{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30bc4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "JIRA_CREDENTIALS = {\n",
    "    \"token_auth\": \"\",\n",
    "    \"server\": \"https://issues.apache.org/jira/\",\n",
    "    \"async_\": True\n",
    "}\n",
    "\n",
    "GITHUB_CREDENTIALS = {\n",
    "    \"Authorization\": \"\", \"User-Agent\": \"rok\",\n",
    "    \"Accept\": \"application/vnd.github.golden-comet-preview+json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d89d64",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "1. Download issues, comments, watchers and external links (~20min)\n",
    "2. Create full GitHub issues (with Jira referring issue links) (~3hrs 20min)\n",
    "2. Collect Jira Issue url to GitHub issue url map (~30min)\n",
    "3. Post GitHub issue links to Jira issue comments (30min?)\n",
    "4. Lock Jira issues (manual)\n",
    "5. Update GitHub issues with corrected issue / subtask links (20min?)\n",
    "\n",
    "# NOTES\n",
    "\n",
    "* Only active milestones can be added to. Activate relevant github milestones before import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006cff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, pickle, re, time\n",
    "from string import punctuation\n",
    "import jira2markdown\n",
    "from jira2markdown.markup.links import Mention\n",
    "from jira2markdown.markup.base import AbstractMarkup\n",
    "from jira import JIRA\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pyparsing import (\n",
    "    CaselessLiteral,\n",
    "    Char,\n",
    "    Combine,\n",
    "    FollowedBy,\n",
    "    Optional,\n",
    "    ParserElement,\n",
    "    ParseResults,\n",
    "    PrecededBy,\n",
    "    SkipTo,\n",
    "    StringEnd,\n",
    "    StringStart,\n",
    "    Suppress,\n",
    "    White,\n",
    "    Word,\n",
    "    alphanums,\n",
    ")\n",
    "import requests\n",
    "\n",
    "\n",
    "def is_completed(item):\n",
    "    return item.fields.status.name in [\"Closed\", \"Resolved\"]\n",
    "\n",
    "\n",
    "def extract_linked_issues(linked_issue):\n",
    "    if hasattr(linked_issue, \"outwardIssue\"):\n",
    "        return {\n",
    "            \"key\": linked_issue.outwardIssue.key,\n",
    "            \"relationship\": linked_issue.type.outward,\n",
    "            \"summary\": linked_issue.outwardIssue.fields.summary,\n",
    "            \"url\": linked_issue.outwardIssue.permalink(),\n",
    "            \"completed\": is_completed(linked_issue.outwardIssue)\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"key\": linked_issue.inwardIssue.key,\n",
    "            \"relationship\": linked_issue.type.inward,\n",
    "            \"summary\": linked_issue.inwardIssue.fields.summary,\n",
    "            \"url\": linked_issue.inwardIssue.permalink(),\n",
    "            \"completed\": is_completed(linked_issue.inwardIssue)\n",
    "        }\n",
    "\n",
    "\n",
    "def get_user_string(jira_author, jira_url):\n",
    "    if jira_author.name in USER_MAPPING:\n",
    "        github_id = f\" / @{USER_MAPPING[jira_author.name]}\"\n",
    "    else:\n",
    "        github_id = \"\"\n",
    "    return f\"[{jira_author.displayName}]({jira_url})\" + github_id\n",
    "\n",
    "\n",
    "def get_comments(issue):\n",
    "    comments = []\n",
    "    for comment in issue.fields.comment.comments:\n",
    "        # Skip ASF GitHub Bot comments per https://github.com/apache/arrow/issues/14648\n",
    "        if comment.author.name == \"githubbot\":\n",
    "            continue\n",
    "\n",
    "        jira_url = f\"{issue.permalink()}?focusedCommentId={comment.id}\"\n",
    "        user_string = get_user_string(comment.author, jira_url)\n",
    "        \n",
    "        fixed_comments = TRANSLATED_MARKUP[issue.key][\"comments\"]\n",
    "        comments.append({\n",
    "            \"body\": f\"{user_string}:\\n{fixed_comments[comment.id]}\",\n",
    "            \"created_at\": comment.created[:-5] + \"Z\"\n",
    "        })\n",
    "    return comments\n",
    "\n",
    "\n",
    "def request_to_github(params, session):\n",
    "    while True:\n",
    "        r = session.request(**params)\n",
    "\n",
    "        if r.status_code in (200, 202, 204):\n",
    "            # all is good\n",
    "            return r\n",
    "        elif r.status_code == 403:\n",
    "            # throttling\n",
    "            print(\"Response was: \", r.json())\n",
    "            reset_time = int(r.headers[\"X-RateLimit-Reset\"])\n",
    "            wait_time = reset_time - round(time.time() + .5)\n",
    "            if wait_time > 0:\n",
    "                request_data = \", \".join((params[\"method\"], params[\"url\"]))\n",
    "                print(f\"Throttled on {request_data}, call:  {r.text}\\nSleeping for {wait_time // 60} minutes.\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            # something is wrong\n",
    "            request_data = \", \".join((params[\"method\"], params[\"url\"], params.get(\"body\", \"\")))\n",
    "            print(f\"Request {request_data} returned status code {r.status_code} and {r.text}\")\n",
    "\n",
    "\n",
    "class MigratedMention(AbstractMarkup):\n",
    "    def action(self, tokens: ParseResults) -> str:\n",
    "        username = self.usernames.get(tokens.accountid)\n",
    "        return f\"`[~{tokens.accountid}]`\" if username is None else f\"@{username}\"\n",
    "\n",
    "    @property\n",
    "    def expr(self) -> ParserElement:\n",
    "        MENTION = Combine(\n",
    "            \"[\"\n",
    "            + Optional(\n",
    "                SkipTo(\"|\", failOn=\"]\") + Suppress(\"|\"),\n",
    "                default=\"\",\n",
    "                )\n",
    "            + \"~\"\n",
    "            + Optional(CaselessLiteral(\"accountid:\"))\n",
    "            + Word(alphanums + \":-\").setResultsName(\"accountid\")\n",
    "            + \"]\",\n",
    "            )\n",
    "        return (\n",
    "                (StringStart() | Optional(PrecededBy(White(), retreat=1), default=\" \"))\n",
    "                + MENTION.setParseAction(self.action)\n",
    "                + (StringEnd() | Optional(FollowedBy(White() | Char(punctuation, excludeChars=\"[\") | MENTION), default=\" \"))\n",
    "        )\n",
    "\n",
    "\n",
    "LEADING_SPACE_HASH_PATTERN = re.compile(r\"\\n\\s(#+\\s+\\S.*)\")\n",
    "ELEMENTS = jira2markdown.elements.MarkupElements()\n",
    "ELEMENTS.replace(Mention, MigratedMention)\n",
    "\n",
    "\n",
    "def translate_markup(issue):\n",
    "    if issue.fields.description:\n",
    "        description = issue.fields.description\n",
    "    else:\n",
    "        description = \"\"\n",
    "\n",
    "    description = re.sub(LEADING_SPACE_HASH_PATTERN, r\"\\n\\1\", description)\n",
    "    text = jira2markdown.convert(description, elements=ELEMENTS, usernames=USER_MAPPING)\n",
    "\n",
    "    for attachment in issue.fields.attachment:\n",
    "        text = text.replace(f\"![{attachment.filename}]({attachment.filename})\",\n",
    "                            f\"![{attachment.filename}]({attachment.content})\")\n",
    "\n",
    "    comments = {}\n",
    "    for comment in issue.fields.comment.comments:\n",
    "        # Skip ASF GitHub Bot comments per https://github.com/apache/arrow/issues/14648\n",
    "        if comment.author.name == \"githubbot\":\n",
    "            continue\n",
    "        comment_body = re.sub(LEADING_SPACE_HASH_PATTERN, r\"\\n\\1\", comment.body)\n",
    "        comment_text = jira2markdown.convert(comment_body, elements=ELEMENTS, usernames=USER_MAPPING)\n",
    "\n",
    "        for attachment in issue.fields.attachment:\n",
    "            comment_text = comment_text.replace(f\"![{attachment.filename}]({attachment.filename})\",\n",
    "                                                f\"![{attachment.filename}]({attachment.content})\")\n",
    "        comments[comment.id] = comment_text\n",
    "\n",
    "    return (issue.key, {\"description\": text, \"comments\": comments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c91b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_jira_issues_filename = 'raw_jira_issues.pickle'\n",
    "raw_jira_watchers_filename = 'raw_jira_watchers.pickle'\n",
    "raw_jira_remote_links_filename = \"raw_jira_remote_links.pickle\"\n",
    "raw_github_prs_filename = \"raw_github_prs.pickle\"\n",
    "translated_markup_filename = \"translated_markdown.pickle\"\n",
    "jira_to_github_user_mapping_file = 'jira-to-github-user-mapping.csv'\n",
    "\n",
    "RELEASE_ORDER = (\n",
    "    '0.1.0', '0.2.0', '0.3.0', 'JS-0.3.0', 'JS-0.3.1', '0.4.0', 'JS-0.4.0',\n",
    "    '0.4.1', 'JS-0.4.1', '0.5.0', '0.6.0', '0.7.0', '0.7.1', '0.8.0',\n",
    "    '0.9.0', '0.10.0', '0.11.0', '0.11.1', '0.12.0', '0.12.1', '0.13.0',\n",
    "    '0.14.0', '0.14.1', '0.15.0', '0.15.1', '0.16.0', '0.17.0', '0.17.1',\n",
    "    '1.0.0', '1.0.1', '2.0.0', '3.0.0', '3.0.1', '4.0.0', '4.0.1', '5.0.0',\n",
    "    '5.0.1', '6.0.0', '6.0.1', '6.0.2', '6.0.3', '7.0.0', '7.0.1', '7.0.2',\n",
    "    '8.0.0', '8.0.1', '9.0.0', '9.0.1', '10.0.0', '10.0.1', '10.0.2',\n",
    "    '11.0.0', '12.0.0'\n",
    ")\n",
    "\n",
    "ISSUETYPE_MAP = {\n",
    "    \"Bug\": \"Type: bug\",\n",
    "    \"Improvement\": \"Type: enhancement\",\n",
    "    \"Wish\": \"Type: enhancement\",\n",
    "    \"New Feature\": \"Type: enhancement\",\n",
    "    \"Task\": \"Type: task\",\n",
    "    \"Sub-task\": \"Type: task\",\n",
    "    \"Test\": \"Type: test\"\n",
    "}\n",
    "\n",
    "GITHUB_LABELS = (\n",
    "    \"Component: Archery\", \"Component: Benchmarking\", \"Component: C\",\n",
    "    \"Component: C#\", \"Component: C++\", \"Component: C++ - Gandiva\",\n",
    "    \"Component: C++ - Plasma\", \"Component: Continuous Integration\",\n",
    "    \"Component: Developer Tools\", \"Component: Documentation\",\n",
    "    \"Component: FlightRPC\", \"Component: Format\", \"Component: GLib\",\n",
    "    \"Component: Go\", \"Component: GPU\", \"Component: Integration\",\n",
    "    \"Component: Java\", \"Component: JavaScript\", \"Component: Julia\",\n",
    "    \"Component: MATLAB\", \"Component: Other\", \"Component: Packaging\",\n",
    "    \"Component: Parquet\", \"Component: Python\", \"Component: R\",\n",
    "    \"Component: Release\", \"Component: Ruby\", \"Component: Rust\",\n",
    "    \"Component: Rust - Ballista\", \"Component: Rust - DataFusion\",\n",
    "    \"Component: Website\", \"Component: Wiki\", \"dependencies\",\n",
    "    \"good-first-issue\", \"hacktoberfest-accepted\", \"java\", \"javascript\",\n",
    "    \"lang-go\", \"needs-rebase\", \"ready-for-review\", \"Type: bug\",\n",
    "    \"Type: enhancement\", \"Type: task\", \"Type: test\", \"Type: usage\",\n",
    "    \"WIP\"\n",
    ")\n",
    "\n",
    "milestone_url = \"https://api.github.com/repos/apache/arrow/milestones\"\n",
    "raw_milestone_map = requests.get(milestone_url, params={\"state\": \"all\"}, headers=GITHUB_CREDENTIALS)\n",
    "MILESTONE_MAP = {x[\"title\"]: x[\"number\"] for x in raw_milestone_map.json()}\n",
    "\n",
    "testing_milestone_url = \"https://api.github.com/repos/datatart/import_dry_run_4/milestones\"\n",
    "raw_testing_milestone_map = requests.get(testing_milestone_url, params={\"state\": \"all\"}, headers=GITHUB_CREDENTIALS)\n",
    "TESTING_MILESTONE_MAP = {x[\"title\"]: x[\"number\"] for x in raw_testing_milestone_map.json()}\n",
    "\n",
    "MILESTONE_MAP = TESTING_MILESTONE_MAP\n",
    "\n",
    "MIGRATION_NOTE = \"\\n\\n**Note**: *This issue was originally created as [{issue_key}]({jira_url}). \" \\\n",
    "    \"Please see the \" \\\n",
    "    \"[migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) \" \\\n",
    "    \"for further details.*\"\n",
    "\n",
    "JIRA_MIGRATION_NOTE = \"This issue has been migrated to [issue #{gh_id}|{gh_url}] on GitHub. \" \\\n",
    "    \"Please see the \" \\\n",
    "    \"[migration documentation|https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3] \" \\\n",
    "    \"for further details.\"\n",
    "\n",
    "USER_MAPPING = {}\n",
    "with open(jira_to_github_user_mapping_file, newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        USER_MAPPING[row[0]] = row[2]\n",
    "        USER_MAPPING[row[1]] = row[2]\n",
    "\n",
    "\n",
    "OWNER = \"datatart\"\n",
    "REPO = \"import_dry_run_4\"\n",
    "IMPORT_URL = f\"https://api.github.com/repos/{OWNER}/{REPO}/import/issues\"\n",
    "ISSUE_URL_TEMPLATE = f\"https://github.com/{OWNER}/{REPO}/issues/{{}}\"\n",
    "\n",
    "GITHUB_PROJECT_URL = \"https://github.com/apache/arrow/pull/\"\n",
    "\n",
    "PROJECT_NAME = \"ARROW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda2a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to check for GitHubs assignable users. We currently don't need it.\n",
    "#\n",
    "# def get_assignable_users(users):\n",
    "#     user_can_be_assignee = []\n",
    "#     with requests.Session() as s:\n",
    "#         for user in users:\n",
    "#             url = f\"https://api.github.com/repos/apache/arrow/assignees/{user}\"\n",
    "#             params = {\"method\": \"GET\", \"url\": url, \"headers\": GITHUB_CREDENTIALS}\n",
    "#             response = request_to_github(params, s)\n",
    "#             if response.status_code == 204:\n",
    "#                 user_can_be_assignee.append(user)\n",
    "#\n",
    "# USER_CAN_BE_ASSIGNEE = get_assignable_users(USER_MAPPING.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c59d3a",
   "metadata": {},
   "source": [
    "# Get Jira issue data and cache it to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5137bbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-08 02:00:59] Getting watchers for ARROW-1 [0/18291].\n",
      "[2023-01-08 02:01:31] Getting watchers for ARROW-1070 [1000/18291].\n",
      "[2023-01-08 02:02:05] Getting watchers for ARROW-2076 [2000/18291].\n",
      "[2023-01-08 02:02:37] Getting watchers for ARROW-3080 [3000/18291].\n",
      "[2023-01-08 02:03:10] Getting watchers for ARROW-4083 [4000/18291].\n",
      "[2023-01-08 02:03:41] Getting watchers for ARROW-5087 [5000/18291].\n",
      "[2023-01-08 02:04:14] Getting watchers for ARROW-6091 [6000/18291].\n",
      "[2023-01-08 02:04:46] Getting watchers for ARROW-7092 [7000/18291].\n",
      "[2023-01-08 02:05:18] Getting watchers for ARROW-8094 [8000/18291].\n",
      "[2023-01-08 02:05:51] Getting watchers for ARROW-9095 [9000/18291].\n",
      "[2023-01-08 02:06:24] Getting watchers for ARROW-10096 [10000/18291].\n",
      "[2023-01-08 02:06:55] Getting watchers for ARROW-11098 [11000/18291].\n",
      "[2023-01-08 02:07:29] Getting watchers for ARROW-12098 [12000/18291].\n",
      "[2023-01-08 02:08:01] Getting watchers for ARROW-13122 [13000/18291].\n",
      "[2023-01-08 02:08:34] Getting watchers for ARROW-14139 [14000/18291].\n",
      "[2023-01-08 02:09:07] Getting watchers for ARROW-15139 [15000/18291].\n",
      "[2023-01-08 02:09:39] Getting watchers for ARROW-16143 [16000/18291].\n",
      "[2023-01-08 02:10:11] Getting watchers for ARROW-17145 [17000/18291].\n",
      "[2023-01-08 02:10:43] Getting watchers for ARROW-18145 [18000/18291].\n",
      "[2023-01-08 02:10:52] Getting remote links for ARROW-1 [0/18291].\n",
      "[2023-01-08 02:11:24] Getting remote links for ARROW-1070 [1000/18291].\n",
      "[2023-01-08 02:11:56] Getting remote links for ARROW-2076 [2000/18291].\n",
      "[2023-01-08 02:12:27] Getting remote links for ARROW-3080 [3000/18291].\n",
      "[2023-01-08 02:12:59] Getting remote links for ARROW-4083 [4000/18291].\n",
      "[2023-01-08 02:13:31] Getting remote links for ARROW-5087 [5000/18291].\n",
      "[2023-01-08 02:14:03] Getting remote links for ARROW-6091 [6000/18291].\n",
      "[2023-01-08 02:25:46] Getting remote links for ARROW-7092 [7000/18291].\n",
      "[2023-01-08 02:47:57] Getting remote links for ARROW-8094 [8000/18291].\n",
      "[2023-01-08 03:10:08] Getting remote links for ARROW-9095 [9000/18291].\n",
      "[2023-01-08 03:32:19] Getting remote links for ARROW-10096 [10000/18291].\n",
      "[2023-01-08 03:54:30] Getting remote links for ARROW-11098 [11000/18291].\n",
      "[2023-01-08 04:14:31] Getting remote links for ARROW-12098 [12000/18291].\n",
      "[2023-01-08 04:36:46] Getting remote links for ARROW-13122 [13000/18291].\n",
      "[2023-01-08 04:58:59] Getting remote links for ARROW-14139 [14000/18291].\n",
      "[2023-01-08 05:21:10] Getting remote links for ARROW-15139 [15000/18291].\n",
      "[2023-01-08 05:43:25] Getting remote links for ARROW-16143 [16000/18291].\n",
      "[2023-01-08 06:05:37] Getting remote links for ARROW-17145 [17000/18291].\n",
      "[2023-01-08 06:27:50] Getting remote links for ARROW-18145 [18000/18291].\n",
      "CPU times: user 1min 46s, sys: 4.97 s, total: 1min 51s\n",
      "Wall time: 4h 37min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CONN = JIRA(**JIRA_CREDENTIALS)\n",
    "\n",
    "\n",
    "ISSUES = CONN.search_issues(f\"project = {PROJECT_NAME} order by key\", maxResults = False, fields = '*all')\n",
    "with open(raw_jira_issues_filename, 'wb') as handle:\n",
    "    pickle.dump(ISSUES, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "WATCHERS = {}\n",
    "for i, issue in enumerate(ISSUES):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Getting watchers for {issue.key} [{i}/{len(ISSUES)}].\")\n",
    "    WATCHERS[issue.id] = CONN.watchers(issue.id)\n",
    "with open(raw_jira_watchers_filename, 'wb') as handle:\n",
    "    pickle.dump(WATCHERS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "REMOTE_LINKS = {}\n",
    "for i, issue in enumerate(ISSUES):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Getting remote links for {issue.key} [{i}/{len(ISSUES)}].\")\n",
    "    REMOTE_LINKS[issue.id] = CONN.remote_links(issue)\n",
    "with open(raw_jira_remote_links_filename, 'wb') as handle:\n",
    "    pickle.dump(REMOTE_LINKS, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open(raw_jira_issues_filename, 'rb') as handle:\n",
    "    ISSUES = pickle.load(handle)\n",
    "\n",
    "with open(raw_jira_watchers_filename, 'rb') as handle:\n",
    "    WATCHERS = pickle.load(handle)\n",
    "\n",
    "with open(raw_jira_remote_links_filename, 'rb') as handle:\n",
    "    REMOTE_LINKS = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1957b2b",
   "metadata": {},
   "source": [
    "# Jira -> GitHub markdown translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17483e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.2 s, sys: 1.02 s, total: 6.23 s\n",
      "Wall time: 20min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Pool(processes=int(cpu_count() / 2)) as pool:\n",
    "    TRANSLATED_MARKUP = pool.map_async(translate_markup, ISSUES, chunksize=100).get()\n",
    "TRANSLATED_MARKUP = {k: v for k, v in TRANSLATED_MARKUP}\n",
    "\n",
    "with open(translated_markup_filename, 'wb') as handle:\n",
    "    pickle.dump(TRANSLATED_MARKUP, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(translated_markup_filename, 'rb') as handle:\n",
    "    TRANSLATED_MARKUP = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc97c4",
   "metadata": {},
   "source": [
    "# Generate GitHub import payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c615ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_import_payload(issue):\n",
    "    issue_type = ISSUETYPE_MAP[issue.fields.issuetype.name]\n",
    "    labels = [f\"Component: {c.name}\" for c in issue.fields.components] + issue.fields.labels + [issue_type]\n",
    "    # Filter out nonexisting labels\n",
    "    labels = [label for label in labels if label in GITHUB_LABELS]\n",
    "    \n",
    "    # Get the earliest fix version and map it to a milestone\n",
    "    fix_versions = sorted((x.name for x in issue.fields.fixVersions), key=lambda x: RELEASE_ORDER.index(x))\n",
    "    fix_version = fix_versions[0] if fix_versions else None\n",
    "    milestone = MILESTONE_MAP.get(fix_version, None)\n",
    "    \n",
    "    jira_url = issue.permalink()\n",
    "\n",
    "    # Get watchers \n",
    "    watchers = [get_user_string(watcher, jira_url) for watcher in WATCHERS[issue.id].watchers]\n",
    "    watchers = \", \".join(watchers)\n",
    "   \n",
    "    remote_links = [remote_link.object for remote_link in REMOTE_LINKS[issue.id]]\n",
    "\n",
    "    body = TRANSLATED_MARKUP[issue.key][\"description\"] + \"\\n\"\n",
    "\n",
    "    if issue.fields.environment:\n",
    "        body += \"\\n**Environment**: \" + issue.fields.environment\n",
    "    if issue.fields.reporter:\n",
    "        body += \"\\n**Reporter**: \" + get_user_string(issue.fields.reporter, jira_url)\n",
    "    if issue.fields.assignee:\n",
    "        body += \"\\n**Assignee**: \" + get_user_string(issue.fields.assignee, jira_url)\n",
    "    if watchers:\n",
    "        body += f\"\\n**Watchers**: {watchers}\"\n",
    "    \n",
    "    if issue.fields.subtasks:\n",
    "        body += \"\\n#### Subtasks:\"\n",
    "        for subtask in issue.fields.subtasks:\n",
    "            body += f\"\\n- [{'X' if is_completed(subtask) else ' '}] \" \\\n",
    "                f\"[{subtask.fields.summary}]({subtask.permalink()})\"\n",
    "\n",
    "    linked_issues = [extract_linked_issues(linked_issue) for linked_issue in issue.fields.issuelinks]\n",
    "    \n",
    "    if linked_issues:\n",
    "        body += \"\\n#### Related issues:\"\n",
    "        for li in linked_issues:\n",
    "            body += \\\n",
    "                f\"\\n- [{li['summary']}]({li['url']}) ({li['relationship']})\"\n",
    "    \n",
    "\n",
    "    if issue.fields.attachment:\n",
    "        body += f\"\\n#### Original Issue Attachments:\"\n",
    "\n",
    "        for attachment in issue.fields.attachment:\n",
    "            body += f\"\\n- [{attachment.filename}]({attachment.content})\"\n",
    "\n",
    "    if issue.fields.customfield_12311020:\n",
    "        body += \"\\n#### Externally tracked issue: \" \\\n",
    "            f\"[{issue.fields.customfield_12311020}]({issue.fields.customfield_12311020})\"\n",
    "\n",
    "    if remote_links:\n",
    "        body += \"\\n#### PRs and other links:\"\n",
    "        for pr in remote_links:\n",
    "            body += f\"\\n- [{pr.title}]({pr.url})\"\n",
    "\n",
    "    body += MIGRATION_NOTE.format(issue_key=issue.key, jira_url=jira_url)\n",
    "    \n",
    "    data = {\n",
    "        \"issue\": {\n",
    "            \"title\": f\"{issue.fields.summary}\",\n",
    "            \"labels\": labels,\n",
    "            \"body\": body,\n",
    "            \"created_at\": issue.fields.created[:-5] + \"Z\",\n",
    "            \"updated_at\": issue.fields.updated[:-5] + \"Z\",\n",
    "            \"closed\": is_completed(issue),\n",
    "        },\n",
    "      \"comments\": get_comments(issue)\n",
    "    }\n",
    "\n",
    "    if issue.fields.resolutiondate:\n",
    "        data[\"issue\"][\"closed_at\"] = issue.fields.resolutiondate[:-5] + \"Z\"\n",
    "    if milestone:\n",
    "        data[\"issue\"][\"milestone\"] = milestone\n",
    "#     if issue.fields.assignee:\n",
    "#         assignee = USER_MAPPING.get(issue.fields.assignee.name, None)\n",
    "#         if assignee and assignee in USER_CAN_BE_ASSIGNEE:\n",
    "#         data[\"issue\"][\"assignee\"] = assignee\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a40dbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 482 ms, sys: 20.1 ms, total: 502 ms\n",
      "Wall time: 502 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_payloads = [(issue.key, generate_import_payload(issue)) for issue in ISSUES]\n",
    "import_responses = {}\n",
    "payloads = all_payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904b542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Drill vector module\n",
      "\n",
      "**Reporter**: [Jacques Nadeau](https://issues.apache.org/jira/browse/ARROW-1) / @jacques-n\n",
      "**Assignee**: [Steven Phillips](https://issues.apache.org/jira/browse/ARROW-1) / @StevenMPhillips\n",
      "**Watchers**: [Jacques Nadeau](https://issues.apache.org/jira/browse/ARROW-1) / @jacques-n\n",
      "\n",
      "**Note**: *This issue was originally created as [ARROW-1](https://issues.apache.org/jira/browse/ARROW-1). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n"
     ]
    }
   ],
   "source": [
    "print(all_payloads[0][1][\"issue\"][\"body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f2b58",
   "metadata": {},
   "source": [
    "# Import issues into GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b7df449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-08 06:55:13] importing  0 / 18291\n",
      "[2023-01-08 06:55:39] importing  100 / 18291\n",
      "[2023-01-08 06:56:05] importing  200 / 18291\n",
      "[2023-01-08 06:56:31] importing  300 / 18291\n",
      "[2023-01-08 06:56:56] importing  400 / 18291\n",
      "[2023-01-08 06:57:22] importing  500 / 18291\n",
      "[2023-01-08 06:57:48] importing  600 / 18291\n",
      "[2023-01-08 06:58:14] importing  700 / 18291\n",
      "[2023-01-08 06:58:39] importing  800 / 18291\n",
      "[2023-01-08 06:59:05] importing  900 / 18291\n",
      "[2023-01-08 06:59:31] importing  1000 / 18291\n",
      "[2023-01-08 06:59:56] importing  1100 / 18291\n",
      "[2023-01-08 07:00:22] importing  1200 / 18291\n",
      "[2023-01-08 07:00:47] importing  1300 / 18291\n",
      "[2023-01-08 07:01:13] importing  1400 / 18291\n",
      "[2023-01-08 07:01:38] importing  1500 / 18291\n",
      "[2023-01-08 07:02:03] importing  1600 / 18291\n",
      "[2023-01-08 07:02:29] importing  1700 / 18291\n",
      "[2023-01-08 07:02:56] importing  1800 / 18291\n",
      "[2023-01-08 07:03:21] importing  1900 / 18291\n",
      "[2023-01-08 07:03:47] importing  2000 / 18291\n",
      "[2023-01-08 07:04:12] importing  2100 / 18291\n",
      "[2023-01-08 07:04:37] importing  2200 / 18291\n",
      "[2023-01-08 07:05:02] importing  2300 / 18291\n",
      "[2023-01-08 07:05:28] importing  2400 / 18291\n",
      "[2023-01-08 07:05:54] importing  2500 / 18291\n",
      "[2023-01-08 07:06:20] importing  2600 / 18291\n",
      "[2023-01-08 07:06:47] importing  2700 / 18291\n",
      "[2023-01-08 07:07:12] importing  2800 / 18291\n",
      "[2023-01-08 07:07:38] importing  2900 / 18291\n",
      "[2023-01-08 07:08:03] importing  3000 / 18291\n",
      "[2023-01-08 07:08:29] importing  3100 / 18291\n",
      "[2023-01-08 07:08:55] importing  3200 / 18291\n",
      "[2023-01-08 07:09:22] importing  3300 / 18291\n",
      "[2023-01-08 07:09:47] importing  3400 / 18291\n",
      "[2023-01-08 07:10:13] importing  3500 / 18291\n",
      "[2023-01-08 07:10:38] importing  3600 / 18291\n",
      "[2023-01-08 07:11:04] importing  3700 / 18291\n",
      "[2023-01-08 07:11:29] importing  3800 / 18291\n",
      "[2023-01-08 07:11:54] importing  3900 / 18291\n",
      "[2023-01-08 07:12:20] importing  4000 / 18291\n",
      "[2023-01-08 07:12:45] importing  4100 / 18291\n",
      "[2023-01-08 07:13:12] importing  4200 / 18291\n",
      "[2023-01-08 07:13:38] importing  4300 / 18291\n",
      "[2023-01-08 07:14:03] importing  4400 / 18291\n",
      "[2023-01-08 07:14:29] importing  4500 / 18291\n",
      "[2023-01-08 07:14:54] importing  4600 / 18291\n",
      "[2023-01-08 07:15:20] importing  4700 / 18291\n",
      "[2023-01-08 07:15:46] importing  4800 / 18291\n",
      "[2023-01-08 07:16:11] importing  4900 / 18291\n",
      "[2023-01-08 07:16:37] importing  5000 / 18291\n",
      "Response was:  {'message': 'API rate limit exceeded for user ID 54589.', 'documentation_url': 'https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting'}\n",
      "Throttled on {'method': 'POST', 'url': 'https://api.github.com/repos/datatart/import_dry_run_4/import/issues'} call. Sleeping for 38 minutes.\n",
      "Response was:  {'message': 'API rate limit exceeded for user ID 54589.', 'documentation_url': 'https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting'}\n",
      "[2023-01-08 07:55:39] importing  5100 / 18291\n",
      "[2023-01-08 07:56:05] importing  5200 / 18291\n",
      "[2023-01-08 07:56:30] importing  5300 / 18291\n",
      "[2023-01-08 07:56:57] importing  5400 / 18291\n",
      "[2023-01-08 07:57:23] importing  5500 / 18291\n",
      "[2023-01-08 07:57:49] importing  5600 / 18291\n",
      "[2023-01-08 07:58:15] importing  5700 / 18291\n",
      "[2023-01-08 07:58:40] importing  5800 / 18291\n",
      "[2023-01-08 07:59:06] importing  5900 / 18291\n",
      "[2023-01-08 07:59:31] importing  6000 / 18291\n",
      "[2023-01-08 07:59:57] importing  6100 / 18291\n",
      "[2023-01-08 08:00:24] importing  6200 / 18291\n",
      "[2023-01-08 08:00:49] importing  6300 / 18291\n",
      "[2023-01-08 08:01:15] importing  6400 / 18291\n",
      "[2023-01-08 08:01:41] importing  6500 / 18291\n",
      "[2023-01-08 08:02:07] importing  6600 / 18291\n",
      "[2023-01-08 08:02:33] importing  6700 / 18291\n",
      "[2023-01-08 08:03:00] importing  6800 / 18291\n",
      "[2023-01-08 08:03:26] importing  6900 / 18291\n",
      "[2023-01-08 08:03:52] importing  7000 / 18291\n",
      "[2023-01-08 08:04:17] importing  7100 / 18291\n",
      "[2023-01-08 08:04:42] importing  7200 / 18291\n",
      "[2023-01-08 08:05:08] importing  7300 / 18291\n",
      "[2023-01-08 08:05:34] importing  7400 / 18291\n",
      "[2023-01-08 08:06:00] importing  7500 / 18291\n",
      "[2023-01-08 08:06:26] importing  7600 / 18291\n",
      "[2023-01-08 08:06:51] importing  7700 / 18291\n",
      "[2023-01-08 08:07:17] importing  7800 / 18291\n",
      "[2023-01-08 08:07:43] importing  7900 / 18291\n",
      "[2023-01-08 08:08:09] importing  8000 / 18291\n",
      "[2023-01-08 08:08:34] importing  8100 / 18291\n",
      "[2023-01-08 08:09:00] importing  8200 / 18291\n",
      "[2023-01-08 08:09:25] importing  8300 / 18291\n",
      "[2023-01-08 08:09:52] importing  8400 / 18291\n",
      "[2023-01-08 08:10:18] importing  8500 / 18291\n",
      "[2023-01-08 08:10:45] importing  8600 / 18291\n",
      "[2023-01-08 08:11:11] importing  8700 / 18291\n",
      "[2023-01-08 08:11:36] importing  8800 / 18291\n",
      "[2023-01-08 08:12:01] importing  8900 / 18291\n",
      "[2023-01-08 08:12:28] importing  9000 / 18291\n",
      "[2023-01-08 08:12:53] importing  9100 / 18291\n",
      "[2023-01-08 08:13:19] importing  9200 / 18291\n",
      "[2023-01-08 08:13:45] importing  9300 / 18291\n",
      "[2023-01-08 08:14:10] importing  9400 / 18291\n",
      "[2023-01-08 08:14:36] importing  9500 / 18291\n",
      "[2023-01-08 08:15:02] importing  9600 / 18291\n",
      "[2023-01-08 08:15:29] importing  9700 / 18291\n",
      "[2023-01-08 08:15:54] importing  9800 / 18291\n",
      "[2023-01-08 08:16:19] importing  9900 / 18291\n",
      "[2023-01-08 08:16:45] importing  10000 / 18291\n",
      "Response was:  {'message': 'API rate limit exceeded for user ID 54589.', 'documentation_url': 'https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting'}\n",
      "Throttled on {'method': 'POST', 'url': 'https://api.github.com/repos/datatart/import_dry_run_4/import/issues', } call. Sleeping for 38 minutes.\n",
      "Response was:  {'message': 'API rate limit exceeded for user ID 54589.', 'documentation_url': 'https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting'}\n",
      "[2023-01-08 08:55:40] importing  10100 / 18291\n",
      "[2023-01-08 08:56:05] importing  10200 / 18291\n",
      "[2023-01-08 08:56:31] importing  10300 / 18291\n",
      "[2023-01-08 08:56:56] importing  10400 / 18291\n",
      "[2023-01-08 08:57:22] importing  10500 / 18291\n",
      "[2023-01-08 08:57:47] importing  10600 / 18291\n",
      "[2023-01-08 08:58:12] importing  10700 / 18291\n",
      "[2023-01-08 08:58:37] importing  10800 / 18291\n",
      "[2023-01-08 08:59:02] importing  10900 / 18291\n",
      "[2023-01-08 08:59:27] importing  11000 / 18291\n",
      "[2023-01-08 08:59:53] importing  11100 / 18291\n",
      "[2023-01-08 09:00:19] importing  11200 / 18291\n",
      "[2023-01-08 09:00:44] importing  11300 / 18291\n",
      "[2023-01-08 09:01:10] importing  11400 / 18291\n",
      "[2023-01-08 09:01:35] importing  11500 / 18291\n",
      "[2023-01-08 09:02:01] importing  11600 / 18291\n",
      "[2023-01-08 09:02:26] importing  11700 / 18291\n",
      "[2023-01-08 09:02:51] importing  11800 / 18291\n",
      "[2023-01-08 09:03:17] importing  11900 / 18291\n",
      "[2023-01-08 09:03:42] importing  12000 / 18291\n",
      "[2023-01-08 09:04:08] importing  12100 / 18291\n",
      "[2023-01-08 09:04:33] importing  12200 / 18291\n",
      "[2023-01-08 09:04:59] importing  12300 / 18291\n",
      "[2023-01-08 09:05:24] importing  12400 / 18291\n",
      "[2023-01-08 09:05:49] importing  12500 / 18291\n",
      "[2023-01-08 09:06:15] importing  12600 / 18291\n",
      "[2023-01-08 09:06:40] importing  12700 / 18291\n",
      "[2023-01-08 09:07:05] importing  12800 / 18291\n",
      "[2023-01-08 09:07:31] importing  12900 / 18291\n",
      "[2023-01-08 09:07:56] importing  13000 / 18291\n",
      "[2023-01-08 09:08:21] importing  13100 / 18291\n",
      "[2023-01-08 09:08:47] importing  13200 / 18291\n",
      "[2023-01-08 09:09:13] importing  13300 / 18291\n",
      "[2023-01-08 09:09:39] importing  13400 / 18291\n",
      "[2023-01-08 09:10:04] importing  13500 / 18291\n",
      "[2023-01-08 09:10:29] importing  13600 / 18291\n",
      "[2023-01-08 09:10:54] importing  13700 / 18291\n",
      "[2023-01-08 09:11:20] importing  13800 / 18291\n",
      "[2023-01-08 09:11:47] importing  13900 / 18291\n",
      "[2023-01-08 09:12:13] importing  14000 / 18291\n",
      "[2023-01-08 09:12:39] importing  14100 / 18291\n",
      "[2023-01-08 09:13:05] importing  14200 / 18291\n",
      "[2023-01-08 09:13:31] importing  14300 / 18291\n",
      "[2023-01-08 09:13:58] importing  14400 / 18291\n",
      "[2023-01-08 09:14:23] importing  14500 / 18291\n",
      "[2023-01-08 09:14:49] importing  14600 / 18291\n",
      "[2023-01-08 09:15:14] importing  14700 / 18291\n",
      "[2023-01-08 09:15:40] importing  14800 / 18291\n",
      "[2023-01-08 09:16:05] importing  14900 / 18291\n",
      "[2023-01-08 09:16:30] importing  15000 / 18291\n",
      "Response was:  {'message': 'API rate limit exceeded for user ID 54589.', 'documentation_url': 'https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting'}\n",
      "Throttled on {'method': 'POST', 'url': 'https://api.github.com/repos/datatart/import_dry_run_4/import/issues'} call. Sleeping for 38 minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response was:  {'message': 'API rate limit exceeded for user ID 54589.', 'documentation_url': 'https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting'}\n",
      "[2023-01-08 09:55:41] importing  15100 / 18291\n",
      "[2023-01-08 09:56:08] importing  15200 / 18291\n",
      "[2023-01-08 09:56:36] importing  15300 / 18291\n",
      "[2023-01-08 09:57:03] importing  15400 / 18291\n",
      "[2023-01-08 09:57:29] importing  15500 / 18291\n",
      "[2023-01-08 09:57:56] importing  15600 / 18291\n",
      "[2023-01-08 09:58:24] importing  15700 / 18291\n",
      "[2023-01-08 09:58:51] importing  15800 / 18291\n",
      "[2023-01-08 09:59:19] importing  15900 / 18291\n",
      "[2023-01-08 09:59:47] importing  16000 / 18291\n",
      "[2023-01-08 10:00:16] importing  16100 / 18291\n",
      "[2023-01-08 10:00:44] importing  16200 / 18291\n",
      "[2023-01-08 10:01:12] importing  16300 / 18291\n",
      "[2023-01-08 10:01:41] importing  16400 / 18291\n",
      "[2023-01-08 10:02:09] importing  16500 / 18291\n",
      "[2023-01-08 10:02:36] importing  16600 / 18291\n",
      "[2023-01-08 10:03:03] importing  16700 / 18291\n",
      "[2023-01-08 10:03:29] importing  16800 / 18291\n",
      "[2023-01-08 10:03:56] importing  16900 / 18291\n",
      "[2023-01-08 10:04:24] importing  17000 / 18291\n",
      "[2023-01-08 10:04:50] importing  17100 / 18291\n",
      "[2023-01-08 10:05:16] importing  17200 / 18291\n",
      "[2023-01-08 10:05:42] importing  17300 / 18291\n",
      "[2023-01-08 10:06:08] importing  17400 / 18291\n",
      "[2023-01-08 10:06:34] importing  17500 / 18291\n",
      "[2023-01-08 10:07:00] importing  17600 / 18291\n",
      "[2023-01-08 10:07:26] importing  17700 / 18291\n",
      "[2023-01-08 10:07:54] importing  17800 / 18291\n",
      "[2023-01-08 10:08:20] importing  17900 / 18291\n",
      "[2023-01-08 10:08:46] importing  18000 / 18291\n",
      "[2023-01-08 10:09:12] importing  18100 / 18291\n",
      "[2023-01-08 10:09:38] importing  18200 / 18291\n",
      "CPU times: user 29.6 s, sys: 1.87 s, total: 31.5 s\n",
      "Wall time: 3h 14min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with requests.Session() as s:\n",
    "    for i, (key, payload) in enumerate(payloads):\n",
    "        if key in import_responses:\n",
    "            continue\n",
    "        if (i % 100 == 0):\n",
    "            print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] importing \", i, \"/\", len(payloads))\n",
    "\n",
    "        params = {\"method\": \"POST\", \"url\": IMPORT_URL, \"json\": payload, \"headers\": GITHUB_CREDENTIALS}\n",
    "        response = request_to_github(params, s)\n",
    "        import_responses[key] = {\"import_response\": response, \"status\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b827eb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-08 15:43:28] importing  0 / 18291\n",
      "[2023-01-08 15:43:28] importing  1000 / 18291\n",
      "[2023-01-08 15:43:28] importing  2000 / 18291\n",
      "[2023-01-08 15:43:28] importing  3000 / 18291\n",
      "[2023-01-08 15:43:28] importing  4000 / 18291\n",
      "[2023-01-08 15:43:28] importing  5000 / 18291\n",
      "[2023-01-08 15:43:28] importing  6000 / 18291\n",
      "[2023-01-08 15:43:28] importing  7000 / 18291\n",
      "Import of ARROW-7760 failed. Please add mapping to it manually.\n",
      "Import of ARROW-7954 failed. Please add mapping to it manually.\n",
      "[2023-01-08 15:43:28] importing  8000 / 18291\n",
      "[2023-01-08 15:43:28] importing  9000 / 18291\n",
      "[2023-01-08 15:43:28] importing  10000 / 18291\n",
      "[2023-01-08 15:43:28] importing  11000 / 18291\n",
      "[2023-01-08 15:43:28] importing  12000 / 18291\n",
      "[2023-01-08 15:43:28] importing  13000 / 18291\n",
      "[2023-01-08 15:43:28] importing  14000 / 18291\n",
      "[2023-01-08 15:43:28] importing  15000 / 18291\n",
      "[2023-01-08 15:43:28] importing  16000 / 18291\n",
      "[2023-01-08 15:43:28] importing  17000 / 18291\n",
      "Import of ARROW-17879 failed. Please add mapping to it manually.\n",
      "[2023-01-08 15:43:29] importing  18000 / 18291\n",
      "CPU times: user 47.7 ms, sys: 3.76 ms, total: 51.5 ms\n",
      "Wall time: 665 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Check import statuses to get github issue links\n",
    "with requests.Session() as s:\n",
    "    for i, key in enumerate(import_responses.keys()):\n",
    "        if (i % 1000 == 0):\n",
    "            print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] importing \", i, \"/\", len(payloads))\n",
    "\n",
    "        if import_responses[key][\"status\"] != \"imported\":\n",
    "            status_url = import_responses[key][\"import_response\"].json()[\"url\"]\n",
    "\n",
    "            params = {\"method\": \"GET\", \"url\": status_url, \"headers\": GITHUB_CREDENTIALS}\n",
    "            response = request_to_github(params, s)\n",
    "            import_responses[key][\"status\"] = response.json()[\"status\"]\n",
    "\n",
    "            if import_responses[key][\"status\"] == \"failed\":\n",
    "                print(f\"Import of {key} failed. Please add mapping to it manually.\")\n",
    "\n",
    "            if import_responses[key][\"status\"] == \"imported\":\n",
    "                print(key)\n",
    "                import_responses[key][\"issue_url\"] = \\\n",
    "                    response.json()[\"issue_url\"].replace(\"https://api.github.com/repos/\", \"https://github.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8db83",
   "metadata": {},
   "source": [
    "**TODO: Issues ARROW-7760, ARROW-7954, ARROW-17879 have comments that have import problems. Issues are imported and we need to manually add their urls before proceeding. THIS MUST BE DONE MANUALLY FOR EVERY IMPORT.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "45a11778",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_issue_map = {\n",
    "    \"ARROW-7760\": 8660,\n",
    "    \"ARROW-7954\": 8834,\n",
    "    \"ARROW-17879\": 17816\n",
    "}\n",
    "manual_issue_map = {k: {\"issue_url\": ISSUE_URL_TEMPLATE.format(v)} for k, v in manual_issue_map.items()}\n",
    "\n",
    "GITHUB_URLS = {k: v[\"issue_url\"] for k, v in {**import_responses, **manual_issue_map}.items()}\n",
    "JIRA_TO_GITHUB_URL_MAP = {issue.permalink(): GITHUB_URLS[issue.key] for issue in ISSUES}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e81474",
   "metadata": {},
   "source": [
    "# Update Jira issues to link to new GitHub Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "98eee89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This is currently untested\n",
    "\n",
    "def update_source_jira(issue, gh_url):\n",
    "    gh_id = gh_url.split(\"/\")[-1]\n",
    "    comment = JIRA_MIGRATION_NOTE.format(gh_id=gh_id, gh_url=gh_url)\n",
    "    CONN.add_comment(issue, comment)\n",
    "\n",
    "    if not issue.fields.customfield_12311020:\n",
    "        issue.update(fields={\"customfield_12311020\" : gh_url})\n",
    "\n",
    "for issue in ISSUES:\n",
    "    # Please verify all issues were successfully imported and we can post links to Jira\n",
    "    break\n",
    "    update_source_jira(issue, GITHUB_URLS[issue.key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a189c2",
   "metadata": {},
   "source": [
    "# Lock Jira comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c35c6",
   "metadata": {},
   "source": [
    "# Update cross issue links on GitHub to link to GitHub issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1626942e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66.2 ms, sys: 0 ns, total: 66.2 ms\n",
      "Wall time: 65.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def fix_issue_bodies(issues, payloads):\n",
    "    issue_bodies = {key: payload[\"issue\"][\"body\"] for key, payload in payloads}    \n",
    "    new_issue_bodies = {}\n",
    "\n",
    "    for issue in issues:\n",
    "        if issue.fields.issuelinks or issue.fields.subtasks:\n",
    "            body = issue_bodies[issue.key]\n",
    "\n",
    "            if issue.fields.issuelinks:\n",
    "                for li in issue.fields.issuelinks:\n",
    "                    linked_issue = li.outwardIssue if hasattr(li, \"outwardIssue\") else li.inwardIssue\n",
    "                    jira_url = linked_issue.permalink()\n",
    "                    github_url = JIRA_TO_GITHUB_URL_MAP.get(jira_url, jira_url)\n",
    "                    body = body.replace(jira_url, github_url)\n",
    "\n",
    "\n",
    "            if issue.fields.subtasks:\n",
    "                for subtask in issue.fields.subtasks:\n",
    "                    jira_url = subtask.permalink()\n",
    "                    github_url = JIRA_TO_GITHUB_URL_MAP.get(jira_url, jira_url)\n",
    "                    body = body.replace(jira_url, github_url)\n",
    "\n",
    "            new_issue_bodies[issue.key] = body\n",
    "\n",
    "    return new_issue_bodies\n",
    "\n",
    "def update_gh_issue_links(issue_bodies):\n",
    "    responses = {}\n",
    "\n",
    "    with requests.Session() as s:\n",
    "        for key, body in issue_bodies.items():\n",
    "            url = GITHUB_URLS[key].replace(\"https://github.com/\", \"https://api.github.com/repos/\")\n",
    "            params = {\"method\": \"POST\", \"url\": url, \"json\": {\"body\": body}, \"headers\": GITHUB_CREDENTIALS}\n",
    "            responses[key] = request_to_github(params, s)\n",
    "\n",
    "    return responses\n",
    "\n",
    "print(f\"Updating {len(new_issue_bodies)} issue bodies with corrected links.\")\n",
    "\n",
    "new_issue_bodies = fix_issue_bodies(ISSUES, payloads)\n",
    "# update_gh_issue_links(new_issue_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cbe75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
