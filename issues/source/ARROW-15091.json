{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13417297",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297",
    "key": "ARROW-15091",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350591",
                "id": "12350591",
                "description": "",
                "name": "7.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-02-03"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
            "name": "vibhatha",
            "key": "vibhatha",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
            },
            "displayName": "Vibhatha Lakmal Abeykoon",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332532",
                "id": "12332532",
                "name": "Documentation"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 91200,
            "total": 91200,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 91200,
            "total": 91200,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15091/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 152,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700513",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#issuecomment-1000239055\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-23T11:27:27.735+0000",
                    "updated": "2021-12-23T11:27:27.735+0000",
                    "started": "2021-12-23T11:27:27.735+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700513",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700605",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774556534\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n\nReview comment:\n       These could be functions and/or constants (`constexpr char kSep[] = \"...\"`)\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n\nReview comment:\n       In general, try to be consistent with include style. Within Arrow code we use \"\" includes, but examples are \"external\" and generally use <> includes.\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n\nReview comment:\n       Does this not get called automatically? User code should never have to call this.\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::internal::ThreadPool> MakeIOThreadPool() {\n+  auto maybe_pool = arrow::internal::ThreadPool::MakeEternal(/*threads=*/8);\n+  if (!maybe_pool.ok()) {\n+    maybe_pool.status().Abort(\"Failed to create global IO thread pool\");\n+  }\n+  return *std::move(maybe_pool);\n+}\n+\n+arrow::internal::ThreadPool* GetIOThreadPool() {\n+  static std::shared_ptr<arrow::internal::ThreadPool> pool = MakeIOThreadPool();\n+  return pool.get();\n+}\n+\n+arrow::Status source_sink_example() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n\nReview comment:\n       Hmm, are these arguments not the defaults?\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n\nReview comment:\n       I'm not sure we want to use the internal JSON format, both because it's not documented and because its format differs from the JSONL format the actual JSON reader uses. Can we use either the actual JSON reader or the CSV reader?\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n\nReview comment:\n       nit, but usually we use UpperCamelCase for function names (\"Consume\", \"ScanSinkNodeExample\"). We use snake_case only for const getters.\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n\nReview comment:\n       Similarly, there is a TableFromJSON.\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n\nReview comment:\n       Would it make more sense to demonstrate InMemoryDataset here? This is using a lot of semi-internal machinery that I think will be confusing.\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n\nReview comment:\n       FWIW, there is a RecordBatchFromJSON.\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n\nReview comment:\n       You can also directly create a RecordBatch from an ArrayVector (vector<shared_ptr<Array>>) instead of jumping through StructArray.\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::internal::ThreadPool> MakeIOThreadPool() {\n+  auto maybe_pool = arrow::internal::ThreadPool::MakeEternal(/*threads=*/8);\n+  if (!maybe_pool.ok()) {\n+    maybe_pool.status().Abort(\"Failed to create global IO thread pool\");\n+  }\n+  return *std::move(maybe_pool);\n+}\n+\n+arrow::internal::ThreadPool* GetIOThreadPool() {\n+  static std::shared_ptr<arrow::internal::ThreadPool> pool = MakeIOThreadPool();\n+  return pool.get();\n+}\n+\n+arrow::Status source_sink_example() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                               ::arrow::internal::GetCpuThreadPool());\n+\n+  // ensure arrow::dataset node factories are in the registry\n+  arrow::dataset::internal::Initialize();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+  cp::ExecPlan::Make(&exec_context));\n+\n+  auto basic_data = MakeBasicBatches();\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+  auto source_node_options =\n+      cp::SourceNodeOptions{basic_data.schema, basic_data.gen(false)};\n+\n+  ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source,\n+                        cp::MakeExecNode(\"source\", plan.get(), {}, source_node_options));\n+\n+  cp::ExecNode* sink;\n+\n+  ARROW_ASSIGN_OR_RAISE(sink, cp::MakeExecNode(\"sink\", plan.get(), {source},\n+                                               cp::SinkNodeOptions{&sink_gen}));\n+\n+  // // // translate sink_gen (async) to sink_reader (sync)\n+  std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+  // // validate the ExecPlan\n+  ABORT_ON_FAILURE(plan->Validate());\n+  PRINT_LINE(\"Exec Plan Created: \" << plan->ToString());\n+  // // // start the ExecPlan\n+  ABORT_ON_FAILURE(plan->StartProducing());\n+\n+  // // collect sink_reader into a Table\n+  std::shared_ptr<arrow::Table> response_table;\n+\n+  ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+  PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+  // // plan stop producing\n+  plan->StopProducing();\n+  // // plan mark finished\n+  plan->finished().Wait();\n+\n+  return arrow::Status::OK();\n+}\n+\n+arrow::Status scan_filter_sink_example() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                               ::arrow::internal::GetCpuThreadPool());\n+\n+  // ensure arrow::dataset node factories are in the registry\n+  arrow::dataset::internal::Initialize();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                        cp::ExecPlan::Make(&exec_context));\n+\n+  std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  // sync scanning is not supported by ScanNode\n+  options->use_async = true;\n+  // specify the filter\n+  cp::Expression b_is_true = cp::field_ref(\"b\");\n+  options->filter = b_is_true;\n+  // empty projection\n+  options->projection = Materialize({});\n+\n+  // construct the scan node\n+  PRINT_LINE(\"Initialized Scanning Options\");\n+\n+  cp::ExecNode* scan;\n+\n+  auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+  PRINT_LINE(\"Scan node options created\");\n+\n+  ARROW_ASSIGN_OR_RAISE(scan,\n+                        cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+  // pipe the scan node into a filter node\n+  cp::ExecNode* filter;\n+  ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                 cp::FilterNodeOptions{b_is_true}));\n+\n+  // // finally, pipe the project node into a sink node\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+  ARROW_ASSIGN_OR_RAISE(\n+      cp::ExecNode * sink,\n+      cp::MakeExecNode(\"sink\", plan.get(), {filter}, cp::SinkNodeOptions{&sink_gen}));\n+\n+  ABORT_ON_FAILURE(sink->Validate());\n+  // // translate sink_gen (async) to sink_reader (sync)\n+  std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+  // // validate the ExecPlan\n+  ABORT_ON_FAILURE(plan->Validate());\n+  PRINT_LINE(\"Exec Plan created \" << plan->ToString());\n+  // // start the ExecPlan\n+  ABORT_ON_FAILURE(plan->StartProducing());\n+\n+  // // collect sink_reader into a Table\n+  std::shared_ptr<arrow::Table> response_table;\n+  ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+  PRINT_LINE(\"Results : \" << response_table->ToString());\n+  // // plan stop producing\n+  plan->StopProducing();\n+  // /// plan marked finished\n+  plan->finished().Wait();\n+\n+  return arrow::Status::OK();\n+}\n+\n+arrow::Status scan_project_sink_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+    cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // projection\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"),\n+    cp::literal(2)}); options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    cp::ExecNode *scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan, cp::MakeExecNode(\"scan\", plan.get(), {},\n+    scan_node_options));\n+\n+    cp::ExecNode *project;\n+    ARROW_ASSIGN_OR_RAISE(project, cp::MakeExecNode(\"project\", plan.get(), {scan},\n+                                                    cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                          cp::MakeExecNode(\"sink\", plan.get(), {project},\n+                                           cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+        std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+\n+    PRINT_LINE(\"Exec Plan Created : \" << plan->ToString());\n+\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop producing\n+    plan->StopProducing();\n+    // // plan marked finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_aggregate_sink_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeBasicBatches();\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source, cp::MakeExecNode(\"source\",\n+                                                                  plan.get(), {},\n+                                                                  source_node_options));\n+\n+    cp::CountOptions options(cp::CountOptions::ONLY_VALID);\n+    auto aggregate_options = cp::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                          cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                          aggregate_options));\n+\n+    cp::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, cp::MakeExecNode(\"sink\", plan.get(), {aggregate},\n+                                                 cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({\n+            arrow::field(\"count(a)\", arrow::int32()),\n+            arrow::field(\"b\", arrow::boolean()),\n+        }),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    //plan stop producing\n+    plan->StopProducing();\n+    // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_consuming_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeBasicBatches();\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source, cp::MakeExecNode(\"source\",\n+                                                                  plan.get(), {},\n+                                                                  source_node_options));\n+\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>\n+        finish)\n+            : batches_seen(batches_seen), finish(std::move(finish)) {}\n+\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+            (*batches_seen)++;\n+            return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+    };\n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+        std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    cp::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+    {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+    ABORT_ON_FAILURE(consuming_sink->Validate());\n+\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // plan start producing\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+    // Source should finish fairly quickly\n+    ABORT_ON_FAILURE(source->finished().status());\n+    PRINT_LINE(\"Source Finished!\");\n+    // Mark consumption complete, plan should finish\n+    arrow::Status finish_status;\n+    //finish.Wait();\n+    finish.MarkFinished(finish_status);\n+    ABORT_ON_FAILURE(plan->finished().status());\n+    ABORT_ON_FAILURE(finish_status);\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_order_by_sink_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+     ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeSortTestBasicBatches();\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source,\n+    cp::MakeExecNode(\"source\", plan.get(), {}, source_node_options));\n+\n+    cp::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    cp::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, cp::OrderBySinkNodeOptions{\n+        cp::SortOptions{{cp::SortKey{\"a\",\n+        cp::SortOrder::Descending}}}, &sink_gen}));\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        basic_data.schema,\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_hash_join_sink_example() {\n+    auto input = MakeGroupableBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode *left_source;\n+    cp::ExecNode *right_source;\n+    for (auto source : {&left_source, &right_source}) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            *source,\n+            MakeExecNode(\"source\", plan.get(), {},\n+                                  cp::SourceNodeOptions{\n+                                      input.schema,\n+                                      input.gen(/*parallel=*/true)}));\n+    }\n+    // TODO: decide whether to keep the filters or remove\n+\n+    // ARROW_ASSIGN_OR_RAISE(\n+    //     auto left_filter,\n+    //     cp::MakeExecNode(\"filter\", plan.get(), {left_source},\n+    //                      cp::FilterNodeOptions{\n+    //                          cp::greater_equal(\n+    //                              cp::field_ref(\"i32\"),\n+    //                              cp::literal(-1))}));\n+    // ARROW_ASSIGN_OR_RAISE(\n+    //     auto right_filter,\n+    //     cp::MakeExecNode(\"filter\", plan.get(), {right_source},\n+    //                      cp::FilterNodeOptions{\n+    //                          cp::less_equal(\n+    //                              cp::field_ref(\"i32\"),\n+    //                              cp::literal(2))}));\n+    // PRINT_LINE(\"left and right filter nodes created\");\n+\n+    cp::HashJoinNodeOptions join_opts{cp::JoinType::INNER,\n+                                      /*left_keys=*/{\"str\"},\n+                                      /*right_keys=*/{\"str\"}, cp::literal(true), \"l_\",\n+                                      \"r_\"};\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        auto hashjoin,\n+        cp::MakeExecNode(\"hashjoin\", plan.get(), {left_source, right_source},\n+        join_opts));\n+\n+    ARROW_ASSIGN_OR_RAISE(std::ignore, cp::MakeExecNode(\"sink\", plan.get(), {hashjoin},\n+                                                        cp::SinkNodeOptions{&sink_gen}));\n+    // expected columns i32, str, l_str, r_str\n+\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"i32\", arrow::int32()),\n+                       arrow::field(\"str\", arrow::utf8()),\n+                       arrow::field(\"l_str\", arrow::utf8()),\n+                       arrow::field(\"r_str\", arrow::utf8())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_kselect_example() {\n+    auto input = MakeGroupableBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        cp::ExecNode * source,\n+        cp::MakeExecNode(\"source\",\n+            plan.get(), {},\n+                cp::SourceNodeOptions{\n+                    input.schema,\n+                    input.gen(/*parallel=*/true)}));\n+\n+    cp::SelectKOptions options = cp::SelectKOptions::TopKDefault(/*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        cp::ExecNode * k_sink_node,\n+        cp::MakeExecNode(\"select_k_sink\",\n+            plan.get(), {source},\n+                cp::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+    k_sink_node->finished().Wait();\n+\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"i32\", arrow::int32()),\n+                       arrow::field(\"str\", arrow::utf8())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop proudcing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status scan_filter_write_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    //cp::Expression b_is_true = cp::field_ref(\"b\");\n+    //options->filter = b_is_true;\n+    // empty projection\n+    options->projection = Materialize({});\n+\n+    cp::ExecNode *scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan, cp::MakeExecNode(\"scan\", plan.get(), {},\n+    scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    std::string root_path = \"\";\n+    std::string uri = \"file:///Users/vibhatha/sandbox/test\";\n\nReview comment:\n       Remember to un-hardcode this :) \r\n   \r\n   We can link to gflags for command line argument parsing if necessary.\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::internal::ThreadPool> MakeIOThreadPool() {\n+  auto maybe_pool = arrow::internal::ThreadPool::MakeEternal(/*threads=*/8);\n+  if (!maybe_pool.ok()) {\n+    maybe_pool.status().Abort(\"Failed to create global IO thread pool\");\n+  }\n+  return *std::move(maybe_pool);\n+}\n+\n+arrow::internal::ThreadPool* GetIOThreadPool() {\n+  static std::shared_ptr<arrow::internal::ThreadPool> pool = MakeIOThreadPool();\n+  return pool.get();\n+}\n\nReview comment:\n       This seems unused.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-23T13:07:57.929+0000",
                    "updated": "2021-12-23T13:07:57.929+0000",
                    "started": "2021-12-23T13:07:57.929+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700605",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700616",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774563765\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n\nReview comment:\n       Yes, we can do that. Should we hard code some values or use a real world dataset for this? \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-23T13:16:24.997+0000",
                    "updated": "2021-12-23T13:16:24.997+0000",
                    "started": "2021-12-23T13:16:24.997+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700616",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700618",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774564422\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n\nReview comment:\n       I think we can keep hardcoding values just to keep it self-contained.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-23T13:17:44.447+0000",
                    "updated": "2021-12-23T13:17:44.447+0000",
                    "started": "2021-12-23T13:17:44.446+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700618",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700619",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774564502\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n\nReview comment:\n       This is WIP function as same as some other generators. Extracted some from internals. Will replace this with better functions. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-23T13:17:59.119+0000",
                    "updated": "2021-12-23T13:17:59.119+0000",
                    "started": "2021-12-23T13:17:59.118+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700619",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700641",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774580069\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::internal::ThreadPool> MakeIOThreadPool() {\n+  auto maybe_pool = arrow::internal::ThreadPool::MakeEternal(/*threads=*/8);\n+  if (!maybe_pool.ok()) {\n+    maybe_pool.status().Abort(\"Failed to create global IO thread pool\");\n+  }\n+  return *std::move(maybe_pool);\n+}\n+\n+arrow::internal::ThreadPool* GetIOThreadPool() {\n+  static std::shared_ptr<arrow::internal::ThreadPool> pool = MakeIOThreadPool();\n+  return pool.get();\n+}\n\nReview comment:\n       Need to clean up some of these functions \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-23T13:45:06.307+0000",
                    "updated": "2021-12-23T13:45:06.307+0000",
                    "started": "2021-12-23T13:45:06.307+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700641",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700908",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774893208\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n\nReview comment:\n       Updating code. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-24T06:28:31.862+0000",
                    "updated": "2021-12-24T06:28:31.862+0000",
                    "started": "2021-12-24T06:28:31.862+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700908",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700913",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774920857\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n\nReview comment:\n       Fixed with a csv based reader using in-memory string data. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-24T08:06:05.673+0000",
                    "updated": "2021-12-24T08:06:05.673+0000",
                    "started": "2021-12-24T08:06:05.672+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700913",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700914",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774921831\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n\nReview comment:\n       Removed. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-24T08:09:20.881+0000",
                    "updated": "2021-12-24T08:09:20.881+0000",
                    "started": "2021-12-24T08:09:20.880+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700914",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700916",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774921952\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n\nReview comment:\n       Updated\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-24T08:09:46.471+0000",
                    "updated": "2021-12-24T08:09:46.471+0000",
                    "started": "2021-12-24T08:09:46.471+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700916",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700934",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774947371\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n\nReview comment:\n       Updated Naming convention\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-24T09:26:51.980+0000",
                    "updated": "2021-12-24T09:26:51.980+0000",
                    "started": "2021-12-24T09:26:51.979+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700934",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700936",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774947652\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::internal::ThreadPool> MakeIOThreadPool() {\n+  auto maybe_pool = arrow::internal::ThreadPool::MakeEternal(/*threads=*/8);\n+  if (!maybe_pool.ok()) {\n+    maybe_pool.status().Abort(\"Failed to create global IO thread pool\");\n+  }\n+  return *std::move(maybe_pool);\n+}\n+\n+arrow::internal::ThreadPool* GetIOThreadPool() {\n+  static std::shared_ptr<arrow::internal::ThreadPool> pool = MakeIOThreadPool();\n+  return pool.get();\n+}\n\nReview comment:\n       Removed.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-24T09:27:28.056+0000",
                    "updated": "2021-12-24T09:27:28.056+0000",
                    "started": "2021-12-24T09:27:28.056+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700936",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700937",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774947746\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::internal::ThreadPool> MakeIOThreadPool() {\n+  auto maybe_pool = arrow::internal::ThreadPool::MakeEternal(/*threads=*/8);\n+  if (!maybe_pool.ok()) {\n+    maybe_pool.status().Abort(\"Failed to create global IO thread pool\");\n+  }\n+  return *std::move(maybe_pool);\n+}\n+\n+arrow::internal::ThreadPool* GetIOThreadPool() {\n+  static std::shared_ptr<arrow::internal::ThreadPool> pool = MakeIOThreadPool();\n+  return pool.get();\n+}\n+\n+arrow::Status source_sink_example() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                               ::arrow::internal::GetCpuThreadPool());\n+\n+  // ensure arrow::dataset node factories are in the registry\n+  arrow::dataset::internal::Initialize();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+  cp::ExecPlan::Make(&exec_context));\n+\n+  auto basic_data = MakeBasicBatches();\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+  auto source_node_options =\n+      cp::SourceNodeOptions{basic_data.schema, basic_data.gen(false)};\n+\n+  ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source,\n+                        cp::MakeExecNode(\"source\", plan.get(), {}, source_node_options));\n+\n+  cp::ExecNode* sink;\n+\n+  ARROW_ASSIGN_OR_RAISE(sink, cp::MakeExecNode(\"sink\", plan.get(), {source},\n+                                               cp::SinkNodeOptions{&sink_gen}));\n+\n+  // // // translate sink_gen (async) to sink_reader (sync)\n+  std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+  // // validate the ExecPlan\n+  ABORT_ON_FAILURE(plan->Validate());\n+  PRINT_LINE(\"Exec Plan Created: \" << plan->ToString());\n+  // // // start the ExecPlan\n+  ABORT_ON_FAILURE(plan->StartProducing());\n+\n+  // // collect sink_reader into a Table\n+  std::shared_ptr<arrow::Table> response_table;\n+\n+  ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+  PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+  // // plan stop producing\n+  plan->StopProducing();\n+  // // plan mark finished\n+  plan->finished().Wait();\n+\n+  return arrow::Status::OK();\n+}\n+\n+arrow::Status scan_filter_sink_example() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                               ::arrow::internal::GetCpuThreadPool());\n+\n+  // ensure arrow::dataset node factories are in the registry\n+  arrow::dataset::internal::Initialize();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                        cp::ExecPlan::Make(&exec_context));\n+\n+  std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  // sync scanning is not supported by ScanNode\n+  options->use_async = true;\n+  // specify the filter\n+  cp::Expression b_is_true = cp::field_ref(\"b\");\n+  options->filter = b_is_true;\n+  // empty projection\n+  options->projection = Materialize({});\n+\n+  // construct the scan node\n+  PRINT_LINE(\"Initialized Scanning Options\");\n+\n+  cp::ExecNode* scan;\n+\n+  auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+  PRINT_LINE(\"Scan node options created\");\n+\n+  ARROW_ASSIGN_OR_RAISE(scan,\n+                        cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+  // pipe the scan node into a filter node\n+  cp::ExecNode* filter;\n+  ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                 cp::FilterNodeOptions{b_is_true}));\n+\n+  // // finally, pipe the project node into a sink node\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+  ARROW_ASSIGN_OR_RAISE(\n+      cp::ExecNode * sink,\n+      cp::MakeExecNode(\"sink\", plan.get(), {filter}, cp::SinkNodeOptions{&sink_gen}));\n+\n+  ABORT_ON_FAILURE(sink->Validate());\n+  // // translate sink_gen (async) to sink_reader (sync)\n+  std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+  // // validate the ExecPlan\n+  ABORT_ON_FAILURE(plan->Validate());\n+  PRINT_LINE(\"Exec Plan created \" << plan->ToString());\n+  // // start the ExecPlan\n+  ABORT_ON_FAILURE(plan->StartProducing());\n+\n+  // // collect sink_reader into a Table\n+  std::shared_ptr<arrow::Table> response_table;\n+  ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+  PRINT_LINE(\"Results : \" << response_table->ToString());\n+  // // plan stop producing\n+  plan->StopProducing();\n+  // /// plan marked finished\n+  plan->finished().Wait();\n+\n+  return arrow::Status::OK();\n+}\n+\n+arrow::Status scan_project_sink_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+    cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // projection\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"),\n+    cp::literal(2)}); options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    cp::ExecNode *scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan, cp::MakeExecNode(\"scan\", plan.get(), {},\n+    scan_node_options));\n+\n+    cp::ExecNode *project;\n+    ARROW_ASSIGN_OR_RAISE(project, cp::MakeExecNode(\"project\", plan.get(), {scan},\n+                                                    cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                          cp::MakeExecNode(\"sink\", plan.get(), {project},\n+                                           cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+        std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+\n+    PRINT_LINE(\"Exec Plan Created : \" << plan->ToString());\n+\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop producing\n+    plan->StopProducing();\n+    // // plan marked finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_aggregate_sink_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeBasicBatches();\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source, cp::MakeExecNode(\"source\",\n+                                                                  plan.get(), {},\n+                                                                  source_node_options));\n+\n+    cp::CountOptions options(cp::CountOptions::ONLY_VALID);\n+    auto aggregate_options = cp::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                          cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                          aggregate_options));\n+\n+    cp::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, cp::MakeExecNode(\"sink\", plan.get(), {aggregate},\n+                                                 cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({\n+            arrow::field(\"count(a)\", arrow::int32()),\n+            arrow::field(\"b\", arrow::boolean()),\n+        }),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    //plan stop producing\n+    plan->StopProducing();\n+    // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_consuming_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeBasicBatches();\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source, cp::MakeExecNode(\"source\",\n+                                                                  plan.get(), {},\n+                                                                  source_node_options));\n+\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>\n+        finish)\n+            : batches_seen(batches_seen), finish(std::move(finish)) {}\n+\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+            (*batches_seen)++;\n+            return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+    };\n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+        std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    cp::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+    {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+    ABORT_ON_FAILURE(consuming_sink->Validate());\n+\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // plan start producing\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+    // Source should finish fairly quickly\n+    ABORT_ON_FAILURE(source->finished().status());\n+    PRINT_LINE(\"Source Finished!\");\n+    // Mark consumption complete, plan should finish\n+    arrow::Status finish_status;\n+    //finish.Wait();\n+    finish.MarkFinished(finish_status);\n+    ABORT_ON_FAILURE(plan->finished().status());\n+    ABORT_ON_FAILURE(finish_status);\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_order_by_sink_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+     ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeSortTestBasicBatches();\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source,\n+    cp::MakeExecNode(\"source\", plan.get(), {}, source_node_options));\n+\n+    cp::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    cp::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, cp::OrderBySinkNodeOptions{\n+        cp::SortOptions{{cp::SortKey{\"a\",\n+        cp::SortOrder::Descending}}}, &sink_gen}));\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        basic_data.schema,\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_hash_join_sink_example() {\n+    auto input = MakeGroupableBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode *left_source;\n+    cp::ExecNode *right_source;\n+    for (auto source : {&left_source, &right_source}) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            *source,\n+            MakeExecNode(\"source\", plan.get(), {},\n+                                  cp::SourceNodeOptions{\n+                                      input.schema,\n+                                      input.gen(/*parallel=*/true)}));\n+    }\n+    // TODO: decide whether to keep the filters or remove\n+\n+    // ARROW_ASSIGN_OR_RAISE(\n+    //     auto left_filter,\n+    //     cp::MakeExecNode(\"filter\", plan.get(), {left_source},\n+    //                      cp::FilterNodeOptions{\n+    //                          cp::greater_equal(\n+    //                              cp::field_ref(\"i32\"),\n+    //                              cp::literal(-1))}));\n+    // ARROW_ASSIGN_OR_RAISE(\n+    //     auto right_filter,\n+    //     cp::MakeExecNode(\"filter\", plan.get(), {right_source},\n+    //                      cp::FilterNodeOptions{\n+    //                          cp::less_equal(\n+    //                              cp::field_ref(\"i32\"),\n+    //                              cp::literal(2))}));\n+    // PRINT_LINE(\"left and right filter nodes created\");\n+\n+    cp::HashJoinNodeOptions join_opts{cp::JoinType::INNER,\n+                                      /*left_keys=*/{\"str\"},\n+                                      /*right_keys=*/{\"str\"}, cp::literal(true), \"l_\",\n+                                      \"r_\"};\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        auto hashjoin,\n+        cp::MakeExecNode(\"hashjoin\", plan.get(), {left_source, right_source},\n+        join_opts));\n+\n+    ARROW_ASSIGN_OR_RAISE(std::ignore, cp::MakeExecNode(\"sink\", plan.get(), {hashjoin},\n+                                                        cp::SinkNodeOptions{&sink_gen}));\n+    // expected columns i32, str, l_str, r_str\n+\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"i32\", arrow::int32()),\n+                       arrow::field(\"str\", arrow::utf8()),\n+                       arrow::field(\"l_str\", arrow::utf8()),\n+                       arrow::field(\"r_str\", arrow::utf8())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status source_kselect_example() {\n+    auto input = MakeGroupableBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        cp::ExecNode * source,\n+        cp::MakeExecNode(\"source\",\n+            plan.get(), {},\n+                cp::SourceNodeOptions{\n+                    input.schema,\n+                    input.gen(/*parallel=*/true)}));\n+\n+    cp::SelectKOptions options = cp::SelectKOptions::TopKDefault(/*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        cp::ExecNode * k_sink_node,\n+        cp::MakeExecNode(\"select_k_sink\",\n+            plan.get(), {source},\n+                cp::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+    k_sink_node->finished().Wait();\n+\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"i32\", arrow::int32()),\n+                       arrow::field(\"str\", arrow::utf8())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop proudcing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status scan_filter_write_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    //cp::Expression b_is_true = cp::field_ref(\"b\");\n+    //options->filter = b_is_true;\n+    // empty projection\n+    options->projection = Materialize({});\n+\n+    cp::ExecNode *scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan, cp::MakeExecNode(\"scan\", plan.get(), {},\n+    scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    std::string root_path = \"\";\n+    std::string uri = \"file:///Users/vibhatha/sandbox/test\";\n\nReview comment:\n       Removed hardcoded values and added cmd args. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-24T09:27:51.010+0000",
                    "updated": "2021-12-24T09:27:51.010+0000",
                    "started": "2021-12-24T09:27:51.010+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700937",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/700938",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r774952365\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n\nReview comment:\n       One question related to this, the `SourceNodeOptions` requires the input\r\n   \r\n   ```c++\r\n   SourceNodeOptions(std::shared_ptr<Schema> output_schema,\r\n                       std::function<Future<util::optional<ExecBatch>>()> generator) {...}\r\n   ```\r\n   \r\n   Are you referring to the return type of this function or about the internal components used in the function body. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-24T09:41:16.390+0000",
                    "updated": "2021-12-24T09:41:16.390+0000",
                    "started": "2021-12-24T09:41:16.390+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "700938",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/701220",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r775317369\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::internal::ThreadPool> MakeIOThreadPool() {\n+  auto maybe_pool = arrow::internal::ThreadPool::MakeEternal(/*threads=*/8);\n+  if (!maybe_pool.ok()) {\n+    maybe_pool.status().Abort(\"Failed to create global IO thread pool\");\n+  }\n+  return *std::move(maybe_pool);\n+}\n+\n+arrow::internal::ThreadPool* GetIOThreadPool() {\n+  static std::shared_ptr<arrow::internal::ThreadPool> pool = MakeIOThreadPool();\n+  return pool.get();\n+}\n+\n+arrow::Status source_sink_example() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n\nReview comment:\n       Yes, you're correct. Will remove this. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-27T02:49:39.419+0000",
                    "updated": "2021-12-27T02:49:39.419+0000",
                    "started": "2021-12-27T02:49:39.419+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "701220",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/701311",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#issuecomment-1001577440\n\n\n   BTW, to get CI to run,  the [WIP] needs to be removed from the issue title (and the branch needs to have a new commit/be rebased)\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-27T13:46:13.010+0000",
                    "updated": "2021-12-27T13:46:13.010+0000",
                    "started": "2021-12-27T13:46:13.010+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "701311",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/701314",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r775506160\n\n\n\n##########\nFile path: cpp/examples/arrow/exec_plan_examples.cc\n##########\n@@ -0,0 +1,1122 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/ir_consumer.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/plan.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/dataset_writer.h\"\n+\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/io/slow.h\"\n+#include \"arrow/io/transform.h\"\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include \"arrow/util/range.h\"\n+#include \"arrow/util/thread_pool.h\"\n+#include \"arrow/util/vector.h\"\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define SEP_STR \"******\"\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << SEP_STR << \" \" << msg << \" \" << SEP_STR << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+  std::shared_ptr<arrow::Array> out;\n+\n+  ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::shared_ptr<arrow::Table> GetTableFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, const std::vector<std::string>& json) {\n+  std::vector<std::shared_ptr<arrow::RecordBatch>> batches;\n+  for (const std::string& batch_json : json) {\n+    batches.push_back(GetRecordBatchFromJSON(schema, batch_json));\n+  }\n+  return *arrow::Table::FromRecordBatches(schema, std::move(batches));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateTable() {\n+    auto schema =\n+        arrow::schema({arrow::field(\"a\", arrow::int64()),\n+        arrow::field(\"b\", arrow::int64()),\n+        arrow::field(\"c\", arrow::int64())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    std::shared_ptr<arrow::Array> array_c;\n+    arrow::NumericBuilder<arrow::Int64Type> builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 1, 2, 1, 2, 1, 2, 1, 2}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_c));\n+    return arrow::Table::Make(schema, {array_a, array_b, array_c});\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    return std::make_shared<arrow::dataset::InMemoryDataset>(\n+        GetTableFromJSON(arrow::schema({arrow::field(\"a\", arrow::int32()),\n+                                        arrow::field(\"b\", arrow::boolean())}),\n+                        {\n+                            R\"([{\"a\": 1,    \"b\": null},\n+                                {\"a\": 2,    \"b\": true}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 3,    \"b\": false}])\",\n+                            R\"([{\"a\": null, \"b\": true},\n+                                {\"a\": 4,    \"b\": false}])\",\n+                            R\"([{\"a\": 5,    \"b\": null},\n+                                {\"a\": 6,    \"b\": false},\n+                                {\"a\": 7,    \"b\": false},\n+                                {\"a\": 8,    \"b\": true}])\",\n+                        }));\n+}\n+\n+std::shared_ptr<arrow::Table> CreateSimpleTable() {\n+    auto schema = arrow::schema(\n+        {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+    std::shared_ptr<arrow::Array> array_a;\n+    std::shared_ptr<arrow::Array> array_b;\n+    arrow::NumericBuilder<arrow::Int32Type> builder;\n+    arrow::BooleanBuilder b_builder;\n+    ABORT_ON_FAILURE(builder.AppendValues({1, 2, 3, 4, 5, 6, 7}));\n+    ABORT_ON_FAILURE(builder.Finish(&array_a));\n+    builder.Reset();\n+\n+    std::vector<bool> bool_vec{false, true, false, true, false, true, false};\n+    ABORT_ON_FAILURE(b_builder.AppendValues(bool_vec));\n+    ABORT_ON_FAILURE(builder.Finish(&array_b));\n+    builder.Reset();\n+    return arrow::Table::Make(schema, {array_a, array_b});\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status scan_sink_node_example() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n\nReview comment:\n       Ah, right. A `Dataset` can be turned into a `Scanner` and the `Scanner` can _internally_ build an ExecPlan, but that isn't exposed. However, that means we're stuck using internal APIs in an example which we should avoid as well.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-27T13:48:01.496+0000",
                    "updated": "2021-12-27T13:48:01.496+0000",
                    "started": "2021-12-27T13:48:01.496+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "701314",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/701332",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r775511319\n\n\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n\nReview comment:\n       ```suggestion\r\n   Using the execution plan we can construct various queries. \r\n   ```\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n\nReview comment:\n       This isn't valid C++ or clear pesudocode (missing semicolons, also the declaration of `gen`)\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n\nReview comment:\n       General comment: each of these sections can be made more consistent, in terms of wording and structure.\r\n   \r\n   > (node name) does (X). It is constructed from (options class), which requires (parameters). (More description of parameters if necessary)\r\n   > \r\n   > For example, to create a (node name) that does (specific example)::\r\n   >     (code example follows)\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n\nReview comment:\n       If some are undocumented, we should add the documentation so we can link them.\r\n   \r\n   That said, I'm not sure how useful the API docs for nodes are, it might even be confusing.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n+\n+    arrow::compute::Expression Materialize(std::vector<std::string> names,\n+                              bool include_aug_fields = false) {\n+        if (include_aug_fields) {\n+            for (auto aug_name : {\"__fragment_index\",\n+                \"__batch_index\", \"__last_in_fragment\"}) {\n+            names.emplace_back(aug_name);\n+            }\n+        }\n+\n+        std::vector<arrow::compute::Expression> exprs;\n+        for (const auto& name : names) {\n+            exprs.push_back(arrow::compute::field_ref(name));\n+        }\n+\n+        return arrow::compute::project(exprs, names);\n+    }\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{\n+                              /*dataset*/dataset, \n+                              /*scan_options*/options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            arrow::compute::MakeExecNode(\"scan\", \n+                              plan.get(), \n+                              {}, \n+                              scan_node_options));\n+\n+``SinkNode``\n+------------\n+\n+:class:`SinkNode` can be considered as the output or final node of an streaming \n+execution definition. :class:`arrow::compute::SinkNodeOptions` interface is used to pass \n+the required options. Requires \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>* generator`\n+and `arrow::util::BackpressureOptions backpressure`. \n+\n+Example::\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    arrow::compute::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, arrow::compute::MakeExecNode(\"sink\", plan.get(), {source},\n+                                                  arrow::compute::SinkNodeOptions{&sink_gen}));\n+\n+\n+The output can be obtained as a table::\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::cout << \"Exec Plan Created: \" << plan->ToString() << std::endl;\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+\n+``ConsumingSinkNode``\n+---------------------\n+\n+:class:`arrow::compute::ConsumingSinkNode` is a sink node that owns consuming the data and \n+will not finish until the consumption is finished.  Use SinkNode if you are\n+transferring the ownership of the data to another system.  \n+Use :class:`arrow::compute::ConsumingSinkNode` if the data is being consumed within the exec \n+plan (i.e. the exec plan should not complete until the consumption has completed).\n+\n+Example::\n+\n+    // define a Custom SinkNodeConsumer\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>finish): \n+        batches_seen(batches_seen), finish(std::move(finish)) {}\n+        // Consumption logic can be written here\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+        // data can be consumed in the expected way\n+        // transfer to another system or just do some work \n+        // and write to disk\n+        (*batches_seen)++;\n+        return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+        \n+    };\n+    \n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+            std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    arrow::compute::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+        {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+\n+``OrderBySinkNode``\n+-------------------\n+\n+This is an extension to the :class:`SinkNode` definition and provides the ability\n+to guarantee the ordering of the stream by providing the,\n+:class:`arrow::compute::OrderBySinkNodeOptions`. \n+Here the :class:`arrow::compute::SortOptions` are provided to define which columns \n+are used for sorting and under which criterion.\n+\n+Example::\n+\n+    arrow::compute::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    arrow::compute::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, \n+    arrow::compute::OrderBySinkNodeOptions{\n+    /*sort_options*/arrow::compute::SortOptions{\n+    {\tarrow::compute::SortKey{\n+    //Column key(s) to order by and how to order by these sort keys.\n+    \"a\",\n+    // Sort Order\n+    arrow::compute::SortOrder::Descending \n+    }}},&sink_gen}));\n+\n+\n+SelectK-Node\n+------------\n+\n+There is no Select-K-SinkNode available as an entity within the source, but the behavior \n+is defined with the options :class:`arrow::compute::SelectKOptions` which is a defined by \n+using :struct:`OrderBySinkNode` definition. This option returns a sink node that receives \n+inputs and then compute top_k/bottom_k.\n+\n+Example::\n+\n+    arrow::compute::SelectKOptions options = arrow::compute::SelectKOptions::TopKDefault(\n+                /*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+      arrow::compute::ExecNode * k_sink_node,\n+      arrow::compute::MakeExecNode(\"select_k_sink\",\n+        plan.get(), {source},\n+        arrow::compute::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+Scan-Node\n+---------\n+\n+There is no definition Scan-Node in the source, but the behavior of is defined using \n+:class:`arrow::dataset::ScanNodeOptions`. This option contains a set of definitions. \n+\n+Option definitions for :class:`arrow::dataset::ScanNodeOptions`:: \n+\n+\n+    /// A row filter (which will be pushed down to partitioning/reading if supported).\n\nReview comment:\n       This is formatted oddly, plus, this all comes from ScanOptions, no?\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n+\n+    arrow::compute::Expression Materialize(std::vector<std::string> names,\n+                              bool include_aug_fields = false) {\n+        if (include_aug_fields) {\n+            for (auto aug_name : {\"__fragment_index\",\n+                \"__batch_index\", \"__last_in_fragment\"}) {\n+            names.emplace_back(aug_name);\n+            }\n+        }\n+\n+        std::vector<arrow::compute::Expression> exprs;\n+        for (const auto& name : names) {\n+            exprs.push_back(arrow::compute::field_ref(name));\n+        }\n+\n+        return arrow::compute::project(exprs, names);\n+    }\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{\n+                              /*dataset*/dataset, \n+                              /*scan_options*/options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            arrow::compute::MakeExecNode(\"scan\", \n+                              plan.get(), \n+                              {}, \n+                              scan_node_options));\n+\n+``SinkNode``\n+------------\n+\n+:class:`SinkNode` can be considered as the output or final node of an streaming \n+execution definition. :class:`arrow::compute::SinkNodeOptions` interface is used to pass \n+the required options. Requires \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>* generator`\n+and `arrow::util::BackpressureOptions backpressure`. \n+\n+Example::\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    arrow::compute::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, arrow::compute::MakeExecNode(\"sink\", plan.get(), {source},\n+                                                  arrow::compute::SinkNodeOptions{&sink_gen}));\n+\n+\n+The output can be obtained as a table::\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::cout << \"Exec Plan Created: \" << plan->ToString() << std::endl;\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+\n+``ConsumingSinkNode``\n+---------------------\n+\n+:class:`arrow::compute::ConsumingSinkNode` is a sink node that owns consuming the data and \n+will not finish until the consumption is finished.  Use SinkNode if you are\n+transferring the ownership of the data to another system.  \n+Use :class:`arrow::compute::ConsumingSinkNode` if the data is being consumed within the exec \n+plan (i.e. the exec plan should not complete until the consumption has completed).\n+\n+Example::\n+\n+    // define a Custom SinkNodeConsumer\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>finish): \n+        batches_seen(batches_seen), finish(std::move(finish)) {}\n+        // Consumption logic can be written here\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+        // data can be consumed in the expected way\n+        // transfer to another system or just do some work \n+        // and write to disk\n+        (*batches_seen)++;\n+        return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+        \n+    };\n+    \n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+            std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    arrow::compute::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+        {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+\n+``OrderBySinkNode``\n+-------------------\n+\n+This is an extension to the :class:`SinkNode` definition and provides the ability\n+to guarantee the ordering of the stream by providing the,\n+:class:`arrow::compute::OrderBySinkNodeOptions`. \n+Here the :class:`arrow::compute::SortOptions` are provided to define which columns \n+are used for sorting and under which criterion.\n+\n+Example::\n+\n+    arrow::compute::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    arrow::compute::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, \n+    arrow::compute::OrderBySinkNodeOptions{\n+    /*sort_options*/arrow::compute::SortOptions{\n+    {\tarrow::compute::SortKey{\n+    //Column key(s) to order by and how to order by these sort keys.\n+    \"a\",\n+    // Sort Order\n+    arrow::compute::SortOrder::Descending \n+    }}},&sink_gen}));\n+\n+\n+SelectK-Node\n+------------\n+\n+There is no Select-K-SinkNode available as an entity within the source, but the behavior \n+is defined with the options :class:`arrow::compute::SelectKOptions` which is a defined by \n+using :struct:`OrderBySinkNode` definition. This option returns a sink node that receives \n+inputs and then compute top_k/bottom_k.\n+\n+Example::\n+\n+    arrow::compute::SelectKOptions options = arrow::compute::SelectKOptions::TopKDefault(\n+                /*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+      arrow::compute::ExecNode * k_sink_node,\n+      arrow::compute::MakeExecNode(\"select_k_sink\",\n+        plan.get(), {source},\n+        arrow::compute::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+Scan-Node\n\nReview comment:\n       Didn't we talk about scan node above?\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n\nReview comment:\n       ```suggestion\r\n   :class:`arrow::compute::SourceNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \r\n   ```\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n\nReview comment:\n       ```suggestion\r\n   :class:`FilterNode`, as the name suggests, keeps only rows matching a given expression. \r\n   ```\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n\nReview comment:\n       We shouldn't talk about individual options classes here. We should link to the relevant documentation instead. Most of this should actually be in the API docs, and if not, should be moved to documentation comments in the source code.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n\nReview comment:\n       containers -> maybe \"building blocks\"?\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n\nReview comment:\n       In general, let's consistently use `Result` and `Status` and friends, or consistently abort on each assignment, but let's not mix the two styles.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n\nReview comment:\n       It feels like this should all go _after_ the docs about constructing an ExecPlan below, since that's where the actual code to create an exec node is introduced.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n\nReview comment:\n       I think we can be more concise here. Example\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n\nReview comment:\n       ```suggestion\r\n                               // input node\r\n   ```\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n+    return im_dataset;\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status ScanSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n\nReview comment:\n       Probably we should take `Scanner::ScanBatchesUnorderedAsync` and use `arrow::MakeMapGenerator` to convert it into a generator of ExecBatch, instead of using internal utilities here and making confusing comments about threads (which users should not need to deal with).\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n\nReview comment:\n       async_mode should be treated as internal and we shouldn't talk about it.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n\nReview comment:\n       nit: \"_if_ names are not provided\u2026\"\r\n   \r\n   and it takes a vector of expressions, no?\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n\nReview comment:\n       ```suggestion\r\n   Filters can be written using :class:`arrow::compute::Expression`. For example, if we wish\r\n   to keep rows of column ``b`` greater than 3, then we can use the following expression::\r\n   ```\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n\nReview comment:\n       Is there prose documentation on Expression that we can link to? Or a section in the API docs that lists all the helpers like `greater`, `call`, etc.?\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n\nReview comment:\n       The code example here could be formatted (e.g. there's random indentation on the macro, and missing indentation for the MakeExecNode call)\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n\nReview comment:\n       This is rather vague. If we're explaining what a filter is, then we need to explain what an aggregation is. Or vice versa, if we _don't_ explain what an aggregation is, then we shouldn't be explaining a filter, either.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n\nReview comment:\n       Same comments about the formatting here.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n\nReview comment:\n       We should also link to the general datasets documentation, and perhaps directly to the section about scanning.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n\nReview comment:\n       I think this means we should probably not refer to individual node classes, but only to the general node names, since we only expose them via the factory interface. Then the scan node won't be an odd exception.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n\nReview comment:\n       Hmm. Do we even want to document the scan node, given it has this internal parameter that users can't really provide?\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n\nReview comment:\n       ```suggestion\r\n   ``ScanNode``\r\n   ```\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n\nReview comment:\n       The code block here is rather confusing. Also, we haven't explained what the parameters are.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n+\n+    arrow::compute::Expression Materialize(std::vector<std::string> names,\n+                              bool include_aug_fields = false) {\n+        if (include_aug_fields) {\n+            for (auto aug_name : {\"__fragment_index\",\n+                \"__batch_index\", \"__last_in_fragment\"}) {\n+            names.emplace_back(aug_name);\n+            }\n+        }\n+\n+        std::vector<arrow::compute::Expression> exprs;\n+        for (const auto& name : names) {\n+            exprs.push_back(arrow::compute::field_ref(name));\n+        }\n+\n+        return arrow::compute::project(exprs, names);\n+    }\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{\n+                              /*dataset*/dataset, \n+                              /*scan_options*/options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            arrow::compute::MakeExecNode(\"scan\", \n+                              plan.get(), \n+                              {}, \n+                              scan_node_options));\n+\n+``SinkNode``\n+------------\n+\n+:class:`SinkNode` can be considered as the output or final node of an streaming \n+execution definition. :class:`arrow::compute::SinkNodeOptions` interface is used to pass \n+the required options. Requires \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>* generator`\n+and `arrow::util::BackpressureOptions backpressure`. \n+\n+Example::\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    arrow::compute::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, arrow::compute::MakeExecNode(\"sink\", plan.get(), {source},\n+                                                  arrow::compute::SinkNodeOptions{&sink_gen}));\n+\n+\n+The output can be obtained as a table::\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::cout << \"Exec Plan Created: \" << plan->ToString() << std::endl;\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+\n+``ConsumingSinkNode``\n+---------------------\n+\n+:class:`arrow::compute::ConsumingSinkNode` is a sink node that owns consuming the data and \n+will not finish until the consumption is finished.  Use SinkNode if you are\n+transferring the ownership of the data to another system.  \n+Use :class:`arrow::compute::ConsumingSinkNode` if the data is being consumed within the exec \n+plan (i.e. the exec plan should not complete until the consumption has completed).\n\nReview comment:\n       This is nearly tautological. I think the general purpose is that `ConsumingSinkNode` is used to consume the output within the exec plan, instead of getting a stream of batches to read.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n\nReview comment:\n       component -> class? \r\n   \r\n   We should call a spade a spade and avoid overly vague terminology. Or we can just say, \"ProjectNode is created from ProjectNodeOptions which requires...\"\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n+\n+    arrow::compute::Expression Materialize(std::vector<std::string> names,\n+                              bool include_aug_fields = false) {\n+        if (include_aug_fields) {\n+            for (auto aug_name : {\"__fragment_index\",\n+                \"__batch_index\", \"__last_in_fragment\"}) {\n+            names.emplace_back(aug_name);\n+            }\n+        }\n+\n+        std::vector<arrow::compute::Expression> exprs;\n+        for (const auto& name : names) {\n+            exprs.push_back(arrow::compute::field_ref(name));\n+        }\n+\n+        return arrow::compute::project(exprs, names);\n+    }\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{\n+                              /*dataset*/dataset, \n+                              /*scan_options*/options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            arrow::compute::MakeExecNode(\"scan\", \n+                              plan.get(), \n+                              {}, \n+                              scan_node_options));\n+\n+``SinkNode``\n+------------\n+\n+:class:`SinkNode` can be considered as the output or final node of an streaming \n+execution definition. :class:`arrow::compute::SinkNodeOptions` interface is used to pass \n+the required options. Requires \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>* generator`\n\nReview comment:\n       Note that this has a user-friendly alias, `function<Future<T>()>` can be written as `arrow::AsyncGenerator<T>`\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n\nReview comment:\n       We don't need to talk about implementation, only purpose. And actually, we haven't talked about what a scan node even does.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n\nReview comment:\n       I guess in general, now I'm questioning the goals of what we're documenting here, since a lot of this is inaccessible or confusing to users. And if it's meant as developer documentation, I wonder if there's a better place for it, or if the focus shouldn't be on the documentation comments themselves.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n+\n+    arrow::compute::Expression Materialize(std::vector<std::string> names,\n+                              bool include_aug_fields = false) {\n+        if (include_aug_fields) {\n+            for (auto aug_name : {\"__fragment_index\",\n+                \"__batch_index\", \"__last_in_fragment\"}) {\n+            names.emplace_back(aug_name);\n+            }\n+        }\n+\n+        std::vector<arrow::compute::Expression> exprs;\n+        for (const auto& name : names) {\n+            exprs.push_back(arrow::compute::field_ref(name));\n+        }\n+\n+        return arrow::compute::project(exprs, names);\n+    }\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{\n+                              /*dataset*/dataset, \n+                              /*scan_options*/options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            arrow::compute::MakeExecNode(\"scan\", \n+                              plan.get(), \n+                              {}, \n+                              scan_node_options));\n+\n+``SinkNode``\n+------------\n+\n+:class:`SinkNode` can be considered as the output or final node of an streaming \n+execution definition. :class:`arrow::compute::SinkNodeOptions` interface is used to pass \n+the required options. Requires \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>* generator`\n+and `arrow::util::BackpressureOptions backpressure`. \n+\n+Example::\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    arrow::compute::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, arrow::compute::MakeExecNode(\"sink\", plan.get(), {source},\n+                                                  arrow::compute::SinkNodeOptions{&sink_gen}));\n+\n+\n+The output can be obtained as a table::\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::cout << \"Exec Plan Created: \" << plan->ToString() << std::endl;\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+\n+``ConsumingSinkNode``\n+---------------------\n+\n+:class:`arrow::compute::ConsumingSinkNode` is a sink node that owns consuming the data and \n+will not finish until the consumption is finished.  Use SinkNode if you are\n+transferring the ownership of the data to another system.  \n+Use :class:`arrow::compute::ConsumingSinkNode` if the data is being consumed within the exec \n+plan (i.e. the exec plan should not complete until the consumption has completed).\n+\n+Example::\n+\n+    // define a Custom SinkNodeConsumer\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>finish): \n+        batches_seen(batches_seen), finish(std::move(finish)) {}\n+        // Consumption logic can be written here\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+        // data can be consumed in the expected way\n+        // transfer to another system or just do some work \n+        // and write to disk\n+        (*batches_seen)++;\n+        return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+        \n+    };\n+    \n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+            std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    arrow::compute::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+        {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+\n+``OrderBySinkNode``\n+-------------------\n+\n+This is an extension to the :class:`SinkNode` definition and provides the ability\n+to guarantee the ordering of the stream by providing the,\n+:class:`arrow::compute::OrderBySinkNodeOptions`. \n+Here the :class:`arrow::compute::SortOptions` are provided to define which columns \n+are used for sorting and under which criterion.\n+\n+Example::\n+\n+    arrow::compute::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    arrow::compute::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, \n+    arrow::compute::OrderBySinkNodeOptions{\n+    /*sort_options*/arrow::compute::SortOptions{\n+    {\tarrow::compute::SortKey{\n+    //Column key(s) to order by and how to order by these sort keys.\n+    \"a\",\n+    // Sort Order\n+    arrow::compute::SortOrder::Descending \n+    }}},&sink_gen}));\n+\n+\n+SelectK-Node\n+------------\n+\n+There is no Select-K-SinkNode available as an entity within the source, but the behavior \n+is defined with the options :class:`arrow::compute::SelectKOptions` which is a defined by \n+using :struct:`OrderBySinkNode` definition. This option returns a sink node that receives \n+inputs and then compute top_k/bottom_k.\n+\n+Example::\n+\n+    arrow::compute::SelectKOptions options = arrow::compute::SelectKOptions::TopKDefault(\n+                /*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+      arrow::compute::ExecNode * k_sink_node,\n+      arrow::compute::MakeExecNode(\"select_k_sink\",\n+        plan.get(), {source},\n+        arrow::compute::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+Scan-Node\n+---------\n+\n+There is no definition Scan-Node in the source, but the behavior of is defined using \n+:class:`arrow::dataset::ScanNodeOptions`. This option contains a set of definitions. \n+\n+Option definitions for :class:`arrow::dataset::ScanNodeOptions`:: \n+\n+\n+    /// A row filter (which will be pushed down to partitioning/reading if supported).\n+    arrow::compute::Expression filter // \n+    /// A projection expression (which can add/remove/rename columns).\n+    arrow::compute::Expression projection; // \n+    /// Schema with which batches will be read from fragments. This is also known as the\n+    /// \"reader schema\" it will be used (for example) in constructing CSV file readers to\n+    /// identify column types for parsing. Usually only a subset of its fields (see\n+    /// MaterializedFields) will be materialized during a scan.\n+    std::shared_ptr<arrow::Schema> dataset_schema; \n+    /// Schema of projected record batches. This is independent of dataset_schema as its\n+    /// fields are derived from the projection. For example, let\n+    ///\n+    ///   dataset_schema = {\"a\": int32, \"b\": int32, \"id\": utf8}\n+    ///   projection = project({equal(field_ref(\"a\"), field_ref(\"b\"))}, {\"a_plus_b\"})\n+    ///\n+    /// (no filter specified). In this case, the projected_schema would be\n+    ///\n+    ///   {\"a_plus_b\": int32}\n+    std::shared_ptr<arrow::Schema> projected_schema;\n+\n+    /// Maximum row count for scanned batches.\n+    int64_t batch_size // 1024 * 1024;\n+\n+    /// How many batches to read ahead within a file\n+    ///\n+    /// Set to 0 to disable batch readahead\n+    ///\n+    /// Note: May not be supported by all formats\n+    /// Note: May not be supported by all scanners\n+    /// Note: Will be ignored if use_threads is set to false\n+    int32_t batch_readahead // 32;\n+\n+    /// How many files to read ahead\n+    ///\n+    /// Set to 0 to disable fragment readahead\n+    ///\n+    /// Note: May not be enforced by all scanners\n+    /// Note: Will be ignored if use_threads is set to false\n+    int32_t fragment_readahead // 8;\n+    /// If true the scanner will scan in parallel\n+    ///\n+    /// Note: If true, this will use threads from both the cpu_executor and the\n+    /// io_context.executor\n+    /// Note: This  must be true in order for any readahead to happen\n+    bool use_threads = false;\n+\n+    /// If true then an asycnhronous implementation of the scanner will be used.\n+    /// This implementation is newer and generally performs better.  However, it\n+    /// makes extensive use of threading and is still considered experimental\n+    bool use_async = false;\n+\n+    /// Fragment-specific scan options.\n+    // Some implemented FragementScanOptions are;\n+    // CsvFragmentScanOptions, IpcFragmentScanOptions, ParquetFragmentScanOptions\n+    std::shared_ptr<arrow::dataset::FragmentScanOptions> fragment_scan_options;\n+\n+\n+Creating a Scan `ExecNode`::\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    options->use_async = true; \n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, \n+                              scan_node_options));\n+\n+\n+Write-Node\n+----------\n+\n+The option to write a result to a file format is provided by this execution node type. \n+A definition doesn't exist as an :class:`ExecNode`, but the write options are provided\n+via the :class:`arrow::dataset::WriteNodeOptions` and defined using \n+:class::`arrow::dataset::FileSystemDatasetWriteOptions`, `std::shared_ptr<arrow::Schema>`,\n\nReview comment:\n       We need double backticks here.\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n+\n+    arrow::compute::Expression Materialize(std::vector<std::string> names,\n+                              bool include_aug_fields = false) {\n+        if (include_aug_fields) {\n+            for (auto aug_name : {\"__fragment_index\",\n+                \"__batch_index\", \"__last_in_fragment\"}) {\n+            names.emplace_back(aug_name);\n+            }\n+        }\n+\n+        std::vector<arrow::compute::Expression> exprs;\n+        for (const auto& name : names) {\n+            exprs.push_back(arrow::compute::field_ref(name));\n+        }\n+\n+        return arrow::compute::project(exprs, names);\n+    }\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{\n+                              /*dataset*/dataset, \n+                              /*scan_options*/options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            arrow::compute::MakeExecNode(\"scan\", \n+                              plan.get(), \n+                              {}, \n+                              scan_node_options));\n+\n+``SinkNode``\n+------------\n+\n+:class:`SinkNode` can be considered as the output or final node of an streaming \n+execution definition. :class:`arrow::compute::SinkNodeOptions` interface is used to pass \n+the required options. Requires \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>* generator`\n+and `arrow::util::BackpressureOptions backpressure`. \n+\n+Example::\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    arrow::compute::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, arrow::compute::MakeExecNode(\"sink\", plan.get(), {source},\n+                                                  arrow::compute::SinkNodeOptions{&sink_gen}));\n+\n+\n+The output can be obtained as a table::\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::cout << \"Exec Plan Created: \" << plan->ToString() << std::endl;\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+\n+``ConsumingSinkNode``\n+---------------------\n+\n+:class:`arrow::compute::ConsumingSinkNode` is a sink node that owns consuming the data and \n+will not finish until the consumption is finished.  Use SinkNode if you are\n+transferring the ownership of the data to another system.  \n+Use :class:`arrow::compute::ConsumingSinkNode` if the data is being consumed within the exec \n+plan (i.e. the exec plan should not complete until the consumption has completed).\n+\n+Example::\n+\n+    // define a Custom SinkNodeConsumer\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>finish): \n+        batches_seen(batches_seen), finish(std::move(finish)) {}\n+        // Consumption logic can be written here\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+        // data can be consumed in the expected way\n+        // transfer to another system or just do some work \n+        // and write to disk\n+        (*batches_seen)++;\n+        return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+        \n+    };\n+    \n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+            std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    arrow::compute::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+        {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+\n+``OrderBySinkNode``\n+-------------------\n+\n+This is an extension to the :class:`SinkNode` definition and provides the ability\n+to guarantee the ordering of the stream by providing the,\n+:class:`arrow::compute::OrderBySinkNodeOptions`. \n+Here the :class:`arrow::compute::SortOptions` are provided to define which columns \n+are used for sorting and under which criterion.\n\nReview comment:\n       ```suggestion\r\n   This is variant of :class:`SinkNode` that sorts the results according to\r\n   :class:`arrow::compute::OrderBySinkNodeOptions`. \r\n   Here :class:`arrow::compute::SortOptions` are provided to define which columns \r\n   are used for sorting and whether to sort by ascending or descending values.\r\n   ```\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n+\n+    arrow::compute::Expression Materialize(std::vector<std::string> names,\n+                              bool include_aug_fields = false) {\n+        if (include_aug_fields) {\n+            for (auto aug_name : {\"__fragment_index\",\n+                \"__batch_index\", \"__last_in_fragment\"}) {\n+            names.emplace_back(aug_name);\n+            }\n+        }\n+\n+        std::vector<arrow::compute::Expression> exprs;\n+        for (const auto& name : names) {\n+            exprs.push_back(arrow::compute::field_ref(name));\n+        }\n+\n+        return arrow::compute::project(exprs, names);\n+    }\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{\n+                              /*dataset*/dataset, \n+                              /*scan_options*/options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            arrow::compute::MakeExecNode(\"scan\", \n+                              plan.get(), \n+                              {}, \n+                              scan_node_options));\n+\n+``SinkNode``\n+------------\n+\n+:class:`SinkNode` can be considered as the output or final node of an streaming \n+execution definition. :class:`arrow::compute::SinkNodeOptions` interface is used to pass \n+the required options. Requires \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>* generator`\n+and `arrow::util::BackpressureOptions backpressure`. \n+\n+Example::\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    arrow::compute::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, arrow::compute::MakeExecNode(\"sink\", plan.get(), {source},\n+                                                  arrow::compute::SinkNodeOptions{&sink_gen}));\n+\n+\n+The output can be obtained as a table::\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::cout << \"Exec Plan Created: \" << plan->ToString() << std::endl;\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+\n+``ConsumingSinkNode``\n+---------------------\n+\n+:class:`arrow::compute::ConsumingSinkNode` is a sink node that owns consuming the data and \n+will not finish until the consumption is finished.  Use SinkNode if you are\n+transferring the ownership of the data to another system.  \n+Use :class:`arrow::compute::ConsumingSinkNode` if the data is being consumed within the exec \n+plan (i.e. the exec plan should not complete until the consumption has completed).\n+\n+Example::\n+\n+    // define a Custom SinkNodeConsumer\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>finish): \n+        batches_seen(batches_seen), finish(std::move(finish)) {}\n+        // Consumption logic can be written here\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+        // data can be consumed in the expected way\n+        // transfer to another system or just do some work \n+        // and write to disk\n+        (*batches_seen)++;\n+        return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+        \n+    };\n+    \n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+            std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    arrow::compute::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+        {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+\n+``OrderBySinkNode``\n+-------------------\n+\n+This is an extension to the :class:`SinkNode` definition and provides the ability\n+to guarantee the ordering of the stream by providing the,\n+:class:`arrow::compute::OrderBySinkNodeOptions`. \n+Here the :class:`arrow::compute::SortOptions` are provided to define which columns \n+are used for sorting and under which criterion.\n+\n+Example::\n+\n+    arrow::compute::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    arrow::compute::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, \n+    arrow::compute::OrderBySinkNodeOptions{\n+    /*sort_options*/arrow::compute::SortOptions{\n+    {\tarrow::compute::SortKey{\n+    //Column key(s) to order by and how to order by these sort keys.\n+    \"a\",\n+    // Sort Order\n+    arrow::compute::SortOrder::Descending \n+    }}},&sink_gen}));\n+\n+\n+SelectK-Node\n+------------\n+\n+There is no Select-K-SinkNode available as an entity within the source, but the behavior \n+is defined with the options :class:`arrow::compute::SelectKOptions` which is a defined by \n+using :struct:`OrderBySinkNode` definition. This option returns a sink node that receives \n+inputs and then compute top_k/bottom_k.\n+\n+Example::\n+\n+    arrow::compute::SelectKOptions options = arrow::compute::SelectKOptions::TopKDefault(\n+                /*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+      arrow::compute::ExecNode * k_sink_node,\n+      arrow::compute::MakeExecNode(\"select_k_sink\",\n+        plan.get(), {source},\n+        arrow::compute::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+Scan-Node\n+---------\n+\n+There is no definition Scan-Node in the source, but the behavior of is defined using \n+:class:`arrow::dataset::ScanNodeOptions`. This option contains a set of definitions. \n+\n+Option definitions for :class:`arrow::dataset::ScanNodeOptions`:: \n+\n+\n+    /// A row filter (which will be pushed down to partitioning/reading if supported).\n+    arrow::compute::Expression filter // \n+    /// A projection expression (which can add/remove/rename columns).\n+    arrow::compute::Expression projection; // \n+    /// Schema with which batches will be read from fragments. This is also known as the\n+    /// \"reader schema\" it will be used (for example) in constructing CSV file readers to\n+    /// identify column types for parsing. Usually only a subset of its fields (see\n+    /// MaterializedFields) will be materialized during a scan.\n+    std::shared_ptr<arrow::Schema> dataset_schema; \n+    /// Schema of projected record batches. This is independent of dataset_schema as its\n+    /// fields are derived from the projection. For example, let\n+    ///\n+    ///   dataset_schema = {\"a\": int32, \"b\": int32, \"id\": utf8}\n+    ///   projection = project({equal(field_ref(\"a\"), field_ref(\"b\"))}, {\"a_plus_b\"})\n+    ///\n+    /// (no filter specified). In this case, the projected_schema would be\n+    ///\n+    ///   {\"a_plus_b\": int32}\n+    std::shared_ptr<arrow::Schema> projected_schema;\n+\n+    /// Maximum row count for scanned batches.\n+    int64_t batch_size // 1024 * 1024;\n+\n+    /// How many batches to read ahead within a file\n+    ///\n+    /// Set to 0 to disable batch readahead\n+    ///\n+    /// Note: May not be supported by all formats\n+    /// Note: May not be supported by all scanners\n+    /// Note: Will be ignored if use_threads is set to false\n+    int32_t batch_readahead // 32;\n+\n+    /// How many files to read ahead\n+    ///\n+    /// Set to 0 to disable fragment readahead\n+    ///\n+    /// Note: May not be enforced by all scanners\n+    /// Note: Will be ignored if use_threads is set to false\n+    int32_t fragment_readahead // 8;\n+    /// If true the scanner will scan in parallel\n+    ///\n+    /// Note: If true, this will use threads from both the cpu_executor and the\n+    /// io_context.executor\n+    /// Note: This  must be true in order for any readahead to happen\n+    bool use_threads = false;\n+\n+    /// If true then an asycnhronous implementation of the scanner will be used.\n+    /// This implementation is newer and generally performs better.  However, it\n+    /// makes extensive use of threading and is still considered experimental\n+    bool use_async = false;\n+\n+    /// Fragment-specific scan options.\n+    // Some implemented FragementScanOptions are;\n+    // CsvFragmentScanOptions, IpcFragmentScanOptions, ParquetFragmentScanOptions\n+    std::shared_ptr<arrow::dataset::FragmentScanOptions> fragment_scan_options;\n+\n+\n+Creating a Scan `ExecNode`::\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    options->use_async = true; \n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, \n+                              scan_node_options));\n+\n+\n+Write-Node\n+----------\n+\n+The option to write a result to a file format is provided by this execution node type. \n+A definition doesn't exist as an :class:`ExecNode`, but the write options are provided\n+via the :class:`arrow::dataset::WriteNodeOptions` and defined using \n+:class::`arrow::dataset::FileSystemDatasetWriteOptions`, `std::shared_ptr<arrow::Schema>`,\n+and `std::shared_ptr<arrow::util::AsyncToggle> backpressure_toggle`. Here the \n+:class::`arrow::dataset::FileSystemDatasetWriteOptions` contains the meta-data required \n+to write the data. \n+\n+Creating `WriteNodeOptions`::\n+\n+    std::string root_path = \"\";\n+    std::string uri = \"file://\" + '/path/to/file';\n+    std::shared_ptr<arrow::fs::FileSystem> filesystem =\n+    arrow::fs::FileSystemFromUri(uri, &root_path).ValueOrDie();\n+\n+    auto base_path = root_path + \"/parquet_dataset\";\n+    ABORT_ON_FAILURE(filesystem->DeleteDir(base_path));\n+    ABORT_ON_FAILURE(filesystem->CreateDir(base_path));\n+\n+    // The partition schema determines which fields are part of the partitioning.\n+    auto partition_schema = arrow::schema({arrow::field(\"a\", arrow::int32())});\n+    // We'll use Hive-style partitioning,\n+    // which creates directories with \"key=value\" pairs.\n+\n+    auto partitioning =\n+    std::make_shared<arrow::dataset::HivePartitioning>(partition_schema);\n+    // We'll write Parquet files.\n+    auto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n+\n+    arrow::dataset::FileSystemDatasetWriteOptions write_options;\n+    write_options.file_write_options = format->DefaultWriteOptions();\n+    write_options.filesystem = filesystem;\n+    write_options.base_dir = base_path;\n+    write_options.partitioning = partitioning;\n+    write_options.basename_template = \"part{i}.parquet\";\n+\n+    arrow::dataset::WriteNodeOptions write_node_options {write_options,\n+    dataset->schema()};\n+\n+Creating a `write` `ExecNode`::\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode *wr, cp::MakeExecNode(\"write\", plan.get(),\n+        {scan}, write_node_options));\n+\n+    ABORT_ON_FAILURE(wr->Validate());\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+    plan->finished().Wait(); // make sure to add this method \n+\n+``UnionNode``\n+-------------\n+\n+:class:`UnionNode` is the :class:`ExecNode` interface to perform a union \n+operation on two datasets. The union operation can be executed\n\nReview comment:\n       This can be confusing given the various uses of union in SQL. Specifically, UnionNode combines two streams of data with the same schema into one, and does not modify or look at any of the data itself.\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n\nReview comment:\n       Do we really need to use macros for printing? Why not a function?\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n+But :class:`arrow::compute::ScanNodeOptions` container includes the options\n+passed to `MakeScanNode` internal function which creates an :class:`ExecNode`\n+performing the defined task. This component includes a few options,\n+defined in the :class:`arrow::compute::ScanNodeOptions` and this requires, \n+`std::shared_ptr<arrow::dataset::Dataset>`, \n+`std::shared_ptr<arrow::compute::ScanOptions>`, \n+`std::shared_ptr<arrow::util::AsyncToggle>`. \n+\n+The :class:`arrow::compute::ScanOptions` includes the scaning options::\n+\n+    arrow::compute::Expression Materialize(std::vector<std::string> names,\n+                              bool include_aug_fields = false) {\n+        if (include_aug_fields) {\n+            for (auto aug_name : {\"__fragment_index\",\n+                \"__batch_index\", \"__last_in_fragment\"}) {\n+            names.emplace_back(aug_name);\n+            }\n+        }\n+\n+        std::vector<arrow::compute::Expression> exprs;\n+        for (const auto& name : names) {\n+            exprs.push_back(arrow::compute::field_ref(name));\n+        }\n+\n+        return arrow::compute::project(exprs, names);\n+    }\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{\n+                              /*dataset*/dataset, \n+                              /*scan_options*/options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            arrow::compute::MakeExecNode(\"scan\", \n+                              plan.get(), \n+                              {}, \n+                              scan_node_options));\n+\n+``SinkNode``\n+------------\n+\n+:class:`SinkNode` can be considered as the output or final node of an streaming \n+execution definition. :class:`arrow::compute::SinkNodeOptions` interface is used to pass \n+the required options. Requires \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>* generator`\n+and `arrow::util::BackpressureOptions backpressure`. \n+\n+Example::\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    arrow::compute::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, arrow::compute::MakeExecNode(\"sink\", plan.get(), {source},\n+                                                  arrow::compute::SinkNodeOptions{&sink_gen}));\n+\n+\n+The output can be obtained as a table::\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::cout << \"Exec Plan Created: \" << plan->ToString() << std::endl;\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+\n+``ConsumingSinkNode``\n+---------------------\n+\n+:class:`arrow::compute::ConsumingSinkNode` is a sink node that owns consuming the data and \n+will not finish until the consumption is finished.  Use SinkNode if you are\n+transferring the ownership of the data to another system.  \n+Use :class:`arrow::compute::ConsumingSinkNode` if the data is being consumed within the exec \n+plan (i.e. the exec plan should not complete until the consumption has completed).\n+\n+Example::\n+\n+    // define a Custom SinkNodeConsumer\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>finish): \n+        batches_seen(batches_seen), finish(std::move(finish)) {}\n+        // Consumption logic can be written here\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+        // data can be consumed in the expected way\n+        // transfer to another system or just do some work \n+        // and write to disk\n+        (*batches_seen)++;\n+        return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+        \n+    };\n+    \n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+            std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    arrow::compute::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+        {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+\n+``OrderBySinkNode``\n+-------------------\n+\n+This is an extension to the :class:`SinkNode` definition and provides the ability\n+to guarantee the ordering of the stream by providing the,\n+:class:`arrow::compute::OrderBySinkNodeOptions`. \n+Here the :class:`arrow::compute::SortOptions` are provided to define which columns \n+are used for sorting and under which criterion.\n+\n+Example::\n+\n+    arrow::compute::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    arrow::compute::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, \n+    arrow::compute::OrderBySinkNodeOptions{\n+    /*sort_options*/arrow::compute::SortOptions{\n+    {\tarrow::compute::SortKey{\n+    //Column key(s) to order by and how to order by these sort keys.\n+    \"a\",\n+    // Sort Order\n+    arrow::compute::SortOrder::Descending \n+    }}},&sink_gen}));\n+\n+\n+SelectK-Node\n+------------\n+\n+There is no Select-K-SinkNode available as an entity within the source, but the behavior \n+is defined with the options :class:`arrow::compute::SelectKOptions` which is a defined by \n+using :struct:`OrderBySinkNode` definition. This option returns a sink node that receives \n+inputs and then compute top_k/bottom_k.\n+\n+Example::\n+\n+    arrow::compute::SelectKOptions options = arrow::compute::SelectKOptions::TopKDefault(\n+                /*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+      arrow::compute::ExecNode * k_sink_node,\n+      arrow::compute::MakeExecNode(\"select_k_sink\",\n+        plan.get(), {source},\n+        arrow::compute::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+Scan-Node\n+---------\n+\n+There is no definition Scan-Node in the source, but the behavior of is defined using \n+:class:`arrow::dataset::ScanNodeOptions`. This option contains a set of definitions. \n+\n+Option definitions for :class:`arrow::dataset::ScanNodeOptions`:: \n+\n+\n+    /// A row filter (which will be pushed down to partitioning/reading if supported).\n+    arrow::compute::Expression filter // \n+    /// A projection expression (which can add/remove/rename columns).\n+    arrow::compute::Expression projection; // \n+    /// Schema with which batches will be read from fragments. This is also known as the\n+    /// \"reader schema\" it will be used (for example) in constructing CSV file readers to\n+    /// identify column types for parsing. Usually only a subset of its fields (see\n+    /// MaterializedFields) will be materialized during a scan.\n+    std::shared_ptr<arrow::Schema> dataset_schema; \n+    /// Schema of projected record batches. This is independent of dataset_schema as its\n+    /// fields are derived from the projection. For example, let\n+    ///\n+    ///   dataset_schema = {\"a\": int32, \"b\": int32, \"id\": utf8}\n+    ///   projection = project({equal(field_ref(\"a\"), field_ref(\"b\"))}, {\"a_plus_b\"})\n+    ///\n+    /// (no filter specified). In this case, the projected_schema would be\n+    ///\n+    ///   {\"a_plus_b\": int32}\n+    std::shared_ptr<arrow::Schema> projected_schema;\n+\n+    /// Maximum row count for scanned batches.\n+    int64_t batch_size // 1024 * 1024;\n+\n+    /// How many batches to read ahead within a file\n+    ///\n+    /// Set to 0 to disable batch readahead\n+    ///\n+    /// Note: May not be supported by all formats\n+    /// Note: May not be supported by all scanners\n+    /// Note: Will be ignored if use_threads is set to false\n+    int32_t batch_readahead // 32;\n+\n+    /// How many files to read ahead\n+    ///\n+    /// Set to 0 to disable fragment readahead\n+    ///\n+    /// Note: May not be enforced by all scanners\n+    /// Note: Will be ignored if use_threads is set to false\n+    int32_t fragment_readahead // 8;\n+    /// If true the scanner will scan in parallel\n+    ///\n+    /// Note: If true, this will use threads from both the cpu_executor and the\n+    /// io_context.executor\n+    /// Note: This  must be true in order for any readahead to happen\n+    bool use_threads = false;\n+\n+    /// If true then an asycnhronous implementation of the scanner will be used.\n+    /// This implementation is newer and generally performs better.  However, it\n+    /// makes extensive use of threading and is still considered experimental\n+    bool use_async = false;\n+\n+    /// Fragment-specific scan options.\n+    // Some implemented FragementScanOptions are;\n+    // CsvFragmentScanOptions, IpcFragmentScanOptions, ParquetFragmentScanOptions\n+    std::shared_ptr<arrow::dataset::FragmentScanOptions> fragment_scan_options;\n+\n+\n+Creating a Scan `ExecNode`::\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    options->use_async = true; \n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, \n+                              scan_node_options));\n+\n+\n+Write-Node\n+----------\n+\n+The option to write a result to a file format is provided by this execution node type. \n\nReview comment:\n       > The \"write\" node writes the results of a query into files on disk. Options for file format (Feather, CSV, Parquet, etc.), partitioning scheme, and so on are provided.\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n+    return im_dataset;\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n\nReview comment:\n       In general, if we're calling anything labeled `internal` in an example, then something else needs fixing, I think.\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n+    return im_dataset;\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n\nReview comment:\n       For each sample, can we have a brief doc comment explaining what the purpose of the sample is?\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n\nReview comment:\n       Hmm, we really shouldn't use the internal JSON parser.\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n+    return im_dataset;\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n\nReview comment:\n       For consistency, use UpperCamelCase naming. \n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n+    return im_dataset;\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n\nReview comment:\n       We shouldn't need to call this, right?\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n+    return im_dataset;\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status ScanSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+// std::shared_ptr<arrow::dataset::InMemoryDataset> GetBatchDataset() {\n+  \n+//   std::shared_ptr<arrow::dataset::Dataset> dataset = NULLPTR;\n+  \n+\n+//   return dataset;\n+// }\n+\n+\n+arrow::Status SourceSinkExample() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                               ::arrow::internal::GetCpuThreadPool());\n+\n+  // ensure arrow::dataset node factories are in the registry\n+  arrow::dataset::internal::Initialize();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+  cp::ExecPlan::Make(&exec_context));\n+\n+  auto basic_data = MakeBasicBatches();\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+  auto source_node_options =\n+      cp::SourceNodeOptions{basic_data.schema, basic_data.gen(false)};\n+\n+  ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source,\n+                        cp::MakeExecNode(\"source\", plan.get(), {}, source_node_options));\n+\n+  cp::ExecNode* sink;\n+\n+  ARROW_ASSIGN_OR_RAISE(sink, cp::MakeExecNode(\"sink\", plan.get(), {source},\n+                                               cp::SinkNodeOptions{&sink_gen}));\n+\n+  // // // translate sink_gen (async) to sink_reader (sync)\n+  std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+  // // validate the ExecPlan\n+  ABORT_ON_FAILURE(plan->Validate());\n+  PRINT_LINE(\"Exec Plan Created: \" << plan->ToString());\n+  // // // start the ExecPlan\n+  ABORT_ON_FAILURE(plan->StartProducing());\n+\n+  // // collect sink_reader into a Table\n+  std::shared_ptr<arrow::Table> response_table;\n+\n+  ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+  PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+  // // plan stop producing\n+  plan->StopProducing();\n+  // // plan mark finished\n+  plan->finished().Wait();\n+\n+  return arrow::Status::OK();\n+}\n+\n+arrow::Status ScanFilterSinkExample() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                               ::arrow::internal::GetCpuThreadPool());\n+\n+  // ensure arrow::dataset node factories are in the registry\n+  arrow::dataset::internal::Initialize();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                        cp::ExecPlan::Make(&exec_context));\n+\n+  std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  // sync scanning is not supported by ScanNode\n+  options->use_async = true;\n+  // specify the filter\n+  cp::Expression filter_opt = cp::greater(cp::field_ref(\"a\"), cp::literal(3));\n+  \n+  options->filter = filter_opt;\n+  // empty projection\n+  options->projection = Materialize({});\n+\n+  // construct the scan node\n+  PRINT_LINE(\"Initialized Scanning Options\");\n+\n+  cp::ExecNode* scan;\n+\n+  auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+  PRINT_LINE(\"Scan node options created\");\n+\n+  ARROW_ASSIGN_OR_RAISE(scan,\n+                        cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+  // pipe the scan node into a filter node\n+  cp::ExecNode* filter;\n+  ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                 cp::FilterNodeOptions{filter_opt}));\n+\n+  // // finally, pipe the project node into a sink node\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+  ARROW_ASSIGN_OR_RAISE(\n+      cp::ExecNode * sink,\n+      cp::MakeExecNode(\"sink\", plan.get(), {filter}, cp::SinkNodeOptions{&sink_gen}));\n+\n+  ABORT_ON_FAILURE(sink->Validate());\n+  // // translate sink_gen (async) to sink_reader (sync)\n+  std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+  // // validate the ExecPlan\n+  ABORT_ON_FAILURE(plan->Validate());\n+  PRINT_LINE(\"Exec Plan created \" << plan->ToString());\n+  // // start the ExecPlan\n+  ABORT_ON_FAILURE(plan->StartProducing());\n+\n+  // // collect sink_reader into a Table\n+  std::shared_ptr<arrow::Table> response_table;\n+  ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+  PRINT_LINE(\"Results : \" << response_table->ToString());\n+  // // plan stop producing\n+  plan->StopProducing();\n+  // /// plan marked finished\n+  plan->finished().Wait();\n+\n+  return arrow::Status::OK();\n+}\n+\n+arrow::Status ScanProjectSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+    cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // projection\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"),\n+    cp::literal(2)}); \n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    cp::ExecNode *scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan, cp::MakeExecNode(\"scan\", plan.get(), {},\n+    scan_node_options));\n+\n+    cp::ExecNode *project;\n+    ARROW_ASSIGN_OR_RAISE(project, cp::MakeExecNode(\"project\", plan.get(), {scan},\n+                                                    cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                          cp::MakeExecNode(\"sink\", plan.get(), {project},\n+                                           cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+        std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+\n+    PRINT_LINE(\"Exec Plan Created : \" << plan->ToString());\n+\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop producing\n+    plan->StopProducing();\n+    // // plan marked finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceAggregateSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeBasicBatches();\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source, cp::MakeExecNode(\"source\",\n+                                                                  plan.get(), {},\n+                                                                  source_node_options));\n+    cp::CountOptions options(cp::CountOptions::ONLY_VALID);\n+    auto aggregate_options = cp::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                          cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                          aggregate_options));\n+\n+    cp::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, cp::MakeExecNode(\"sink\", plan.get(), {aggregate},\n+                                                 cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({\n+            arrow::field(\"count(a)\", arrow::int32()),\n+            arrow::field(\"b\", arrow::boolean()),\n+        }),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    //plan stop producing\n+    plan->StopProducing();\n+    // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceConsumingSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeBasicBatches();\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source, cp::MakeExecNode(\"source\",\n+                                                                  plan.get(), {},\n+                                                                  source_node_options));\n+\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>\n+        finish)\n+            : batches_seen(batches_seen), finish(std::move(finish)) {}\n+\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+            (*batches_seen)++;\n+            return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+    };\n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+        std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    cp::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+    {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+    ABORT_ON_FAILURE(consuming_sink->Validate());\n+\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // plan start producing\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+    // Source should finish fairly quickly\n+    ABORT_ON_FAILURE(source->finished().status());\n+    PRINT_LINE(\"Source Finished!\");\n+    // Mark consumption complete, plan should finish\n+    arrow::Status finish_status;\n+    //finish.Wait();\n+    finish.MarkFinished(finish_status);\n+    ABORT_ON_FAILURE(plan->finished().status());\n+    ABORT_ON_FAILURE(finish_status);\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceOrderBySinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+     ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeSortTestBasicBatches();\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source,\n+    cp::MakeExecNode(\"source\", plan.get(), {}, source_node_options));\n+\n+    cp::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    cp::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, cp::OrderBySinkNodeOptions{\n+        cp::SortOptions{{cp::SortKey{\"a\",\n+        cp::SortOrder::Descending}}}, &sink_gen}));\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        basic_data.schema,\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceHashJoinSinkExample() {\n+    auto input = MakeGroupableBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode *left_source;\n+    cp::ExecNode *right_source;\n+    for (auto source : {&left_source, &right_source}) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            *source,\n+            MakeExecNode(\"source\", plan.get(), {},\n+                                  cp::SourceNodeOptions{\n+                                      input.schema,\n+                                      input.gen(/*parallel=*/true)}));\n+    }\n+    // TODO: decide whether to keep the filters or remove\n+\n+    // ARROW_ASSIGN_OR_RAISE(\n+    //     auto left_filter,\n+    //     cp::MakeExecNode(\"filter\", plan.get(), {left_source},\n+    //                      cp::FilterNodeOptions{\n+    //                          cp::greater_equal(\n+    //                              cp::field_ref(\"i32\"),\n+    //                              cp::literal(-1))}));\n+    // ARROW_ASSIGN_OR_RAISE(\n+    //     auto right_filter,\n+    //     cp::MakeExecNode(\"filter\", plan.get(), {right_source},\n+    //                      cp::FilterNodeOptions{\n+    //                          cp::less_equal(\n+    //                              cp::field_ref(\"i32\"),\n+    //                              cp::literal(2))}));\n+    // PRINT_LINE(\"left and right filter nodes created\");\n+\n+    cp::HashJoinNodeOptions join_opts{cp::JoinType::INNER,\n+                                      /*left_keys=*/{\"str\"},\n+                                      /*right_keys=*/{\"str\"}, cp::literal(true), \"l_\",\n+                                      \"r_\"};\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        auto hashjoin,\n+        cp::MakeExecNode(\"hashjoin\", plan.get(), {left_source, right_source},\n+        join_opts));\n+\n+    ARROW_ASSIGN_OR_RAISE(std::ignore, cp::MakeExecNode(\"sink\", plan.get(), {hashjoin},\n+                                                        cp::SinkNodeOptions{&sink_gen}));\n+    // expected columns i32, str, l_str, r_str\n+\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"i32\", arrow::int32()),\n+                       arrow::field(\"str\", arrow::utf8()),\n+                       arrow::field(\"l_str\", arrow::utf8()),\n+                       arrow::field(\"r_str\", arrow::utf8())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceKSelectExample() {\n+    auto input = MakeGroupableBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        cp::ExecNode * source,\n+        cp::MakeExecNode(\"source\",\n+            plan.get(), {},\n+                cp::SourceNodeOptions{\n+                    input.schema,\n+                    input.gen(/*parallel=*/true)}));\n+\n+    cp::SelectKOptions options = cp::SelectKOptions::TopKDefault(/*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        cp::ExecNode * k_sink_node,\n+        cp::MakeExecNode(\"select_k_sink\",\n+            plan.get(), {source},\n+                cp::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+    k_sink_node->finished().Wait();\n+\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"i32\", arrow::int32()),\n+                       arrow::field(\"str\", arrow::utf8())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop proudcing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status ScanFilterWriteExample(std::string file_path) {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    //cp::Expression b_is_true = cp::field_ref(\"b\");\n+    //options->filter = b_is_true;\n+    // empty projection\n+    options->projection = Materialize({});\n+\n+    cp::ExecNode *scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan, cp::MakeExecNode(\"scan\", plan.get(), {},\n+    scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    std::string root_path = \"\";\n+    std::string uri = \"file://\" + file_path;\n+    std::shared_ptr<arrow::fs::FileSystem> filesystem =\n+    arrow::fs::FileSystemFromUri(uri, &root_path).ValueOrDie();\n+\n+    auto base_path = root_path + \"/parquet_dataset\";\n+    ABORT_ON_FAILURE(filesystem->DeleteDir(base_path));\n+    ABORT_ON_FAILURE(filesystem->CreateDir(base_path));\n+\n+    // The partition schema determines which fields are part of the partitioning.\n+    auto partition_schema = arrow::schema({arrow::field(\"a\", arrow::int32())});\n+    // We'll use Hive-style partitioning,\n+    // which creates directories with \"key=value\" pairs.\n+\n+    auto partitioning =\n+    std::make_shared<arrow::dataset::HivePartitioning>(partition_schema);\n+    // We'll write Parquet files.\n+    auto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n+\n+    arrow::dataset::FileSystemDatasetWriteOptions write_options;\n+    write_options.file_write_options = format->DefaultWriteOptions();\n+    write_options.filesystem = filesystem;\n+    write_options.base_dir = base_path;\n+    write_options.partitioning = partitioning;\n+    write_options.basename_template = \"part{i}.parquet\";\n+\n+    arrow::dataset::WriteNodeOptions write_node_options {write_options,\n+    dataset->schema()};\n+\n+    PRINT_LINE(\"Write Options created\");\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode *wr, cp::MakeExecNode(\"write\", plan.get(),\n+    {scan}, write_node_options));\n+\n+    ABORT_ON_FAILURE(wr->Validate());\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Execution Plan Created : \" << plan->ToString());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+    plan->finished().Wait();\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceUnionSinkExample() {\n+    auto basic_data = MakeBasicBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    std::shared_ptr<cp::ExecPlan> plan =\n+    cp::ExecPlan::Make(&exec_context).ValueOrDie();\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::Declaration union_node{\"union\", cp::ExecNodeOptions{}};\n+    cp::Declaration lhs{\"source\",\n+                  cp::SourceNodeOptions{basic_data.schema,\n+                                    basic_data.gen(/*parallel=*/false)}};\n+    lhs.label = \"lhs\";\n+    cp::Declaration rhs{\"source\",\n+                    cp::SourceNodeOptions{basic_data.schema,\n+                                      basic_data.gen(/*parallel=*/false)}};\n+    rhs.label = \"rhs\";\n+    union_node.inputs.emplace_back(lhs);\n+    union_node.inputs.emplace_back(rhs);\n+\n+    cp::CountOptions options(cp::CountOptions::ONLY_VALID);\n+    ARROW_ASSIGN_OR_RAISE(auto declr,\n+    cp::Declaration::Sequence(\n+            {\n+                union_node,\n+                {\"aggregate\", cp::AggregateNodeOptions{\n+                  /*aggregates=*/{{\"count\", &options}},\n+                  /*targets=*/{\"a\"},\n+                  /*names=*/{\"count(a)\"},\n+                  /*keys=*/{}}},\n+                {\"sink\", cp::SinkNodeOptions{&sink_gen}},\n+            })\n+            .AddToPlan(plan.get()));\n+\n+    ABORT_ON_FAILURE(declr->Validate());\n+\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"count(a)\", arrow::int32())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+    return arrow::Status::OK();\n+}\n+\n+enum ExampleMode {\n+  SOURCE_SINK = 0,\n+  SCAN_SINK = 1,\n+  SCAN_FILTER_SINK = 2,\n+  SCAN_PROJECT_SINK = 3,\n+  SOURCE_AGGREGATE_SINK = 4,\n+  SCAN_CONSUMING_SINK = 5,\n+  SCAN_ORDER_BY_SINK = 6,\n+  SCAN_HASHJOIN_SINK = 7,\n+  SCAN_SELECT_SINK = 8,\n+  SCAN_FILTER_WRITE = 9,\n+  SOURCE_UNION_SINK = 10,\n+};\n+\n+int main(int argc, char** argv) {\n+  if (argc < 2) {\n+    // Fake success for CI purposes.\n+    return EXIT_SUCCESS;\n+  }\n+\n+  int mode = std::atoi(argv[1]);\n+  std::string base_save_path = argv[2];\n+\n+  switch (mode) {\n+    case SOURCE_SINK:\n+      PRINT_BLOCK(\"Source Sink Example\");\n+      CHECK_AND_CONTINUE(SourceSinkExample())\n\nReview comment:\n       We can mostly avoid defining new macros if we instead refactor this into a function that returns Status, and then just check the status once in `main`.\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n+    return im_dataset;\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::Expression Materialize(std::vector<std::string> names,\n+                           bool include_aug_fields = false) {\n+    if (include_aug_fields) {\n+        for (auto aug_name : {\"__fragment_index\",\n+            \"__batch_index\", \"__last_in_fragment\"}) {\n+        names.emplace_back(aug_name);\n+        }\n+    }\n+\n+    std::vector<cp::Expression> exprs;\n+    for (const auto& name : names) {\n+        exprs.push_back(cp::field_ref(name));\n+    }\n+\n+    return cp::project(exprs, names);\n+}\n+\n+arrow::Status consume(std::shared_ptr<arrow::Schema> schema,\n+    std::function<arrow::Future<arrow::util::optional<cp::ExecBatch>>()>* sink_gen) {\n+    auto iterator = MakeGeneratorIterator(*sink_gen);\n+    while (true) {\n+        ARROW_ASSIGN_OR_RAISE(auto exec_batch, iterator.Next());\n+        if (!exec_batch.has_value()) {\n+            break;\n+        }\n+        ARROW_ASSIGN_OR_RAISE(auto record_batch, exec_batch->ToRecordBatch(schema));\n+        std::cout << record_batch->ToString() << '\\n';\n+    }\n+    return arrow::Status::OK();\n+}\n+\n+\n+arrow::Status ScanSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    // Execution plan created\n+    ARROW_ASSIGN_OR_RAISE(\n+        std::shared_ptr<cp::ExecPlan> plan, cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    options->projection = Materialize({});  // create empty projection\n+\n+    // construct the scan node\n+    cp::ExecNode* scan;\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode* sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        sink, cp::MakeExecNode(\"sink\", plan.get(), {scan},\n+        cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+    // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+cp::ExecBatch GetExecBatchFromJSON(const std::vector<arrow::ValueDescr>& descrs,\n+                                   arrow::util::string_view json) {\n+  auto fields = ::arrow::internal::MapVector(\n+      [](const arrow::ValueDescr& descr) { return arrow::field(\"\", descr.type); },\n+      descrs);\n+\n+  cp::ExecBatch batch{*GetRecordBatchFromJSON(arrow::schema(std::move(fields)), json)};\n+\n+  auto value_it = batch.values.begin();\n+  for (const auto& descr : descrs) {\n+    if (descr.shape == arrow::ValueDescr::SCALAR) {\n+      if (batch.length == 0) {\n+        *value_it = arrow::MakeNullScalar(value_it->type());\n+      } else {\n+        *value_it = value_it->make_array()->GetScalar(0).ValueOrDie();\n+      }\n+    }\n+    ++value_it;\n+  }\n+\n+  return batch;\n+}\n+\n+struct BatchesWithSchema {\n+  std::vector<cp::ExecBatch> batches;\n+  std::shared_ptr<arrow::Schema> schema;\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen(bool parallel) const {\n+    auto opt_batches = ::arrow::internal::MapVector(\n+        [](cp::ExecBatch batch) { return arrow::util::make_optional(std::move(batch)); },\n+        batches);\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen;\n+\n+    if (parallel) {\n+      // emulate batches completing initial decode-after-scan on a cpu thread\n+      gen = arrow::MakeBackgroundGenerator(\n+                arrow::MakeVectorIterator(std::move(opt_batches)),\n+                ::arrow::internal::GetCpuThreadPool())\n+                .ValueOrDie();\n+\n+      // ensure that callbacks are not executed immediately on a background thread\n+      gen = arrow::MakeTransferredGenerator(std::move(gen),\n+                                            ::arrow::internal::GetCpuThreadPool());\n+    } else {\n+      gen = arrow::MakeVectorGenerator(std::move(opt_batches));\n+    }\n+\n+    return gen;\n+  }\n+};\n+\n+\n+\n+BatchesWithSchema MakeBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[null, true], [4, false]]\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::boolean()},\n+                                      \"[[5, null], [6, false], [7, false]]\")};\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::boolean())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeSortTestBasicBatches() {\n+  BatchesWithSchema out;\n+  out.batches = {\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[1, 3, 0, 2], [121, 101, 120, 12], [10, 110, 210, 121], [51, 101, 2, 34]]\"),\n+      GetExecBatchFromJSON(\n+          {arrow::int32(), arrow::int32(), arrow::int32(), arrow::int32()},\n+          \"[[11, 31, 1, 12], [12, 101, 120, 12], [0, 110, 210, 11], [51, 10, 2, 3]]\")\n+  };\n+  out.schema = arrow::schema(\n+      {arrow::field(\"a\", arrow::int32()), arrow::field(\"b\", arrow::int32()),\n+       arrow::field(\"c\", arrow::int32()), arrow::field(\"d\", arrow::int32())});\n+  return out;\n+}\n+\n+BatchesWithSchema MakeGroupableBatches(int multiplicity = 1) {\n+  BatchesWithSchema out;\n+\n+  out.batches = {GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [12, \"alfa\"],\n+                   [7,  \"beta\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [-2, \"alfa\"],\n+                   [-1, \"gama\"],\n+                   [3,  \"alfa\"]\n+                 ])\"),\n+                 GetExecBatchFromJSON({arrow::int32(), arrow::utf8()}, R\"([\n+                   [5,  \"gama\"],\n+                   [3,  \"beta\"],\n+                   [-8, \"alfa\"]\n+                 ])\")};\n+\n+  size_t batch_count = out.batches.size();\n+  for (int repeat = 1; repeat < multiplicity; ++repeat) {\n+    for (size_t i = 0; i < batch_count; ++i) {\n+      out.batches.push_back(out.batches[i]);\n+    }\n+  }\n+\n+  out.schema = arrow::schema(\n+      {arrow::field(\"i32\", arrow::int32()), arrow::field(\"str\", arrow::utf8())});\n+\n+  return out;\n+}\n+\n+// std::shared_ptr<arrow::dataset::InMemoryDataset> GetBatchDataset() {\n+  \n+//   std::shared_ptr<arrow::dataset::Dataset> dataset = NULLPTR;\n+  \n+\n+//   return dataset;\n+// }\n+\n+\n+arrow::Status SourceSinkExample() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                               ::arrow::internal::GetCpuThreadPool());\n+\n+  // ensure arrow::dataset node factories are in the registry\n+  arrow::dataset::internal::Initialize();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+  cp::ExecPlan::Make(&exec_context));\n+\n+  auto basic_data = MakeBasicBatches();\n+\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+  auto source_node_options =\n+      cp::SourceNodeOptions{basic_data.schema, basic_data.gen(false)};\n+\n+  ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source,\n+                        cp::MakeExecNode(\"source\", plan.get(), {}, source_node_options));\n+\n+  cp::ExecNode* sink;\n+\n+  ARROW_ASSIGN_OR_RAISE(sink, cp::MakeExecNode(\"sink\", plan.get(), {source},\n+                                               cp::SinkNodeOptions{&sink_gen}));\n+\n+  // // // translate sink_gen (async) to sink_reader (sync)\n+  std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      basic_data.schema, std::move(sink_gen), exec_context.memory_pool());\n+\n+  // // validate the ExecPlan\n+  ABORT_ON_FAILURE(plan->Validate());\n+  PRINT_LINE(\"Exec Plan Created: \" << plan->ToString());\n+  // // // start the ExecPlan\n+  ABORT_ON_FAILURE(plan->StartProducing());\n+\n+  // // collect sink_reader into a Table\n+  std::shared_ptr<arrow::Table> response_table;\n+\n+  ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+  PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+  // // plan stop producing\n+  plan->StopProducing();\n+  // // plan mark finished\n+  plan->finished().Wait();\n+\n+  return arrow::Status::OK();\n+}\n+\n+arrow::Status ScanFilterSinkExample() {\n+  cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                               ::arrow::internal::GetCpuThreadPool());\n+\n+  // ensure arrow::dataset node factories are in the registry\n+  arrow::dataset::internal::Initialize();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                        cp::ExecPlan::Make(&exec_context));\n+\n+  std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  // sync scanning is not supported by ScanNode\n+  options->use_async = true;\n+  // specify the filter\n+  cp::Expression filter_opt = cp::greater(cp::field_ref(\"a\"), cp::literal(3));\n+  \n+  options->filter = filter_opt;\n+  // empty projection\n+  options->projection = Materialize({});\n+\n+  // construct the scan node\n+  PRINT_LINE(\"Initialized Scanning Options\");\n+\n+  cp::ExecNode* scan;\n+\n+  auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+  PRINT_LINE(\"Scan node options created\");\n+\n+  ARROW_ASSIGN_OR_RAISE(scan,\n+                        cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+  // pipe the scan node into a filter node\n+  cp::ExecNode* filter;\n+  ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                 cp::FilterNodeOptions{filter_opt}));\n+\n+  // // finally, pipe the project node into a sink node\n+  arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+  ARROW_ASSIGN_OR_RAISE(\n+      cp::ExecNode * sink,\n+      cp::MakeExecNode(\"sink\", plan.get(), {filter}, cp::SinkNodeOptions{&sink_gen}));\n+\n+  ABORT_ON_FAILURE(sink->Validate());\n+  // // translate sink_gen (async) to sink_reader (sync)\n+  std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+      dataset->schema(), std::move(sink_gen), exec_context.memory_pool());\n+\n+  // // validate the ExecPlan\n+  ABORT_ON_FAILURE(plan->Validate());\n+  PRINT_LINE(\"Exec Plan created \" << plan->ToString());\n+  // // start the ExecPlan\n+  ABORT_ON_FAILURE(plan->StartProducing());\n+\n+  // // collect sink_reader into a Table\n+  std::shared_ptr<arrow::Table> response_table;\n+  ARROW_ASSIGN_OR_RAISE(response_table,\n+                        arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+  PRINT_LINE(\"Results : \" << response_table->ToString());\n+  // // plan stop producing\n+  plan->StopProducing();\n+  // /// plan marked finished\n+  plan->finished().Wait();\n+\n+  return arrow::Status::OK();\n+}\n+\n+arrow::Status ScanProjectSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+    cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // projection\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"),\n+    cp::literal(2)}); \n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    cp::ExecNode *scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan, cp::MakeExecNode(\"scan\", plan.get(), {},\n+    scan_node_options));\n+\n+    cp::ExecNode *project;\n+    ARROW_ASSIGN_OR_RAISE(project, cp::MakeExecNode(\"project\", plan.get(), {scan},\n+                                                    cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                          cp::MakeExecNode(\"sink\", plan.get(), {project},\n+                                           cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+        std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+\n+    PRINT_LINE(\"Exec Plan Created : \" << plan->ToString());\n+\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop producing\n+    plan->StopProducing();\n+    // // plan marked finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceAggregateSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeBasicBatches();\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source, cp::MakeExecNode(\"source\",\n+                                                                  plan.get(), {},\n+                                                                  source_node_options));\n+    cp::CountOptions options(cp::CountOptions::ONLY_VALID);\n+    auto aggregate_options = cp::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                          cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                          aggregate_options));\n+\n+    cp::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink, cp::MakeExecNode(\"sink\", plan.get(), {aggregate},\n+                                                 cp::SinkNodeOptions{&sink_gen}));\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({\n+            arrow::field(\"count(a)\", arrow::int32()),\n+            arrow::field(\"b\", arrow::boolean()),\n+        }),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"ExecPlan created : \" << plan->ToString());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    //plan stop producing\n+    plan->StopProducing();\n+    // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceConsumingSinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeBasicBatches();\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source, cp::MakeExecNode(\"source\",\n+                                                                  plan.get(), {},\n+                                                                  source_node_options));\n+\n+    std::atomic<uint32_t> batches_seen{0};\n+    arrow::Future<> finish = arrow::Future<>::Make();\n+    struct CustomSinkNodeConsumer : public cp::SinkNodeConsumer {\n+        CustomSinkNodeConsumer(std::atomic<uint32_t> *batches_seen, arrow::Future<>\n+        finish)\n+            : batches_seen(batches_seen), finish(std::move(finish)) {}\n+\n+        arrow::Status Consume(cp::ExecBatch batch) override {\n+            (*batches_seen)++;\n+            return arrow::Status::OK();\n+        }\n+\n+        arrow::Future<> Finish() override { return finish; }\n+\n+        std::atomic<uint32_t> *batches_seen;\n+        arrow::Future<> finish;\n+    };\n+    std::shared_ptr<CustomSinkNodeConsumer> consumer =\n+        std::make_shared<CustomSinkNodeConsumer>(&batches_seen, finish);\n+\n+    cp::ExecNode *consuming_sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(consuming_sink, MakeExecNode(\"consuming_sink\", plan.get(),\n+    {source}, cp::ConsumingSinkNodeOptions(consumer)));\n+\n+    ABORT_ON_FAILURE(consuming_sink->Validate());\n+\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // plan start producing\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+    // Source should finish fairly quickly\n+    ABORT_ON_FAILURE(source->finished().status());\n+    PRINT_LINE(\"Source Finished!\");\n+    // Mark consumption complete, plan should finish\n+    arrow::Status finish_status;\n+    //finish.Wait();\n+    finish.MarkFinished(finish_status);\n+    ABORT_ON_FAILURE(plan->finished().status());\n+    ABORT_ON_FAILURE(finish_status);\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceOrderBySinkExample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+     ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    auto basic_data = MakeSortTestBasicBatches();\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    auto source_node_options = cp::SourceNodeOptions{\n+        basic_data.schema, basic_data.gen(true)};\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * source,\n+    cp::MakeExecNode(\"source\", plan.get(), {}, source_node_options));\n+\n+    cp::ExecNode *sink;\n+\n+    ARROW_ASSIGN_OR_RAISE(sink,\n+    cp::MakeExecNode(\"order_by_sink\", plan.get(),\n+    {source}, cp::OrderBySinkNodeOptions{\n+        cp::SortOptions{{cp::SortKey{\"a\",\n+        cp::SortOrder::Descending}}}, &sink_gen}));\n+\n+    // // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        basic_data.schema,\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceHashJoinSinkExample() {\n+    auto input = MakeGroupableBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::ExecNode *left_source;\n+    cp::ExecNode *right_source;\n+    for (auto source : {&left_source, &right_source}) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            *source,\n+            MakeExecNode(\"source\", plan.get(), {},\n+                                  cp::SourceNodeOptions{\n+                                      input.schema,\n+                                      input.gen(/*parallel=*/true)}));\n+    }\n+    // TODO: decide whether to keep the filters or remove\n+\n+    // ARROW_ASSIGN_OR_RAISE(\n+    //     auto left_filter,\n+    //     cp::MakeExecNode(\"filter\", plan.get(), {left_source},\n+    //                      cp::FilterNodeOptions{\n+    //                          cp::greater_equal(\n+    //                              cp::field_ref(\"i32\"),\n+    //                              cp::literal(-1))}));\n+    // ARROW_ASSIGN_OR_RAISE(\n+    //     auto right_filter,\n+    //     cp::MakeExecNode(\"filter\", plan.get(), {right_source},\n+    //                      cp::FilterNodeOptions{\n+    //                          cp::less_equal(\n+    //                              cp::field_ref(\"i32\"),\n+    //                              cp::literal(2))}));\n+    // PRINT_LINE(\"left and right filter nodes created\");\n+\n+    cp::HashJoinNodeOptions join_opts{cp::JoinType::INNER,\n+                                      /*left_keys=*/{\"str\"},\n+                                      /*right_keys=*/{\"str\"}, cp::literal(true), \"l_\",\n+                                      \"r_\"};\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        auto hashjoin,\n+        cp::MakeExecNode(\"hashjoin\", plan.get(), {left_source, right_source},\n+        join_opts));\n+\n+    ARROW_ASSIGN_OR_RAISE(std::ignore, cp::MakeExecNode(\"sink\", plan.get(), {hashjoin},\n+                                                        cp::SinkNodeOptions{&sink_gen}));\n+    // expected columns i32, str, l_str, r_str\n+\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"i32\", arrow::int32()),\n+                       arrow::field(\"str\", arrow::utf8()),\n+                       arrow::field(\"l_str\", arrow::utf8()),\n+                       arrow::field(\"r_str\", arrow::utf8())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop producing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceKSelectExample() {\n+    auto input = MakeGroupableBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        cp::ExecNode * source,\n+        cp::MakeExecNode(\"source\",\n+            plan.get(), {},\n+                cp::SourceNodeOptions{\n+                    input.schema,\n+                    input.gen(/*parallel=*/true)}));\n+\n+    cp::SelectKOptions options = cp::SelectKOptions::TopKDefault(/*k=*/2, {\"i32\"});\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        cp::ExecNode * k_sink_node,\n+        cp::MakeExecNode(\"select_k_sink\",\n+            plan.get(), {source},\n+                cp::SelectKSinkNodeOptions{options, &sink_gen}));\n+\n+    k_sink_node->finished().Wait();\n+\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"i32\", arrow::int32()),\n+                       arrow::field(\"str\", arrow::utf8())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // validate the ExecPlan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+    arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    PRINT_LINE(\"Results : \" << response_table->ToString());\n+\n+    // // plan stop proudcing\n+    plan->StopProducing();\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status ScanFilterWriteExample(std::string file_path) {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+     cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    //cp::Expression b_is_true = cp::field_ref(\"b\");\n+    //options->filter = b_is_true;\n+    // empty projection\n+    options->projection = Materialize({});\n+\n+    cp::ExecNode *scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan, cp::MakeExecNode(\"scan\", plan.get(), {},\n+    scan_node_options));\n+\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    std::string root_path = \"\";\n+    std::string uri = \"file://\" + file_path;\n+    std::shared_ptr<arrow::fs::FileSystem> filesystem =\n+    arrow::fs::FileSystemFromUri(uri, &root_path).ValueOrDie();\n+\n+    auto base_path = root_path + \"/parquet_dataset\";\n+    ABORT_ON_FAILURE(filesystem->DeleteDir(base_path));\n+    ABORT_ON_FAILURE(filesystem->CreateDir(base_path));\n+\n+    // The partition schema determines which fields are part of the partitioning.\n+    auto partition_schema = arrow::schema({arrow::field(\"a\", arrow::int32())});\n+    // We'll use Hive-style partitioning,\n+    // which creates directories with \"key=value\" pairs.\n+\n+    auto partitioning =\n+    std::make_shared<arrow::dataset::HivePartitioning>(partition_schema);\n+    // We'll write Parquet files.\n+    auto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n+\n+    arrow::dataset::FileSystemDatasetWriteOptions write_options;\n+    write_options.file_write_options = format->DefaultWriteOptions();\n+    write_options.filesystem = filesystem;\n+    write_options.base_dir = base_path;\n+    write_options.partitioning = partitioning;\n+    write_options.basename_template = \"part{i}.parquet\";\n+\n+    arrow::dataset::WriteNodeOptions write_node_options {write_options,\n+    dataset->schema()};\n+\n+    PRINT_LINE(\"Write Options created\");\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode *wr, cp::MakeExecNode(\"write\", plan.get(),\n+    {scan}, write_node_options));\n+\n+    ABORT_ON_FAILURE(wr->Validate());\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Execution Plan Created : \" << plan->ToString());\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+    plan->finished().Wait();\n+    return arrow::Status::OK();\n+}\n+\n+arrow::Status SourceUnionSinkExample() {\n+    auto basic_data = MakeBasicBatches();\n+\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                 ::arrow::internal::GetCpuThreadPool());\n+\n+    std::shared_ptr<cp::ExecPlan> plan =\n+    cp::ExecPlan::Make(&exec_context).ValueOrDie();\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+\n+    cp::Declaration union_node{\"union\", cp::ExecNodeOptions{}};\n+    cp::Declaration lhs{\"source\",\n+                  cp::SourceNodeOptions{basic_data.schema,\n+                                    basic_data.gen(/*parallel=*/false)}};\n+    lhs.label = \"lhs\";\n+    cp::Declaration rhs{\"source\",\n+                    cp::SourceNodeOptions{basic_data.schema,\n+                                      basic_data.gen(/*parallel=*/false)}};\n+    rhs.label = \"rhs\";\n+    union_node.inputs.emplace_back(lhs);\n+    union_node.inputs.emplace_back(rhs);\n+\n+    cp::CountOptions options(cp::CountOptions::ONLY_VALID);\n+    ARROW_ASSIGN_OR_RAISE(auto declr,\n+    cp::Declaration::Sequence(\n+            {\n+                union_node,\n+                {\"aggregate\", cp::AggregateNodeOptions{\n+                  /*aggregates=*/{{\"count\", &options}},\n+                  /*targets=*/{\"a\"},\n+                  /*names=*/{\"count(a)\"},\n+                  /*keys=*/{}}},\n+                {\"sink\", cp::SinkNodeOptions{&sink_gen}},\n+            })\n+            .AddToPlan(plan.get()));\n+\n+    ABORT_ON_FAILURE(declr->Validate());\n+\n+    ABORT_ON_FAILURE(plan->Validate());\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader = cp::MakeGeneratorReader(\n+        arrow::schema({arrow::field(\"count(a)\", arrow::int32())}),\n+        std::move(sink_gen),\n+        exec_context.memory_pool());\n+\n+    // // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n\nReview comment:\n       Why abort here instead of `ARROW_RETURN_NOT_OK`?\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n\nReview comment:\n       (It's likely to go away entirely sooner or later.)\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n+9. Scan-Node\n+10. :class:`HashJoinNode`\n+11. Write-Node\n+12. :class:`UnionNode`\n+\n+There are a set of :class:`ExecNode` s designed to provide various operations required\n+in designing a streaming execution plan. \n+\n+``SourceNode``\n+--------------\n+\n+:struct:`arrow::compute::SourceNode` can be considered as an entry point to create a streaming execution plan. \n+A source node can be constructed as follows.\n+\n+:class:`arrow::compute::SoureNodeOptions` are used to create the :struct:`arrow::compute::SourceNode`. \n+The :class:`Schema` of the data passing through and a function to generate data \n+`std::function<arrow::Future<arrow::util::optional<arrow::compute::ExecBatch>>()>` \n+are required to create this option::\n+\n+    // data generator\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> gen() { ... }\n+    \n+    // data schema \n+    auto schema = arrow::schema({...})\n+    \n+    // source node options\n+    auto source_node_options = arrow::compute::SourceNodeOptions{schema, gen};\n+    \n+    // create a source node\n+    ARROW_ASSIGN_OR_RAISE(arrow::compute::ExecNode * source,\n+                            arrow::compute::MakeExecNode(\"source\", plan.get(), {}, \n+                            source_node_options));\n+\n+``FilterNode``\n+--------------\n+\n+:class:`FilterNode`, as the name suggests, provide a container to define a data filtering criteria. \n+Filter can be written using :class:`arrow::compute::Expression`. For instance if the row values\n+of a particular column needs to be filtered by a boundary value, ex: all values of column b\n+greater than 3, can be written using :class:`arrow::compute::FilterNodeOptions` as follows::\n+\n+    // a > 3\n+    arrow::compute::Expression filter_opt = arrow::compute::greater(\n+                                  arrow::compute::field_ref(\"a\"), \n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tarrow::compute::literal(3));\n+\n+Using this option, the filter node can be constructed as follows::\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n+\n+    // creating filter node\n+    arrow::compute::ExecNode* filter;\n+      ARROW_ASSIGN_OR_RAISE(filter, arrow::compute::MakeExecNode(\"filter\", \n+                            // plan\n+                            plan.get(),\n+                            // previous input\n+                            {scan}, \n+                            //filter node options\n+                            arrow::compute::FilterNodeOptions{filter_opt}));\n+\n+``ProjectNode``\n+---------------\n+\n+:class:`ProjectNode` executes expressions on input batches and produces new batches. \n+Each expression will be evaluated against each batch which is pushed to this \n+node to produce a corresponding output column. This is exposed via \n+:class:`arrow::compute::ProjectNodeOptions` component which requires, \n+a :class:`arrow::compute::Expression`, names of the project columns (names are not provided, \n+the string representations of exprs will be used) and a boolean flag to determine \n+synchronous/asynchronous nature (by default asynchronous option is set to `true`). \n+\n+Sample Expression for projection::\n+\n+    // a * 2 (multiply values in a column by 2)\n+    arrow::compute::Expression a_times_2 = arrow::compute::call(\"multiply\", \n+\t\t\t\t\t\t\t{arrow::compute::field_ref(\"a\"), arrow::compute::literal(2)});\n+\n+\n+Creating a project node::\n+\n+    arrow::compute::ExecNode *project;\n+        ARROW_ASSIGN_OR_RAISE(project, \n+            arrow::compute::MakeExecNode(\"project\", \n+            // plan\n+            plan.get(),\n+            // previous node \n+            {scan},\n+            // project node options \n+            arrow::compute::ProjectNodeOptions{{a_times_2}}));\n+\n+``ScalarAggregateNode``\n+-----------------------\n+\n+:class:`ScalarAggregateNode` is an :class:`ExecNode` which provides various \n+aggregation options. The :class:`arrow::compute::AggregateNodeOptions` provides the \n+container to define the aggregation criterion. These options can be \n+selected from `arrow::compute` options. \n+\n+1. `ScalarAggregateOptions`\n+\n+In this aggregation mode, using option, `skip_nulls` the null values are ignored.\n+Also checks with another flag `min_count`, if less than this many non-null values \n+are observed, emit null. \n+\n+Example::\n+\n+    auto agg_options = cp::ScalarAggregateOptions agg_opt(false, 2);\n+\n+2. `CountOptions`\n+   \n+:class:`arrow::compute::CountOptions` aggregation option provides three sub-options to \n+determine the counting approach. \n+\n+a. `ONLY_VALID` : Count only non-null values\n+b. `ONLY_NULL` : Count both non-null and null values\n+c. `ALL` : Count both non-null and null values\n+\n+Example::\n+\n+    arrow::compute::CountOptions options(cp::CountOptions::ONLY_VALID);\n+\n+3. `ModeOptions`\n+\n+:class:`arrow::compute::ModeOptions` aggregation option computes mode for a distribution,\n+by returns top-n common values and counts. \n+By default, returns the most common value and count\n+\n+Example::\n+\n+    // n: top value `n` values\n+    // skip_nulls: if true (the default), null values are ignored. \n+    // \t\t\t\t\t\tOtherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::ModeOptions mode_option(/*n*/5, /*skip_nulls*/true, /*min_count*/2);\n+\n+4. `VarianceOptions`\n+\n+:class:`arrow::compute::VarianceOptions` option controls the Delta Degrees of Freedom \n+(ddof) of Variance and Stddev kernel. The divisor used in calculations is N - ddof, \n+where N is the number of elements. By default, ddof is zero, and population variance \n+or stddev is returned.\n+\n+Example::\n+\n+    // ddof: \n+    // skip_nulss: If true (the default), null values are ignored. \n+    //////Otherwise, if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::VarianceOptions variance_option(/*ddof/*1, \n+                                                    /*skip_nulls*/true, \n+                                                    /*min_count*/3);\n+\n+5. `QuantileOptions`\n+\n+:class:`arrow::compute::QuantileOptions` This option controls the Quantile kernel behavior. \n+By default, returns the median value. There is an interpolation method to use when quantile \n+lies between two data points. The provided options for interpolation are; `LINEAE`, `LOWER`, `HIGHER`,\n+`NEAREST` and `MIDPOINT`.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive \n+    ////// (scalar value or a std::vector as input)\n+    // interpolation: one of `LINEAER`, `LOWER`, 'HIGHER', \n+    ////// `NEAREST`, `MIDPOINT`\n+    // skip_nulls: If true (the default), null values are ignored. Otherwise, \n+    ////// if any value is null, emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::QuantileOptions quantile_options(/*q*/0.50, \n+      /*interpolation*/cp::QuantileOptions::Interpolation::LINEAR, \n+      /*skip_nulls*/true, \n+      /*min_count*/3);\n+\n+6. `TDigestOptions`\n+\n+`arrow::compute::TDigestOptions` option controls TDigest approximate quantile kernel behavior.\n+By default, returns the median value.\n+\n+Example::\n+\n+    // q: quantile must be between 0 and 1 inclusive\n+    // delta: compression parameter, default 100\n+    // buffer_size: input buffer size, default 500\n+    // skip_nulls: if true (the default), null values are ignored. Otherwise, if any value is null,\n+    ////// emit null.\n+    // min_count: If less than this many non-null values are observed, emit null.\n+    arrow::compute::TDigestOptions tdigest_option(/*q*/0.5, \n+      /*delta*/200, \n+      /*buffer_size*/600, \n+      /*skip_nulls*/true, \n+      /*min_count*/5);\n+\n+7. IndexOptions\n+\n+:class:`arrow::compute::IndexOptions` This option controls Index kernel behavior. \n+This is used to find the index of a particular scalar value. \n+\n+Example::\n+\n+    arrow::compute::IndexOptions index_options(arrow::MakeScalar(\"1\"));\n+\n+An example for creating an aggregate node::\n+\n+    arrow::compute::CountOptions options(arrow::compute::CountOptions::ONLY_VALID);\n+\n+    auto aggregate_options = arrow::compute::AggregateNodeOptions{\n+        /*aggregates=*/{{\"hash_count\", &options}},\n+        /*targets=*/{\"a\"},\n+        /*names=*/{\"count(a)\"},\n+        /*keys=*/{\"b\"}};\n+\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * aggregate,\n+                              cp::MakeExecNode(\"aggregate\", plan.get(), {source},\n+                              aggregate_options));\n+\n+\n+Scan-Node\n+---------\n+\n+There is no class or struct defined as ScanNode in the source. \n\nReview comment:\n       Also, again, because the API docs for individual node classes are useless at best, and potentially misleading at worst.\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n+    arrow::io::IOContext io_context = arrow::io::default_io_context();\n+    std::shared_ptr<arrow::io::InputStream> input;\n+    std::string csv_data = GetDataAsCsvString();\n+    arrow::util::string_view sv = csv_data;\n+    input = std::make_shared<arrow::io::BufferReader>(sv);\n+\n+    auto read_options = arrow::csv::ReadOptions::Defaults();\n+    auto parse_options = arrow::csv::ParseOptions::Defaults();\n+    auto convert_options = arrow::csv::ConvertOptions::Defaults();\n+\n+    // Instantiate TableReader from input stream and options\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<arrow::csv::TableReader> table_reader,\n+      arrow::csv::TableReader::Make(io_context,\n+                                    input,\n+                                    read_options,\n+                                    parse_options,\n+                                    convert_options));\n+\n+    std::shared_ptr<arrow::csv::TableReader> reader = table_reader;\n+\n+    // Read table from CSV file\n+    ARROW_ASSIGN_OR_RAISE(auto maybe_table,\n+      reader->Read());\n+    auto ds = std::make_shared<arrow::dataset::InMemoryDataset>(maybe_table);\n+    dataset = std::move(ds);\n+    return arrow::Status::OK();\n+}\n+\n+std::shared_ptr<arrow::dataset::Dataset> CreateDataset() {\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> im_dataset;\n+    ABORT_ON_FAILURE(CreateDataSetFromCSVData(im_dataset));\n+    return im_dataset;\n+}\n+\n+arrow::Status exec_plan_end_to_end_sample() {\n+    cp::ExecContext exec_context(arrow::default_memory_pool(),\n+                                ::arrow::internal::GetCpuThreadPool());\n+\n+    // ensure arrow::dataset node factories are in the registry\n+    arrow::dataset::internal::Initialize();\n+\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<cp::ExecPlan> plan,\n+                            cp::ExecPlan::Make(&exec_context));\n+\n+    std::shared_ptr<arrow::dataset::Dataset> dataset = CreateDataset();\n+\n+    auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+    // sync scanning is not supported by ScanNode\n+    options->use_async = true;\n+    // specify the filter\n+    cp::Expression b_is_true = cp::field_ref(\"b\");\n+    options->filter = b_is_true;\n+    // for now, specify the projection as the full project expression (eventually this can\n+    // just be a list of materialized field names)\n+\n+    cp::Expression a_times_2 = cp::call(\"multiply\", {cp::field_ref(\"a\"), cp::literal(2)});\n+    options->projection =\n+        cp::call(\"make_struct\", {a_times_2}, cp::MakeStructOptions{{\"a * 2\"}});\n+\n+    // // construct the scan node\n+    cp::ExecNode* scan;\n+\n+    auto scan_node_options = arrow::dataset::ScanNodeOptions{dataset, options};\n+\n+    ARROW_ASSIGN_OR_RAISE(scan,\n+                            cp::MakeExecNode(\"scan\", plan.get(), {}, scan_node_options));\n+\n+    // pipe the scan node into a filter node\n+    cp::ExecNode* filter;\n+    ARROW_ASSIGN_OR_RAISE(filter, cp::MakeExecNode(\"filter\", plan.get(), {scan},\n+                                                    cp::FilterNodeOptions{b_is_true}));\n+\n+    cp::ExecNode* project;\n+\n+    ARROW_ASSIGN_OR_RAISE(project,\n+                            cp::MakeExecNode(\"augmented_project\", plan.get(), {filter},\n+                                            cp::ProjectNodeOptions{{a_times_2}}));\n+\n+    // // finally, pipe the project node into a sink node\n+    arrow::AsyncGenerator<arrow::util::optional<cp::ExecBatch>> sink_gen;\n+    ARROW_ASSIGN_OR_RAISE(cp::ExecNode * sink,\n+                            cp::MakeExecNode(\"sink\",\n+                            plan.get(), {project},\n+                            cp::SinkNodeOptions{&sink_gen}));\n+\n+    ABORT_ON_FAILURE(sink->Validate());\n+\n+    // // translate sink_gen (async) to sink_reader (sync)\n+    std::shared_ptr<arrow::RecordBatchReader> sink_reader =\n+        cp::MakeGeneratorReader(arrow::schema({arrow::field(\"a * 2\", arrow::int32())}),\n+                                std::move(sink_gen), exec_context.memory_pool());\n+\n+    // // validate the plan\n+    ABORT_ON_FAILURE(plan->Validate());\n+    PRINT_LINE(\"Exec Plan created: \" << plan->ToString());\n+    // // start the ExecPlan\n+    ABORT_ON_FAILURE(plan->StartProducing());\n+\n+    // // collect sink_reader into a Table\n+    std::shared_ptr<arrow::Table> response_table;\n+    ARROW_ASSIGN_OR_RAISE(response_table,\n+                            arrow::Table::FromRecordBatchReader(sink_reader.get()));\n+\n+    std::cout << \"Results : \" << response_table->ToString() << std::endl;\n+\n+    // // stop producing\n+    plan->StopProducing();\n+\n+    // // plan mark finished\n+    plan->finished().Wait();\n+\n+    return arrow::Status::OK();\n\nReview comment:\n       We should return the status from `plan->finished()` to catch if the plan errors.\n\n##########\nFile path: cpp/examples/arrow/execution_plan_documentation_examples.cc\n##########\n@@ -0,0 +1,1160 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements. See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership. The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License. You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied. See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <utility>\n+\n+#include <arrow/compute/api.h>\n+#include <arrow/compute/api_scalar.h>\n+#include <arrow/compute/api_vector.h>\n+#include <arrow/compute/cast.h>\n+#include <arrow/compute/exec/exec_plan.h>\n+#include <arrow/compute/exec/ir_consumer.h>\n+#include <arrow/compute/exec/test_util.h>\n+\n+#include <arrow/csv/api.h>\n+\n+#include <arrow/dataset/dataset.h>\n+#include <arrow/dataset/file_parquet.h>\n+#include <arrow/dataset/file_base.h>\n+#include <arrow/dataset/plan.h>\n+#include <arrow/dataset/scanner.h>\n+#include <arrow/dataset/dataset_writer.h>\n+\n+#include <arrow/io/interfaces.h>\n+#include <arrow/io/memory.h>\n+#include <arrow/io/slow.h>\n+#include <arrow/io/transform.h>\n+#include <arrow/io/stdio.h>\n+\n+#include <arrow/result.h>\n+#include <arrow/status.h>\n+#include <arrow/table.h>\n+\n+#include <arrow/ipc/api.h>\n+\n+#include <arrow/util/future.h>\n+#include <arrow/util/range.h>\n+#include <arrow/util/thread_pool.h>\n+#include <arrow/util/vector.h>\n+\n+// Demonstrate various operators in Arrow Streaming Execution Engine\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      abort();                                     \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_RETURN(expr)                     \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    } else {                                       \\\n+      return EXIT_SUCCESS;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+#define CHECK_AND_CONTINUE(expr)                   \\\n+  do {                                             \\\n+    arrow::Status status_ = (expr);                \\\n+    if (!status_.ok()) {                           \\\n+      std::cerr << status_.message() << std::endl; \\\n+      return EXIT_FAILURE;                         \\\n+    }                                              \\\n+  } while (0);\n+\n+constexpr char kSep[] = \"******\";\n+\n+#define PRINT_BLOCK(msg)                                                     \\\n+  std::cout << \"\" << std::endl;                                              \\\n+  std::cout << \"\\t\" << kSep << \" \" << msg << \" \" << kSep << std::endl; \\\n+  std::cout << \"\" << std::endl;\n+\n+#define PRINT_LINE(msg) std::cout << msg << std::endl;\n+\n+namespace cp = ::arrow::compute;\n+\n+std::shared_ptr<arrow::Array> GetArrayFromJSON(\n+    const std::shared_ptr<arrow::DataType>& type, arrow::util::string_view json) {\n+    std::shared_ptr<arrow::Array> out;\n+    ABORT_ON_FAILURE(arrow::ipc::internal::json::ArrayFromJSON(type, json, &out));\n+  return out;\n+}\n+\n+std::shared_ptr<arrow::RecordBatch> GetRecordBatchFromJSON(\n+    const std::shared_ptr<arrow::Schema>& schema, arrow::util::string_view json) {\n+  // Parse as a StructArray\n+  auto struct_type = struct_(schema->fields());\n+  std::shared_ptr<arrow::Array> struct_array = GetArrayFromJSON(struct_type, json);\n+\n+  // Convert StructArray to RecordBatch\n+  return *arrow::RecordBatch::FromStructArray(struct_array);\n+}\n+\n+std::string GetDataAsCsvString() {\n+    std::string data_str = \"\";\n+\n+    data_str.append(\"a,b\\n\");\n+    data_str.append(\"1,null\\n\");\n+    data_str.append(\"2,true\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"3,false\\n\");\n+    data_str.append(\"null,true\\n\");\n+    data_str.append(\"4,false\\n\");\n+    data_str.append(\"5,null\\n\");\n+    data_str.append(\"6,false\\n\");\n+    data_str.append(\"7,false\\n\");\n+    data_str.append(\"8,true\\n\");\n+\n+    return data_str;\n+}\n+\n+arrow::Status CreateDataSetFromCSVData(\n+    std::shared_ptr<arrow::dataset::InMemoryDataset> &dataset) {\n\nReview comment:\n       Or really, just return `Result<std::shared_ptr<Dataset>>`.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-27T14:46:07.874+0000",
                    "updated": "2021-12-27T14:46:07.874+0000",
                    "started": "2021-12-27T14:46:07.874+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "701332",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/701389",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r775666772\n\n\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n\nReview comment:\n       Yeah that sounds better. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-27T23:00:36.384+0000",
                    "updated": "2021-12-27T23:00:36.384+0000",
                    "started": "2021-12-27T23:00:36.384+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "701389",
                    "issueId": "13417297"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/worklog/701390",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12033:\nURL: https://github.com/apache/arrow/pull/12033#discussion_r775667075\n\n\n\n##########\nFile path: docs/source/cpp/streaming_execution.rst\n##########\n@@ -175,9 +175,607 @@ their completion::\n     // alive until this future is marked finished.\n     Future<> complete = plan->finished();\n \n+Constructing ``ExecNode`` using Options\n+=======================================\n+\n+Using the execution plan we can construct varioud execution queries. \n+To construct such queries, we have provided a set of containers or \n+referred as :class:`ExecNode` s. These nodes provide the ability to \n+construct operations like filtering, projection, join, etc. \n+\n+This is the list of :class:`ExecutionNode` s exposed;\n+\n+1. :class:`SourceNode`\n+2. :class:`FilterNode`\n+3. :class:`ProjectNode`\n+4. :class:`ScalarAggregateNode`\n+5. :class:`SinkNode`\n+6. :class:`ConsumingSinkNode`\n+7. :struct:`OrderBySinkNode`\n+8. SelectK-SinkNode\n\nReview comment:\n       May be we can say about node operation without saying the node names. For instance join operation, filter operation, etc. I think it\u2019s important to expose the available operations ? WDYT?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-27T23:02:40.450+0000",
                    "updated": "2021-12-27T23:02:40.450+0000",
                    "started": "2021-12-27T23:02:40.450+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "701390",
                    "issueId": "13417297"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 91200,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@5f31b899[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@19f15195[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@447afb23[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@579477c0[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@311bb753[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@4cdb6895[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5feb21ce[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@2bb56784[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4778fc8e[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@5070bb34[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@561305a5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@569ad1a4[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 91200,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Jan 27 18:43:34 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-01-27T18:43:34.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15091/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-12-14T06:57:17.000+0000",
        "updated": "2022-01-29T06:01:31.000+0000",
        "timeoriginalestimate": null,
        "description": "ARROW-13227 Added documentation about the exec plan which is the workflow engine for the streaming execution engine.  Since then we have added a number of nodes (filter, project, hash-join, etc) which each have their own behavior and options.  We should document these nodes and give some examples how the low level C++ API can be used to create query plans.  This would be intended for fairly advanced users.  Most users will probably be getting to the exec plan through some kind of front end.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "25h 20m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 91200
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Doc] Document nodes in C++ streaming execution engine",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13417297/comment/17483374",
                    "id": "17483374",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Issue resolved by pull request 12033\n[https://github.com/apache/arrow/pull/12033]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2022-01-27T18:43:34.651+0000",
                    "updated": "2022-01-27T18:43:34.651+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0xo0w:",
        "customfield_12314139": null
    }
}