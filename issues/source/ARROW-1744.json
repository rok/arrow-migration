{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13112779",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779",
    "key": "ARROW-1744",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12342562",
                "id": "12342562",
                "description": "",
                "name": "0.10.0",
                "archived": false,
                "released": true,
                "releaseDate": "2018-08-06"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=pcmoritz",
            "name": "pcmoritz",
            "key": "pcmoritz",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Philipp Moritz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332956",
                "id": "12332956",
                "name": "C++ - Plasma"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=pcmoritz",
            "name": "pcmoritz",
            "key": "pcmoritz",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Philipp Moritz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=pcmoritz",
            "name": "pcmoritz",
            "key": "pcmoritz",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Philipp Moritz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 63000,
            "total": 63000,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 63000,
            "total": 63000,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1744/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 117,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103765",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pcmoritz commented on issue #1379: ARROW-1744: [WIP] Add plasma tensorflow op\nURL: https://github.com/apache/arrow/pull/1379#issuecomment-390504067\n \n \n   replaced by https://github.com/apache/arrow/pull/2046\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T19:08:47.110+0000",
                    "updated": "2018-05-20T19:08:47.110+0000",
                    "started": "2018-05-20T19:08:47.109+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103765",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103766",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pcmoritz closed pull request #1379: ARROW-1744: [WIP] Add plasma tensorflow op\nURL: https://github.com/apache/arrow/pull/1379\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/cpp/src/plasma/tf/make.sh b/cpp/src/plasma/tf/make.sh\nnew file mode 100644\nindex 0000000000..61f7a7f7f6\n--- /dev/null\n+++ b/cpp/src/plasma/tf/make.sh\n@@ -0,0 +1,3 @@\n+TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\n+\n+g++ -std=c++11 -g -shared plasma_op.cc -o plasma_op.so `pkg-config --cflags --libs plasma` -undefined dynamic_lookup -fPIC -I $TF_INC -O2\ndiff --git a/cpp/src/plasma/tf/plasma_op.cc b/cpp/src/plasma/tf/plasma_op.cc\nnew file mode 100644\nindex 0000000000..32bfb37c82\n--- /dev/null\n+++ b/cpp/src/plasma/tf/plasma_op.cc\n@@ -0,0 +1,70 @@\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/ipc/reader.h\"\n+#include \"arrow/tensor.h\"\n+#include \"plasma/client.h\"\n+\n+using namespace tensorflow;\n+\n+REGISTER_OP(\"PlasmaData\")\n+    .Input(\"object_id: string\")\n+    .Output(\"output: float32\")\n+    .Attr(\"socket: string\");\n+\n+// TODO(pcm): Make this zero-copy if possible\n+\n+class PlasmaDataOp : public OpKernel {\n+ public:\n+  explicit PlasmaDataOp(OpKernelConstruction* context) : OpKernel(context) {\n+    std::cout << \"called constructor\" << std::endl;\n+    std::string socket;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"socket\", &socket));\n+    // Connect to plasma\n+    ARROW_CHECK_OK(client_.Connect(socket, \"\", PLASMA_DEFAULT_RELEASE_DELAY));\n+    std::cout << \"constructor finished\" << std::endl;\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    // Grab the input tensor\n+    const Tensor& input_tensor = context->input(0);\n+    auto input = input_tensor.flat<string>();\n+\n+    // Get the object\n+    plasma::ObjectID object_id = plasma::ObjectID::from_binary(input(0));\n+    plasma::ObjectBuffer object_buffer;\n+    ARROW_CHECK_OK(client_.Get(&object_id, 1, -1, &object_buffer));\n+\n+    // Get the tensor\n+    std::shared_ptr<arrow::Tensor> result;\n+    arrow::io::BufferReader reader(object_buffer.data, object_buffer.data_size);\n+    int64_t offset;\n+    ARROW_CHECK_OK(reader.Tell(&offset));\n+    ARROW_CHECK_OK(arrow::ipc::ReadTensor(0, &reader, &result));\n+\n+    std::cout << \"shape is\" << result->shape()[0] << \" , \" << result->shape()[1]\n+              << std::endl;\n+\n+    // Create an output tensor\n+    TensorShape shape(result->shape());\n+    Tensor* output_tensor = NULL;\n+    OP_REQUIRES_OK(context, context->allocate_output(0, shape, &output_tensor));\n+    auto output_flat = output_tensor->flat<float>();\n+\n+    // Set all but the first element of the output tensor to 0.\n+    const int64_t N = result->size();\n+    std::cout << \"size is \" << N << std::endl;\n+    const float* data = reinterpret_cast<const float*>(result->data()->data());\n+    for (int i = 0; i < N; i++) {\n+      output_flat(i) = data[i];\n+    }\n+  }\n+  ~PlasmaDataOp() { ARROW_CHECK_OK(client_.Disconnect()); }\n+\n+ private:\n+  plasma::PlasmaClient client_;\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"PlasmaData\").Device(DEVICE_CPU), PlasmaDataOp);\ndiff --git a/cpp/src/plasma/tf/test.py b/cpp/src/plasma/tf/test.py\nnew file mode 100644\nindex 0000000000..2a1056a737\n--- /dev/null\n+++ b/cpp/src/plasma/tf/test.py\n@@ -0,0 +1,46 @@\n+import numpy as np\n+import pyarrow as pa\n+import pyarrow.plasma as plasma\n+import tensorflow as tf\n+\n+import time\n+\n+zero_out_module = tf.load_op_library('./plasma_op.so')\n+\n+client = plasma.connect(\"/tmp/plasma\", \"\", 64)\n+\n+data = np.random.randn(10000, 4000).astype(\"float32\")\n+tensor = pa.Tensor.from_numpy(data)\n+\n+data_id = client.put(tensor)\n+\n+# plasma.ObjectID(np.random.bytes(20))\n+# data_size = pa.get_tensor_size(tensor)\n+# buf = client.create(object_id, data_size)\n+# stream = pa.FixedSizeBufferWriter(buf)\n+# pa.write_tensor(tensor, stream)\n+# client.seal(object_id)\n+\n+sess = tf.Session()\n+object_id = tf.placeholder(tf.string)\n+load_op = zero_out_module.plasma_data([object_id], socket=\"/tmp/plasma\")\n+a = time.time()\n+print(\"XXX\", sess.run(load_op, feed_dict={object_id: data_id.binary()}))\n+b = time.time() - a\n+print(\"b1\", b)\n+print(\"XXX\", sess.run(load_op, feed_dict={object_id: data_id.binary()}))\n+\n+placeholder = tf.placeholder(tf.float32, shape=(10000, 4000))\n+\n+# variable = tf.Variable(placeholder, trainable=False, initializer=tf.random_uniform_initializer(-1.0, 1.0))\n+\n+# sess.run(tf.global_variables_initializer())\n+a = time.time()\n+d = sess.run(placeholder, feed_dict={placeholder: data})\n+b = time.time() - a\n+print(\"b2\", b)\n+\n+\n+print(\"ZZZ\", d)\n+\n+print(\"YYY\", data)\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T19:08:47.368+0000",
                    "updated": "2018-05-20T19:08:47.368+0000",
                    "started": "2018-05-20T19:08:47.368+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103766",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103775",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "concretevitamin commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472528\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n \n Review comment:\n   group with gpu_event_mgr include\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T22:35:17.854+0000",
                    "updated": "2018-05-20T22:35:17.854+0000",
                    "started": "2018-05-20T22:35:17.853+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103775",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103776",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "concretevitamin commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472452\n \n \n\n ##########\n File path: cpp/src/plasma/CMakeLists.txt\n ##########\n @@ -123,6 +129,18 @@ if (\"${COMPILER_FAMILY}\" STREQUAL \"gcc\")\n     \" -Wno-conversion\")\n endif()\n \n+if (PLASMA_BUILD_TENSORFLOW_OP)\n+  if (NOT TensorFlow_FOUND)\n+    message(FATAL_ERROR \"Need TensorFlow to build the TensorFlow op\")\n+  endif()\n+  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=0\")\n+  include_directories(${TensorFlow_INCLUDE_DIRS})\n+  include_directories(${TensorFlow_INCLUDE_DIRS}/external/nsync/public)\n \n Review comment:\n   Hmm, (1) nsync is not included in the tf.sysconfig output?  (2) orthogonal to last question, is this needed here?  I never needed it to compile by hand.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T22:35:17.857+0000",
                    "updated": "2018-05-20T22:35:17.857+0000",
                    "started": "2018-05-20T22:35:17.856+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103776",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103777",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "concretevitamin commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472483\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n+    // LOG(INFO) << \"Output TensorShape: \" << shape.DebugString();\n+    // LOG(INFO) << \"size_in_bytes of the plasma object: \" << size_in_bytes;\n+\n+    const float* plasma_data = reinterpret_cast<const float*>(object_buffer.data->data());\n+    // for (int i = 0; i < size_in_bytes / sizeof(float); ++i) {\n+    //   LOG(INFO) << plasma_data[i];\n+    // }\n+\n+    Tensor* output_tensor = nullptr;\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, shape, &output_tensor),\n+                         done);\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      std::memcpy(output_tensor->flat<float>().data(),\n+                  reinterpret_cast<const float*>(object_buffer.data->data()),\n+                  size_in_bytes);\n+      done();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      {\n+        mutex_lock l(h2d_stream_mu);\n+        if (h2d_stream == nullptr) {\n+          h2d_stream = new Stream(stream_executor);\n+          CHECK(h2d_stream->Init().ok());\n+        }\n+      }\n+\n+      // #define PLASMA_CLIENT_DOES_NOT_EXIST 3\n \n Review comment:\n   This commented out block can all be removed. \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T22:35:17.859+0000",
                    "updated": "2018-05-20T22:35:17.859+0000",
                    "started": "2018-05-20T22:35:17.859+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103777",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103778",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "concretevitamin commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472493\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n+    // LOG(INFO) << \"Output TensorShape: \" << shape.DebugString();\n+    // LOG(INFO) << \"size_in_bytes of the plasma object: \" << size_in_bytes;\n+\n+    const float* plasma_data = reinterpret_cast<const float*>(object_buffer.data->data());\n+    // for (int i = 0; i < size_in_bytes / sizeof(float); ++i) {\n+    //   LOG(INFO) << plasma_data[i];\n+    // }\n+\n+    Tensor* output_tensor = nullptr;\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, shape, &output_tensor),\n+                         done);\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      std::memcpy(output_tensor->flat<float>().data(),\n+                  reinterpret_cast<const float*>(object_buffer.data->data()),\n+                  size_in_bytes);\n+      done();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      {\n+        mutex_lock l(h2d_stream_mu);\n+        if (h2d_stream == nullptr) {\n+          h2d_stream = new Stream(stream_executor);\n+          CHECK(h2d_stream->Init().ok());\n+        }\n+      }\n+\n+      // #define PLASMA_CLIENT_DOES_NOT_EXIST 3\n+      // #define PLASMA_CLIENT_LOCAL 0\n+\n+      //       // Launch async fetch.\n+      //       {\n+      //         mutex_lock lock(mu_);\n+      //         LOG(INFO) << \"Launching Fetch()\";\n+      //         ARROW_CHECK_OK(client_.Fetch(/*num_object_ids=*/1,\n+      //         &object_id)); LOG(INFO) << \"Done launching Fetch()\";\n+      //       }\n+\n+      //       std::function<void()> WaitForTransfer = [context, this,\n+      //       object_id,\n+      //                                                WaitForTransfer]() ->\n+      //                                                void {\n+      //         int object_status;\n+      //         {\n+      //           mutex_lock lock(mu_);\n+      //           LOG(INFO) << \"Launching Info()\";\n+      //           ARROW_CHECK_OK(client_.Info(object_id, &object_status));\n+      //           LOG(INFO) << \"Done launching Info()\";\n+      //         }\n+      //         CHECK(object_status != PLASMA_CLIENT_DOES_NOT_EXIST);\n+\n+      //         if (object_status == PLASMA_CLIENT_LOCAL) {\n+      //           LOG(INFO) << \"Object is local!\";\n+      //           CHECK(0);\n+      //           // TODO(zongheng):  do real work;\n+      //         } else {\n+\n+      //           LOG(INFO) << \"Object still not local, status: \" <<\n+      //           object_status; std::function<void()> new_func =\n+      //           WaitForTransfer; context->device()\n+      //               ->tensorflow_gpu_device_info()\n+      //               ->event_mgr->ThenExecute(h2d_stream, new_func);\n+      //         }\n+      //       };\n+\n+      //       context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+      //           h2d_stream, WaitForTransfer);\n+\n+      // Important.  See note in T2P op.\n+      // We don't check the return status since the host memory might've been\n+      // already registered (e.g., the TensorToPlasmaOp might've been run).\n+      stream_executor->HostMemoryRegister(\n+          const_cast<void*>(static_cast<const void*>(plasma_data)),\n+          static_cast<uint64>(size_in_bytes));\n+\n+      perftools::gputools::DeviceMemoryBase wrapped_dst(\n+          static_cast<void*>(output_tensor->flat<float>().data()));\n+      const bool success =\n+          h2d_stream\n+              ->ThenMemcpy(&wrapped_dst, static_cast<const void*>(plasma_data),\n+                           static_cast<uint64>(size_in_bytes))\n+              .ok();\n+      OP_REQUIRES_ASYNC(context, success,\n+                        errors::Internal(\"H2D memcpy failed to be enqueued.\"), done);\n+\n+      // Without this sync the main compute stream might proceed to use the\n+      // Tensor buffer, but its contents might still be in-flight from our\n+      // h2d_stream.\n+      CHECK(orig_stream->ThenWaitFor(h2d_stream).ok());\n+\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          h2d_stream, std::move(done));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+};\n+\n+REGISTER_OP(\"TensorToPlasma\")\n+    .Input(\"input_tensor: dtypes\")\n+    .Input(\"plasma_object_id: string\")\n+    .Attr(\"dtypes: list(type)\")\n+    .Attr(\"plasma_store_socket_name: string\")\n+    .Attr(\"plasma_manager_socket_name: string\");\n+\n+REGISTER_KERNEL_BUILDER(Name(\"TensorToPlasma\").Device(DEVICE_CPU),\n+                        TensorToPlasmaOp<CPUDevice>);\n+REGISTER_KERNEL_BUILDER(Name(\"TensorToPlasma\").Device(DEVICE_GPU),\n \n Review comment:\n   Please wrap these two GPU registrations with GOOGLE_CUDA guard\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T22:35:17.884+0000",
                    "updated": "2018-05-20T22:35:17.884+0000",
                    "started": "2018-05-20T22:35:17.884+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103778",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103779",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "concretevitamin commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472511\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n \n Review comment:\n   can remove\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T22:35:17.947+0000",
                    "updated": "2018-05-20T22:35:17.947+0000",
                    "started": "2018-05-20T22:35:17.946+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103779",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103780",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "concretevitamin commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472496\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n+    // LOG(INFO) << \"Output TensorShape: \" << shape.DebugString();\n+    // LOG(INFO) << \"size_in_bytes of the plasma object: \" << size_in_bytes;\n+\n+    const float* plasma_data = reinterpret_cast<const float*>(object_buffer.data->data());\n+    // for (int i = 0; i < size_in_bytes / sizeof(float); ++i) {\n+    //   LOG(INFO) << plasma_data[i];\n+    // }\n+\n+    Tensor* output_tensor = nullptr;\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, shape, &output_tensor),\n+                         done);\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      std::memcpy(output_tensor->flat<float>().data(),\n+                  reinterpret_cast<const float*>(object_buffer.data->data()),\n+                  size_in_bytes);\n+      done();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      {\n+        mutex_lock l(h2d_stream_mu);\n+        if (h2d_stream == nullptr) {\n+          h2d_stream = new Stream(stream_executor);\n+          CHECK(h2d_stream->Init().ok());\n+        }\n+      }\n+\n+      // #define PLASMA_CLIENT_DOES_NOT_EXIST 3\n+      // #define PLASMA_CLIENT_LOCAL 0\n+\n+      //       // Launch async fetch.\n+      //       {\n+      //         mutex_lock lock(mu_);\n+      //         LOG(INFO) << \"Launching Fetch()\";\n+      //         ARROW_CHECK_OK(client_.Fetch(/*num_object_ids=*/1,\n+      //         &object_id)); LOG(INFO) << \"Done launching Fetch()\";\n+      //       }\n+\n+      //       std::function<void()> WaitForTransfer = [context, this,\n+      //       object_id,\n+      //                                                WaitForTransfer]() ->\n+      //                                                void {\n+      //         int object_status;\n+      //         {\n+      //           mutex_lock lock(mu_);\n+      //           LOG(INFO) << \"Launching Info()\";\n+      //           ARROW_CHECK_OK(client_.Info(object_id, &object_status));\n+      //           LOG(INFO) << \"Done launching Info()\";\n+      //         }\n+      //         CHECK(object_status != PLASMA_CLIENT_DOES_NOT_EXIST);\n+\n+      //         if (object_status == PLASMA_CLIENT_LOCAL) {\n+      //           LOG(INFO) << \"Object is local!\";\n+      //           CHECK(0);\n+      //           // TODO(zongheng):  do real work;\n+      //         } else {\n+\n+      //           LOG(INFO) << \"Object still not local, status: \" <<\n+      //           object_status; std::function<void()> new_func =\n+      //           WaitForTransfer; context->device()\n+      //               ->tensorflow_gpu_device_info()\n+      //               ->event_mgr->ThenExecute(h2d_stream, new_func);\n+      //         }\n+      //       };\n+\n+      //       context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+      //           h2d_stream, WaitForTransfer);\n+\n+      // Important.  See note in T2P op.\n+      // We don't check the return status since the host memory might've been\n+      // already registered (e.g., the TensorToPlasmaOp might've been run).\n+      stream_executor->HostMemoryRegister(\n+          const_cast<void*>(static_cast<const void*>(plasma_data)),\n+          static_cast<uint64>(size_in_bytes));\n+\n+      perftools::gputools::DeviceMemoryBase wrapped_dst(\n+          static_cast<void*>(output_tensor->flat<float>().data()));\n+      const bool success =\n+          h2d_stream\n+              ->ThenMemcpy(&wrapped_dst, static_cast<const void*>(plasma_data),\n+                           static_cast<uint64>(size_in_bytes))\n+              .ok();\n+      OP_REQUIRES_ASYNC(context, success,\n+                        errors::Internal(\"H2D memcpy failed to be enqueued.\"), done);\n+\n+      // Without this sync the main compute stream might proceed to use the\n+      // Tensor buffer, but its contents might still be in-flight from our\n+      // h2d_stream.\n+      CHECK(orig_stream->ThenWaitFor(h2d_stream).ok());\n+\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          h2d_stream, std::move(done));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+};\n+\n+REGISTER_OP(\"TensorToPlasma\")\n+    .Input(\"input_tensor: dtypes\")\n+    .Input(\"plasma_object_id: string\")\n+    .Attr(\"dtypes: list(type)\")\n+    .Attr(\"plasma_store_socket_name: string\")\n+    .Attr(\"plasma_manager_socket_name: string\");\n+\n+REGISTER_KERNEL_BUILDER(Name(\"TensorToPlasma\").Device(DEVICE_CPU),\n+                        TensorToPlasmaOp<CPUDevice>);\n+REGISTER_KERNEL_BUILDER(Name(\"TensorToPlasma\").Device(DEVICE_GPU),\n+                        TensorToPlasmaOp<GPUDevice>);\n+\n+REGISTER_OP(\"PlasmaToTensor\")\n+    .Input(\"plasma_object_id: string\")\n+    .Output(\"tensor: float\")\n+    .Attr(\"plasma_store_socket_name: string\")\n+    .Attr(\"plasma_manager_socket_name: string\");\n+\n+REGISTER_KERNEL_BUILDER(Name(\"PlasmaToTensor\").Device(DEVICE_CPU),\n+                        PlasmaToTensorOp<CPUDevice>);\n+REGISTER_KERNEL_BUILDER(Name(\"PlasmaToTensor\").Device(DEVICE_GPU),\n \n Review comment:\n   ditto\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T22:35:17.954+0000",
                    "updated": "2018-05-20T22:35:17.954+0000",
                    "started": "2018-05-20T22:35:17.954+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103780",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103781",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "concretevitamin commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472505\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n+    // LOG(INFO) << \"Output TensorShape: \" << shape.DebugString();\n \n Review comment:\n   all commented out code can be removed\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T22:35:17.971+0000",
                    "updated": "2018-05-20T22:35:17.971+0000",
                    "started": "2018-05-20T22:35:17.970+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103781",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103783",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "robertnishihara commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472695\n \n \n\n ##########\n File path: python/pyarrow/tests/test_plasma_tf_op.py\n ##########\n @@ -0,0 +1,78 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+import numpy as np\n+import pytest\n+\n+\n+@pytest.mark.plasma\n+def test_plasma_tf_op(use_gpu=False):\n+    import pyarrow.plasma as plasma\n+\n+    if not plasma.has_tf_plasma_op:\n+        return\n+\n+    try:\n+        import tensorflow as tf\n+    except ImportError:\n+        pytest.skip(\"TensorFlow not installed\")\n+\n+    with plasma.start_plasma_store(10**8) as (plasma_store_name, p):\n+        FORCE_DEVICE = '/gpu' if use_gpu else '/cpu'\n+\n+        object_id = np.random.bytes(20)\n+\n+        data = np.random.randn(3, 244, 244).astype(np.float32)\n+        ones = np.ones((3, 244, 244)).astype(np.float32)\n+\n+        sess = tf.Session(config=tf.ConfigProto(\n+            allow_soft_placement=True, log_device_placement=True))\n+\n+        def ToPlasma():\n+            data_tensor = tf.constant(data)\n+            ones_tensor = tf.constant(ones)\n+            return plasma.tf_plasma_op.tensor_to_plasma(\n+                [data_tensor, ones_tensor],\n+                object_id,\n+                plasma_store_socket_name=plasma_store_name,\n+                plasma_manager_socket_name=\"\")\n+\n+        def FromPlasma():\n+            return plasma.tf_plasma_op.plasma_to_tensor(\n+                object_id,\n+                plasma_store_socket_name=plasma_store_name,\n+                plasma_manager_socket_name=\"\")\n+\n+        with tf.device(FORCE_DEVICE):\n+            to_plasma = ToPlasma()\n+            from_plasma = FromPlasma()\n+\n+        z = from_plasma + 1\n+\n+        sess.run(to_plasma)\n+        # NOTE(zongheng): currently it returns a flat 1D tensor.\n+        # So reshape manually.\n \n Review comment:\n   If we serialize data as Arrow tensors, it should be easy to get rid of this limitation.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:04:05.442+0000",
                    "updated": "2018-05-20T23:04:05.442+0000",
                    "started": "2018-05-20T23:04:05.442+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103783",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103784",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "robertnishihara commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472662\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n \n Review comment:\n   Please document all of the private fields. In particular, `mu_` guards more than just `client_`, right?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:04:05.444+0000",
                    "updated": "2018-05-20T23:04:05.444+0000",
                    "started": "2018-05-20T23:04:05.443+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103784",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103785",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "robertnishihara commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189473200\n \n \n\n ##########\n File path: python/pyarrow/tests/test_plasma_tf_op.py\n ##########\n @@ -0,0 +1,78 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+import numpy as np\n+import pytest\n+\n+\n+@pytest.mark.plasma\n+def test_plasma_tf_op(use_gpu=False):\n+    import pyarrow.plasma as plasma\n+\n+    if not plasma.has_tf_plasma_op:\n+        return\n+\n+    try:\n+        import tensorflow as tf\n+    except ImportError:\n+        pytest.skip(\"TensorFlow not installed\")\n+\n+    with plasma.start_plasma_store(10**8) as (plasma_store_name, p):\n+        FORCE_DEVICE = '/gpu' if use_gpu else '/cpu'\n+\n+        object_id = np.random.bytes(20)\n+\n+        data = np.random.randn(3, 244, 244).astype(np.float32)\n+        ones = np.ones((3, 244, 244)).astype(np.float32)\n+\n+        sess = tf.Session(config=tf.ConfigProto(\n+            allow_soft_placement=True, log_device_placement=True))\n+\n+        def ToPlasma():\n+            data_tensor = tf.constant(data)\n+            ones_tensor = tf.constant(ones)\n+            return plasma.tf_plasma_op.tensor_to_plasma(\n+                [data_tensor, ones_tensor],\n+                object_id,\n+                plasma_store_socket_name=plasma_store_name,\n+                plasma_manager_socket_name=\"\")\n+\n+        def FromPlasma():\n+            return plasma.tf_plasma_op.plasma_to_tensor(\n \n Review comment:\n   Can you add a page documenting the API and giving code examples for how to use it?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:04:05.454+0000",
                    "updated": "2018-05-20T23:04:05.454+0000",
                    "started": "2018-05-20T23:04:05.453+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103785",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103786",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "robertnishihara commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189473208\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n+    // LOG(INFO) << \"Output TensorShape: \" << shape.DebugString();\n+    // LOG(INFO) << \"size_in_bytes of the plasma object: \" << size_in_bytes;\n \n Review comment:\n   Let's remove all the dead code.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:04:05.458+0000",
                    "updated": "2018-05-20T23:04:05.458+0000",
                    "started": "2018-05-20T23:04:05.457+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103786",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103787",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "robertnishihara commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189472943\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n \n Review comment:\n   Let's serialize data as Arrow tensors.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:04:05.458+0000",
                    "updated": "2018-05-20T23:04:05.458+0000",
                    "started": "2018-05-20T23:04:05.458+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103787",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103788",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "robertnishihara commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189473244\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n \n Review comment:\n   We should make sure we support different dtypes and preserve the tensor type.\r\n   \r\n   This code looks like it might fail for tensors whose dtype is smaller than 4 bytes.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:04:05.463+0000",
                    "updated": "2018-05-20T23:04:05.463+0000",
                    "started": "2018-05-20T23:04:05.462+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103788",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103789",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "robertnishihara commented on issue #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#issuecomment-390520412\n \n \n   It'd be nice to ship GPU and CPU versions of the op with pyarrow. Is that feasible?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:17:12.901+0000",
                    "updated": "2018-05-20T23:17:12.901+0000",
                    "started": "2018-05-20T23:17:12.900+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103789",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103791",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pcmoritz commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189474774\n \n \n\n ##########\n File path: cpp/src/plasma/CMakeLists.txt\n ##########\n @@ -123,6 +129,18 @@ if (\"${COMPILER_FAMILY}\" STREQUAL \"gcc\")\n     \" -Wno-conversion\")\n endif()\n \n+if (PLASMA_BUILD_TENSORFLOW_OP)\n+  if (NOT TensorFlow_FOUND)\n+    message(FATAL_ERROR \"Need TensorFlow to build the TensorFlow op\")\n+  endif()\n+  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=0\")\n+  include_directories(${TensorFlow_INCLUDE_DIRS})\n+  include_directories(${TensorFlow_INCLUDE_DIRS}/external/nsync/public)\n \n Review comment:\n   This is only needed for python 2.7 and yes, it's not included in the tf.sysconfig output. See https://github.com/sadeepj/crfasrnn_keras/issues/19 which I'm putting in.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:54:44.069+0000",
                    "updated": "2018-05-20T23:54:44.069+0000",
                    "started": "2018-05-20T23:54:44.068+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103791",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103792",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pcmoritz commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189474797\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n+    // LOG(INFO) << \"Output TensorShape: \" << shape.DebugString();\n+    // LOG(INFO) << \"size_in_bytes of the plasma object: \" << size_in_bytes;\n+\n+    const float* plasma_data = reinterpret_cast<const float*>(object_buffer.data->data());\n+    // for (int i = 0; i < size_in_bytes / sizeof(float); ++i) {\n+    //   LOG(INFO) << plasma_data[i];\n+    // }\n+\n+    Tensor* output_tensor = nullptr;\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, shape, &output_tensor),\n+                         done);\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      std::memcpy(output_tensor->flat<float>().data(),\n+                  reinterpret_cast<const float*>(object_buffer.data->data()),\n+                  size_in_bytes);\n+      done();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      {\n+        mutex_lock l(h2d_stream_mu);\n+        if (h2d_stream == nullptr) {\n+          h2d_stream = new Stream(stream_executor);\n+          CHECK(h2d_stream->Init().ok());\n+        }\n+      }\n+\n+      // #define PLASMA_CLIENT_DOES_NOT_EXIST 3\n \n Review comment:\n   doing this\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:55:35.176+0000",
                    "updated": "2018-05-20T23:55:35.176+0000",
                    "started": "2018-05-20T23:55:35.176+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103792",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103793",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pcmoritz commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189474830\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n+    // LOG(INFO) << \"Output TensorShape: \" << shape.DebugString();\n+    // LOG(INFO) << \"size_in_bytes of the plasma object: \" << size_in_bytes;\n+\n+    const float* plasma_data = reinterpret_cast<const float*>(object_buffer.data->data());\n+    // for (int i = 0; i < size_in_bytes / sizeof(float); ++i) {\n+    //   LOG(INFO) << plasma_data[i];\n+    // }\n+\n+    Tensor* output_tensor = nullptr;\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, shape, &output_tensor),\n+                         done);\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      std::memcpy(output_tensor->flat<float>().data(),\n+                  reinterpret_cast<const float*>(object_buffer.data->data()),\n+                  size_in_bytes);\n+      done();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      {\n+        mutex_lock l(h2d_stream_mu);\n+        if (h2d_stream == nullptr) {\n+          h2d_stream = new Stream(stream_executor);\n+          CHECK(h2d_stream->Init().ok());\n+        }\n+      }\n+\n+      // #define PLASMA_CLIENT_DOES_NOT_EXIST 3\n+      // #define PLASMA_CLIENT_LOCAL 0\n+\n+      //       // Launch async fetch.\n+      //       {\n+      //         mutex_lock lock(mu_);\n+      //         LOG(INFO) << \"Launching Fetch()\";\n+      //         ARROW_CHECK_OK(client_.Fetch(/*num_object_ids=*/1,\n+      //         &object_id)); LOG(INFO) << \"Done launching Fetch()\";\n+      //       }\n+\n+      //       std::function<void()> WaitForTransfer = [context, this,\n+      //       object_id,\n+      //                                                WaitForTransfer]() ->\n+      //                                                void {\n+      //         int object_status;\n+      //         {\n+      //           mutex_lock lock(mu_);\n+      //           LOG(INFO) << \"Launching Info()\";\n+      //           ARROW_CHECK_OK(client_.Info(object_id, &object_status));\n+      //           LOG(INFO) << \"Done launching Info()\";\n+      //         }\n+      //         CHECK(object_status != PLASMA_CLIENT_DOES_NOT_EXIST);\n+\n+      //         if (object_status == PLASMA_CLIENT_LOCAL) {\n+      //           LOG(INFO) << \"Object is local!\";\n+      //           CHECK(0);\n+      //           // TODO(zongheng):  do real work;\n+      //         } else {\n+\n+      //           LOG(INFO) << \"Object still not local, status: \" <<\n+      //           object_status; std::function<void()> new_func =\n+      //           WaitForTransfer; context->device()\n+      //               ->tensorflow_gpu_device_info()\n+      //               ->event_mgr->ThenExecute(h2d_stream, new_func);\n+      //         }\n+      //       };\n+\n+      //       context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+      //           h2d_stream, WaitForTransfer);\n+\n+      // Important.  See note in T2P op.\n+      // We don't check the return status since the host memory might've been\n+      // already registered (e.g., the TensorToPlasmaOp might've been run).\n+      stream_executor->HostMemoryRegister(\n+          const_cast<void*>(static_cast<const void*>(plasma_data)),\n+          static_cast<uint64>(size_in_bytes));\n+\n+      perftools::gputools::DeviceMemoryBase wrapped_dst(\n+          static_cast<void*>(output_tensor->flat<float>().data()));\n+      const bool success =\n+          h2d_stream\n+              ->ThenMemcpy(&wrapped_dst, static_cast<const void*>(plasma_data),\n+                           static_cast<uint64>(size_in_bytes))\n+              .ok();\n+      OP_REQUIRES_ASYNC(context, success,\n+                        errors::Internal(\"H2D memcpy failed to be enqueued.\"), done);\n+\n+      // Without this sync the main compute stream might proceed to use the\n+      // Tensor buffer, but its contents might still be in-flight from our\n+      // h2d_stream.\n+      CHECK(orig_stream->ThenWaitFor(h2d_stream).ok());\n+\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          h2d_stream, std::move(done));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+};\n+\n+REGISTER_OP(\"TensorToPlasma\")\n+    .Input(\"input_tensor: dtypes\")\n+    .Input(\"plasma_object_id: string\")\n+    .Attr(\"dtypes: list(type)\")\n+    .Attr(\"plasma_store_socket_name: string\")\n+    .Attr(\"plasma_manager_socket_name: string\");\n+\n+REGISTER_KERNEL_BUILDER(Name(\"TensorToPlasma\").Device(DEVICE_CPU),\n+                        TensorToPlasmaOp<CPUDevice>);\n+REGISTER_KERNEL_BUILDER(Name(\"TensorToPlasma\").Device(DEVICE_GPU),\n \n Review comment:\n   doing this\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:56:33.044+0000",
                    "updated": "2018-05-20T23:56:33.044+0000",
                    "started": "2018-05-20T23:56:33.043+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103793",
                    "issueId": "13112779"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/worklog/103796",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pcmoritz commented on a change in pull request #2046: ARROW-1744: [Plasma] Provide TensorFlow operator to transfer Tensors between Plasma and TensorFlow\nURL: https://github.com/apache/arrow/pull/2046#discussion_r189474830\n \n \n\n ##########\n File path: cpp/src/plasma/tf_plasma_op.cc\n ##########\n @@ -0,0 +1,383 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"plasma/client.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n+#endif\n+#include \"tensorflow/core/framework/device_base.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/mutex.h\"\n+#ifdef GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#endif\n+#include \"tensorflow/stream_executor/device_memory.h\"\n+#include \"tensorflow/stream_executor/event.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+using namespace tensorflow;  // NOLINT\n+\n+using ArrowStatus = arrow::Status;\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+static plasma::PlasmaClient client_;\n+static bool connected_ = false;\n+static mutex mu_;\n+\n+using Event = perftools::gputools::Event;\n+using Stream = perftools::gputools::Stream;\n+\n+// NOTE(zongheng): for some reason using unique_ptr or shared_ptr results in\n+// CUDA_ERROR_DEINITIALIZED on program exit.  I suspect this is because the\n+// static object's dtor gets called *after* TensorFlow's own CUDA cleanup.\n+// Instead, we use a raw pointer here and manually clean up in the Ops' dtors.\n+static Stream* d2h_stream = nullptr;\n+static mutex d2h_stream_mu;\n+\n+// TODO(zongheng): CPU kernels' std::memcpy might be able to be sped up by\n+// parallelization.\n+\n+// Put:  tf.Tensor -> plasma.\n+template <typename Device>\n+class TensorToPlasmaOp : public AsyncOpKernel {\n+ public:\n+  explicit TensorToPlasmaOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~TensorToPlasmaOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(d2h_stream_mu);\n+      if (d2h_stream != nullptr) {\n+        delete d2h_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const int num_inputs = context->num_inputs();\n+    OP_REQUIRES_ASYNC(\n+        context, num_inputs >= 2,\n+        errors::InvalidArgument(\"Input should have at least 1 tensor and 1 object_id\"),\n+        done);\n+    const int num_tensors = num_inputs - 1;\n+\n+    std::vector<size_t> offsets;\n+    offsets.reserve(num_tensors + 1);\n+    offsets.push_back(0);\n+    size_t total_bytes = 0;\n+    for (int i = 0; i < num_tensors; ++i) {\n+      const size_t s = context->input(i).TotalBytes();\n+      CHECK_EQ(s, context->input(i).NumElements() * sizeof(float));\n+      CHECK_GT(s, 0);\n+      total_bytes += s;\n+      offsets.push_back(total_bytes);\n+    }\n+\n+    const Tensor& plasma_object_id = context->input(num_inputs - 1);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    std::shared_ptr<Buffer> data_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Create(object_id, static_cast<int64_t>(total_bytes),\n+                                    /*metadata=*/nullptr, 0, &data_buffer));\n+    }\n+\n+    float* data = reinterpret_cast<float*>(data_buffer->mutable_data());\n+\n+    auto wrapped_callback = [this, context, done, data_buffer, object_id]() {\n+      {\n+        mutex_lock lock(mu_);\n+        ARROW_CHECK_OK(client_.Seal(object_id));\n+      }\n+      context->SetStatus(tensorflow::Status::OK());\n+      done();\n+    };\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        std::memcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                    input_tensor.flat<float>().data(),\n+                    static_cast<uint64>(offsets[i + 1] - offsets[i]));\n+      }\n+      wrapped_callback();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      // NOTE(zongheng): this is critical of getting good performance out of D2H\n+      // async memcpy.  Under the hood it performs cuMemHostRegister(), see:\n+      // http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gf0a9fe11544326dabd743b7aa6b54223\n+      CHECK(stream_executor->HostMemoryRegister(static_cast<void*>(data),\n+                                                static_cast<uint64>(total_bytes)));\n+\n+      {\n+        mutex_lock l(d2h_stream_mu);\n+        if (d2h_stream == nullptr) {\n+          d2h_stream = new Stream(stream_executor);\n+          CHECK(d2h_stream->Init().ok());\n+        }\n+      }\n+\n+      // Needed to make sure the input buffers have been computed.\n+      // NOTE(ekl): this is unnecessary when the op is behind a NCCL allreduce already\n+      CHECK(d2h_stream->ThenWaitFor(orig_stream).ok());\n+\n+      for (int i = 0; i < num_tensors; ++i) {\n+        const auto& input_tensor = context->input(i);\n+        float* input_buffer = const_cast<float*>(input_tensor.flat<float>().data());\n+        perftools::gputools::DeviceMemoryBase wrapped_src(\n+            static_cast<void*>(input_buffer));\n+        const bool success =\n+            d2h_stream\n+                ->ThenMemcpy(static_cast<void*>(data + offsets[i] / sizeof(float)),\n+                             wrapped_src,\n+                             static_cast<uint64>(offsets[i + 1] - offsets[i]))\n+                .ok();\n+        OP_REQUIRES_ASYNC(context, success,\n+                          errors::Internal(\"D2H memcpy failed to be enqueued.\"), done);\n+      }\n+      // TODO(zongheng): does std::move() give better performance?\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          d2h_stream, std::move(wrapped_callback));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+\n+  mutex mu_;\n+  bool connected_ = false;\n+  plasma::PlasmaClient client_ GUARDED_BY(mu_);\n+};\n+\n+static Stream* h2d_stream = nullptr;\n+static mutex h2d_stream_mu;\n+\n+// Get:  plasma -> tf.Tensor.\n+template <typename Device>\n+class PlasmaToTensorOp : public AsyncOpKernel {\n+ public:\n+  explicit PlasmaToTensorOp(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_store_socket_name\",\n+                                             &plasma_store_socket_name_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"plasma_manager_socket_name\",\n+                                             &plasma_manager_socket_name_));\n+    mutex_lock lock(mu_);\n+    if (!connected_) {\n+      VLOG(1) << \"Connecting to Plasma...\";\n+      ARROW_CHECK_OK(client_.Connect(plasma_store_socket_name_,\n+                                     plasma_manager_socket_name_,\n+                                     PLASMA_DEFAULT_RELEASE_DELAY));\n+      VLOG(1) << \"Connected!\";\n+      connected_ = true;\n+    }\n+  }\n+\n+  ~PlasmaToTensorOp() override {\n+    {\n+      mutex_lock lock(mu_);\n+      ARROW_CHECK_OK(client_.Disconnect());\n+    }\n+    {\n+      mutex_lock lock(h2d_stream_mu);\n+      if (h2d_stream != nullptr) {\n+        delete h2d_stream;\n+      }\n+    }\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) override {\n+    const Tensor& plasma_object_id = context->input(0);\n+    CHECK_EQ(plasma_object_id.NumElements(), 1);\n+    const string& plasma_object_id_str = plasma_object_id.flat<std::string>()(0);\n+\n+    VLOG(1) << \"plasma_object_id_str: '\" << plasma_object_id_str << \"'\";\n+    const plasma::ObjectID object_id =\n+        plasma::ObjectID::from_binary(plasma_object_id_str);\n+\n+    plasma::ObjectBuffer object_buffer;\n+    {\n+      mutex_lock lock(mu_);\n+      // NOTE(zongheng): this is a blocking call.  We might want to (1) make\n+      // Plasma asynchronous, (2) launch a thread / event here ourselves, or\n+      // something like that...\n+      ARROW_CHECK_OK(client_.Get(&object_id, /*num_objects=*/1,\n+                                 /*timeout_ms=*/-1, &object_buffer));\n+    }\n+\n+    const int64_t size_in_bytes = object_buffer.data->size();\n+    TensorShape shape({size_in_bytes / sizeof(float)});\n+    // LOG(INFO) << \"Output TensorShape: \" << shape.DebugString();\n+    // LOG(INFO) << \"size_in_bytes of the plasma object: \" << size_in_bytes;\n+\n+    const float* plasma_data = reinterpret_cast<const float*>(object_buffer.data->data());\n+    // for (int i = 0; i < size_in_bytes / sizeof(float); ++i) {\n+    //   LOG(INFO) << plasma_data[i];\n+    // }\n+\n+    Tensor* output_tensor = nullptr;\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, shape, &output_tensor),\n+                         done);\n+\n+    if (std::is_same<Device, CPUDevice>::value) {\n+      std::memcpy(output_tensor->flat<float>().data(),\n+                  reinterpret_cast<const float*>(object_buffer.data->data()),\n+                  size_in_bytes);\n+      done();\n+    } else {\n+#ifdef GOOGLE_CUDA\n+      auto orig_stream = context->op_device_context()->stream();\n+      OP_REQUIRES_ASYNC(context, orig_stream != nullptr,\n+                        errors::Internal(\"No GPU stream available.\"), done);\n+      auto stream_executor = orig_stream->parent();\n+\n+      {\n+        mutex_lock l(h2d_stream_mu);\n+        if (h2d_stream == nullptr) {\n+          h2d_stream = new Stream(stream_executor);\n+          CHECK(h2d_stream->Init().ok());\n+        }\n+      }\n+\n+      // #define PLASMA_CLIENT_DOES_NOT_EXIST 3\n+      // #define PLASMA_CLIENT_LOCAL 0\n+\n+      //       // Launch async fetch.\n+      //       {\n+      //         mutex_lock lock(mu_);\n+      //         LOG(INFO) << \"Launching Fetch()\";\n+      //         ARROW_CHECK_OK(client_.Fetch(/*num_object_ids=*/1,\n+      //         &object_id)); LOG(INFO) << \"Done launching Fetch()\";\n+      //       }\n+\n+      //       std::function<void()> WaitForTransfer = [context, this,\n+      //       object_id,\n+      //                                                WaitForTransfer]() ->\n+      //                                                void {\n+      //         int object_status;\n+      //         {\n+      //           mutex_lock lock(mu_);\n+      //           LOG(INFO) << \"Launching Info()\";\n+      //           ARROW_CHECK_OK(client_.Info(object_id, &object_status));\n+      //           LOG(INFO) << \"Done launching Info()\";\n+      //         }\n+      //         CHECK(object_status != PLASMA_CLIENT_DOES_NOT_EXIST);\n+\n+      //         if (object_status == PLASMA_CLIENT_LOCAL) {\n+      //           LOG(INFO) << \"Object is local!\";\n+      //           CHECK(0);\n+      //           // TODO(zongheng):  do real work;\n+      //         } else {\n+\n+      //           LOG(INFO) << \"Object still not local, status: \" <<\n+      //           object_status; std::function<void()> new_func =\n+      //           WaitForTransfer; context->device()\n+      //               ->tensorflow_gpu_device_info()\n+      //               ->event_mgr->ThenExecute(h2d_stream, new_func);\n+      //         }\n+      //       };\n+\n+      //       context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+      //           h2d_stream, WaitForTransfer);\n+\n+      // Important.  See note in T2P op.\n+      // We don't check the return status since the host memory might've been\n+      // already registered (e.g., the TensorToPlasmaOp might've been run).\n+      stream_executor->HostMemoryRegister(\n+          const_cast<void*>(static_cast<const void*>(plasma_data)),\n+          static_cast<uint64>(size_in_bytes));\n+\n+      perftools::gputools::DeviceMemoryBase wrapped_dst(\n+          static_cast<void*>(output_tensor->flat<float>().data()));\n+      const bool success =\n+          h2d_stream\n+              ->ThenMemcpy(&wrapped_dst, static_cast<const void*>(plasma_data),\n+                           static_cast<uint64>(size_in_bytes))\n+              .ok();\n+      OP_REQUIRES_ASYNC(context, success,\n+                        errors::Internal(\"H2D memcpy failed to be enqueued.\"), done);\n+\n+      // Without this sync the main compute stream might proceed to use the\n+      // Tensor buffer, but its contents might still be in-flight from our\n+      // h2d_stream.\n+      CHECK(orig_stream->ThenWaitFor(h2d_stream).ok());\n+\n+      context->device()->tensorflow_gpu_device_info()->event_mgr->ThenExecute(\n+          h2d_stream, std::move(done));\n+#endif\n+    }\n+  }\n+\n+ private:\n+  std::string plasma_store_socket_name_;\n+  std::string plasma_manager_socket_name_;\n+};\n+\n+REGISTER_OP(\"TensorToPlasma\")\n+    .Input(\"input_tensor: dtypes\")\n+    .Input(\"plasma_object_id: string\")\n+    .Attr(\"dtypes: list(type)\")\n+    .Attr(\"plasma_store_socket_name: string\")\n+    .Attr(\"plasma_manager_socket_name: string\");\n+\n+REGISTER_KERNEL_BUILDER(Name(\"TensorToPlasma\").Device(DEVICE_CPU),\n+                        TensorToPlasmaOp<CPUDevice>);\n+REGISTER_KERNEL_BUILDER(Name(\"TensorToPlasma\").Device(DEVICE_GPU),\n \n Review comment:\n   doing this now\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-05-20T23:56:39.340+0000",
                    "updated": "2018-05-20T23:56:39.340+0000",
                    "started": "2018-05-20T23:56:39.339+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "103796",
                    "issueId": "13112779"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 63000,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@50ebc365[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@41c5a8b0[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@716252ff[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@1d0ef44b[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2a063c54[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@37b34740[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@54250ee3[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@3e631480[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@251c6b84[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@8b8b120[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7416e4c2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@52f739d4[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 63000,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Aug 28 16:25:02 UTC 2018",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2018-07-17T07:03:32.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1744/watchers",
            "watchCount": 7,
            "isWatching": false
        },
        "created": "2017-10-28T08:07:40.000+0000",
        "updated": "2019-01-22T09:34:12.000+0000",
        "timeoriginalestimate": null,
        "description": "see https://www.tensorflow.org/extend/adding_an_op",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "17.5h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 63000
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Plasma] Provide TensorFlow operator to read tensors from plasma",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16274849",
                    "id": "16274849",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=holden",
                        "name": "holden",
                        "key": "holdenk",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=holdenk&avatarId=19457",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=holdenk&avatarId=19457",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=holdenk&avatarId=19457",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=holdenk&avatarId=19457"
                        },
                        "displayName": "Holden Karau",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Are you working on this [~pcmoritz]?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=holden",
                        "name": "holden",
                        "key": "holdenk",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=holdenk&avatarId=19457",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=holdenk&avatarId=19457",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=holdenk&avatarId=19457",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=holdenk&avatarId=19457"
                        },
                        "displayName": "Holden Karau",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2017-12-01T19:49:55.766+0000",
                    "updated": "2017-12-01T19:49:55.766+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16274905",
                    "id": "16274905",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "pcmoritz opened a new pull request #1379: ARROW-1744: [WIP] Add plasma tensorflow op\nURL: https://github.com/apache/arrow/pull/1379\n \n \n   Before this can be merged, it needs some profiling and comparison to other ways of loading data in TensorFlow.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-01T20:15:28.198+0000",
                    "updated": "2017-12-01T20:15:28.198+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16274910",
                    "id": "16274910",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=pcmoritz",
                        "name": "pcmoritz",
                        "key": "pcmoritz",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Philipp Moritz",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "[~holdenk] I submitted a WIP version of what I have so far; it works but needs some profiling. Unfortunately I won't have time to work on it before next week, feel free to try it out and let me know what you think/keep developing it in the meantime.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=pcmoritz",
                        "name": "pcmoritz",
                        "key": "pcmoritz",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Philipp Moritz",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-01T20:18:40.042+0000",
                    "updated": "2017-12-01T20:18:40.042+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16287530",
                    "id": "16287530",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "holdenk commented on a change in pull request #1379: ARROW-1744: [WIP] Add plasma tensorflow op\nURL: https://github.com/apache/arrow/pull/1379#discussion_r156349402\n \n \n\n ##########\n File path: cpp/src/plasma/tf/test.py\n ##########\n @@ -0,0 +1,46 @@\n+import numpy as np\n+import pyarrow as pa\n+import pyarrow.plasma as plasma\n+import tensorflow as tf\n+\n+import time\n+\n+zero_out_module = tf.load_op_library('./plasma_op.so')\n+\n+client = plasma.connect(\"/tmp/plasma\", \"\", 64)\n \n Review comment:\n   Minor comment, but maybe do a mktemp type thing here instead? I know some folks run tests in parallel on the same machine.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-12T12:29:50.758+0000",
                    "updated": "2017-12-12T12:29:50.758+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16287531",
                    "id": "16287531",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "holdenk commented on a change in pull request #1379: ARROW-1744: [WIP] Add plasma tensorflow op\nURL: https://github.com/apache/arrow/pull/1379#discussion_r156349604\n \n \n\n ##########\n File path: cpp/src/plasma/tf/plasma_op.cc\n ##########\n @@ -0,0 +1,70 @@\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/shape_inference.h\"\n+\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/ipc/reader.h\"\n+#include \"arrow/tensor.h\"\n+#include \"plasma/client.h\"\n+\n+using namespace tensorflow;\n+\n+REGISTER_OP(\"PlasmaData\")\n+    .Input(\"object_id: string\")\n+    .Output(\"output: float32\")\n+    .Attr(\"socket: string\");\n+\n+// TODO(pcm): Make this zero-copy if possible\n+\n+class PlasmaDataOp : public OpKernel {\n+ public:\n+  explicit PlasmaDataOp(OpKernelConstruction* context) : OpKernel(context) {\n+    std::cout << \"called constructor\" << std::endl;\n+    std::string socket;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"socket\", &socket));\n+    // Connect to plasma\n+    ARROW_CHECK_OK(client_.Connect(socket, \"\", PLASMA_DEFAULT_RELEASE_DELAY));\n+    std::cout << \"constructor finished\" << std::endl;\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    // Grab the input tensor\n+    const Tensor& input_tensor = context->input(0);\n+    auto input = input_tensor.flat<string>();\n+\n+    // Get the object\n+    plasma::ObjectID object_id = plasma::ObjectID::from_binary(input(0));\n+    plasma::ObjectBuffer object_buffer;\n+    ARROW_CHECK_OK(client_.Get(&object_id, 1, -1, &object_buffer));\n+\n+    // Get the tensor\n+    std::shared_ptr<arrow::Tensor> result;\n+    arrow::io::BufferReader reader(object_buffer.data, object_buffer.data_size);\n+    int64_t offset;\n+    ARROW_CHECK_OK(reader.Tell(&offset));\n+    ARROW_CHECK_OK(arrow::ipc::ReadTensor(0, &reader, &result));\n+\n+    std::cout << \"shape is\" << result->shape()[0] << \" , \" << result->shape()[1]\n+              << std::endl;\n+\n+    // Create an output tensor\n+    TensorShape shape(result->shape());\n+    Tensor* output_tensor = NULL;\n+    OP_REQUIRES_OK(context, context->allocate_output(0, shape, &output_tensor));\n+    auto output_flat = output_tensor->flat<float>();\n+\n+    // Set all but the first element of the output tensor to 0.\n \n Review comment:\n   Is this comment perhaps about an older version of the code?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-12T12:29:50.840+0000",
                    "updated": "2017-12-12T12:29:50.840+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16383630",
                    "id": "16383630",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on issue #1379: ARROW-1744: [WIP] Add plasma tensorflow op\nURL: https://github.com/apache/arrow/pull/1379#issuecomment-369934686\n \n \n   @pcmoritz Any progress on this topic here?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2018-03-02T14:26:37.691+0000",
                    "updated": "2018-03-02T14:26:37.691+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16538719",
                    "id": "16538719",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "I moved this to 0.11. If it gets merged before 0.10, that's fine, but since Ray (where this would be most immediately used) does not use Arrow's versioned releases they aren't blocking on getting this into 0.10",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-07-10T14:54:56.361+0000",
                    "updated": "2018-07-10T14:54:56.361+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16546108",
                    "id": "16546108",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=robertnishihara",
                        "name": "robertnishihara",
                        "key": "robertnishihara",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Robert Nishihara",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Issue resolved by pull request 2104\n[https://github.com/apache/arrow/pull/2104]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=robertnishihara",
                        "name": "robertnishihara",
                        "key": "robertnishihara",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Robert Nishihara",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2018-07-17T07:03:32.129+0000",
                    "updated": "2018-07-17T07:03:32.129+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16583285",
                    "id": "16583285",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bhulette",
                        "name": "bhulette",
                        "key": "bhulette",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Brian Hulette",
                        "active": true,
                        "timeZone": "America/Vancouver"
                    },
                    "body": "It looks like this was actually merged for 0.10 (and certainly not JS-0.4.0) - is it too late to update the fix version?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bhulette",
                        "name": "bhulette",
                        "key": "bhulette",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Brian Hulette",
                        "active": true,
                        "timeZone": "America/Vancouver"
                    },
                    "created": "2018-08-17T02:30:55.599+0000",
                    "updated": "2018-08-17T02:30:55.599+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16583295",
                    "id": "16583295",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bhulette",
                        "name": "bhulette",
                        "key": "bhulette",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Brian Hulette",
                        "active": true,
                        "timeZone": "America/Vancouver"
                    },
                    "body": "I think the same thing happened in ARROW-2940, ARROW-2451, ARROW-2437, ARROW-2458, and ARROW-2397 - I went ahead and updated them all. It looks like these mistakes prevented them from being added to CHANGELOG.md for v0.10.0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bhulette",
                        "name": "bhulette",
                        "key": "bhulette",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Brian Hulette",
                        "active": true,
                        "timeZone": "America/Vancouver"
                    },
                    "created": "2018-08-17T02:44:47.892+0000",
                    "updated": "2018-08-17T02:44:47.892+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16595192",
                    "id": "16595192",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bhulette",
                        "name": "bhulette",
                        "key": "bhulette",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Brian Hulette",
                        "active": true,
                        "timeZone": "America/Vancouver"
                    },
                    "body": "[~wesmckinn] not sure if you saw this - I think the issues I mentioned above were incorrectly marked with fix version JS-0.4.0 rather than 0.10.0. I've fixed them in Jira now, but I don't think they made it into the CHANGELOG.md as a result. Is there anything we can/should do about that now?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bhulette",
                        "name": "bhulette",
                        "key": "bhulette",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Brian Hulette",
                        "active": true,
                        "timeZone": "America/Vancouver"
                    },
                    "created": "2018-08-28T15:54:23.417+0000",
                    "updated": "2018-08-28T15:54:23.417+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13112779/comment/16595231",
                    "id": "16595231",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "We can definitely generate the changelog. ARROW-3132",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-08-28T16:25:02.147+0000",
                    "updated": "2018-08-28T16:25:02.147+0000"
                }
            ],
            "maxResults": 12,
            "total": 12,
            "startAt": 0
        },
        "customfield_12311820": "0|i3ltpb:",
        "customfield_12314139": null
    }
}