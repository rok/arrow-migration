{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13358015",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015",
    "key": "ARROW-11591",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12349493",
                "id": "12349493",
                "description": "",
                "name": "4.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-04-26"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "compute",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
            "name": "npr",
            "key": "npr",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Neal Richardson",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
            "name": "npr",
            "key": "npr",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Neal Richardson",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 31800,
            "total": 31800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 31800,
            "total": 31800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11591/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 53,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/560153",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz opened a new pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621\n\n\n   In order to keep this patch simpler, the execution framework for scalar aggregate kernels is reused for grouped aggregations. This is not intended as a permanent arrangement.\r\n   \r\n   A `compute::Function` is added which implements grouped aggregation.\r\n   `GroupByOptions::aggregates` is a vector specifying which\r\n   aggregations will be performed: each element is a\r\n   GroupByOptions::Aggregate` containing the name of an aggregate\r\n   function and a pointer to a `FunctionOptions`. The first arguments to\r\n   `group_by` are interpreted as the corresponding aggregands and the remainder\r\n   will be used as grouping keys. The output will be an array with the same\r\n   number of fields where each slot contains the aggregation result and keys\r\n   for a group:\r\n   \r\n   ```c++\r\n   GroupByOptions options{\r\n       {\"sum\", nullptr},  // first argument will be summed\r\n       {\"min_max\",\r\n        &min_max_options},  // second argument's extrema will be found\r\n   };\r\n   \r\n   std::shared_ptr<arrow::Array> needs_sum = ...;\r\n   std::shared_ptr<arrow::Array> needs_min_max = ...;\r\n   std::shared_ptr<arrow::Array> key_0 = ...;\r\n   std::shared_ptr<arrow::Array> key_1 = ...;\r\n   \r\n   ARROW_ASSIGN_OR_RAISE(arrow::Datum out,\r\n                         arrow::compute::CallFunction(\"group_by\",\r\n                                                      {\r\n                                                          needs_sum,\r\n                                                          needs_min_max,\r\n                                                          key_0,\r\n                                                          key_1,\r\n                                                      },\r\n                                                      &options));\r\n   \r\n   // Unpack struct array result (a four-field array)\r\n   auto out_array = out.array_as<StructArray>();\r\n   std::shared_ptr<arrow::Array> sums = out_array->field(0);\r\n   std::shared_ptr<arrow::Array> mins_and_maxes = out_array->field(1);\r\n   std::shared_ptr<arrow::Array> group_key_0 = out_array->field(2);\r\n   std::shared_ptr<arrow::Array> group_key_1 = out_array->field(3);\r\n   ```\r\n   \r\n   TODO:\r\n   - [ ] Only sum, count, and min_max aggregators are implemented\r\n   - [ ] Add an aggregator which returns a list of row indices of members for use in partitioned dataset writing\r\n   - [ ] Reorganization\r\n   - [ ] Comments\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-02T21:42:32.518+0000",
                    "updated": "2021-03-02T21:42:32.518+0000",
                    "started": "2021-03-02T21:42:32.518+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "560153",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/560198",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r585974907\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n\nReview comment:\n       Is this right?\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-02T23:23:56.179+0000",
                    "updated": "2021-03-02T23:23:56.179+0000",
                    "started": "2021-03-02T23:23:56.179+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "560198",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/560205",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#issuecomment-789303321\n\n\n   https://issues.apache.org/jira/browse/ARROW-11591\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-02T23:40:10.602+0000",
                    "updated": "2021-03-02T23:40:10.602+0000",
                    "started": "2021-03-02T23:40:10.602+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "560205",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/560262",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#issuecomment-789380392\n\n\n   Regarding the `HASH_AGGREGATE` function type, one of the inputs on each invocation should be the current hash table cardinality so you do not need to inspect the group ids to infer the cardinality. \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-03T02:34:04.688+0000",
                    "updated": "2021-03-03T02:34:04.688+0000",
                    "started": "2021-03-03T02:34:04.688+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "560262",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/561009",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r587455622\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec.cc\n##########\n@@ -838,6 +838,13 @@ class ScalarAggExecutor : public KernelExecutorImpl<ScalarAggregateKernel> {\n \n  private:\n   Status Consume(const ExecBatch& batch) {\n+    if (kernel_->nomerge) {\n+      kernel_->consume(kernel_ctx_, batch);\n+      ARROW_CTX_RETURN_IF_ERROR(kernel_ctx_);\n+      return Status::OK();\n+    }\n\nReview comment:\n       Hmm... this is weird. The `init` function isn't even called?\n\n##########\nFile path: cpp/src/arrow/compute/api_aggregate.h\n##########\n@@ -306,5 +326,34 @@ Result<Datum> TDigest(const Datum& value,\n                       const TDigestOptions& options = TDigestOptions::Defaults(),\n                       ExecContext* ctx = NULLPTR);\n \n+/// \\brief Calculate multiple aggregations grouped on multiple keys\n+///\n+/// \\param[in] aggregands datums to which aggregations will be applied\n+/// \\param[in] keys datums which will be used to group the aggregations\n+/// \\param[in] options GroupByOptions, encapsulating the names and options of aggregate\n+///            functions to be applied and the field names for results in the output.\n+/// \\return a StructArray with len(aggregands) + len(keys) fields. The first\n+///         len(aggregands) fields are the results of the aggregations for the group\n+///         specified by keys in the final len(keys) fields.\n+///\n+/// For example:\n+///   GroupByOptions options = {\n+///     .aggregates = {\n+///       {\"sum\", nullptr, \"sum result\"},\n+///       {\"mean\", nullptr, \"mean result\"},\n+///     },\n+///     .key_names = {\"str key\", \"date key\"},\n+///   };\n+/// assert(*GroupBy({[2, 5, 8], [1.5, 2.0, 3.0]},\n+///                 {[\"a\", \"b\", \"a\"], [today, today, today]},\n+///                 options).Equals([\n+///   {\"sum result\": 10, \"mean result\": 2.25, \"str key\": \"a\", \"date key\": today},\n+///   {\"sum result\": 5,  \"mean result\": 2.0,  \"str key\": \"b\", \"date key\": today},\n+/// ]))\n\nReview comment:\n       I would expect `GroupBy` to simply return lists of indices, not to perform aggregations.\r\n   Do we maybe want to call this `GroupedAggregate`? Or do you want to make this a swiss-knife function?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n\nReview comment:\n       Here as well, unless there's a lot of sharing going on, it would probably be better to put grouped aggregation in its own file.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n #include \"arrow/chunked_array.h\"\n #include \"arrow/compute/api_aggregate.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/kernels/aggregate_internal.h\"\n #include \"arrow/compute/kernels/test_util.h\"\n+#include \"arrow/compute/registry.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bitmap_reader.h\"\n #include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/int_util_internal.h\"\n \n #include \"arrow/testing/gtest_common.h\"\n #include \"arrow/testing/gtest_util.h\"\n #include \"arrow/testing/random.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n+using internal::BitmapReader;\n using internal::checked_cast;\n using internal::checked_pointer_cast;\n \n namespace compute {\n \n+// Copy-pasta from partition.cc\n+//\n+// In the finished product this will only be a test helper for group_by\n+// and partition.cc will rely on a no-aggregate call to group_by.\n+namespace group_helpers {\n+namespace {\n+\n+// Transform an array of counts to offsets which will divide a ListArray\n+// into an equal number of slices with corresponding lengths.\n+Result<std::shared_ptr<Buffer>> CountsToOffsets(std::shared_ptr<Int64Array> counts) {\n+  TypedBufferBuilder<int32_t> offset_builder;\n+  RETURN_NOT_OK(offset_builder.Resize(counts->length() + 1));\n+\n+  int32_t current_offset = 0;\n+  offset_builder.UnsafeAppend(current_offset);\n+\n+  for (int64_t i = 0; i < counts->length(); ++i) {\n+    DCHECK_NE(counts->Value(i), 0);\n+    current_offset += static_cast<int32_t>(counts->Value(i));\n+    offset_builder.UnsafeAppend(current_offset);\n+  }\n+\n+  std::shared_ptr<Buffer> offsets;\n+  RETURN_NOT_OK(offset_builder.Finish(&offsets));\n+  return offsets;\n+}\n+\n+class StructDictionary {\n+ public:\n+  struct Encoded {\n+    std::shared_ptr<Int32Array> indices;\n+    std::shared_ptr<StructDictionary> dictionary;\n+  };\n+\n+  static Result<Encoded> Encode(const ArrayVector& columns) {\n+    Encoded out{nullptr, std::make_shared<StructDictionary>()};\n+\n+    for (const auto& column : columns) {\n+      if (column->null_count() != 0) {\n+        return Status::NotImplemented(\"Grouping on a field with nulls\");\n+      }\n+\n+      RETURN_NOT_OK(out.dictionary->AddOne(column, &out.indices));\n+    }\n+\n+    return out;\n+  }\n+\n+  Result<std::shared_ptr<StructArray>> Decode(std::shared_ptr<Int32Array> fused_indices,\n+                                              FieldVector fields) {\n+    std::vector<Int32Builder> builders(dictionaries_.size());\n+    for (Int32Builder& b : builders) {\n+      RETURN_NOT_OK(b.Resize(fused_indices->length()));\n+    }\n+\n+    std::vector<int32_t> codes(dictionaries_.size());\n+    for (int64_t i = 0; i < fused_indices->length(); ++i) {\n+      Expand(fused_indices->Value(i), codes.data());\n+\n+      auto builder_it = builders.begin();\n+      for (int32_t index : codes) {\n+        builder_it++->UnsafeAppend(index);\n+      }\n+    }\n+\n+    ArrayVector columns(dictionaries_.size());\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      std::shared_ptr<ArrayData> indices;\n+      RETURN_NOT_OK(builders[i].FinishInternal(&indices));\n+\n+      ARROW_ASSIGN_OR_RAISE(Datum column, compute::Take(dictionaries_[i], indices));\n+\n+      if (fields[i]->type()->id() == Type::DICTIONARY) {\n+        RETURN_NOT_OK(RestoreDictionaryEncoding(\n+            checked_pointer_cast<DictionaryType>(fields[i]->type()), &column));\n+      }\n+\n+      columns[i] = column.make_array();\n+    }\n+\n+    return StructArray::Make(std::move(columns), std::move(fields));\n+  }\n+\n+ private:\n+  Status AddOne(Datum column, std::shared_ptr<Int32Array>* fused_indices) {\n+    if (column.type()->id() != Type::DICTIONARY) {\n+      ARROW_ASSIGN_OR_RAISE(column, compute::DictionaryEncode(std::move(column)));\n+    }\n+\n+    auto dict_column = column.array_as<DictionaryArray>();\n+    dictionaries_.push_back(dict_column->dictionary());\n+    ARROW_ASSIGN_OR_RAISE(auto indices, compute::Cast(*dict_column->indices(), int32()));\n+\n+    if (*fused_indices == nullptr) {\n+      *fused_indices = checked_pointer_cast<Int32Array>(std::move(indices));\n+      return IncreaseSize();\n+    }\n+\n+    // It's useful to think about the case where each of dictionaries_ has size 10.\n+    // In this case the decimal digit in the ones place is the code in dictionaries_[0],\n+    // the tens place corresponds to the code in dictionaries_[1], etc.\n+    // The incumbent indices must be shifted to the hundreds place so as not to collide.\n+    ARROW_ASSIGN_OR_RAISE(Datum new_fused_indices,\n+                          compute::Multiply(indices, MakeScalar(size_)));\n+\n+    ARROW_ASSIGN_OR_RAISE(new_fused_indices,\n+                          compute::Add(new_fused_indices, *fused_indices));\n+\n+    *fused_indices = checked_pointer_cast<Int32Array>(new_fused_indices.make_array());\n+    return IncreaseSize();\n+  }\n+\n+  // expand a fused code into component dict codes, order is in order of addition\n+  void Expand(int32_t fused_code, int32_t* codes) {\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      auto dictionary_size = static_cast<int32_t>(dictionaries_[i]->length());\n+      codes[i] = fused_code % dictionary_size;\n+      fused_code /= dictionary_size;\n+    }\n+  }\n+\n+  Status RestoreDictionaryEncoding(std::shared_ptr<DictionaryType> expected_type,\n+                                   Datum* column) {\n+    DCHECK_NE(column->type()->id(), Type::DICTIONARY);\n+    ARROW_ASSIGN_OR_RAISE(*column, compute::DictionaryEncode(std::move(*column)));\n+\n+    if (expected_type->index_type()->id() == Type::INT32) {\n+      // dictionary_encode has already yielded the expected index_type\n+      return Status::OK();\n+    }\n+\n+    // cast the indices to the expected index type\n+    auto dictionary = std::move(column->mutable_array()->dictionary);\n+    column->mutable_array()->type = int32();\n+\n+    ARROW_ASSIGN_OR_RAISE(*column,\n+                          compute::Cast(std::move(*column), expected_type->index_type()));\n+\n+    column->mutable_array()->dictionary = std::move(dictionary);\n+    column->mutable_array()->type = expected_type;\n+    return Status::OK();\n+  }\n+\n+  Status IncreaseSize() {\n+    auto factor = static_cast<int32_t>(dictionaries_.back()->length());\n+\n+    if (arrow::internal::MultiplyWithOverflow(size_, factor, &size_)) {\n+      return Status::CapacityError(\"Max groups exceeded\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  int32_t size_ = 1;\n+  ArrayVector dictionaries_;\n+};\n+\n+Result<std::shared_ptr<StructArray>> MakeGroupings(const StructArray& keys) {\n+  if (keys.num_fields() == 0) {\n+    return Status::Invalid(\"Grouping with no keys\");\n+  }\n+\n+  if (keys.null_count() != 0) {\n+    return Status::Invalid(\"Grouping with null keys\");\n+  }\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused, StructDictionary::Encode(keys.fields()));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto sort_indices, compute::SortIndices(*fused.indices));\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted, compute::Take(fused.indices, *sort_indices));\n+  fused.indices = checked_pointer_cast<Int32Array>(sorted.make_array());\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused_counts_and_values,\n+                        compute::ValueCounts(fused.indices));\n+  fused.indices.reset();\n+\n+  auto unique_fused_indices =\n+      checked_pointer_cast<Int32Array>(fused_counts_and_values->GetFieldByName(\"values\"));\n+  ARROW_ASSIGN_OR_RAISE(\n+      auto unique_rows,\n+      fused.dictionary->Decode(std::move(unique_fused_indices), keys.type()->fields()));\n+\n+  auto counts =\n+      checked_pointer_cast<Int64Array>(fused_counts_and_values->GetFieldByName(\"counts\"));\n+  ARROW_ASSIGN_OR_RAISE(auto offsets, CountsToOffsets(std::move(counts)));\n+\n+  auto grouped_sort_indices =\n+      std::make_shared<ListArray>(list(sort_indices->type()), unique_rows->length(),\n+                                  std::move(offsets), std::move(sort_indices));\n+\n+  return StructArray::Make(\n+      ArrayVector{std::move(unique_rows), std::move(grouped_sort_indices)},\n+      std::vector<std::string>{\"values\", \"groupings\"});\n+}\n+\n+Result<std::shared_ptr<ListArray>> ApplyGroupings(const ListArray& groupings,\n+                                                  const Array& array) {\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted,\n+                        compute::Take(array, groupings.data()->child_data[0]));\n+\n+  return std::make_shared<ListArray>(list(array.type()), groupings.length(),\n+                                     groupings.value_offsets(), sorted.make_array());\n+}\n+\n+struct ScalarVectorToArray {\n+  template <typename T, typename AppendScalar,\n+            typename BuilderType = typename TypeTraits<T>::BuilderType,\n+            typename ScalarType = typename TypeTraits<T>::ScalarType>\n+  Status UseBuilder(const AppendScalar& append) {\n+    BuilderType builder(type(), default_memory_pool());\n+    for (const auto& s : scalars_) {\n+      if (s->is_valid) {\n+        RETURN_NOT_OK(append(checked_cast<const ScalarType&>(*s), &builder));\n+      } else {\n+        RETURN_NOT_OK(builder.AppendNull());\n+      }\n+    }\n+    return builder.FinishInternal(&data_);\n+  }\n+\n+  struct AppendValue {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      return builder->Append(s.value);\n+    }\n+  };\n+\n+  struct AppendBuffer {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      const Buffer& buffer = *s.value;\n+      return builder->Append(util::string_view{buffer});\n+    }\n+  };\n+\n+  template <typename T>\n+  enable_if_primitive_ctype<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendValue{});\n+  }\n+\n+  template <typename T>\n+  enable_if_has_string_view<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendBuffer{});\n+  }\n+\n+  Status Visit(const DataType& type) {\n+    return Status::NotImplemented(\"ScalarVectorToArray for type \", type);\n+  }\n+\n+  Result<Datum> Convert(ScalarVector scalars) && {\n+    if (scalars.size() == 0) {\n+      return Status::NotImplemented(\"ScalarVectorToArray with no scalars\");\n+    }\n+    scalars_ = std::move(scalars);\n+    RETURN_NOT_OK(VisitTypeInline(*type(), this));\n+    return Datum(std::move(data_));\n+  }\n+\n+  const std::shared_ptr<DataType>& type() { return scalars_[0]->type; }\n+\n+  ScalarVector scalars_;\n+  std::shared_ptr<ArrayData> data_;\n+};\n+\n+Result<Datum> NaiveGroupBy(std::vector<Datum> aggregands, std::vector<Datum> keys,\n+                           GroupByOptions options) {\n+  ArrayVector keys_arrays;\n+  for (const Datum& key : keys) keys_arrays.push_back(key.make_array());\n+  std::vector<std::string> key_names(keys_arrays.size(), \"\");\n+  ARROW_ASSIGN_OR_RAISE(auto keys_struct,\n+                        StructArray::Make(std::move(keys_arrays), std::move(key_names)));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto groupings_and_values, MakeGroupings(*keys_struct));\n+\n+  auto groupings =\n+      checked_pointer_cast<ListArray>(groupings_and_values->GetFieldByName(\"groupings\"));\n+\n+  int64_t n_groups = groupings->length();\n+\n+  ArrayVector out_columns;\n+\n+  for (size_t i_agg = 0; i_agg < aggregands.size(); ++i_agg) {\n+    const Datum& aggregand = aggregands[i_agg];\n+    const std::string& function = options.aggregates[i_agg].function;\n+\n+    ScalarVector aggregated_scalars;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto grouped_aggregand,\n+                          ApplyGroupings(*groupings, *aggregand.make_array()));\n+\n+    for (int64_t i_group = 0; i_group < n_groups; ++i_group) {\n+      ARROW_ASSIGN_OR_RAISE(\n+          Datum d, CallFunction(function, {grouped_aggregand->value_slice(i_group)}));\n+      aggregated_scalars.push_back(d.scalar());\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(Datum aggregated_column,\n+                          ScalarVectorToArray{}.Convert(std::move(aggregated_scalars)));\n+    out_columns.push_back(aggregated_column.make_array());\n+  }\n+\n+  keys_struct =\n+      checked_pointer_cast<StructArray>(groupings_and_values->GetFieldByName(\"values\"));\n+  for (size_t i_key = 0; i_key < aggregands.size(); ++i_key) {\n+    out_columns.push_back(keys_struct->field(i_key));\n+  }\n+\n+  std::vector<std::string> out_names(out_columns.size(), \"\");\n+  return StructArray::Make(std::move(out_columns), std::move(out_names));\n+}\n+\n+void ValidateGroupBy(GroupByOptions options, std::vector<Datum> aggregands,\n+                     std::vector<Datum> keys) {\n+  ASSERT_OK_AND_ASSIGN(Datum expected,\n+                       group_helpers::NaiveGroupBy(aggregands, keys, options));\n+\n+  ASSERT_OK_AND_ASSIGN(Datum actual, GroupBy(aggregands, keys, options));\n+\n+  // Ordering of groups is not important, so sort by key columns to ensure the comparison\n+  // doesn't fail spuriously\n+\n+  for (Datum* out : {&expected, &actual}) {\n+    auto out_columns = out->array_as<StructArray>()->fields();\n+\n+    SortOptions sort_options;\n+    FieldVector key_fields;\n+    ArrayVector key_columns;\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      auto name = std::to_string(i);\n+      sort_options.sort_keys.emplace_back(name);\n+      key_fields.push_back(field(name, out_columns[0]->type()));\n+      key_columns.push_back(out_columns[0]);\n+    }\n+    auto key_batch = RecordBatch::Make(schema(std::move(key_fields)), out->length(),\n+                                       std::move(key_columns));\n+\n+    ASSERT_OK_AND_ASSIGN(Datum sort_indices, SortIndices(key_batch, sort_options));\n+    ASSERT_OK_AND_ASSIGN(*out, Take(*out, sort_indices, TakeOptions::NoBoundsCheck()));\n+  }\n+\n+  AssertDatumsEqual(expected, actual, /*verbose=*/true);\n+}\n+\n+}  // namespace\n+}  // namespace group_helpers\n+\n+TEST(GroupBy, SumOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped, GroupBy({aggregand}, {key},\n+                                                             GroupByOptions{\n+                                                                 {\"sum\", nullptr},\n+                                                             }));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", float64()),\n+                                      field(\"\", int64()),\n+                                  }),\n+                                  R\"([\n+    [4.25,   1],\n+    [-0.125, 2],\n+    [null,   3],\n+    [0.75,   null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, MinMaxOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped, GroupBy({aggregand}, {key},\n+                                                             GroupByOptions{\n+                                                                 {\"min_max\", nullptr},\n+                                                             }));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", struct_({\n+                                                    field(\"min\", float64()),\n+                                                    field(\"max\", float64()),\n+                                                })),\n+                                      field(\"\", int64()),\n+                                  }),\n+                                  R\"([\n+    [{\"min\": 1.0,   \"max\": 3.25},  1],\n+    [{\"min\": -0.25, \"max\": 0.125}, 2],\n+    [{\"min\": null,  \"max\": null},  3],\n+    [{\"min\": 0.75,  \"max\": 0.75},  null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, CountAndSum) {\n+  auto aggregand = ArrayFromJSON(float32(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 1, 3, 2, 3, null]\");\n+\n+  CountOptions count_options;\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      Datum aggregated_and_grouped,\n+      // NB: passing an aggregand twice or also using it as a key is legal\n+      GroupBy({aggregand, aggregand, key}, {key},\n+              GroupByOptions{\n+                  {\"count\", &count_options},\n+                  {\"sum\", nullptr},\n+                  {\"sum\", nullptr},\n+              }));\n+\n+  AssertDatumsEqual(\n+      ArrayFromJSON(struct_({\n+                        field(\"\", int64()),\n+                        // NB: summing a float32 array results in float64 sums\n+                        field(\"\", float64()),\n+                        field(\"\", int64()),\n+                        field(\"\", int64()),\n+                    }),\n+                    R\"([\n+    [1, 1.0,   2,    1],\n+    [2, 0.125, 4,    2],\n+    [2, 3.0,   6,    3],\n+    [1, 0.75,  null, null]\n+  ])\"),\n+      aggregated_and_grouped,\n+      /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, StringKey) {\n+  auto aggregand = ArrayFromJSON(int64(), \"[10, 5, 4, 2, 12, 9]\");\n+  auto key = ArrayFromJSON(utf8(), R\"([\"alfa\", \"beta\", \"gamma\", \"gamma\", null, \"beta\"])\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped,\n+                       GroupBy({aggregand}, {key}, GroupByOptions{{\"sum\", nullptr}}));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", int64()),\n+                                      field(\"\", utf8()),\n+                                  }),\n+                                  R\"([\n+    [10,   \"alfa\"],\n+    [14,   \"beta\"],\n+    [6,    \"gamma\"],\n+    [12,   null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, MultipleKeys) {\n+  auto aggregand = ArrayFromJSON(float32(), \"[0.125, 0.5, -0.75, 8, 1.0, 2.0]\");\n+  auto int_key = ArrayFromJSON(int32(), \"[0, 1, 0, 1, 0, 1]\");\n+  auto str_key =\n+      ArrayFromJSON(utf8(), R\"([\"beta\", \"beta\", \"gamma\", \"gamma\", null, \"beta\"])\");\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      Datum aggregated_and_grouped,\n+      GroupBy({aggregand}, {int_key, str_key}, GroupByOptions{{\"sum\", nullptr}}));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", float64()),\n+                                      field(\"\", int32()),\n+                                      field(\"\", utf8()),\n+                                  }),\n+                                  R\"([\n+    [0.125, 0, \"beta\"],\n+    [2.5,   1, \"beta\"],\n+    [-0.75, 0, \"gamma\"],\n+    [8,     1, \"gamma\"],\n+    [1.0,   0, null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, ConcreteCaseWithValidateGroupBy) {\n\nReview comment:\n       I suppose we're going to add more of those?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n\nReview comment:\n       Should you resize to shrink the overallocated buffer?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n\nReview comment:\n       Could do a simple `memset`... though perhaps the compiler generates it anyway.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n\nReview comment:\n       Ideally we would use a `string_view` here and only copy into the map if not found...\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n+          reinterpret_cast<const char*>(key_bytes_batch_.data() + offsets_batch_[i]),\n+          key_length);\n+      auto iter = map_.find(key);\n+      if (iter == map_.end()) {\n+        group_ids_batch_[i] = n_groups++;\n+        auto next_key_offset = static_cast<int32_t>(key_bytes_.size());\n+        key_bytes_.resize(next_key_offset + key_length);\n+        offsets_.push_back(next_key_offset + key_length);\n+        memcpy(key_bytes_.data() + next_key_offset, key.c_str(), key_length);\n+        map_.insert(std::make_pair(key, group_ids_batch_[i]));\n+      } else {\n+        group_ids_batch_[i] = iter->second;\n+      }\n+    }\n+\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      aggregators[i]->Consume(ctx, aggregands[i], group_ids_batch_.data());\n+      if (ctx->HasError()) return;\n+    }\n+  }\n+\n+  void MergeFrom(KernelContext* ctx, KernelState&& src) override {\n+    // TODO(ARROW-11840) merge two hash tables\n+    ctx->SetStatus(Status::NotImplemented(\"merging grouped aggregations\"));\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    size_t n_keys = decode_next_impl.size();\n+    ArrayDataVector out_columns(aggregators.size() + n_keys);\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      Datum aggregand;\n+      aggregators[i]->Finalize(ctx, &aggregand);\n+      if (ctx->HasError()) return;\n+      out_columns[i] = aggregand.array();\n+    }\n+\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(n_groups);\n+    for (int64_t i = 0; i < n_groups; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_.data() + offsets_[i];\n+    }\n+\n+    int64_t length = n_groups;\n+    for (size_t i = 0; i < n_keys; ++i) {\n+      std::shared_ptr<ArrayData> key_array;\n+      decode_next_impl[i].decode_next_impl(ctx, static_cast<int32_t>(length),\n+                                           key_buf_ptrs_.data(), &key_array);\n+      out_columns[aggregators.size() + i] = std::move(key_array);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type), length, {/*null_bitmap=*/nullptr},\n+                           std::move(out_columns));\n+  }\n+  std::vector<int32_t> offsets_batch_;\n+  std::vector<uint8_t> key_bytes_batch_;\n+  std::vector<uint8_t*> key_buf_ptrs_;\n+  std::vector<uint32_t> group_ids_batch_;\n+\n+  std::unordered_map<std::string, uint32_t> map_;\n+  std::vector<int32_t> offsets_;\n\nReview comment:\n       `int64_t`?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n+          reinterpret_cast<const char*>(key_bytes_batch_.data() + offsets_batch_[i]),\n+          key_length);\n+      auto iter = map_.find(key);\n+      if (iter == map_.end()) {\n+        group_ids_batch_[i] = n_groups++;\n+        auto next_key_offset = static_cast<int32_t>(key_bytes_.size());\n+        key_bytes_.resize(next_key_offset + key_length);\n+        offsets_.push_back(next_key_offset + key_length);\n+        memcpy(key_bytes_.data() + next_key_offset, key.c_str(), key_length);\n+        map_.insert(std::make_pair(key, group_ids_batch_[i]));\n+      } else {\n+        group_ids_batch_[i] = iter->second;\n+      }\n+    }\n+\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      aggregators[i]->Consume(ctx, aggregands[i], group_ids_batch_.data());\n+      if (ctx->HasError()) return;\n+    }\n+  }\n+\n+  void MergeFrom(KernelContext* ctx, KernelState&& src) override {\n+    // TODO(ARROW-11840) merge two hash tables\n+    ctx->SetStatus(Status::NotImplemented(\"merging grouped aggregations\"));\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    size_t n_keys = decode_next_impl.size();\n+    ArrayDataVector out_columns(aggregators.size() + n_keys);\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      Datum aggregand;\n+      aggregators[i]->Finalize(ctx, &aggregand);\n+      if (ctx->HasError()) return;\n+      out_columns[i] = aggregand.array();\n+    }\n+\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(n_groups);\n+    for (int64_t i = 0; i < n_groups; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_.data() + offsets_[i];\n+    }\n+\n+    int64_t length = n_groups;\n+    for (size_t i = 0; i < n_keys; ++i) {\n+      std::shared_ptr<ArrayData> key_array;\n+      decode_next_impl[i].decode_next_impl(ctx, static_cast<int32_t>(length),\n+                                           key_buf_ptrs_.data(), &key_array);\n+      out_columns[aggregators.size() + i] = std::move(key_array);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type), length, {/*null_bitmap=*/nullptr},\n+                           std::move(out_columns));\n+  }\n+  std::vector<int32_t> offsets_batch_;\n+  std::vector<uint8_t> key_bytes_batch_;\n+  std::vector<uint8_t*> key_buf_ptrs_;\n+  std::vector<uint32_t> group_ids_batch_;\n+\n+  std::unordered_map<std::string, uint32_t> map_;\n+  std::vector<int32_t> offsets_;\n+  std::vector<uint8_t> key_bytes_;\n+  uint32_t n_groups;\n+\n+  std::shared_ptr<DataType> out_type;\n+  GroupByOptions options;\n+  std::vector<std::unique_ptr<GroupedAggregator>> aggregators;\n+\n+  std::vector<GetAddLengthImpl> add_length_impl;\n+  std::vector<GetEncodeNextImpl> encode_next_impl;\n+  std::vector<GetDecodeNextImpl> decode_next_impl;\n+};\n+\n+template <typename Aggregator>\n+std::unique_ptr<Aggregator> MakeAggregator(KernelContext* ctx,\n+                                           const std::string& function_name,\n+                                           const std::shared_ptr<DataType>& input_type,\n+                                           const FunctionOptions* options) {\n+  if (options == nullptr) {\n+    if (auto function = ctx->exec_context()\n+                            ->func_registry()\n+                            ->GetFunction(function_name)\n+                            .ValueOr(nullptr)) {\n+      options = function->default_options();\n+    }\n+  }\n+\n+  return Aggregator::Make(ctx, input_type, options);\n+}\n+\n+std::unique_ptr<KernelState> GroupByInit(KernelContext* ctx, const KernelInitArgs& args) {\n+  auto impl = ::arrow::internal::make_unique<GroupByImpl>();\n+  impl->options = *checked_cast<const GroupByOptions*>(args.options);\n+  const auto& aggregates = impl->options.aggregates;\n+\n+  impl->n_groups = 0;\n+  impl->offsets_.push_back(0);\n\nReview comment:\n       Shouldn't this go into the `Impl` constructor?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_benchmark.cc\n##########\n@@ -300,6 +300,70 @@ BENCHMARK_TEMPLATE(ReferenceSum, SumBitmapVectorizeUnroll<int64_t>)\n     ->Apply(BenchmarkSetArgs);\n #endif  // ARROW_WITH_BENCHMARKS_REFERENCE\n \n+//\n+// GroupBy\n+//\n+\n+static void BenchmarkGroupBy(benchmark::State& state,\n+                             std::vector<GroupByOptions::Aggregate> aggregates,\n+                             std::vector<Datum> aggregands, std::vector<Datum> keys) {\n+  std::vector<Datum> arguments;\n+  for (const Datum& aggregand : aggregands) {\n+    arguments.push_back(aggregand);\n+  }\n+\n+  for (const Datum& key : keys) {\n+    arguments.push_back(key);\n+  }\n+\n+  GroupByOptions options;\n+  options.aggregates = aggregates;\n+  options.key_names.resize(keys.size(), \"ignored\");\n+\n+  for (auto _ : state) {\n+    ABORT_NOT_OK(CallFunction(\"group_by\", arguments, &options).status());\n+  }\n+}\n+\n+#define GROUP_BY_BENCHMARK(Name, Impl)                               \\\n+  static void Name(benchmark::State& state) {                        \\\n+    RegressionArgs args(state, false);                               \\\n+    auto rng = random::RandomArrayGenerator(1923);                   \\\n+    (Impl)();                                                        \\\n+  }                                                                  \\\n+  BENCHMARK(Name)->Apply([](benchmark::internal::Benchmark* bench) { \\\n+    BenchmarkSetArgsWithSizes(bench, {1 * 1024 * 1024});             \\\n+  })\n+\n+GROUP_BY_BENCHMARK(SumDoublesGroupedBySmallStringSet, [&] {\n+  auto summand = rng.Float64(args.size,\n+                             /*min=*/0.0,\n+                             /*max=*/1.0e14,\n+                             /*null_probability=*/args.null_proportion,\n+                             /*nan_probability=*/args.null_proportion / 10);\n+\n+  auto key = rng.StringWithRepeats(args.size,\n+                                   /*unique=*/16,\n+                                   /*min_length=*/3,\n+                                   /*max_length=*/32);\n+\n+  BenchmarkGroupBy(state, {{\"sum\", NULLPTR, \"summed f64\"}}, {summand}, {key});\n\nReview comment:\n       `GroupByOptions::Aggregate` has only two fields, how does `{\"sum\", NULLPTR, \"summed f64\"}` work?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n #include \"arrow/chunked_array.h\"\n #include \"arrow/compute/api_aggregate.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/kernels/aggregate_internal.h\"\n #include \"arrow/compute/kernels/test_util.h\"\n+#include \"arrow/compute/registry.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bitmap_reader.h\"\n #include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/int_util_internal.h\"\n \n #include \"arrow/testing/gtest_common.h\"\n #include \"arrow/testing/gtest_util.h\"\n #include \"arrow/testing/random.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n+using internal::BitmapReader;\n using internal::checked_cast;\n using internal::checked_pointer_cast;\n \n namespace compute {\n \n+// Copy-pasta from partition.cc\n+//\n+// In the finished product this will only be a test helper for group_by\n+// and partition.cc will rely on a no-aggregate call to group_by.\n+namespace group_helpers {\n+namespace {\n+\n+// Transform an array of counts to offsets which will divide a ListArray\n+// into an equal number of slices with corresponding lengths.\n+Result<std::shared_ptr<Buffer>> CountsToOffsets(std::shared_ptr<Int64Array> counts) {\n+  TypedBufferBuilder<int32_t> offset_builder;\n+  RETURN_NOT_OK(offset_builder.Resize(counts->length() + 1));\n+\n+  int32_t current_offset = 0;\n+  offset_builder.UnsafeAppend(current_offset);\n+\n+  for (int64_t i = 0; i < counts->length(); ++i) {\n+    DCHECK_NE(counts->Value(i), 0);\n+    current_offset += static_cast<int32_t>(counts->Value(i));\n+    offset_builder.UnsafeAppend(current_offset);\n+  }\n+\n+  std::shared_ptr<Buffer> offsets;\n+  RETURN_NOT_OK(offset_builder.Finish(&offsets));\n+  return offsets;\n+}\n+\n+class StructDictionary {\n+ public:\n+  struct Encoded {\n+    std::shared_ptr<Int32Array> indices;\n+    std::shared_ptr<StructDictionary> dictionary;\n+  };\n+\n+  static Result<Encoded> Encode(const ArrayVector& columns) {\n+    Encoded out{nullptr, std::make_shared<StructDictionary>()};\n+\n+    for (const auto& column : columns) {\n+      if (column->null_count() != 0) {\n+        return Status::NotImplemented(\"Grouping on a field with nulls\");\n+      }\n+\n+      RETURN_NOT_OK(out.dictionary->AddOne(column, &out.indices));\n+    }\n+\n+    return out;\n+  }\n+\n+  Result<std::shared_ptr<StructArray>> Decode(std::shared_ptr<Int32Array> fused_indices,\n+                                              FieldVector fields) {\n+    std::vector<Int32Builder> builders(dictionaries_.size());\n+    for (Int32Builder& b : builders) {\n+      RETURN_NOT_OK(b.Resize(fused_indices->length()));\n+    }\n+\n+    std::vector<int32_t> codes(dictionaries_.size());\n+    for (int64_t i = 0; i < fused_indices->length(); ++i) {\n+      Expand(fused_indices->Value(i), codes.data());\n+\n+      auto builder_it = builders.begin();\n+      for (int32_t index : codes) {\n+        builder_it++->UnsafeAppend(index);\n+      }\n+    }\n+\n+    ArrayVector columns(dictionaries_.size());\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      std::shared_ptr<ArrayData> indices;\n+      RETURN_NOT_OK(builders[i].FinishInternal(&indices));\n+\n+      ARROW_ASSIGN_OR_RAISE(Datum column, compute::Take(dictionaries_[i], indices));\n+\n+      if (fields[i]->type()->id() == Type::DICTIONARY) {\n+        RETURN_NOT_OK(RestoreDictionaryEncoding(\n+            checked_pointer_cast<DictionaryType>(fields[i]->type()), &column));\n+      }\n+\n+      columns[i] = column.make_array();\n+    }\n+\n+    return StructArray::Make(std::move(columns), std::move(fields));\n+  }\n+\n+ private:\n+  Status AddOne(Datum column, std::shared_ptr<Int32Array>* fused_indices) {\n+    if (column.type()->id() != Type::DICTIONARY) {\n+      ARROW_ASSIGN_OR_RAISE(column, compute::DictionaryEncode(std::move(column)));\n+    }\n+\n+    auto dict_column = column.array_as<DictionaryArray>();\n+    dictionaries_.push_back(dict_column->dictionary());\n+    ARROW_ASSIGN_OR_RAISE(auto indices, compute::Cast(*dict_column->indices(), int32()));\n+\n+    if (*fused_indices == nullptr) {\n+      *fused_indices = checked_pointer_cast<Int32Array>(std::move(indices));\n+      return IncreaseSize();\n+    }\n+\n+    // It's useful to think about the case where each of dictionaries_ has size 10.\n+    // In this case the decimal digit in the ones place is the code in dictionaries_[0],\n+    // the tens place corresponds to the code in dictionaries_[1], etc.\n+    // The incumbent indices must be shifted to the hundreds place so as not to collide.\n+    ARROW_ASSIGN_OR_RAISE(Datum new_fused_indices,\n+                          compute::Multiply(indices, MakeScalar(size_)));\n+\n+    ARROW_ASSIGN_OR_RAISE(new_fused_indices,\n+                          compute::Add(new_fused_indices, *fused_indices));\n+\n+    *fused_indices = checked_pointer_cast<Int32Array>(new_fused_indices.make_array());\n+    return IncreaseSize();\n+  }\n+\n+  // expand a fused code into component dict codes, order is in order of addition\n+  void Expand(int32_t fused_code, int32_t* codes) {\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      auto dictionary_size = static_cast<int32_t>(dictionaries_[i]->length());\n+      codes[i] = fused_code % dictionary_size;\n+      fused_code /= dictionary_size;\n+    }\n+  }\n+\n+  Status RestoreDictionaryEncoding(std::shared_ptr<DictionaryType> expected_type,\n+                                   Datum* column) {\n+    DCHECK_NE(column->type()->id(), Type::DICTIONARY);\n+    ARROW_ASSIGN_OR_RAISE(*column, compute::DictionaryEncode(std::move(*column)));\n+\n+    if (expected_type->index_type()->id() == Type::INT32) {\n+      // dictionary_encode has already yielded the expected index_type\n+      return Status::OK();\n+    }\n+\n+    // cast the indices to the expected index type\n+    auto dictionary = std::move(column->mutable_array()->dictionary);\n+    column->mutable_array()->type = int32();\n+\n+    ARROW_ASSIGN_OR_RAISE(*column,\n+                          compute::Cast(std::move(*column), expected_type->index_type()));\n+\n+    column->mutable_array()->dictionary = std::move(dictionary);\n+    column->mutable_array()->type = expected_type;\n+    return Status::OK();\n+  }\n+\n+  Status IncreaseSize() {\n+    auto factor = static_cast<int32_t>(dictionaries_.back()->length());\n+\n+    if (arrow::internal::MultiplyWithOverflow(size_, factor, &size_)) {\n+      return Status::CapacityError(\"Max groups exceeded\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  int32_t size_ = 1;\n+  ArrayVector dictionaries_;\n+};\n+\n+Result<std::shared_ptr<StructArray>> MakeGroupings(const StructArray& keys) {\n+  if (keys.num_fields() == 0) {\n+    return Status::Invalid(\"Grouping with no keys\");\n+  }\n+\n+  if (keys.null_count() != 0) {\n+    return Status::Invalid(\"Grouping with null keys\");\n+  }\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused, StructDictionary::Encode(keys.fields()));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto sort_indices, compute::SortIndices(*fused.indices));\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted, compute::Take(fused.indices, *sort_indices));\n+  fused.indices = checked_pointer_cast<Int32Array>(sorted.make_array());\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused_counts_and_values,\n+                        compute::ValueCounts(fused.indices));\n+  fused.indices.reset();\n+\n+  auto unique_fused_indices =\n+      checked_pointer_cast<Int32Array>(fused_counts_and_values->GetFieldByName(\"values\"));\n+  ARROW_ASSIGN_OR_RAISE(\n+      auto unique_rows,\n+      fused.dictionary->Decode(std::move(unique_fused_indices), keys.type()->fields()));\n+\n+  auto counts =\n+      checked_pointer_cast<Int64Array>(fused_counts_and_values->GetFieldByName(\"counts\"));\n+  ARROW_ASSIGN_OR_RAISE(auto offsets, CountsToOffsets(std::move(counts)));\n+\n+  auto grouped_sort_indices =\n+      std::make_shared<ListArray>(list(sort_indices->type()), unique_rows->length(),\n+                                  std::move(offsets), std::move(sort_indices));\n+\n+  return StructArray::Make(\n+      ArrayVector{std::move(unique_rows), std::move(grouped_sort_indices)},\n+      std::vector<std::string>{\"values\", \"groupings\"});\n+}\n+\n+Result<std::shared_ptr<ListArray>> ApplyGroupings(const ListArray& groupings,\n+                                                  const Array& array) {\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted,\n+                        compute::Take(array, groupings.data()->child_data[0]));\n+\n+  return std::make_shared<ListArray>(list(array.type()), groupings.length(),\n+                                     groupings.value_offsets(), sorted.make_array());\n+}\n+\n+struct ScalarVectorToArray {\n\nReview comment:\n       Looks like it would be nice to have `AppendScalar` methods on common builders?\r\n   (perhaps even as a virtual function on the base builder class)\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n #include \"arrow/chunked_array.h\"\n #include \"arrow/compute/api_aggregate.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/kernels/aggregate_internal.h\"\n #include \"arrow/compute/kernels/test_util.h\"\n+#include \"arrow/compute/registry.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bitmap_reader.h\"\n #include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/int_util_internal.h\"\n \n #include \"arrow/testing/gtest_common.h\"\n #include \"arrow/testing/gtest_util.h\"\n #include \"arrow/testing/random.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n+using internal::BitmapReader;\n using internal::checked_cast;\n using internal::checked_pointer_cast;\n \n namespace compute {\n \n+// Copy-pasta from partition.cc\n+//\n+// In the finished product this will only be a test helper for group_by\n+// and partition.cc will rely on a no-aggregate call to group_by.\n+namespace group_helpers {\n+namespace {\n+\n+// Transform an array of counts to offsets which will divide a ListArray\n+// into an equal number of slices with corresponding lengths.\n+Result<std::shared_ptr<Buffer>> CountsToOffsets(std::shared_ptr<Int64Array> counts) {\n+  TypedBufferBuilder<int32_t> offset_builder;\n+  RETURN_NOT_OK(offset_builder.Resize(counts->length() + 1));\n+\n+  int32_t current_offset = 0;\n+  offset_builder.UnsafeAppend(current_offset);\n+\n+  for (int64_t i = 0; i < counts->length(); ++i) {\n+    DCHECK_NE(counts->Value(i), 0);\n+    current_offset += static_cast<int32_t>(counts->Value(i));\n+    offset_builder.UnsafeAppend(current_offset);\n+  }\n+\n+  std::shared_ptr<Buffer> offsets;\n+  RETURN_NOT_OK(offset_builder.Finish(&offsets));\n+  return offsets;\n+}\n+\n+class StructDictionary {\n+ public:\n+  struct Encoded {\n+    std::shared_ptr<Int32Array> indices;\n+    std::shared_ptr<StructDictionary> dictionary;\n+  };\n+\n+  static Result<Encoded> Encode(const ArrayVector& columns) {\n+    Encoded out{nullptr, std::make_shared<StructDictionary>()};\n+\n+    for (const auto& column : columns) {\n+      if (column->null_count() != 0) {\n+        return Status::NotImplemented(\"Grouping on a field with nulls\");\n+      }\n+\n+      RETURN_NOT_OK(out.dictionary->AddOne(column, &out.indices));\n+    }\n+\n+    return out;\n+  }\n+\n+  Result<std::shared_ptr<StructArray>> Decode(std::shared_ptr<Int32Array> fused_indices,\n+                                              FieldVector fields) {\n+    std::vector<Int32Builder> builders(dictionaries_.size());\n+    for (Int32Builder& b : builders) {\n+      RETURN_NOT_OK(b.Resize(fused_indices->length()));\n+    }\n+\n+    std::vector<int32_t> codes(dictionaries_.size());\n+    for (int64_t i = 0; i < fused_indices->length(); ++i) {\n+      Expand(fused_indices->Value(i), codes.data());\n+\n+      auto builder_it = builders.begin();\n+      for (int32_t index : codes) {\n+        builder_it++->UnsafeAppend(index);\n+      }\n+    }\n+\n+    ArrayVector columns(dictionaries_.size());\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      std::shared_ptr<ArrayData> indices;\n+      RETURN_NOT_OK(builders[i].FinishInternal(&indices));\n+\n+      ARROW_ASSIGN_OR_RAISE(Datum column, compute::Take(dictionaries_[i], indices));\n+\n+      if (fields[i]->type()->id() == Type::DICTIONARY) {\n+        RETURN_NOT_OK(RestoreDictionaryEncoding(\n+            checked_pointer_cast<DictionaryType>(fields[i]->type()), &column));\n+      }\n+\n+      columns[i] = column.make_array();\n+    }\n+\n+    return StructArray::Make(std::move(columns), std::move(fields));\n+  }\n+\n+ private:\n+  Status AddOne(Datum column, std::shared_ptr<Int32Array>* fused_indices) {\n+    if (column.type()->id() != Type::DICTIONARY) {\n+      ARROW_ASSIGN_OR_RAISE(column, compute::DictionaryEncode(std::move(column)));\n+    }\n+\n+    auto dict_column = column.array_as<DictionaryArray>();\n+    dictionaries_.push_back(dict_column->dictionary());\n+    ARROW_ASSIGN_OR_RAISE(auto indices, compute::Cast(*dict_column->indices(), int32()));\n+\n+    if (*fused_indices == nullptr) {\n+      *fused_indices = checked_pointer_cast<Int32Array>(std::move(indices));\n+      return IncreaseSize();\n+    }\n+\n+    // It's useful to think about the case where each of dictionaries_ has size 10.\n+    // In this case the decimal digit in the ones place is the code in dictionaries_[0],\n+    // the tens place corresponds to the code in dictionaries_[1], etc.\n+    // The incumbent indices must be shifted to the hundreds place so as not to collide.\n+    ARROW_ASSIGN_OR_RAISE(Datum new_fused_indices,\n+                          compute::Multiply(indices, MakeScalar(size_)));\n+\n+    ARROW_ASSIGN_OR_RAISE(new_fused_indices,\n+                          compute::Add(new_fused_indices, *fused_indices));\n+\n+    *fused_indices = checked_pointer_cast<Int32Array>(new_fused_indices.make_array());\n+    return IncreaseSize();\n+  }\n+\n+  // expand a fused code into component dict codes, order is in order of addition\n+  void Expand(int32_t fused_code, int32_t* codes) {\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      auto dictionary_size = static_cast<int32_t>(dictionaries_[i]->length());\n+      codes[i] = fused_code % dictionary_size;\n+      fused_code /= dictionary_size;\n+    }\n+  }\n+\n+  Status RestoreDictionaryEncoding(std::shared_ptr<DictionaryType> expected_type,\n+                                   Datum* column) {\n+    DCHECK_NE(column->type()->id(), Type::DICTIONARY);\n+    ARROW_ASSIGN_OR_RAISE(*column, compute::DictionaryEncode(std::move(*column)));\n+\n+    if (expected_type->index_type()->id() == Type::INT32) {\n+      // dictionary_encode has already yielded the expected index_type\n+      return Status::OK();\n+    }\n+\n+    // cast the indices to the expected index type\n+    auto dictionary = std::move(column->mutable_array()->dictionary);\n+    column->mutable_array()->type = int32();\n+\n+    ARROW_ASSIGN_OR_RAISE(*column,\n+                          compute::Cast(std::move(*column), expected_type->index_type()));\n+\n+    column->mutable_array()->dictionary = std::move(dictionary);\n+    column->mutable_array()->type = expected_type;\n+    return Status::OK();\n+  }\n+\n+  Status IncreaseSize() {\n+    auto factor = static_cast<int32_t>(dictionaries_.back()->length());\n+\n+    if (arrow::internal::MultiplyWithOverflow(size_, factor, &size_)) {\n+      return Status::CapacityError(\"Max groups exceeded\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  int32_t size_ = 1;\n+  ArrayVector dictionaries_;\n+};\n+\n+Result<std::shared_ptr<StructArray>> MakeGroupings(const StructArray& keys) {\n+  if (keys.num_fields() == 0) {\n+    return Status::Invalid(\"Grouping with no keys\");\n+  }\n+\n+  if (keys.null_count() != 0) {\n+    return Status::Invalid(\"Grouping with null keys\");\n+  }\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused, StructDictionary::Encode(keys.fields()));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto sort_indices, compute::SortIndices(*fused.indices));\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted, compute::Take(fused.indices, *sort_indices));\n+  fused.indices = checked_pointer_cast<Int32Array>(sorted.make_array());\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused_counts_and_values,\n+                        compute::ValueCounts(fused.indices));\n+  fused.indices.reset();\n+\n+  auto unique_fused_indices =\n+      checked_pointer_cast<Int32Array>(fused_counts_and_values->GetFieldByName(\"values\"));\n+  ARROW_ASSIGN_OR_RAISE(\n+      auto unique_rows,\n+      fused.dictionary->Decode(std::move(unique_fused_indices), keys.type()->fields()));\n+\n+  auto counts =\n+      checked_pointer_cast<Int64Array>(fused_counts_and_values->GetFieldByName(\"counts\"));\n+  ARROW_ASSIGN_OR_RAISE(auto offsets, CountsToOffsets(std::move(counts)));\n+\n+  auto grouped_sort_indices =\n+      std::make_shared<ListArray>(list(sort_indices->type()), unique_rows->length(),\n+                                  std::move(offsets), std::move(sort_indices));\n+\n+  return StructArray::Make(\n+      ArrayVector{std::move(unique_rows), std::move(grouped_sort_indices)},\n+      std::vector<std::string>{\"values\", \"groupings\"});\n+}\n+\n+Result<std::shared_ptr<ListArray>> ApplyGroupings(const ListArray& groupings,\n+                                                  const Array& array) {\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted,\n+                        compute::Take(array, groupings.data()->child_data[0]));\n+\n+  return std::make_shared<ListArray>(list(array.type()), groupings.length(),\n+                                     groupings.value_offsets(), sorted.make_array());\n+}\n+\n+struct ScalarVectorToArray {\n+  template <typename T, typename AppendScalar,\n+            typename BuilderType = typename TypeTraits<T>::BuilderType,\n+            typename ScalarType = typename TypeTraits<T>::ScalarType>\n+  Status UseBuilder(const AppendScalar& append) {\n+    BuilderType builder(type(), default_memory_pool());\n+    for (const auto& s : scalars_) {\n+      if (s->is_valid) {\n+        RETURN_NOT_OK(append(checked_cast<const ScalarType&>(*s), &builder));\n+      } else {\n+        RETURN_NOT_OK(builder.AppendNull());\n+      }\n+    }\n+    return builder.FinishInternal(&data_);\n+  }\n+\n+  struct AppendValue {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      return builder->Append(s.value);\n+    }\n+  };\n+\n+  struct AppendBuffer {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      const Buffer& buffer = *s.value;\n+      return builder->Append(util::string_view{buffer});\n+    }\n+  };\n+\n+  template <typename T>\n+  enable_if_primitive_ctype<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendValue{});\n+  }\n+\n+  template <typename T>\n+  enable_if_has_string_view<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendBuffer{});\n+  }\n+\n+  Status Visit(const DataType& type) {\n+    return Status::NotImplemented(\"ScalarVectorToArray for type \", type);\n+  }\n+\n+  Result<Datum> Convert(ScalarVector scalars) && {\n+    if (scalars.size() == 0) {\n+      return Status::NotImplemented(\"ScalarVectorToArray with no scalars\");\n+    }\n+    scalars_ = std::move(scalars);\n+    RETURN_NOT_OK(VisitTypeInline(*type(), this));\n+    return Datum(std::move(data_));\n+  }\n+\n+  const std::shared_ptr<DataType>& type() { return scalars_[0]->type; }\n+\n+  ScalarVector scalars_;\n+  std::shared_ptr<ArrayData> data_;\n+};\n+\n+Result<Datum> NaiveGroupBy(std::vector<Datum> aggregands, std::vector<Datum> keys,\n+                           GroupByOptions options) {\n+  ArrayVector keys_arrays;\n+  for (const Datum& key : keys) keys_arrays.push_back(key.make_array());\n+  std::vector<std::string> key_names(keys_arrays.size(), \"\");\n+  ARROW_ASSIGN_OR_RAISE(auto keys_struct,\n+                        StructArray::Make(std::move(keys_arrays), std::move(key_names)));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto groupings_and_values, MakeGroupings(*keys_struct));\n+\n+  auto groupings =\n+      checked_pointer_cast<ListArray>(groupings_and_values->GetFieldByName(\"groupings\"));\n+\n+  int64_t n_groups = groupings->length();\n+\n+  ArrayVector out_columns;\n+\n+  for (size_t i_agg = 0; i_agg < aggregands.size(); ++i_agg) {\n+    const Datum& aggregand = aggregands[i_agg];\n+    const std::string& function = options.aggregates[i_agg].function;\n+\n+    ScalarVector aggregated_scalars;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto grouped_aggregand,\n+                          ApplyGroupings(*groupings, *aggregand.make_array()));\n+\n+    for (int64_t i_group = 0; i_group < n_groups; ++i_group) {\n+      ARROW_ASSIGN_OR_RAISE(\n+          Datum d, CallFunction(function, {grouped_aggregand->value_slice(i_group)}));\n+      aggregated_scalars.push_back(d.scalar());\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(Datum aggregated_column,\n+                          ScalarVectorToArray{}.Convert(std::move(aggregated_scalars)));\n+    out_columns.push_back(aggregated_column.make_array());\n+  }\n+\n+  keys_struct =\n+      checked_pointer_cast<StructArray>(groupings_and_values->GetFieldByName(\"values\"));\n+  for (size_t i_key = 0; i_key < aggregands.size(); ++i_key) {\n+    out_columns.push_back(keys_struct->field(i_key));\n+  }\n+\n+  std::vector<std::string> out_names(out_columns.size(), \"\");\n+  return StructArray::Make(std::move(out_columns), std::move(out_names));\n+}\n+\n+void ValidateGroupBy(GroupByOptions options, std::vector<Datum> aggregands,\n+                     std::vector<Datum> keys) {\n+  ASSERT_OK_AND_ASSIGN(Datum expected,\n+                       group_helpers::NaiveGroupBy(aggregands, keys, options));\n+\n+  ASSERT_OK_AND_ASSIGN(Datum actual, GroupBy(aggregands, keys, options));\n+\n+  // Ordering of groups is not important, so sort by key columns to ensure the comparison\n+  // doesn't fail spuriously\n+\n+  for (Datum* out : {&expected, &actual}) {\n+    auto out_columns = out->array_as<StructArray>()->fields();\n+\n+    SortOptions sort_options;\n+    FieldVector key_fields;\n+    ArrayVector key_columns;\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      auto name = std::to_string(i);\n+      sort_options.sort_keys.emplace_back(name);\n+      key_fields.push_back(field(name, out_columns[0]->type()));\n+      key_columns.push_back(out_columns[0]);\n+    }\n+    auto key_batch = RecordBatch::Make(schema(std::move(key_fields)), out->length(),\n+                                       std::move(key_columns));\n+\n+    ASSERT_OK_AND_ASSIGN(Datum sort_indices, SortIndices(key_batch, sort_options));\n+    ASSERT_OK_AND_ASSIGN(*out, Take(*out, sort_indices, TakeOptions::NoBoundsCheck()));\n+  }\n+\n+  AssertDatumsEqual(expected, actual, /*verbose=*/true);\n+}\n+\n+}  // namespace\n+}  // namespace group_helpers\n+\n+TEST(GroupBy, SumOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n\nReview comment:\n       It seems it would be better to test with more than one null in the keys array.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n\nReview comment:\n       Maybe indeed? It seems weird to call max every time on a bunch of group ids we just computed.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n\nReview comment:\n       Since I would expect grouped aggregation to have many tests, it may be better to create a new test module for it.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n #include \"arrow/chunked_array.h\"\n #include \"arrow/compute/api_aggregate.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/kernels/aggregate_internal.h\"\n #include \"arrow/compute/kernels/test_util.h\"\n+#include \"arrow/compute/registry.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bitmap_reader.h\"\n #include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/int_util_internal.h\"\n \n #include \"arrow/testing/gtest_common.h\"\n #include \"arrow/testing/gtest_util.h\"\n #include \"arrow/testing/random.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n+using internal::BitmapReader;\n using internal::checked_cast;\n using internal::checked_pointer_cast;\n \n namespace compute {\n \n+// Copy-pasta from partition.cc\n+//\n+// In the finished product this will only be a test helper for group_by\n+// and partition.cc will rely on a no-aggregate call to group_by.\n+namespace group_helpers {\n+namespace {\n+\n+// Transform an array of counts to offsets which will divide a ListArray\n+// into an equal number of slices with corresponding lengths.\n+Result<std::shared_ptr<Buffer>> CountsToOffsets(std::shared_ptr<Int64Array> counts) {\n+  TypedBufferBuilder<int32_t> offset_builder;\n+  RETURN_NOT_OK(offset_builder.Resize(counts->length() + 1));\n+\n+  int32_t current_offset = 0;\n+  offset_builder.UnsafeAppend(current_offset);\n+\n+  for (int64_t i = 0; i < counts->length(); ++i) {\n+    DCHECK_NE(counts->Value(i), 0);\n+    current_offset += static_cast<int32_t>(counts->Value(i));\n+    offset_builder.UnsafeAppend(current_offset);\n+  }\n+\n+  std::shared_ptr<Buffer> offsets;\n+  RETURN_NOT_OK(offset_builder.Finish(&offsets));\n+  return offsets;\n+}\n+\n+class StructDictionary {\n+ public:\n+  struct Encoded {\n+    std::shared_ptr<Int32Array> indices;\n+    std::shared_ptr<StructDictionary> dictionary;\n+  };\n+\n+  static Result<Encoded> Encode(const ArrayVector& columns) {\n+    Encoded out{nullptr, std::make_shared<StructDictionary>()};\n+\n+    for (const auto& column : columns) {\n+      if (column->null_count() != 0) {\n+        return Status::NotImplemented(\"Grouping on a field with nulls\");\n+      }\n+\n+      RETURN_NOT_OK(out.dictionary->AddOne(column, &out.indices));\n+    }\n+\n+    return out;\n+  }\n+\n+  Result<std::shared_ptr<StructArray>> Decode(std::shared_ptr<Int32Array> fused_indices,\n+                                              FieldVector fields) {\n+    std::vector<Int32Builder> builders(dictionaries_.size());\n+    for (Int32Builder& b : builders) {\n+      RETURN_NOT_OK(b.Resize(fused_indices->length()));\n+    }\n+\n+    std::vector<int32_t> codes(dictionaries_.size());\n+    for (int64_t i = 0; i < fused_indices->length(); ++i) {\n+      Expand(fused_indices->Value(i), codes.data());\n+\n+      auto builder_it = builders.begin();\n+      for (int32_t index : codes) {\n+        builder_it++->UnsafeAppend(index);\n+      }\n+    }\n+\n+    ArrayVector columns(dictionaries_.size());\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      std::shared_ptr<ArrayData> indices;\n+      RETURN_NOT_OK(builders[i].FinishInternal(&indices));\n+\n+      ARROW_ASSIGN_OR_RAISE(Datum column, compute::Take(dictionaries_[i], indices));\n+\n+      if (fields[i]->type()->id() == Type::DICTIONARY) {\n+        RETURN_NOT_OK(RestoreDictionaryEncoding(\n+            checked_pointer_cast<DictionaryType>(fields[i]->type()), &column));\n+      }\n+\n+      columns[i] = column.make_array();\n+    }\n+\n+    return StructArray::Make(std::move(columns), std::move(fields));\n+  }\n+\n+ private:\n+  Status AddOne(Datum column, std::shared_ptr<Int32Array>* fused_indices) {\n+    if (column.type()->id() != Type::DICTIONARY) {\n+      ARROW_ASSIGN_OR_RAISE(column, compute::DictionaryEncode(std::move(column)));\n+    }\n+\n+    auto dict_column = column.array_as<DictionaryArray>();\n+    dictionaries_.push_back(dict_column->dictionary());\n+    ARROW_ASSIGN_OR_RAISE(auto indices, compute::Cast(*dict_column->indices(), int32()));\n+\n+    if (*fused_indices == nullptr) {\n+      *fused_indices = checked_pointer_cast<Int32Array>(std::move(indices));\n+      return IncreaseSize();\n+    }\n+\n+    // It's useful to think about the case where each of dictionaries_ has size 10.\n+    // In this case the decimal digit in the ones place is the code in dictionaries_[0],\n+    // the tens place corresponds to the code in dictionaries_[1], etc.\n+    // The incumbent indices must be shifted to the hundreds place so as not to collide.\n+    ARROW_ASSIGN_OR_RAISE(Datum new_fused_indices,\n+                          compute::Multiply(indices, MakeScalar(size_)));\n+\n+    ARROW_ASSIGN_OR_RAISE(new_fused_indices,\n+                          compute::Add(new_fused_indices, *fused_indices));\n+\n+    *fused_indices = checked_pointer_cast<Int32Array>(new_fused_indices.make_array());\n+    return IncreaseSize();\n+  }\n+\n+  // expand a fused code into component dict codes, order is in order of addition\n+  void Expand(int32_t fused_code, int32_t* codes) {\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      auto dictionary_size = static_cast<int32_t>(dictionaries_[i]->length());\n+      codes[i] = fused_code % dictionary_size;\n+      fused_code /= dictionary_size;\n+    }\n+  }\n+\n+  Status RestoreDictionaryEncoding(std::shared_ptr<DictionaryType> expected_type,\n+                                   Datum* column) {\n+    DCHECK_NE(column->type()->id(), Type::DICTIONARY);\n+    ARROW_ASSIGN_OR_RAISE(*column, compute::DictionaryEncode(std::move(*column)));\n+\n+    if (expected_type->index_type()->id() == Type::INT32) {\n+      // dictionary_encode has already yielded the expected index_type\n+      return Status::OK();\n+    }\n+\n+    // cast the indices to the expected index type\n+    auto dictionary = std::move(column->mutable_array()->dictionary);\n+    column->mutable_array()->type = int32();\n+\n+    ARROW_ASSIGN_OR_RAISE(*column,\n+                          compute::Cast(std::move(*column), expected_type->index_type()));\n+\n+    column->mutable_array()->dictionary = std::move(dictionary);\n+    column->mutable_array()->type = expected_type;\n+    return Status::OK();\n+  }\n+\n+  Status IncreaseSize() {\n+    auto factor = static_cast<int32_t>(dictionaries_.back()->length());\n+\n+    if (arrow::internal::MultiplyWithOverflow(size_, factor, &size_)) {\n+      return Status::CapacityError(\"Max groups exceeded\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  int32_t size_ = 1;\n+  ArrayVector dictionaries_;\n+};\n+\n+Result<std::shared_ptr<StructArray>> MakeGroupings(const StructArray& keys) {\n+  if (keys.num_fields() == 0) {\n+    return Status::Invalid(\"Grouping with no keys\");\n+  }\n+\n+  if (keys.null_count() != 0) {\n+    return Status::Invalid(\"Grouping with null keys\");\n+  }\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused, StructDictionary::Encode(keys.fields()));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto sort_indices, compute::SortIndices(*fused.indices));\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted, compute::Take(fused.indices, *sort_indices));\n+  fused.indices = checked_pointer_cast<Int32Array>(sorted.make_array());\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused_counts_and_values,\n+                        compute::ValueCounts(fused.indices));\n+  fused.indices.reset();\n+\n+  auto unique_fused_indices =\n+      checked_pointer_cast<Int32Array>(fused_counts_and_values->GetFieldByName(\"values\"));\n+  ARROW_ASSIGN_OR_RAISE(\n+      auto unique_rows,\n+      fused.dictionary->Decode(std::move(unique_fused_indices), keys.type()->fields()));\n+\n+  auto counts =\n+      checked_pointer_cast<Int64Array>(fused_counts_and_values->GetFieldByName(\"counts\"));\n+  ARROW_ASSIGN_OR_RAISE(auto offsets, CountsToOffsets(std::move(counts)));\n+\n+  auto grouped_sort_indices =\n+      std::make_shared<ListArray>(list(sort_indices->type()), unique_rows->length(),\n+                                  std::move(offsets), std::move(sort_indices));\n+\n+  return StructArray::Make(\n+      ArrayVector{std::move(unique_rows), std::move(grouped_sort_indices)},\n+      std::vector<std::string>{\"values\", \"groupings\"});\n+}\n+\n+Result<std::shared_ptr<ListArray>> ApplyGroupings(const ListArray& groupings,\n+                                                  const Array& array) {\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted,\n+                        compute::Take(array, groupings.data()->child_data[0]));\n+\n+  return std::make_shared<ListArray>(list(array.type()), groupings.length(),\n+                                     groupings.value_offsets(), sorted.make_array());\n+}\n+\n+struct ScalarVectorToArray {\n+  template <typename T, typename AppendScalar,\n+            typename BuilderType = typename TypeTraits<T>::BuilderType,\n+            typename ScalarType = typename TypeTraits<T>::ScalarType>\n+  Status UseBuilder(const AppendScalar& append) {\n+    BuilderType builder(type(), default_memory_pool());\n+    for (const auto& s : scalars_) {\n+      if (s->is_valid) {\n+        RETURN_NOT_OK(append(checked_cast<const ScalarType&>(*s), &builder));\n+      } else {\n+        RETURN_NOT_OK(builder.AppendNull());\n+      }\n+    }\n+    return builder.FinishInternal(&data_);\n+  }\n+\n+  struct AppendValue {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      return builder->Append(s.value);\n+    }\n+  };\n+\n+  struct AppendBuffer {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      const Buffer& buffer = *s.value;\n+      return builder->Append(util::string_view{buffer});\n+    }\n+  };\n+\n+  template <typename T>\n+  enable_if_primitive_ctype<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendValue{});\n+  }\n+\n+  template <typename T>\n+  enable_if_has_string_view<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendBuffer{});\n+  }\n+\n+  Status Visit(const DataType& type) {\n+    return Status::NotImplemented(\"ScalarVectorToArray for type \", type);\n+  }\n+\n+  Result<Datum> Convert(ScalarVector scalars) && {\n+    if (scalars.size() == 0) {\n+      return Status::NotImplemented(\"ScalarVectorToArray with no scalars\");\n+    }\n+    scalars_ = std::move(scalars);\n+    RETURN_NOT_OK(VisitTypeInline(*type(), this));\n+    return Datum(std::move(data_));\n+  }\n+\n+  const std::shared_ptr<DataType>& type() { return scalars_[0]->type; }\n+\n+  ScalarVector scalars_;\n+  std::shared_ptr<ArrayData> data_;\n+};\n+\n+Result<Datum> NaiveGroupBy(std::vector<Datum> aggregands, std::vector<Datum> keys,\n+                           GroupByOptions options) {\n+  ArrayVector keys_arrays;\n+  for (const Datum& key : keys) keys_arrays.push_back(key.make_array());\n+  std::vector<std::string> key_names(keys_arrays.size(), \"\");\n+  ARROW_ASSIGN_OR_RAISE(auto keys_struct,\n+                        StructArray::Make(std::move(keys_arrays), std::move(key_names)));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto groupings_and_values, MakeGroupings(*keys_struct));\n+\n+  auto groupings =\n+      checked_pointer_cast<ListArray>(groupings_and_values->GetFieldByName(\"groupings\"));\n+\n+  int64_t n_groups = groupings->length();\n+\n+  ArrayVector out_columns;\n+\n+  for (size_t i_agg = 0; i_agg < aggregands.size(); ++i_agg) {\n+    const Datum& aggregand = aggregands[i_agg];\n+    const std::string& function = options.aggregates[i_agg].function;\n+\n+    ScalarVector aggregated_scalars;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto grouped_aggregand,\n+                          ApplyGroupings(*groupings, *aggregand.make_array()));\n+\n+    for (int64_t i_group = 0; i_group < n_groups; ++i_group) {\n+      ARROW_ASSIGN_OR_RAISE(\n+          Datum d, CallFunction(function, {grouped_aggregand->value_slice(i_group)}));\n+      aggregated_scalars.push_back(d.scalar());\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(Datum aggregated_column,\n+                          ScalarVectorToArray{}.Convert(std::move(aggregated_scalars)));\n+    out_columns.push_back(aggregated_column.make_array());\n+  }\n+\n+  keys_struct =\n+      checked_pointer_cast<StructArray>(groupings_and_values->GetFieldByName(\"values\"));\n+  for (size_t i_key = 0; i_key < aggregands.size(); ++i_key) {\n+    out_columns.push_back(keys_struct->field(i_key));\n+  }\n+\n+  std::vector<std::string> out_names(out_columns.size(), \"\");\n+  return StructArray::Make(std::move(out_columns), std::move(out_names));\n+}\n+\n+void ValidateGroupBy(GroupByOptions options, std::vector<Datum> aggregands,\n+                     std::vector<Datum> keys) {\n+  ASSERT_OK_AND_ASSIGN(Datum expected,\n+                       group_helpers::NaiveGroupBy(aggregands, keys, options));\n+\n+  ASSERT_OK_AND_ASSIGN(Datum actual, GroupBy(aggregands, keys, options));\n+\n+  // Ordering of groups is not important, so sort by key columns to ensure the comparison\n+  // doesn't fail spuriously\n+\n+  for (Datum* out : {&expected, &actual}) {\n+    auto out_columns = out->array_as<StructArray>()->fields();\n+\n+    SortOptions sort_options;\n+    FieldVector key_fields;\n+    ArrayVector key_columns;\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      auto name = std::to_string(i);\n+      sort_options.sort_keys.emplace_back(name);\n+      key_fields.push_back(field(name, out_columns[0]->type()));\n+      key_columns.push_back(out_columns[0]);\n+    }\n+    auto key_batch = RecordBatch::Make(schema(std::move(key_fields)), out->length(),\n+                                       std::move(key_columns));\n+\n+    ASSERT_OK_AND_ASSIGN(Datum sort_indices, SortIndices(key_batch, sort_options));\n+    ASSERT_OK_AND_ASSIGN(*out, Take(*out, sort_indices, TakeOptions::NoBoundsCheck()));\n+  }\n+\n+  AssertDatumsEqual(expected, actual, /*verbose=*/true);\n+}\n+\n+}  // namespace\n+}  // namespace group_helpers\n+\n+TEST(GroupBy, SumOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped, GroupBy({aggregand}, {key},\n+                                                             GroupByOptions{\n+                                                                 {\"sum\", nullptr},\n+                                                             }));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", float64()),\n+                                      field(\"\", int64()),\n+                                  }),\n+                                  R\"([\n+    [4.25,   1],\n+    [-0.125, 2],\n+    [null,   3],\n+    [0.75,   null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, MinMaxOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped, GroupBy({aggregand}, {key},\n+                                                             GroupByOptions{\n+                                                                 {\"min_max\", nullptr},\n+                                                             }));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", struct_({\n+                                                    field(\"min\", float64()),\n+                                                    field(\"max\", float64()),\n+                                                })),\n+                                      field(\"\", int64()),\n+                                  }),\n+                                  R\"([\n+    [{\"min\": 1.0,   \"max\": 3.25},  1],\n+    [{\"min\": -0.25, \"max\": 0.125}, 2],\n+    [{\"min\": null,  \"max\": null},  3],\n+    [{\"min\": 0.75,  \"max\": 0.75},  null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, CountAndSum) {\n+  auto aggregand = ArrayFromJSON(float32(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 1, 3, 2, 3, null]\");\n+\n+  CountOptions count_options;\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      Datum aggregated_and_grouped,\n+      // NB: passing an aggregand twice or also using it as a key is legal\n+      GroupBy({aggregand, aggregand, key}, {key},\n+              GroupByOptions{\n+                  {\"count\", &count_options},\n+                  {\"sum\", nullptr},\n+                  {\"sum\", nullptr},\n+              }));\n+\n+  AssertDatumsEqual(\n+      ArrayFromJSON(struct_({\n+                        field(\"\", int64()),\n+                        // NB: summing a float32 array results in float64 sums\n+                        field(\"\", float64()),\n+                        field(\"\", int64()),\n+                        field(\"\", int64()),\n+                    }),\n+                    R\"([\n+    [1, 1.0,   2,    1],\n+    [2, 0.125, 4,    2],\n+    [2, 3.0,   6,    3],\n+    [1, 0.75,  null, null]\n+  ])\"),\n+      aggregated_and_grouped,\n+      /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, StringKey) {\n+  auto aggregand = ArrayFromJSON(int64(), \"[10, 5, 4, 2, 12, 9]\");\n+  auto key = ArrayFromJSON(utf8(), R\"([\"alfa\", \"beta\", \"gamma\", \"gamma\", null, \"beta\"])\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped,\n+                       GroupBy({aggregand}, {key}, GroupByOptions{{\"sum\", nullptr}}));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", int64()),\n+                                      field(\"\", utf8()),\n+                                  }),\n+                                  R\"([\n+    [10,   \"alfa\"],\n+    [14,   \"beta\"],\n+    [6,    \"gamma\"],\n+    [12,   null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, MultipleKeys) {\n+  auto aggregand = ArrayFromJSON(float32(), \"[0.125, 0.5, -0.75, 8, 1.0, 2.0]\");\n+  auto int_key = ArrayFromJSON(int32(), \"[0, 1, 0, 1, 0, 1]\");\n+  auto str_key =\n+      ArrayFromJSON(utf8(), R\"([\"beta\", \"beta\", \"gamma\", \"gamma\", null, \"beta\"])\");\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      Datum aggregated_and_grouped,\n+      GroupBy({aggregand}, {int_key, str_key}, GroupByOptions{{\"sum\", nullptr}}));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", float64()),\n+                                      field(\"\", int32()),\n+                                      field(\"\", utf8()),\n+                                  }),\n+                                  R\"([\n+    [0.125, 0, \"beta\"],\n+    [2.5,   1, \"beta\"],\n+    [-0.75, 0, \"gamma\"],\n+    [8,     1, \"gamma\"],\n+    [1.0,   0, null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, ConcreteCaseWithValidateGroupBy) {\n+  auto aggregand = ArrayFromJSON(int64(), \"[10, 5, 4, 2, 12]\");\n+  auto key = ArrayFromJSON(utf8(), R\"([\"alfa\", \"beta\", \"gamma\", \"gamma\", \"beta\"])\");\n+\n+  group_helpers::ValidateGroupBy(GroupByOptions{{\"sum\", nullptr}}, {aggregand}, {key});\n+}\n+\n+TEST(GroupBy, RandomArraySum) {\n+  auto rand = random::RandomArrayGenerator(0xdeadbeef);\n+\n+  for (size_t i = 3; i < 14; i += 2) {\n+    for (auto null_probability : {0.0, 0.001, 0.1, 0.5, 0.999, 1.0}) {\n+      int64_t length = 1UL << i;\n\nReview comment:\n       Do you expect it to be useful to test all these different lengths and null probabilities?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n\nReview comment:\n       Should use `sizeof(int64_t)` for this one.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n\nReview comment:\n       So the counts are only used to know whether a group had non-null values in it, right?\r\n   Perhaps it could be an UInt8Array of \"boolean\" flags, then, indicating that a non-null value was encountered in the given group?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n+\n+      if (null_bitmap == nullptr) {\n+        KERNEL_ASSIGN_OR_RAISE(null_bitmap, ctx, ctx->AllocateBitmap(num_groups()));\n+        BitUtil::SetBitsTo(null_bitmap->mutable_data(), 0, num_groups(), true);\n+      }\n+\n+      null_count += 1;\n+      BitUtil::SetBitTo(null_bitmap->mutable_data(), i, false);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type_), num_groups(),\n+                           {std::move(null_bitmap), std::move(sums_)}, null_count);\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return out_type_; }\n+\n+  std::shared_ptr<ResizableBuffer> sums_, counts_;\n+  std::shared_ptr<DataType> out_type_;\n+  ConsumeImpl consume_impl_;\n+};\n+\n+struct GroupedMinMaxImpl : public GroupedAggregator {\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, BufferVector*)>;\n+\n+  using ResizeImpl = std::function<Status(Buffer*, int64_t)>;\n+\n+  struct GetImpl {\n+    template <typename T, typename CType = typename TypeTraits<T>::CType>\n+    enable_if_number<T, Status> Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, BufferVector* buffers) {\n+        auto raw_inputs = reinterpret_cast<const CType*>(input->buffers[1]->data());\n+\n+        auto raw_mins = reinterpret_cast<CType*>(buffers->at(0)->mutable_data());\n+        auto raw_maxes = reinterpret_cast<CType*>(buffers->at(1)->mutable_data());\n+\n+        auto raw_has_nulls = buffers->at(2)->mutable_data();\n+        auto raw_has_values = buffers->at(3)->mutable_data();\n+\n+        for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+          auto g = group_ids[i];\n+          bool is_valid = BitUtil::GetBit(input->buffers[0]->data(), input_i);\n\nReview comment:\n       What if there is no null bitmap? You probably want to use something like `VisitBitBlocks` or `VisitArrayDataInline` instead.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n\nReview comment:\n       Should add some general comments. Is this implementing the \"pivot\" operation?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n\nReview comment:\n       Convenient :-)\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n+\n+      if (null_bitmap == nullptr) {\n+        KERNEL_ASSIGN_OR_RAISE(null_bitmap, ctx, ctx->AllocateBitmap(num_groups()));\n+        BitUtil::SetBitsTo(null_bitmap->mutable_data(), 0, num_groups(), true);\n+      }\n+\n+      null_count += 1;\n+      BitUtil::SetBitTo(null_bitmap->mutable_data(), i, false);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type_), num_groups(),\n+                           {std::move(null_bitmap), std::move(sums_)}, null_count);\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return out_type_; }\n+\n+  std::shared_ptr<ResizableBuffer> sums_, counts_;\n+  std::shared_ptr<DataType> out_type_;\n+  ConsumeImpl consume_impl_;\n+};\n+\n+struct GroupedMinMaxImpl : public GroupedAggregator {\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, BufferVector*)>;\n+\n+  using ResizeImpl = std::function<Status(Buffer*, int64_t)>;\n+\n+  struct GetImpl {\n+    template <typename T, typename CType = typename TypeTraits<T>::CType>\n+    enable_if_number<T, Status> Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, BufferVector* buffers) {\n+        auto raw_inputs = reinterpret_cast<const CType*>(input->buffers[1]->data());\n+\n+        auto raw_mins = reinterpret_cast<CType*>(buffers->at(0)->mutable_data());\n+        auto raw_maxes = reinterpret_cast<CType*>(buffers->at(1)->mutable_data());\n+\n+        auto raw_has_nulls = buffers->at(2)->mutable_data();\n+        auto raw_has_values = buffers->at(3)->mutable_data();\n+\n+        for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+          auto g = group_ids[i];\n+          bool is_valid = BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+          if (is_valid) {\n+            raw_maxes[g] = std::max(raw_maxes[g], raw_inputs[input_i]);\n+            raw_mins[g] = std::min(raw_mins[g], raw_inputs[input_i]);\n+            BitUtil::SetBit(raw_has_values, g);\n+          } else {\n+            BitUtil::SetBit(raw_has_nulls, g);\n+          }\n+        }\n+      };\n+\n+      for (auto pair :\n+           {std::make_pair(&resize_min_impl, std::numeric_limits<CType>::max()),\n+            std::make_pair(&resize_max_impl, std::numeric_limits<CType>::min())}) {\n+        *pair.first = [pair](Buffer* vals, int64_t new_num_groups) {\n\nReview comment:\n       I think this a bit convoluted, though I understand it allows you to write a nice iteration-based `for` loop.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n\nReview comment:\n       Use `VisitArrayDataInline`?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n+\n+      if (null_bitmap == nullptr) {\n+        KERNEL_ASSIGN_OR_RAISE(null_bitmap, ctx, ctx->AllocateBitmap(num_groups()));\n+        BitUtil::SetBitsTo(null_bitmap->mutable_data(), 0, num_groups(), true);\n+      }\n+\n+      null_count += 1;\n+      BitUtil::SetBitTo(null_bitmap->mutable_data(), i, false);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type_), num_groups(),\n+                           {std::move(null_bitmap), std::move(sums_)}, null_count);\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return out_type_; }\n+\n+  std::shared_ptr<ResizableBuffer> sums_, counts_;\n+  std::shared_ptr<DataType> out_type_;\n+  ConsumeImpl consume_impl_;\n+};\n+\n+struct GroupedMinMaxImpl : public GroupedAggregator {\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, BufferVector*)>;\n+\n+  using ResizeImpl = std::function<Status(Buffer*, int64_t)>;\n+\n+  struct GetImpl {\n+    template <typename T, typename CType = typename TypeTraits<T>::CType>\n+    enable_if_number<T, Status> Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, BufferVector* buffers) {\n+        auto raw_inputs = reinterpret_cast<const CType*>(input->buffers[1]->data());\n+\n+        auto raw_mins = reinterpret_cast<CType*>(buffers->at(0)->mutable_data());\n+        auto raw_maxes = reinterpret_cast<CType*>(buffers->at(1)->mutable_data());\n+\n+        auto raw_has_nulls = buffers->at(2)->mutable_data();\n+        auto raw_has_values = buffers->at(3)->mutable_data();\n+\n+        for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+          auto g = group_ids[i];\n+          bool is_valid = BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+          if (is_valid) {\n+            raw_maxes[g] = std::max(raw_maxes[g], raw_inputs[input_i]);\n+            raw_mins[g] = std::min(raw_mins[g], raw_inputs[input_i]);\n+            BitUtil::SetBit(raw_has_values, g);\n+          } else {\n+            BitUtil::SetBit(raw_has_nulls, g);\n+          }\n+        }\n+      };\n+\n+      for (auto pair :\n+           {std::make_pair(&resize_min_impl, std::numeric_limits<CType>::max()),\n+            std::make_pair(&resize_max_impl, std::numeric_limits<CType>::min())}) {\n+        *pair.first = [pair](Buffer* vals, int64_t new_num_groups) {\n+          int64_t old_num_groups = vals->size() / sizeof(CType);\n+\n+          int64_t new_size = new_num_groups * sizeof(CType);\n+          RETURN_NOT_OK(checked_cast<ResizableBuffer*>(vals)->Resize(new_size));\n+\n+          auto raw_vals = reinterpret_cast<CType*>(vals->mutable_data());\n+          for (int64_t i = old_num_groups; i != new_num_groups; ++i) {\n+            raw_vals[i] = pair.second;\n+          }\n+          return Status::OK();\n+        };\n+      }\n+\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType& type) {\n+      return Status::NotImplemented(\"Grouped MinMax data of type \", type);\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Grouped MinMax data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Grouped MinMax data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    ResizeImpl resize_min_impl, resize_max_impl;\n+  };\n+\n+  static std::unique_ptr<GroupedMinMaxImpl> Make(\n+      KernelContext* ctx, const std::shared_ptr<DataType>& input_type,\n+      const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedMinMaxImpl>();\n+    out->options_ = *checked_cast<const MinMaxOptions*>(options);\n+    out->type_ = input_type;\n+\n+    out->buffers_.resize(4);\n+    for (auto& buf : out->buffers_) {\n+      ctx->SetStatus(ctx->Allocate(0).Value(&buf));\n+      if (ctx->HasError()) return nullptr;\n+    }\n+\n+    GetImpl get_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_impl));\n+\n+    out->consume_impl_ = std::move(get_impl.consume_impl);\n+    out->resize_min_impl_ = std::move(get_impl.resize_min_impl);\n+    out->resize_max_impl_ = std::move(get_impl.resize_max_impl);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_num_groups = num_groups_;\n+    num_groups_ = new_num_groups;\n+\n+    KERNEL_RETURN_IF_ERROR(ctx, resize_min_impl_(buffers_[0].get(), new_num_groups));\n+    KERNEL_RETURN_IF_ERROR(ctx, resize_max_impl_(buffers_[1].get(), new_num_groups));\n+\n+    for (auto buffer : {buffers_[2].get(), buffers_[3].get()}) {\n+      KERNEL_RETURN_IF_ERROR(ctx, checked_cast<ResizableBuffer*>(buffer)->Resize(\n+                                      BitUtil::BytesForBits(new_num_groups)));\n+      BitUtil::SetBitsTo(buffer->mutable_data(), old_num_groups, new_num_groups, false);\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, &buffers_);\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    // aggregation for group is valid if there was at least one value in that group\n+    std::shared_ptr<Buffer> null_bitmap = std::move(buffers_[3]);\n+\n+    if (options_.null_handling == MinMaxOptions::EMIT_NULL) {\n+      // ... and there were no nulls in that group\n\nReview comment:\n       Could perhaps use `BitmapAndNot`?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n\nReview comment:\n       Hmm... this looks like it's the _physical_ size of the buffer, not the logical number of groups (which may be smaller because of overallocation using `GrowByFactor`)?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n\nReview comment:\n       This doesn't seem useful, you're `memset`'ing just below.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n\nReview comment:\n       It is weird to hardcode the width switch like this. My two cents, but I would expect the following kind of code organization:\r\n   ```c++\r\n   template <int kByteWidth>\r\n   struct ConstantWidthPivotImpl {\r\n     static void AddEncodedLengths(const ArrayData& data, int32_t* lengths) {\r\n       for (int64_t i = 0; i < num_repeats; ++i) {\r\n         lengths[i] += kByteWidth + null_extra_byte;\r\n       }\r\n     }\r\n   \r\n     static void EncodeData(const ArrayData& data, uint8_t** encoded_bytes) {\r\n       int64_t i = 0;\r\n       const uint8_t* valid_bits = data.GetValues<uint8_t>(0, 0);\r\n       const uint8_t* values = data.GetValues<uint8_t>(1, 0) + data.offset * kByteWidth;\r\n       VisitNullBitmapInline(valid_bits, data.offset, data.length, data.null_count,\r\n         [&]() {\r\n           auto& encoded_ptr = encoded_bytes[i];\r\n           *encoded_ptr = 1;\r\n           memcpy(encoded_ptr + null_extra_byte, &value, kByteWidth);\r\n           encoded_ptr += kByteWidth + null_extra_byte;\r\n         },\r\n         [&]() {\r\n           auto& encoded_ptr = encoded_bytes[i];\r\n           memset(encoded_ptr, 0, kByteWidth + null_extra_byte);\r\n           encoded_ptr += kByteWidth + null_extra_byte;\r\n         }\r\n       );\r\n     }\r\n   \r\n     static void DecodeData(KernelContext* ctx, std::shared<DataType> type,\r\n         int64_t length, uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\r\n       TypedBufferBuilder<bool> null_builder;\r\n       BufferBuilder values_builder;\r\n       KERNEL_RETURN_IF_ERROR(ctx, null_builder.Reserve(length));\r\n       KERNEL_RETURN_IF_ERROR(ctx, values_builder.Reserve(length));\r\n       for (int64_t i = 0; i < length; ++i) {\r\n         auto& encoded_ptr = encoded_bytes[i];\r\n         if (*encoded_ptr) {\r\n           null_builder.UnsafeAppend(true);\r\n           values_builder.UnsafeAppend(encoded_ptr + 1, kNumBytes);  // memcpy\r\n         } else {\r\n           null_builder.UnsafeAppend(false);\r\n           values_builder.UnsafeAppend(kNumBytes, 0);  // memset\r\n         }\r\n         encoded_ptr += kByteWidth + null_extra_byte;\r\n         BufferVector buffers[2];\r\n         KERNEL_RETURN_IF_ERROR(ctx, null_builder.Finish(&buffers[0]));\r\n         KERNEL_RETURN_IF_ERROR(ctx, values_builder.Finish(&buffers[1]));\r\n         return std::make_shared<ArrayData>(type, length, std::move(buffers));\r\n       }\r\n     }\r\n   };\r\n   \r\n   template <typename T, typename Enable>\r\n   struct Pivot;\r\n   \r\n   template <typename T>\r\n   struct Pivot<T, enable_if_t<std::is_base_of<PrimitiveCType, T>::value>>\r\n     : public ConstantWidthPivotImpl<sizeof(CType)> {\r\n   };\r\n   \r\n   template <>\r\n   struct Pivot<Decimal128Type>\r\n     : public ConstantWidthPivotImpl<16> {\r\n   };\r\n   \r\n   ```\r\n   \n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n+          reinterpret_cast<const char*>(key_bytes_batch_.data() + offsets_batch_[i]),\n+          key_length);\n+      auto iter = map_.find(key);\n\nReview comment:\n       You can use `map_.insert` here to avoid making a second lookup. If the key is already in the map, you get an iterator whose value gives you the group id:\r\n   ```c++\r\n         const auto pair = map_.emplace(std::move(key), n_groups);\r\n         if (pair.second) {\r\n           // New group inserted\r\n           group_ids_batch_[i] = n_groups++;\r\n           auto next_key_offset = static_cast<int32_t>(key_bytes_.size());\r\n           key_bytes_.resize(next_key_offset + key_length);\r\n           offsets_.push_back(next_key_offset + key_length);\r\n           memcpy(key_bytes_.data() + next_key_offset, pair.first->first.data(), key_length);\r\n         } else {\r\n           // Existing group found\r\n           group_ids_batch_[i] = pair.first->second;\r\n         }\r\n   ```\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n\nReview comment:\n       `BitUtil::BytesForBits`?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n+\n+      if (null_bitmap == nullptr) {\n+        KERNEL_ASSIGN_OR_RAISE(null_bitmap, ctx, ctx->AllocateBitmap(num_groups()));\n+        BitUtil::SetBitsTo(null_bitmap->mutable_data(), 0, num_groups(), true);\n+      }\n+\n+      null_count += 1;\n+      BitUtil::SetBitTo(null_bitmap->mutable_data(), i, false);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type_), num_groups(),\n+                           {std::move(null_bitmap), std::move(sums_)}, null_count);\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return out_type_; }\n+\n+  std::shared_ptr<ResizableBuffer> sums_, counts_;\n+  std::shared_ptr<DataType> out_type_;\n+  ConsumeImpl consume_impl_;\n+};\n+\n+struct GroupedMinMaxImpl : public GroupedAggregator {\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, BufferVector*)>;\n+\n+  using ResizeImpl = std::function<Status(Buffer*, int64_t)>;\n+\n+  struct GetImpl {\n+    template <typename T, typename CType = typename TypeTraits<T>::CType>\n+    enable_if_number<T, Status> Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, BufferVector* buffers) {\n+        auto raw_inputs = reinterpret_cast<const CType*>(input->buffers[1]->data());\n+\n+        auto raw_mins = reinterpret_cast<CType*>(buffers->at(0)->mutable_data());\n+        auto raw_maxes = reinterpret_cast<CType*>(buffers->at(1)->mutable_data());\n+\n+        auto raw_has_nulls = buffers->at(2)->mutable_data();\n+        auto raw_has_values = buffers->at(3)->mutable_data();\n+\n+        for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+          auto g = group_ids[i];\n+          bool is_valid = BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+          if (is_valid) {\n+            raw_maxes[g] = std::max(raw_maxes[g], raw_inputs[input_i]);\n+            raw_mins[g] = std::min(raw_mins[g], raw_inputs[input_i]);\n+            BitUtil::SetBit(raw_has_values, g);\n+          } else {\n+            BitUtil::SetBit(raw_has_nulls, g);\n+          }\n+        }\n+      };\n+\n+      for (auto pair :\n+           {std::make_pair(&resize_min_impl, std::numeric_limits<CType>::max()),\n+            std::make_pair(&resize_max_impl, std::numeric_limits<CType>::min())}) {\n+        *pair.first = [pair](Buffer* vals, int64_t new_num_groups) {\n\nReview comment:\n       Suggestion:\r\n   ```c++\r\n         auto make_resize_impl = [](CType initval) -> ResizeImpl {\r\n           return [initval](Buffer* vals, int64_t new_num_groups) {\r\n             int64_t old_num_groups = vals->size() / sizeof(CType);\r\n   \r\n             int64_t new_size = new_num_groups * sizeof(CType);\r\n             RETURN_NOT_OK(checked_cast<ResizableBuffer*>(vals)->Resize(new_size));\r\n   \r\n             auto raw_vals = reinterpret_cast<CType*>(vals->mutable_data());\r\n             for (int64_t i = old_num_groups; i != new_num_groups; ++i) {\r\n               raw_vals[i] = pair.second;\r\n             }\r\n             return Status::OK();\r\n           };\r\n         };\r\n         resize_min_impl = make_resize_impl(std::numeric_limits<CType>::max());\r\n         resize_max_impl = make_resize_impl(std::numeric_limits<CType>::min());\r\n   ```\r\n   \n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n\nReview comment:\n       You could probably leverage `GetPhysicalType` to avoid repeating similar logic here?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n+          reinterpret_cast<const char*>(key_bytes_batch_.data() + offsets_batch_[i]),\n+          key_length);\n+      auto iter = map_.find(key);\n+      if (iter == map_.end()) {\n+        group_ids_batch_[i] = n_groups++;\n+        auto next_key_offset = static_cast<int32_t>(key_bytes_.size());\n+        key_bytes_.resize(next_key_offset + key_length);\n+        offsets_.push_back(next_key_offset + key_length);\n+        memcpy(key_bytes_.data() + next_key_offset, key.c_str(), key_length);\n+        map_.insert(std::make_pair(key, group_ids_batch_[i]));\n+      } else {\n+        group_ids_batch_[i] = iter->second;\n+      }\n+    }\n+\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      aggregators[i]->Consume(ctx, aggregands[i], group_ids_batch_.data());\n+      if (ctx->HasError()) return;\n+    }\n+  }\n+\n+  void MergeFrom(KernelContext* ctx, KernelState&& src) override {\n+    // TODO(ARROW-11840) merge two hash tables\n+    ctx->SetStatus(Status::NotImplemented(\"merging grouped aggregations\"));\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    size_t n_keys = decode_next_impl.size();\n+    ArrayDataVector out_columns(aggregators.size() + n_keys);\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      Datum aggregand;\n+      aggregators[i]->Finalize(ctx, &aggregand);\n+      if (ctx->HasError()) return;\n+      out_columns[i] = aggregand.array();\n+    }\n+\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(n_groups);\n+    for (int64_t i = 0; i < n_groups; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_.data() + offsets_[i];\n+    }\n+\n+    int64_t length = n_groups;\n+    for (size_t i = 0; i < n_keys; ++i) {\n+      std::shared_ptr<ArrayData> key_array;\n+      decode_next_impl[i].decode_next_impl(ctx, static_cast<int32_t>(length),\n+                                           key_buf_ptrs_.data(), &key_array);\n+      out_columns[aggregators.size() + i] = std::move(key_array);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type), length, {/*null_bitmap=*/nullptr},\n+                           std::move(out_columns));\n+  }\n+  std::vector<int32_t> offsets_batch_;\n+  std::vector<uint8_t> key_bytes_batch_;\n+  std::vector<uint8_t*> key_buf_ptrs_;\n+  std::vector<uint32_t> group_ids_batch_;\n+\n+  std::unordered_map<std::string, uint32_t> map_;\n\nReview comment:\n       Add comments for the non-trivial members here?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n+          reinterpret_cast<const char*>(key_bytes_batch_.data() + offsets_batch_[i]),\n+          key_length);\n+      auto iter = map_.find(key);\n+      if (iter == map_.end()) {\n+        group_ids_batch_[i] = n_groups++;\n+        auto next_key_offset = static_cast<int32_t>(key_bytes_.size());\n+        key_bytes_.resize(next_key_offset + key_length);\n+        offsets_.push_back(next_key_offset + key_length);\n+        memcpy(key_bytes_.data() + next_key_offset, key.c_str(), key_length);\n+        map_.insert(std::make_pair(key, group_ids_batch_[i]));\n+      } else {\n+        group_ids_batch_[i] = iter->second;\n+      }\n+    }\n+\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      aggregators[i]->Consume(ctx, aggregands[i], group_ids_batch_.data());\n+      if (ctx->HasError()) return;\n+    }\n+  }\n+\n+  void MergeFrom(KernelContext* ctx, KernelState&& src) override {\n+    // TODO(ARROW-11840) merge two hash tables\n+    ctx->SetStatus(Status::NotImplemented(\"merging grouped aggregations\"));\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    size_t n_keys = decode_next_impl.size();\n+    ArrayDataVector out_columns(aggregators.size() + n_keys);\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      Datum aggregand;\n+      aggregators[i]->Finalize(ctx, &aggregand);\n+      if (ctx->HasError()) return;\n+      out_columns[i] = aggregand.array();\n+    }\n+\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(n_groups);\n+    for (int64_t i = 0; i < n_groups; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_.data() + offsets_[i];\n+    }\n+\n+    int64_t length = n_groups;\n+    for (size_t i = 0; i < n_keys; ++i) {\n+      std::shared_ptr<ArrayData> key_array;\n+      decode_next_impl[i].decode_next_impl(ctx, static_cast<int32_t>(length),\n+                                           key_buf_ptrs_.data(), &key_array);\n+      out_columns[aggregators.size() + i] = std::move(key_array);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type), length, {/*null_bitmap=*/nullptr},\n+                           std::move(out_columns));\n+  }\n+  std::vector<int32_t> offsets_batch_;\n\nReview comment:\n       Shouldn't this be `int64_t`?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n\nReview comment:\n       Should probably be `uint64_t`.\r\n   ```python\r\n   >>> import pyarrow as pa, pyarrow.compute as pc\r\n   >>> pc.sum([True, False, True, None])\r\n   <pyarrow.UInt64Scalar: 2>\r\n   ```\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n+          reinterpret_cast<const char*>(key_bytes_batch_.data() + offsets_batch_[i]),\n+          key_length);\n+      auto iter = map_.find(key);\n+      if (iter == map_.end()) {\n+        group_ids_batch_[i] = n_groups++;\n+        auto next_key_offset = static_cast<int32_t>(key_bytes_.size());\n+        key_bytes_.resize(next_key_offset + key_length);\n+        offsets_.push_back(next_key_offset + key_length);\n+        memcpy(key_bytes_.data() + next_key_offset, key.c_str(), key_length);\n+        map_.insert(std::make_pair(key, group_ids_batch_[i]));\n+      } else {\n+        group_ids_batch_[i] = iter->second;\n+      }\n+    }\n+\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      aggregators[i]->Consume(ctx, aggregands[i], group_ids_batch_.data());\n+      if (ctx->HasError()) return;\n+    }\n+  }\n+\n+  void MergeFrom(KernelContext* ctx, KernelState&& src) override {\n+    // TODO(ARROW-11840) merge two hash tables\n+    ctx->SetStatus(Status::NotImplemented(\"merging grouped aggregations\"));\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    size_t n_keys = decode_next_impl.size();\n+    ArrayDataVector out_columns(aggregators.size() + n_keys);\n+    for (size_t i = 0; i < aggregators.size(); ++i) {\n+      Datum aggregand;\n+      aggregators[i]->Finalize(ctx, &aggregand);\n+      if (ctx->HasError()) return;\n+      out_columns[i] = aggregand.array();\n+    }\n+\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(n_groups);\n+    for (int64_t i = 0; i < n_groups; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_.data() + offsets_[i];\n+    }\n+\n+    int64_t length = n_groups;\n+    for (size_t i = 0; i < n_keys; ++i) {\n+      std::shared_ptr<ArrayData> key_array;\n+      decode_next_impl[i].decode_next_impl(ctx, static_cast<int32_t>(length),\n+                                           key_buf_ptrs_.data(), &key_array);\n+      out_columns[aggregators.size() + i] = std::move(key_array);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type), length, {/*null_bitmap=*/nullptr},\n+                           std::move(out_columns));\n+  }\n+  std::vector<int32_t> offsets_batch_;\n+  std::vector<uint8_t> key_bytes_batch_;\n+  std::vector<uint8_t*> key_buf_ptrs_;\n\nReview comment:\n       As an optimization, we may also want a `std::vector<int64_t> key_null_counts_`... though this may not be beneficial.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n #include \"arrow/chunked_array.h\"\n #include \"arrow/compute/api_aggregate.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/kernels/aggregate_internal.h\"\n #include \"arrow/compute/kernels/test_util.h\"\n+#include \"arrow/compute/registry.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bitmap_reader.h\"\n #include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/int_util_internal.h\"\n \n #include \"arrow/testing/gtest_common.h\"\n #include \"arrow/testing/gtest_util.h\"\n #include \"arrow/testing/random.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n+using internal::BitmapReader;\n using internal::checked_cast;\n using internal::checked_pointer_cast;\n \n namespace compute {\n \n+// Copy-pasta from partition.cc\n+//\n+// In the finished product this will only be a test helper for group_by\n+// and partition.cc will rely on a no-aggregate call to group_by.\n+namespace group_helpers {\n+namespace {\n+\n+// Transform an array of counts to offsets which will divide a ListArray\n+// into an equal number of slices with corresponding lengths.\n+Result<std::shared_ptr<Buffer>> CountsToOffsets(std::shared_ptr<Int64Array> counts) {\n+  TypedBufferBuilder<int32_t> offset_builder;\n+  RETURN_NOT_OK(offset_builder.Resize(counts->length() + 1));\n+\n+  int32_t current_offset = 0;\n+  offset_builder.UnsafeAppend(current_offset);\n+\n+  for (int64_t i = 0; i < counts->length(); ++i) {\n+    DCHECK_NE(counts->Value(i), 0);\n+    current_offset += static_cast<int32_t>(counts->Value(i));\n+    offset_builder.UnsafeAppend(current_offset);\n+  }\n+\n+  std::shared_ptr<Buffer> offsets;\n+  RETURN_NOT_OK(offset_builder.Finish(&offsets));\n+  return offsets;\n+}\n+\n+class StructDictionary {\n+ public:\n+  struct Encoded {\n+    std::shared_ptr<Int32Array> indices;\n+    std::shared_ptr<StructDictionary> dictionary;\n+  };\n+\n+  static Result<Encoded> Encode(const ArrayVector& columns) {\n+    Encoded out{nullptr, std::make_shared<StructDictionary>()};\n+\n+    for (const auto& column : columns) {\n+      if (column->null_count() != 0) {\n+        return Status::NotImplemented(\"Grouping on a field with nulls\");\n+      }\n+\n+      RETURN_NOT_OK(out.dictionary->AddOne(column, &out.indices));\n+    }\n+\n+    return out;\n+  }\n+\n+  Result<std::shared_ptr<StructArray>> Decode(std::shared_ptr<Int32Array> fused_indices,\n+                                              FieldVector fields) {\n+    std::vector<Int32Builder> builders(dictionaries_.size());\n+    for (Int32Builder& b : builders) {\n+      RETURN_NOT_OK(b.Resize(fused_indices->length()));\n+    }\n+\n+    std::vector<int32_t> codes(dictionaries_.size());\n+    for (int64_t i = 0; i < fused_indices->length(); ++i) {\n+      Expand(fused_indices->Value(i), codes.data());\n+\n+      auto builder_it = builders.begin();\n+      for (int32_t index : codes) {\n+        builder_it++->UnsafeAppend(index);\n+      }\n+    }\n+\n+    ArrayVector columns(dictionaries_.size());\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      std::shared_ptr<ArrayData> indices;\n+      RETURN_NOT_OK(builders[i].FinishInternal(&indices));\n+\n+      ARROW_ASSIGN_OR_RAISE(Datum column, compute::Take(dictionaries_[i], indices));\n+\n+      if (fields[i]->type()->id() == Type::DICTIONARY) {\n+        RETURN_NOT_OK(RestoreDictionaryEncoding(\n+            checked_pointer_cast<DictionaryType>(fields[i]->type()), &column));\n+      }\n+\n+      columns[i] = column.make_array();\n+    }\n+\n+    return StructArray::Make(std::move(columns), std::move(fields));\n+  }\n+\n+ private:\n+  Status AddOne(Datum column, std::shared_ptr<Int32Array>* fused_indices) {\n+    if (column.type()->id() != Type::DICTIONARY) {\n+      ARROW_ASSIGN_OR_RAISE(column, compute::DictionaryEncode(std::move(column)));\n+    }\n+\n+    auto dict_column = column.array_as<DictionaryArray>();\n+    dictionaries_.push_back(dict_column->dictionary());\n+    ARROW_ASSIGN_OR_RAISE(auto indices, compute::Cast(*dict_column->indices(), int32()));\n+\n+    if (*fused_indices == nullptr) {\n+      *fused_indices = checked_pointer_cast<Int32Array>(std::move(indices));\n+      return IncreaseSize();\n+    }\n+\n+    // It's useful to think about the case where each of dictionaries_ has size 10.\n+    // In this case the decimal digit in the ones place is the code in dictionaries_[0],\n+    // the tens place corresponds to the code in dictionaries_[1], etc.\n+    // The incumbent indices must be shifted to the hundreds place so as not to collide.\n+    ARROW_ASSIGN_OR_RAISE(Datum new_fused_indices,\n+                          compute::Multiply(indices, MakeScalar(size_)));\n+\n+    ARROW_ASSIGN_OR_RAISE(new_fused_indices,\n+                          compute::Add(new_fused_indices, *fused_indices));\n+\n+    *fused_indices = checked_pointer_cast<Int32Array>(new_fused_indices.make_array());\n+    return IncreaseSize();\n+  }\n+\n+  // expand a fused code into component dict codes, order is in order of addition\n+  void Expand(int32_t fused_code, int32_t* codes) {\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      auto dictionary_size = static_cast<int32_t>(dictionaries_[i]->length());\n+      codes[i] = fused_code % dictionary_size;\n+      fused_code /= dictionary_size;\n+    }\n+  }\n+\n+  Status RestoreDictionaryEncoding(std::shared_ptr<DictionaryType> expected_type,\n+                                   Datum* column) {\n+    DCHECK_NE(column->type()->id(), Type::DICTIONARY);\n+    ARROW_ASSIGN_OR_RAISE(*column, compute::DictionaryEncode(std::move(*column)));\n+\n+    if (expected_type->index_type()->id() == Type::INT32) {\n+      // dictionary_encode has already yielded the expected index_type\n+      return Status::OK();\n+    }\n+\n+    // cast the indices to the expected index type\n+    auto dictionary = std::move(column->mutable_array()->dictionary);\n+    column->mutable_array()->type = int32();\n+\n+    ARROW_ASSIGN_OR_RAISE(*column,\n+                          compute::Cast(std::move(*column), expected_type->index_type()));\n+\n+    column->mutable_array()->dictionary = std::move(dictionary);\n+    column->mutable_array()->type = expected_type;\n+    return Status::OK();\n+  }\n+\n+  Status IncreaseSize() {\n+    auto factor = static_cast<int32_t>(dictionaries_.back()->length());\n+\n+    if (arrow::internal::MultiplyWithOverflow(size_, factor, &size_)) {\n+      return Status::CapacityError(\"Max groups exceeded\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  int32_t size_ = 1;\n+  ArrayVector dictionaries_;\n+};\n+\n+Result<std::shared_ptr<StructArray>> MakeGroupings(const StructArray& keys) {\n+  if (keys.num_fields() == 0) {\n+    return Status::Invalid(\"Grouping with no keys\");\n+  }\n+\n+  if (keys.null_count() != 0) {\n+    return Status::Invalid(\"Grouping with null keys\");\n+  }\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused, StructDictionary::Encode(keys.fields()));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto sort_indices, compute::SortIndices(*fused.indices));\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted, compute::Take(fused.indices, *sort_indices));\n+  fused.indices = checked_pointer_cast<Int32Array>(sorted.make_array());\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused_counts_and_values,\n+                        compute::ValueCounts(fused.indices));\n+  fused.indices.reset();\n+\n+  auto unique_fused_indices =\n+      checked_pointer_cast<Int32Array>(fused_counts_and_values->GetFieldByName(\"values\"));\n+  ARROW_ASSIGN_OR_RAISE(\n+      auto unique_rows,\n+      fused.dictionary->Decode(std::move(unique_fused_indices), keys.type()->fields()));\n+\n+  auto counts =\n+      checked_pointer_cast<Int64Array>(fused_counts_and_values->GetFieldByName(\"counts\"));\n+  ARROW_ASSIGN_OR_RAISE(auto offsets, CountsToOffsets(std::move(counts)));\n+\n+  auto grouped_sort_indices =\n+      std::make_shared<ListArray>(list(sort_indices->type()), unique_rows->length(),\n+                                  std::move(offsets), std::move(sort_indices));\n+\n+  return StructArray::Make(\n+      ArrayVector{std::move(unique_rows), std::move(grouped_sort_indices)},\n+      std::vector<std::string>{\"values\", \"groupings\"});\n+}\n+\n+Result<std::shared_ptr<ListArray>> ApplyGroupings(const ListArray& groupings,\n+                                                  const Array& array) {\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted,\n+                        compute::Take(array, groupings.data()->child_data[0]));\n+\n+  return std::make_shared<ListArray>(list(array.type()), groupings.length(),\n+                                     groupings.value_offsets(), sorted.make_array());\n+}\n+\n+struct ScalarVectorToArray {\n+  template <typename T, typename AppendScalar,\n+            typename BuilderType = typename TypeTraits<T>::BuilderType,\n+            typename ScalarType = typename TypeTraits<T>::ScalarType>\n+  Status UseBuilder(const AppendScalar& append) {\n+    BuilderType builder(type(), default_memory_pool());\n+    for (const auto& s : scalars_) {\n+      if (s->is_valid) {\n+        RETURN_NOT_OK(append(checked_cast<const ScalarType&>(*s), &builder));\n+      } else {\n+        RETURN_NOT_OK(builder.AppendNull());\n+      }\n+    }\n+    return builder.FinishInternal(&data_);\n+  }\n+\n+  struct AppendValue {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      return builder->Append(s.value);\n+    }\n+  };\n+\n+  struct AppendBuffer {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      const Buffer& buffer = *s.value;\n+      return builder->Append(util::string_view{buffer});\n+    }\n+  };\n+\n+  template <typename T>\n+  enable_if_primitive_ctype<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendValue{});\n+  }\n+\n+  template <typename T>\n+  enable_if_has_string_view<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendBuffer{});\n+  }\n+\n+  Status Visit(const DataType& type) {\n+    return Status::NotImplemented(\"ScalarVectorToArray for type \", type);\n+  }\n+\n+  Result<Datum> Convert(ScalarVector scalars) && {\n+    if (scalars.size() == 0) {\n+      return Status::NotImplemented(\"ScalarVectorToArray with no scalars\");\n+    }\n+    scalars_ = std::move(scalars);\n+    RETURN_NOT_OK(VisitTypeInline(*type(), this));\n+    return Datum(std::move(data_));\n+  }\n+\n+  const std::shared_ptr<DataType>& type() { return scalars_[0]->type; }\n+\n+  ScalarVector scalars_;\n+  std::shared_ptr<ArrayData> data_;\n+};\n+\n+Result<Datum> NaiveGroupBy(std::vector<Datum> aggregands, std::vector<Datum> keys,\n+                           GroupByOptions options) {\n+  ArrayVector keys_arrays;\n+  for (const Datum& key : keys) keys_arrays.push_back(key.make_array());\n+  std::vector<std::string> key_names(keys_arrays.size(), \"\");\n+  ARROW_ASSIGN_OR_RAISE(auto keys_struct,\n+                        StructArray::Make(std::move(keys_arrays), std::move(key_names)));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto groupings_and_values, MakeGroupings(*keys_struct));\n+\n+  auto groupings =\n+      checked_pointer_cast<ListArray>(groupings_and_values->GetFieldByName(\"groupings\"));\n+\n+  int64_t n_groups = groupings->length();\n+\n+  ArrayVector out_columns;\n+\n+  for (size_t i_agg = 0; i_agg < aggregands.size(); ++i_agg) {\n+    const Datum& aggregand = aggregands[i_agg];\n+    const std::string& function = options.aggregates[i_agg].function;\n+\n+    ScalarVector aggregated_scalars;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto grouped_aggregand,\n+                          ApplyGroupings(*groupings, *aggregand.make_array()));\n+\n+    for (int64_t i_group = 0; i_group < n_groups; ++i_group) {\n+      ARROW_ASSIGN_OR_RAISE(\n+          Datum d, CallFunction(function, {grouped_aggregand->value_slice(i_group)}));\n+      aggregated_scalars.push_back(d.scalar());\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(Datum aggregated_column,\n+                          ScalarVectorToArray{}.Convert(std::move(aggregated_scalars)));\n+    out_columns.push_back(aggregated_column.make_array());\n+  }\n+\n+  keys_struct =\n+      checked_pointer_cast<StructArray>(groupings_and_values->GetFieldByName(\"values\"));\n+  for (size_t i_key = 0; i_key < aggregands.size(); ++i_key) {\n+    out_columns.push_back(keys_struct->field(i_key));\n+  }\n+\n+  std::vector<std::string> out_names(out_columns.size(), \"\");\n+  return StructArray::Make(std::move(out_columns), std::move(out_names));\n+}\n+\n+void ValidateGroupBy(GroupByOptions options, std::vector<Datum> aggregands,\n+                     std::vector<Datum> keys) {\n+  ASSERT_OK_AND_ASSIGN(Datum expected,\n+                       group_helpers::NaiveGroupBy(aggregands, keys, options));\n+\n+  ASSERT_OK_AND_ASSIGN(Datum actual, GroupBy(aggregands, keys, options));\n+\n+  // Ordering of groups is not important, so sort by key columns to ensure the comparison\n+  // doesn't fail spuriously\n+\n+  for (Datum* out : {&expected, &actual}) {\n+    auto out_columns = out->array_as<StructArray>()->fields();\n+\n+    SortOptions sort_options;\n+    FieldVector key_fields;\n+    ArrayVector key_columns;\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      auto name = std::to_string(i);\n+      sort_options.sort_keys.emplace_back(name);\n+      key_fields.push_back(field(name, out_columns[0]->type()));\n+      key_columns.push_back(out_columns[0]);\n+    }\n+    auto key_batch = RecordBatch::Make(schema(std::move(key_fields)), out->length(),\n+                                       std::move(key_columns));\n+\n+    ASSERT_OK_AND_ASSIGN(Datum sort_indices, SortIndices(key_batch, sort_options));\n+    ASSERT_OK_AND_ASSIGN(*out, Take(*out, sort_indices, TakeOptions::NoBoundsCheck()));\n+  }\n+\n+  AssertDatumsEqual(expected, actual, /*verbose=*/true);\n+}\n+\n+}  // namespace\n+}  // namespace group_helpers\n+\n+TEST(GroupBy, SumOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n\nReview comment:\n       Also, add a null value in one of the non-null aggregand groups?\r\n   e.g.:\r\n   ```c++\r\n     auto aggregand = ArrayFromJSON(float64(), \"[0.25, 1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75, null]\");\r\n     auto key = ArrayFromJSON(int64(), \"[null, 1, 2, 3, 1, 2, 2, null, 1]\");\r\n   ```\r\n   which should probably give:\r\n   ```c++\r\n       [4.25,   1],\r\n       [-0.125, 2],\r\n       [null,   3],\r\n       [1.0,   null]\r\n   ```\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n\nReview comment:\n       I suppose at some point we'll use a better hash table anyway?\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n\nReview comment:\n       Note that using `BufferBuider` would take care of this, AFAICT.\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -229,6 +604,710 @@ std::unique_ptr<KernelState> AllInit(KernelContext*, const KernelInitArgs& args)\n   return ::arrow::internal::make_unique<BooleanAllImpl>();\n }\n \n+struct GroupByImpl : public ScalarAggregator {\n+  using AddLengthImpl = std::function<void(const std::shared_ptr<ArrayData>&, int32_t*)>;\n+\n+  struct GetAddLengthImpl {\n+    static constexpr int32_t null_extra_byte = 1;\n+\n+    static void AddFixedLength(int32_t fixed_length, int64_t num_repeats,\n+                               int32_t* lengths) {\n+      for (int64_t i = 0; i < num_repeats; ++i) {\n+        lengths[i] += fixed_length + null_extra_byte;\n+      }\n+    }\n+\n+    static void AddVarLength(const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+      using offset_type = typename StringType::offset_type;\n+      constexpr int32_t length_extra_bytes = sizeof(offset_type);\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            lengths[i] += null_extra_byte + length_extra_bytes;\n+          } else {\n+            lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                          offsets[offset + i];\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          lengths[i] += null_extra_byte + length_extra_bytes + offsets[offset + i + 1] -\n+                        offsets[offset + i];\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bytes = (bit_width(input_type.id()) + 7) / 8;\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      add_length_impl = [](const std::shared_ptr<ArrayData>& data, int32_t* lengths) {\n+        AddVarLength(data, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      add_length_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                    int32_t* lengths) {\n+        AddFixedLength(num_bytes, data->length, lengths);\n+      };\n+      return Status::OK();\n+    }\n+\n+    AddLengthImpl add_length_impl;\n+  };\n+\n+  using EncodeNextImpl =\n+      std::function<void(const std::shared_ptr<ArrayData>&, uint8_t**)>;\n+\n+  struct GetEncodeNextImpl {\n+    template <int NumBits>\n+    static void EncodeSmallFixed(const std::shared_ptr<ArrayData>& data,\n+                                 uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          uint64_t null_multiplier = is_null ? 0 : 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = static_cast<uint8_t>(\n+                null_multiplier * (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0));\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] =\n+                static_cast<uint8_t>(null_multiplier * reinterpret_cast<const uint8_t*>(\n+                                                           raw_input)[offset + i]);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                static_cast<uint16_t>(null_multiplier * reinterpret_cast<const uint16_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                static_cast<uint32_t>(null_multiplier * reinterpret_cast<const uint32_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                static_cast<uint64_t>(null_multiplier * reinterpret_cast<const uint64_t*>(\n+                                                            raw_input)[offset + i]);\n+            encoded_ptr += 8;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          if (NumBits == 1) {\n+            encoded_ptr[0] = (BitUtil::GetBit(raw_input, offset + i) ? 1 : 0);\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 8) {\n+            encoded_ptr[0] = reinterpret_cast<const uint8_t*>(raw_input)[offset + i];\n+            encoded_ptr += 1;\n+          }\n+          if (NumBits == 16) {\n+            reinterpret_cast<uint16_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint16_t*>(raw_input)[offset + i];\n+            encoded_ptr += 2;\n+          }\n+          if (NumBits == 32) {\n+            reinterpret_cast<uint32_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint32_t*>(raw_input)[offset + i];\n+            encoded_ptr += 4;\n+          }\n+          if (NumBits == 64) {\n+            reinterpret_cast<uint64_t*>(encoded_ptr)[0] =\n+                reinterpret_cast<const uint64_t*>(raw_input)[offset + i];\n+            encoded_ptr += 8;\n+          }\n+        }\n+      }\n+    }\n+\n+    static void EncodeBigFixed(int num_bytes, const std::shared_ptr<ArrayData>& data,\n+                               uint8_t** encoded_bytes) {\n+      auto raw_input = data->buffers[1]->data();\n+      auto offset = data->offset;\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          encoded_ptr[0] = is_null ? 1 : 0;\n+          encoded_ptr += 1;\n+          if (is_null) {\n+            memset(encoded_ptr, 0, num_bytes);\n+          } else {\n+            memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          }\n+          encoded_ptr += num_bytes;\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr += 1;\n+          memcpy(encoded_ptr, raw_input + num_bytes * (offset + i), num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    static void EncodeVarLength(const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+      using offset_type = typename StringType::offset_type;\n+      auto offset = data->offset;\n+      const auto offsets = data->GetValues<offset_type>(1);\n+      auto raw_input = data->buffers[2]->data();\n+      if (data->MayHaveNulls()) {\n+        const uint8_t* nulls = data->buffers[0]->data();\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          bool is_null = !BitUtil::GetBit(nulls, offset + i);\n+          if (is_null) {\n+            encoded_ptr[0] = 1;\n+            encoded_ptr++;\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = 0;\n+            encoded_ptr += sizeof(offset_type);\n+          } else {\n+            encoded_ptr[0] = 0;\n+            encoded_ptr++;\n+            size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+            reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+            encoded_ptr += sizeof(offset_type);\n+            memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+            encoded_ptr += num_bytes;\n+          }\n+        }\n+      } else {\n+        for (int64_t i = 0; i < data->length; ++i) {\n+          auto& encoded_ptr = encoded_bytes[i];\n+          encoded_ptr[0] = 0;\n+          encoded_ptr++;\n+          size_t num_bytes = offsets[offset + i + 1] - offsets[offset + i];\n+          reinterpret_cast<offset_type*>(encoded_ptr)[0] = num_bytes;\n+          encoded_ptr += sizeof(offset_type);\n+          memcpy(encoded_ptr, raw_input + offsets[offset + i], num_bytes);\n+          encoded_ptr += num_bytes;\n+        }\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      switch (num_bits) {\n+        case 1:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<1>(data, encoded_bytes);\n+          };\n+          break;\n+        case 8:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<8>(data, encoded_bytes);\n+          };\n+          break;\n+        case 16:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<16>(data, encoded_bytes);\n+          };\n+          break;\n+        case 32:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<32>(data, encoded_bytes);\n+          };\n+          break;\n+        case 64:\n+          encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                                uint8_t** encoded_bytes) {\n+            EncodeSmallFixed<64>(data, encoded_bytes);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      encode_next_impl = [](const std::shared_ptr<ArrayData>& data,\n+                            uint8_t** encoded_bytes) {\n+        EncodeVarLength(data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      encode_next_impl = [num_bytes](const std::shared_ptr<ArrayData>& data,\n+                                     uint8_t** encoded_bytes) {\n+        EncodeBigFixed(num_bytes, data, encoded_bytes);\n+      };\n+      return Status::OK();\n+    }\n+\n+    EncodeNextImpl encode_next_impl;\n+  };\n+\n+  using DecodeNextImpl = std::function<void(KernelContext*, int32_t, uint8_t**,\n+                                            std::shared_ptr<ArrayData>*)>;\n+\n+  struct GetDecodeNextImpl {\n+    static Status DecodeNulls(KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                              std::shared_ptr<ResizableBuffer>* null_buf,\n+                              int32_t* null_count) {\n+      // Do we have nulls?\n+      *null_count = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        *null_count += encoded_bytes[i][0];\n+      }\n+      if (*null_count > 0) {\n+        ARROW_ASSIGN_OR_RAISE(*null_buf, ctx->AllocateBitmap(length));\n+        uint8_t* nulls = (*null_buf)->mutable_data();\n+        memset(nulls, 0, (*null_buf)->size());\n+        for (int32_t i = 0; i < length; ++i) {\n+          if (!encoded_bytes[i][0]) {\n+            BitUtil::SetBit(nulls, i);\n+          }\n+          encoded_bytes[i] += 1;\n+        }\n+      } else {\n+        for (int32_t i = 0; i < length; ++i) {\n+          encoded_bytes[i] += 1;\n+        }\n+      }\n+      return Status ::OK();\n+    }\n+\n+    template <int NumBits>\n+    static void DecodeSmallFixed(KernelContext* ctx, const Type::type& output_type,\n+                                 int32_t length, uint8_t** encoded_bytes,\n+                                 std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(\n+          auto key_buf, ctx,\n+          ctx->Allocate(NumBits == 1 ? (length + 7) / 8 : (NumBits / 8) * length));\n+\n+      uint8_t* raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        auto& encoded_ptr = encoded_bytes[i];\n+        if (NumBits == 1) {\n+          BitUtil::SetBitTo(raw_output, i, encoded_ptr[0] != 0);\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 8) {\n+          raw_output[i] = encoded_ptr[0];\n+          encoded_ptr += 1;\n+        }\n+        if (NumBits == 16) {\n+          reinterpret_cast<uint16_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint16_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 2;\n+        }\n+        if (NumBits == 32) {\n+          reinterpret_cast<uint32_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint32_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 4;\n+        }\n+        if (NumBits == 64) {\n+          reinterpret_cast<uint64_t*>(raw_output)[i] =\n+              reinterpret_cast<const uint64_t*>(encoded_bytes[i])[0];\n+          encoded_ptr += 8;\n+        }\n+      }\n+\n+      DCHECK(is_integer(output_type) || output_type == Type::BOOL);\n+      *out = ArrayData::Make(int64(), length, {null_buf, key_buf}, null_count);\n+    }\n+\n+    static void DecodeBigFixed(KernelContext* ctx, int num_bytes, int32_t length,\n+                               uint8_t** encoded_bytes, std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(num_bytes * length));\n+      auto raw_output = key_buf->mutable_data();\n+      for (int32_t i = 0; i < length; ++i) {\n+        memcpy(raw_output + i * num_bytes, encoded_bytes[i], num_bytes);\n+        encoded_bytes[i] += num_bytes;\n+      }\n+\n+      *out = ArrayData::Make(fixed_size_binary(num_bytes), length, {null_buf, key_buf},\n+                             null_count);\n+    }\n+\n+    static void DecodeVarLength(KernelContext* ctx, bool is_string, int32_t length,\n+                                uint8_t** encoded_bytes,\n+                                std::shared_ptr<ArrayData>* out) {\n+      std::shared_ptr<ResizableBuffer> null_buf;\n+      int32_t null_count;\n+      KERNEL_RETURN_IF_ERROR(\n+          ctx, DecodeNulls(ctx, length, encoded_bytes, &null_buf, &null_count));\n+\n+      using offset_type = typename StringType::offset_type;\n+\n+      int32_t length_sum = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        length_sum += reinterpret_cast<offset_type*>(encoded_bytes)[0];\n+      }\n+\n+      KERNEL_ASSIGN_OR_RAISE(auto offset_buf, ctx,\n+                             ctx->Allocate(sizeof(offset_type) * (1 + length)));\n+      KERNEL_ASSIGN_OR_RAISE(auto key_buf, ctx, ctx->Allocate(length_sum));\n+\n+      auto raw_offsets = offset_buf->mutable_data();\n+      auto raw_keys = key_buf->mutable_data();\n+      int32_t current_offset = 0;\n+      for (int32_t i = 0; i < length; ++i) {\n+        offset_type key_length = reinterpret_cast<offset_type*>(encoded_bytes[i])[0];\n+        reinterpret_cast<offset_type*>(raw_offsets)[i] = current_offset;\n+        encoded_bytes[i] += sizeof(offset_type);\n+        memcpy(raw_keys + current_offset, encoded_bytes[i], key_length);\n+        encoded_bytes[i] += key_length;\n+        current_offset += key_length;\n+      }\n+      reinterpret_cast<offset_type*>(raw_offsets)[length] = current_offset;\n+\n+      if (is_string) {\n+        *out = ArrayData::Make(utf8(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      } else {\n+        *out = ArrayData::Make(binary(), length, {null_buf, offset_buf, key_buf},\n+                               null_count, 0);\n+      }\n+    }\n+\n+    template <typename T>\n+    Status Visit(const T& input_type) {\n+      int32_t num_bits = bit_width(input_type.id());\n+      auto type_id = input_type.id();\n+      switch (num_bits) {\n+        case 1:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<1>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 8:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<8>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 16:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<16>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 32:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<32>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+        case 64:\n+          decode_next_impl = [type_id](KernelContext* ctx, int32_t length,\n+                                       uint8_t** encoded_bytes,\n+                                       std::shared_ptr<ArrayData>* out) {\n+            DecodeSmallFixed<64>(ctx, type_id, length, encoded_bytes, out);\n+          };\n+          break;\n+      }\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const StringType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, true, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BinaryType&) {\n+      decode_next_impl = [](KernelContext* ctx, int32_t length, uint8_t** encoded_bytes,\n+                            std::shared_ptr<ArrayData>* out) {\n+        DecodeVarLength(ctx, false, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const FixedSizeBinaryType& type) {\n+      int32_t num_bytes = type.byte_width();\n+      decode_next_impl = [num_bytes](KernelContext* ctx, int32_t length,\n+                                     uint8_t** encoded_bytes,\n+                                     std::shared_ptr<ArrayData>* out) {\n+        DecodeBigFixed(ctx, num_bytes, length, encoded_bytes, out);\n+      };\n+      return Status::OK();\n+    }\n+\n+    DecodeNextImpl decode_next_impl;\n+  };\n+\n+  void Consume(KernelContext* ctx, const ExecBatch& batch) override {\n+    ArrayDataVector aggregands, keys;\n+\n+    size_t i;\n+    for (i = 0; i < aggregators.size(); ++i) {\n+      aggregands.push_back(batch[i].array());\n+    }\n+    while (i < static_cast<size_t>(batch.num_values())) {\n+      keys.push_back(batch[i++].array());\n+    }\n+\n+    offsets_batch_.clear();\n+    offsets_batch_.resize(batch.length + 1);\n+    offsets_batch_[0] = 0;\n+    memset(offsets_batch_.data(), 0, sizeof(offsets_batch_[0]) * offsets_batch_.size());\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      add_length_impl[i].add_length_impl(keys[i], offsets_batch_.data());\n+    }\n+    int32_t total_length = 0;\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      auto total_length_before = total_length;\n+      total_length += offsets_batch_[i];\n+      offsets_batch_[i] = total_length_before;\n+    }\n+    offsets_batch_[batch.length] = total_length;\n+\n+    key_bytes_batch_.clear();\n+    key_bytes_batch_.resize(total_length);\n+    key_buf_ptrs_.clear();\n+    key_buf_ptrs_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      key_buf_ptrs_[i] = key_bytes_batch_.data() + offsets_batch_[i];\n+    }\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      encode_next_impl[i].encode_next_impl(keys[i], key_buf_ptrs_.data());\n+    }\n+\n+    group_ids_batch_.clear();\n+    group_ids_batch_.resize(batch.length);\n+    for (int64_t i = 0; i < batch.length; ++i) {\n+      int32_t key_length = offsets_batch_[i + 1] - offsets_batch_[i];\n+      std::string key(\n+          reinterpret_cast<const char*>(key_bytes_batch_.data() + offsets_batch_[i]),\n+          key_length);\n+      auto iter = map_.find(key);\n\nReview comment:\n       Note that we could use a `std::unordered_map<util::string_view, int32_t>` instead:\r\n   ```c++\r\n         util::string_view key(\r\n             reinterpret_cast<const char*>(key_bytes_batch_.data() + offsets_batch_[i]),\r\n             key_length);\r\n         const auto pair = map_.emplace(key, n_groups);\r\n         if (pair.second) {\r\n           // New group inserted\r\n           group_ids_batch_[i] = n_groups++;\r\n           // Copy key to permanent storage, update map value\r\n           auto next_key_offset = static_cast<int32_t>(key_bytes_.size());\r\n           key_bytes_.resize(next_key_offset + key_length);\r\n           offsets_.push_back(next_key_offset + key_length);\r\n           auto final_key_data = key_bytes_.data() + next_key_offset;\r\n           memcpy(final_key_data, key.data(), key_length);\r\n           // Update map to point to permanent key storage, not temporary\r\n           pair.first->first = util::string_view(reinterpret_cast<char *>(final_key_data), key_length);\r\n         } else {\r\n           // Existing group found\r\n           group_ids_batch_[i] = pair.first->second;\r\n         }\r\n   ```\r\n   \r\n   \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-04T16:14:55.272+0000",
                    "updated": "2021-03-04T16:14:55.272+0000",
                    "started": "2021-03-04T16:14:55.272+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "561009",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/561172",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#issuecomment-790998553\n\n\n   With an assist from @bkietz, I've written a very basic R wrapper that exercises this in https://github.com/apache/arrow/commit/aa530cb586462bee98390d129575fe1622ffb222. It's enough to expose some issues to address, to say nothing of the interface questions.\r\n   \r\n   ```r\r\n   library(arrow)\r\n   library(dplyr)\r\n   \r\n   # The commit uses this option to switch to use the group_by compute function\r\n   options(arrow.summarize = TRUE)\r\n   # If the Arrow aggregation function isn't implemented, or if the Arrow call errors,\r\n   # it falls back to pulling the data in R and evaluating in R.\r\n   \r\n   # mtcars is a standard dataset that ships with R\r\n   mt <- Table$create(mtcars)\r\n   mt %>%\r\n     group_by(cyl) %>%\r\n     summarize(total_hp = sum(hp))\r\n   # Warning: Error : NotImplemented: Key of typedouble\r\n   # ../src/arrow/compute/function.cc:178  kernel_ctx.status()\r\n   # ; pulling data into R\r\n   # # A tibble: 3 x 2\r\n   #     cyl total_hp\r\n   # * <dbl>    <dbl>\r\n   # 1     4      909\r\n   # 2     6      856\r\n   # 3     8     2929\r\n   \r\n   # That's unfortunate. R blurs the distinction for users between integer and double,\r\n   # so it's not uncommon to have integer data stored as a float.\r\n   # (Also, the error message is missing some whitespace.)\r\n   \r\n   # We can cast that to an integer and try again\r\n   \r\n   mt$cyl <- mt$cyl$cast(int32())\r\n   unique(mt$cyl)\r\n   # Array\r\n   # <int32>\r\n   # [\r\n   #   6,\r\n   #   4,\r\n   #   8\r\n   # ]\r\n   \r\n   mt %>%\r\n     group_by(cyl) %>%\r\n     summarize(total_hp = sum(hp))\r\n   # StructArray\r\n   # <struct<: double, : int32>>\r\n   # -- is_valid: all not null\r\n   # -- child 0 type: double\r\n   #   [\r\n   #     856,\r\n   #     909,\r\n   #     2929\r\n   #   ]\r\n   # -- child 1 type: int64\r\n   #   [\r\n   #     17179869190,\r\n   #     8,\r\n   #     0\r\n   #   ]\r\n   \r\n   # Alright, it computed and got the same numbers, but the StructArray\r\n   # is not valid. Type says int32 but data says int64 and we have misplaced bits\r\n   \r\n   # Let's try a different stock dataset\r\n   ir <- Table$create(iris)\r\n   ir %>%\r\n     group_by(Species) %>%\r\n     summarize(total_length = sum(Sepal.Length))\r\n   # Warning: Error : NotImplemented: Key of typedictionary<values=string, indices=int8, ordered=0>\r\n   # ../src/arrow/compute/function.cc:178  kernel_ctx.status()\r\n   # ; pulling data into R\r\n   # # A tibble: 3 x 2\r\n   #   Species    total_length\r\n   # * <fct>             <dbl>\r\n   # 1 setosa             250.\r\n   # 2 versicolor         297.\r\n   # 3 virginica          329.\r\n   \r\n   # Hmm. dictionary types really need to be supported.\r\n   # Let's work around and cast it to string\r\n   \r\n   ir$Species <- ir$Species$cast(utf8())\r\n   unique(ir$Species)\r\n   # Array\r\n   # <string>\r\n   # [\r\n   #   \"setosa\",\r\n   #   \"versicolor\",\r\n   #   \"virginica\"\r\n   # ]\r\n   ir %>%\r\n     group_by(Species) %>%\r\n     summarize(total_length = sum(Sepal.Length))\r\n   # Warning: Error : Invalid: Negative buffer resize: -219443965\r\n   # ../src/arrow/buffer.cc:262  buffer->Resize(size)\r\n   # ../src/arrow/compute/kernels/aggregate_basic.cc:1005  (_error_or_value9).status()\r\n   # ../src/arrow/compute/function.cc:193  executor->Execute(implicitly_cast_args, listener.get())\r\n   # ; pulling data into R\r\n   # # A tibble: 3 x 2\r\n   #   Species    total_length\r\n   # * <chr>             <dbl>\r\n   # 1 setosa             250.\r\n   # 2 versicolor         297.\r\n   # 3 virginica          329.\r\n   ```\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-04T22:45:15.954+0000",
                    "updated": "2021-03-04T22:45:15.954+0000",
                    "started": "2021-03-04T22:45:15.954+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "561172",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/561288",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#issuecomment-791153213\n\n\n   > Before digging into the details too much, my main issue with what I see is that I don't agree with making hash aggregation a callable function through `CallFunction`.\r\n   > \r\n   > In the context of a query engine, the interface for this operator looks something like:\r\n   > \r\n   > ```\r\n   > class ExecNode {\r\n   >  public:\r\n   >   virtual void Push(int node_index, const ExecBatch& batch) = 0;\r\n   >   virtual void PushDone(int node_index) = 0;\r\n   > };\r\n   > \r\n   > class HashAggregationNode : public ExecNode {\r\n   >   ...\r\n   > }; \r\n   > ```\r\n   > \r\n   > (some query engines use a \"pull\"-based model, in which the data flow is inverted \u2014 there are pros and cons to both approaches, see https://www.vldb.org/pvldb/vol11/p2209-kersten.pdf)\r\n   > \r\n   > If you want a simple one-shot version of the algorithm (rather than a general \"streaming\" one like the above), then you can break the input data into record batches of the desired size (e.g. 4K - 64K rows, depending on heuristics) and then push the chunks into the node that you create (note that the HashAggregationNode should push its result into a terminal \"OutputNode\" when you invoke `hash_agg_node->PushDone(0)`).\r\n   > \r\n   > The API can be completely crude / preliminary, but would it be possible to use the query-engine-type approach for this? I think it would be best to start taking some strides in this direction rather than bolting this onto the array function execution machinery which doesn't make sense in a query processing context (because aggregation is fundamentally a streaming algorithm)\r\n   > \r\n   > On the hash aggregation functions themselves, perhaps it makes sense to add a `HASH_AGGREGATE` function type and define the kernel interface for these functions, then look up these functions using the general dispatch machinery?\r\n   \r\n   I like these points. Let me break into subtopics what I think we are facing now and what we should think about for the future. I read the comment above as advocating bringing support for two concepts: a) relational operators (that are building blocks of a query execution pipeline or a more general query execution plan) and b) pipelines / streaming processing. \r\n   \r\n   **1. How we got here**\r\n   \r\n   Part of the problem is that we don't have a problem just yet. We didn't bite a big enough chunk of work to force us to do the appropriate refactoring. And the problem I am referring to is that of pipelining, which we do not have support for yet. From what I understand, currently we cannot bind together multiple operations into a single processing pipeline executed using a single Arrow function call, computing expressions on the fly without persisting their results for an entire set of rows. \r\n   \r\n   Related to the pipelining is the fact that in this PR we do not stream the group by output. That way, at some level of abstraction (squinting eyes) we can treat it as a scalar aggregate (we output a single item which happens to be an entire collection of arrays; variants make it possible).\r\n   \r\n   But to me the bigger problem here is that we are mixing together two separate worlds: scalar operators and relational operators and that muddies the general picture. That mixing happens as a consequence of treating the group by as a modified scalar aggregate in the code.\r\n   \r\n   ** 2. Scalar operators vs relational operators**\r\n   \r\n   One way to think about it is that scalar expressions (compositions of scalar operators) are like C++ lambdas (e.g. comparison of two values) provided in a call to C++ STL algorithms / containers (e.g. std::sort) while relational operators correspond to these algorithms / containers (except that they additionally have support for composability - creating execution pipelines / trees / DAGs). \r\n   \r\n   The point of confusion may come from the fact that once you vectorize scalar expressions (which you probably want to do, for performance reasons), their interfaces start looking very similar (if not the same) as would some special relational operators (namely: filter, project and scalar aggregate). I claim that current kernel executor classes are relational operators in disguise - project (aka compute scalar) and reduce (aka scalar aggregate). \r\n   Relational operators inside current group by\r\n   Interestingly, group by present in this PR, can itself be treated as a DAG of multiple relational operators with some kind of pipelines connecting them. The input batch is first processed by the code that assigns integer group id to every row based on key columns. The output of that is then processed by zero, one or more group by aggregate operators that update aggregate related accumulators in their internal arrays. At the end of the input, the output array related to group id mapping component is concatenated to output arrays for individual group by aggregate operators to produce output collection of arrays. We can treat group id mapping and group by aggregates as separate relational operators or we can choose to treat them as internals of a single hash group by operator. Even hash computation for hash table lookup (part of group id mapping) can be treated as a separate processing step that is using a projection operator. \r\n   \r\n   When we talk about relational operators and their connections we can talk at different levels of granularity. Sometimes it\u2019s simpler to treat building blocks made of multiple operators with fixed, hard-coded connections between them as a single operator, sometimes it brings more opportunities for code reusability to treat smaller blocks separately. \r\n   \r\n   ** 3. Push and pull**\r\n   \r\n   My personal feeling is that the pull model was good in the early query execution engines, based on processing of a single row at a time and using virtual function calls to switch between relational operators within the query. In my experience, the push model is easier to work with in both modern worlds of query execution: JIT compiled query processing and vectorized query processing. \r\n   \r\n   It may seem abstract right now, which model is better push or pull, but I believe that once you have to actually solve an existing problem it becomes more obvious which one you prefer.\r\n   \r\n   From my experience, in a vectorized execution model, it is easy to adapt existing push-based implementation to support pull-based interface. I am guessing that the same would be true the other way around.\r\n   \r\n   Also, intuitively, when we think about consuming input, we think: pull (e.g. scanning in-memory data or a file), and when we think about producing output, we think: push (e.g. sending results of the computation over the network).  \r\n   \r\n   I would probably recommend: at a lower level - use whatever model results in a more readable, simpler, more natural code - and at a higher level - adapt all interfaces to push model.\r\n   \r\n   ** 4. When streaming output is not desired**\r\n   \r\n   It may be a bit forward looking, but in some cases it is beneficial to give one relational operator direct access to internal row storage of another relational operator instead of always assuming that the operators stream their output. Streaming output should always be supported, but in addition to that, the direct access option may be useful.\r\n   \r\n   It\u2019s not unusual to request the output of group by to be sorted on group by key columns. In that case the sort operator, which needs to accumulate all of the input in the buffer before sorting, could work directly on the group by buffers without converting internal row storage of group by to batches and then batches back to internal row storage of sort. \r\n   \r\n   Similarly, window functions, quantiles, may require random access to the entire set of input rows (within a group / partition of data). In this case, again, they may want to work on an internal storage of rows of sort rather than streaming it out and accumulating the output stream in internal buffers.\r\n   \r\n   ** 5. Summary**\r\n   \r\n   I think that what we are talking about here has a goal that overlaps with group by but is wider in scope and somewhat separate and that is: a) refactoring the code to bring the concepts of relational and scalar operators, b) laying ground for the support of pipelining. \r\n   \r\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-05T04:55:41.271+0000",
                    "updated": "2021-03-05T04:55:41.271+0000",
                    "started": "2021-03-05T04:55:41.271+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "561288",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564658",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592436887\n\n\n\n##########\nFile path: cpp/src/arrow/compute/api_aggregate.h\n##########\n@@ -306,5 +326,34 @@ Result<Datum> TDigest(const Datum& value,\n                       const TDigestOptions& options = TDigestOptions::Defaults(),\n                       ExecContext* ctx = NULLPTR);\n \n+/// \\brief Calculate multiple aggregations grouped on multiple keys\n+///\n+/// \\param[in] aggregands datums to which aggregations will be applied\n+/// \\param[in] keys datums which will be used to group the aggregations\n+/// \\param[in] options GroupByOptions, encapsulating the names and options of aggregate\n+///            functions to be applied and the field names for results in the output.\n+/// \\return a StructArray with len(aggregands) + len(keys) fields. The first\n+///         len(aggregands) fields are the results of the aggregations for the group\n+///         specified by keys in the final len(keys) fields.\n+///\n+/// For example:\n+///   GroupByOptions options = {\n+///     .aggregates = {\n+///       {\"sum\", nullptr, \"sum result\"},\n+///       {\"mean\", nullptr, \"mean result\"},\n+///     },\n+///     .key_names = {\"str key\", \"date key\"},\n+///   };\n+/// assert(*GroupBy({[2, 5, 8], [1.5, 2.0, 3.0]},\n+///                 {[\"a\", \"b\", \"a\"], [today, today, today]},\n+///                 options).Equals([\n+///   {\"sum result\": 10, \"mean result\": 2.25, \"str key\": \"a\", \"date key\": today},\n+///   {\"sum result\": 5,  \"mean result\": 2.0,  \"str key\": \"b\", \"date key\": today},\n+/// ]))\n\nReview comment:\n       Since the group id lists are temporary (except in the rare case where we need to partition batches for writing), we will be computing and discarding them on the fly rather than materializing an O(N) set of them.\r\n   \r\n   I'll be removing this compute function; as mentioned above it's not necessary for group by to live in the function registry.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:05:06.057+0000",
                    "updated": "2021-03-11T15:05:06.057+0000",
                    "started": "2021-03-11T15:05:06.056+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564658",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564661",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592443814\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec.cc\n##########\n@@ -838,6 +838,13 @@ class ScalarAggExecutor : public KernelExecutorImpl<ScalarAggregateKernel> {\n \n  private:\n   Status Consume(const ExecBatch& batch) {\n+    if (kernel_->nomerge) {\n+      kernel_->consume(kernel_ctx_, batch);\n+      ARROW_CTX_RETURN_IF_ERROR(kernel_ctx_);\n+      return Status::OK();\n+    }\n\nReview comment:\n       This is a confusing aspect of the KernelExecutor contract which should be documented: `KernelExecutor::Init` initializes the executor but assumes that the kernel's `init` has already been called in the provided `KernelContext`. This is to facilitate the case where `init` may be expensive and does not need to be called again for each execution of the kernel, for example the same lookup table can be re-used for all scanned batches in a dataset filter.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:13:08.323+0000",
                    "updated": "2021-03-11T15:13:08.323+0000",
                    "started": "2021-03-11T15:13:08.323+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564661",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564662",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592443814\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec.cc\n##########\n@@ -838,6 +838,13 @@ class ScalarAggExecutor : public KernelExecutorImpl<ScalarAggregateKernel> {\n \n  private:\n   Status Consume(const ExecBatch& batch) {\n+    if (kernel_->nomerge) {\n+      kernel_->consume(kernel_ctx_, batch);\n+      ARROW_CTX_RETURN_IF_ERROR(kernel_ctx_);\n+      return Status::OK();\n+    }\n\nReview comment:\n       This is a confusing aspect of the KernelExecutor contract which should be documented: `KernelExecutor::Init` initializes the executor but assumes that the kernel's `init` has already been called in the provided `KernelContext`. This is to facilitate the case where `init` may be expensive and does not need to be called again for each execution of the kernel, for example the same lookup table can be re-used for all scanned batches in a dataset filter.\r\n   \r\n   I'll add a comment clarifying this.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:13:21.453+0000",
                    "updated": "2021-03-11T15:13:21.453+0000",
                    "started": "2021-03-11T15:13:21.453+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564662",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564663",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592444462\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n\nReview comment:\n       Agreed. I'll move it to a separate test.cc\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:13:56.485+0000",
                    "updated": "2021-03-11T15:13:56.485+0000",
                    "started": "2021-03-11T15:13:56.484+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564663",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564664",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592445217\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_benchmark.cc\n##########\n@@ -300,6 +300,70 @@ BENCHMARK_TEMPLATE(ReferenceSum, SumBitmapVectorizeUnroll<int64_t>)\n     ->Apply(BenchmarkSetArgs);\n #endif  // ARROW_WITH_BENCHMARKS_REFERENCE\n \n+//\n+// GroupBy\n+//\n+\n+static void BenchmarkGroupBy(benchmark::State& state,\n+                             std::vector<GroupByOptions::Aggregate> aggregates,\n+                             std::vector<Datum> aggregands, std::vector<Datum> keys) {\n+  std::vector<Datum> arguments;\n+  for (const Datum& aggregand : aggregands) {\n+    arguments.push_back(aggregand);\n+  }\n+\n+  for (const Datum& key : keys) {\n+    arguments.push_back(key);\n+  }\n+\n+  GroupByOptions options;\n+  options.aggregates = aggregates;\n+  options.key_names.resize(keys.size(), \"ignored\");\n+\n+  for (auto _ : state) {\n+    ABORT_NOT_OK(CallFunction(\"group_by\", arguments, &options).status());\n+  }\n+}\n+\n+#define GROUP_BY_BENCHMARK(Name, Impl)                               \\\n+  static void Name(benchmark::State& state) {                        \\\n+    RegressionArgs args(state, false);                               \\\n+    auto rng = random::RandomArrayGenerator(1923);                   \\\n+    (Impl)();                                                        \\\n+  }                                                                  \\\n+  BENCHMARK(Name)->Apply([](benchmark::internal::Benchmark* bench) { \\\n+    BenchmarkSetArgsWithSizes(bench, {1 * 1024 * 1024});             \\\n+  })\n+\n+GROUP_BY_BENCHMARK(SumDoublesGroupedBySmallStringSet, [&] {\n+  auto summand = rng.Float64(args.size,\n+                             /*min=*/0.0,\n+                             /*max=*/1.0e14,\n+                             /*null_probability=*/args.null_proportion,\n+                             /*nan_probability=*/args.null_proportion / 10);\n+\n+  auto key = rng.StringWithRepeats(args.size,\n+                                   /*unique=*/16,\n+                                   /*min_length=*/3,\n+                                   /*max_length=*/32);\n+\n+  BenchmarkGroupBy(state, {{\"sum\", NULLPTR, \"summed f64\"}}, {summand}, {key});\n\nReview comment:\n       Well it doesn't work. I'll remove the second string\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:14:39.631+0000",
                    "updated": "2021-03-11T15:14:39.631+0000",
                    "started": "2021-03-11T15:14:39.631+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564664",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564665",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592450325\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n #include \"arrow/chunked_array.h\"\n #include \"arrow/compute/api_aggregate.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/kernels/aggregate_internal.h\"\n #include \"arrow/compute/kernels/test_util.h\"\n+#include \"arrow/compute/registry.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bitmap_reader.h\"\n #include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/int_util_internal.h\"\n \n #include \"arrow/testing/gtest_common.h\"\n #include \"arrow/testing/gtest_util.h\"\n #include \"arrow/testing/random.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n+using internal::BitmapReader;\n using internal::checked_cast;\n using internal::checked_pointer_cast;\n \n namespace compute {\n \n+// Copy-pasta from partition.cc\n+//\n+// In the finished product this will only be a test helper for group_by\n+// and partition.cc will rely on a no-aggregate call to group_by.\n+namespace group_helpers {\n+namespace {\n+\n+// Transform an array of counts to offsets which will divide a ListArray\n+// into an equal number of slices with corresponding lengths.\n+Result<std::shared_ptr<Buffer>> CountsToOffsets(std::shared_ptr<Int64Array> counts) {\n+  TypedBufferBuilder<int32_t> offset_builder;\n+  RETURN_NOT_OK(offset_builder.Resize(counts->length() + 1));\n+\n+  int32_t current_offset = 0;\n+  offset_builder.UnsafeAppend(current_offset);\n+\n+  for (int64_t i = 0; i < counts->length(); ++i) {\n+    DCHECK_NE(counts->Value(i), 0);\n+    current_offset += static_cast<int32_t>(counts->Value(i));\n+    offset_builder.UnsafeAppend(current_offset);\n+  }\n+\n+  std::shared_ptr<Buffer> offsets;\n+  RETURN_NOT_OK(offset_builder.Finish(&offsets));\n+  return offsets;\n+}\n+\n+class StructDictionary {\n+ public:\n+  struct Encoded {\n+    std::shared_ptr<Int32Array> indices;\n+    std::shared_ptr<StructDictionary> dictionary;\n+  };\n+\n+  static Result<Encoded> Encode(const ArrayVector& columns) {\n+    Encoded out{nullptr, std::make_shared<StructDictionary>()};\n+\n+    for (const auto& column : columns) {\n+      if (column->null_count() != 0) {\n+        return Status::NotImplemented(\"Grouping on a field with nulls\");\n+      }\n+\n+      RETURN_NOT_OK(out.dictionary->AddOne(column, &out.indices));\n+    }\n+\n+    return out;\n+  }\n+\n+  Result<std::shared_ptr<StructArray>> Decode(std::shared_ptr<Int32Array> fused_indices,\n+                                              FieldVector fields) {\n+    std::vector<Int32Builder> builders(dictionaries_.size());\n+    for (Int32Builder& b : builders) {\n+      RETURN_NOT_OK(b.Resize(fused_indices->length()));\n+    }\n+\n+    std::vector<int32_t> codes(dictionaries_.size());\n+    for (int64_t i = 0; i < fused_indices->length(); ++i) {\n+      Expand(fused_indices->Value(i), codes.data());\n+\n+      auto builder_it = builders.begin();\n+      for (int32_t index : codes) {\n+        builder_it++->UnsafeAppend(index);\n+      }\n+    }\n+\n+    ArrayVector columns(dictionaries_.size());\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      std::shared_ptr<ArrayData> indices;\n+      RETURN_NOT_OK(builders[i].FinishInternal(&indices));\n+\n+      ARROW_ASSIGN_OR_RAISE(Datum column, compute::Take(dictionaries_[i], indices));\n+\n+      if (fields[i]->type()->id() == Type::DICTIONARY) {\n+        RETURN_NOT_OK(RestoreDictionaryEncoding(\n+            checked_pointer_cast<DictionaryType>(fields[i]->type()), &column));\n+      }\n+\n+      columns[i] = column.make_array();\n+    }\n+\n+    return StructArray::Make(std::move(columns), std::move(fields));\n+  }\n+\n+ private:\n+  Status AddOne(Datum column, std::shared_ptr<Int32Array>* fused_indices) {\n+    if (column.type()->id() != Type::DICTIONARY) {\n+      ARROW_ASSIGN_OR_RAISE(column, compute::DictionaryEncode(std::move(column)));\n+    }\n+\n+    auto dict_column = column.array_as<DictionaryArray>();\n+    dictionaries_.push_back(dict_column->dictionary());\n+    ARROW_ASSIGN_OR_RAISE(auto indices, compute::Cast(*dict_column->indices(), int32()));\n+\n+    if (*fused_indices == nullptr) {\n+      *fused_indices = checked_pointer_cast<Int32Array>(std::move(indices));\n+      return IncreaseSize();\n+    }\n+\n+    // It's useful to think about the case where each of dictionaries_ has size 10.\n+    // In this case the decimal digit in the ones place is the code in dictionaries_[0],\n+    // the tens place corresponds to the code in dictionaries_[1], etc.\n+    // The incumbent indices must be shifted to the hundreds place so as not to collide.\n+    ARROW_ASSIGN_OR_RAISE(Datum new_fused_indices,\n+                          compute::Multiply(indices, MakeScalar(size_)));\n+\n+    ARROW_ASSIGN_OR_RAISE(new_fused_indices,\n+                          compute::Add(new_fused_indices, *fused_indices));\n+\n+    *fused_indices = checked_pointer_cast<Int32Array>(new_fused_indices.make_array());\n+    return IncreaseSize();\n+  }\n+\n+  // expand a fused code into component dict codes, order is in order of addition\n+  void Expand(int32_t fused_code, int32_t* codes) {\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      auto dictionary_size = static_cast<int32_t>(dictionaries_[i]->length());\n+      codes[i] = fused_code % dictionary_size;\n+      fused_code /= dictionary_size;\n+    }\n+  }\n+\n+  Status RestoreDictionaryEncoding(std::shared_ptr<DictionaryType> expected_type,\n+                                   Datum* column) {\n+    DCHECK_NE(column->type()->id(), Type::DICTIONARY);\n+    ARROW_ASSIGN_OR_RAISE(*column, compute::DictionaryEncode(std::move(*column)));\n+\n+    if (expected_type->index_type()->id() == Type::INT32) {\n+      // dictionary_encode has already yielded the expected index_type\n+      return Status::OK();\n+    }\n+\n+    // cast the indices to the expected index type\n+    auto dictionary = std::move(column->mutable_array()->dictionary);\n+    column->mutable_array()->type = int32();\n+\n+    ARROW_ASSIGN_OR_RAISE(*column,\n+                          compute::Cast(std::move(*column), expected_type->index_type()));\n+\n+    column->mutable_array()->dictionary = std::move(dictionary);\n+    column->mutable_array()->type = expected_type;\n+    return Status::OK();\n+  }\n+\n+  Status IncreaseSize() {\n+    auto factor = static_cast<int32_t>(dictionaries_.back()->length());\n+\n+    if (arrow::internal::MultiplyWithOverflow(size_, factor, &size_)) {\n+      return Status::CapacityError(\"Max groups exceeded\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  int32_t size_ = 1;\n+  ArrayVector dictionaries_;\n+};\n+\n+Result<std::shared_ptr<StructArray>> MakeGroupings(const StructArray& keys) {\n+  if (keys.num_fields() == 0) {\n+    return Status::Invalid(\"Grouping with no keys\");\n+  }\n+\n+  if (keys.null_count() != 0) {\n+    return Status::Invalid(\"Grouping with null keys\");\n+  }\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused, StructDictionary::Encode(keys.fields()));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto sort_indices, compute::SortIndices(*fused.indices));\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted, compute::Take(fused.indices, *sort_indices));\n+  fused.indices = checked_pointer_cast<Int32Array>(sorted.make_array());\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused_counts_and_values,\n+                        compute::ValueCounts(fused.indices));\n+  fused.indices.reset();\n+\n+  auto unique_fused_indices =\n+      checked_pointer_cast<Int32Array>(fused_counts_and_values->GetFieldByName(\"values\"));\n+  ARROW_ASSIGN_OR_RAISE(\n+      auto unique_rows,\n+      fused.dictionary->Decode(std::move(unique_fused_indices), keys.type()->fields()));\n+\n+  auto counts =\n+      checked_pointer_cast<Int64Array>(fused_counts_and_values->GetFieldByName(\"counts\"));\n+  ARROW_ASSIGN_OR_RAISE(auto offsets, CountsToOffsets(std::move(counts)));\n+\n+  auto grouped_sort_indices =\n+      std::make_shared<ListArray>(list(sort_indices->type()), unique_rows->length(),\n+                                  std::move(offsets), std::move(sort_indices));\n+\n+  return StructArray::Make(\n+      ArrayVector{std::move(unique_rows), std::move(grouped_sort_indices)},\n+      std::vector<std::string>{\"values\", \"groupings\"});\n+}\n+\n+Result<std::shared_ptr<ListArray>> ApplyGroupings(const ListArray& groupings,\n+                                                  const Array& array) {\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted,\n+                        compute::Take(array, groupings.data()->child_data[0]));\n+\n+  return std::make_shared<ListArray>(list(array.type()), groupings.length(),\n+                                     groupings.value_offsets(), sorted.make_array());\n+}\n+\n+struct ScalarVectorToArray {\n\nReview comment:\n       https://issues.apache.org/jira/browse/ARROW-11932\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:20:20.179+0000",
                    "updated": "2021-03-11T15:20:20.179+0000",
                    "started": "2021-03-11T15:20:20.178+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564665",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564666",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592450678\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n #include \"arrow/chunked_array.h\"\n #include \"arrow/compute/api_aggregate.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/kernels/aggregate_internal.h\"\n #include \"arrow/compute/kernels/test_util.h\"\n+#include \"arrow/compute/registry.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bitmap_reader.h\"\n #include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/int_util_internal.h\"\n \n #include \"arrow/testing/gtest_common.h\"\n #include \"arrow/testing/gtest_util.h\"\n #include \"arrow/testing/random.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n+using internal::BitmapReader;\n using internal::checked_cast;\n using internal::checked_pointer_cast;\n \n namespace compute {\n \n+// Copy-pasta from partition.cc\n+//\n+// In the finished product this will only be a test helper for group_by\n+// and partition.cc will rely on a no-aggregate call to group_by.\n+namespace group_helpers {\n+namespace {\n+\n+// Transform an array of counts to offsets which will divide a ListArray\n+// into an equal number of slices with corresponding lengths.\n+Result<std::shared_ptr<Buffer>> CountsToOffsets(std::shared_ptr<Int64Array> counts) {\n+  TypedBufferBuilder<int32_t> offset_builder;\n+  RETURN_NOT_OK(offset_builder.Resize(counts->length() + 1));\n+\n+  int32_t current_offset = 0;\n+  offset_builder.UnsafeAppend(current_offset);\n+\n+  for (int64_t i = 0; i < counts->length(); ++i) {\n+    DCHECK_NE(counts->Value(i), 0);\n+    current_offset += static_cast<int32_t>(counts->Value(i));\n+    offset_builder.UnsafeAppend(current_offset);\n+  }\n+\n+  std::shared_ptr<Buffer> offsets;\n+  RETURN_NOT_OK(offset_builder.Finish(&offsets));\n+  return offsets;\n+}\n+\n+class StructDictionary {\n+ public:\n+  struct Encoded {\n+    std::shared_ptr<Int32Array> indices;\n+    std::shared_ptr<StructDictionary> dictionary;\n+  };\n+\n+  static Result<Encoded> Encode(const ArrayVector& columns) {\n+    Encoded out{nullptr, std::make_shared<StructDictionary>()};\n+\n+    for (const auto& column : columns) {\n+      if (column->null_count() != 0) {\n+        return Status::NotImplemented(\"Grouping on a field with nulls\");\n+      }\n+\n+      RETURN_NOT_OK(out.dictionary->AddOne(column, &out.indices));\n+    }\n+\n+    return out;\n+  }\n+\n+  Result<std::shared_ptr<StructArray>> Decode(std::shared_ptr<Int32Array> fused_indices,\n+                                              FieldVector fields) {\n+    std::vector<Int32Builder> builders(dictionaries_.size());\n+    for (Int32Builder& b : builders) {\n+      RETURN_NOT_OK(b.Resize(fused_indices->length()));\n+    }\n+\n+    std::vector<int32_t> codes(dictionaries_.size());\n+    for (int64_t i = 0; i < fused_indices->length(); ++i) {\n+      Expand(fused_indices->Value(i), codes.data());\n+\n+      auto builder_it = builders.begin();\n+      for (int32_t index : codes) {\n+        builder_it++->UnsafeAppend(index);\n+      }\n+    }\n+\n+    ArrayVector columns(dictionaries_.size());\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      std::shared_ptr<ArrayData> indices;\n+      RETURN_NOT_OK(builders[i].FinishInternal(&indices));\n+\n+      ARROW_ASSIGN_OR_RAISE(Datum column, compute::Take(dictionaries_[i], indices));\n+\n+      if (fields[i]->type()->id() == Type::DICTIONARY) {\n+        RETURN_NOT_OK(RestoreDictionaryEncoding(\n+            checked_pointer_cast<DictionaryType>(fields[i]->type()), &column));\n+      }\n+\n+      columns[i] = column.make_array();\n+    }\n+\n+    return StructArray::Make(std::move(columns), std::move(fields));\n+  }\n+\n+ private:\n+  Status AddOne(Datum column, std::shared_ptr<Int32Array>* fused_indices) {\n+    if (column.type()->id() != Type::DICTIONARY) {\n+      ARROW_ASSIGN_OR_RAISE(column, compute::DictionaryEncode(std::move(column)));\n+    }\n+\n+    auto dict_column = column.array_as<DictionaryArray>();\n+    dictionaries_.push_back(dict_column->dictionary());\n+    ARROW_ASSIGN_OR_RAISE(auto indices, compute::Cast(*dict_column->indices(), int32()));\n+\n+    if (*fused_indices == nullptr) {\n+      *fused_indices = checked_pointer_cast<Int32Array>(std::move(indices));\n+      return IncreaseSize();\n+    }\n+\n+    // It's useful to think about the case where each of dictionaries_ has size 10.\n+    // In this case the decimal digit in the ones place is the code in dictionaries_[0],\n+    // the tens place corresponds to the code in dictionaries_[1], etc.\n+    // The incumbent indices must be shifted to the hundreds place so as not to collide.\n+    ARROW_ASSIGN_OR_RAISE(Datum new_fused_indices,\n+                          compute::Multiply(indices, MakeScalar(size_)));\n+\n+    ARROW_ASSIGN_OR_RAISE(new_fused_indices,\n+                          compute::Add(new_fused_indices, *fused_indices));\n+\n+    *fused_indices = checked_pointer_cast<Int32Array>(new_fused_indices.make_array());\n+    return IncreaseSize();\n+  }\n+\n+  // expand a fused code into component dict codes, order is in order of addition\n+  void Expand(int32_t fused_code, int32_t* codes) {\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      auto dictionary_size = static_cast<int32_t>(dictionaries_[i]->length());\n+      codes[i] = fused_code % dictionary_size;\n+      fused_code /= dictionary_size;\n+    }\n+  }\n+\n+  Status RestoreDictionaryEncoding(std::shared_ptr<DictionaryType> expected_type,\n+                                   Datum* column) {\n+    DCHECK_NE(column->type()->id(), Type::DICTIONARY);\n+    ARROW_ASSIGN_OR_RAISE(*column, compute::DictionaryEncode(std::move(*column)));\n+\n+    if (expected_type->index_type()->id() == Type::INT32) {\n+      // dictionary_encode has already yielded the expected index_type\n+      return Status::OK();\n+    }\n+\n+    // cast the indices to the expected index type\n+    auto dictionary = std::move(column->mutable_array()->dictionary);\n+    column->mutable_array()->type = int32();\n+\n+    ARROW_ASSIGN_OR_RAISE(*column,\n+                          compute::Cast(std::move(*column), expected_type->index_type()));\n+\n+    column->mutable_array()->dictionary = std::move(dictionary);\n+    column->mutable_array()->type = expected_type;\n+    return Status::OK();\n+  }\n+\n+  Status IncreaseSize() {\n+    auto factor = static_cast<int32_t>(dictionaries_.back()->length());\n+\n+    if (arrow::internal::MultiplyWithOverflow(size_, factor, &size_)) {\n+      return Status::CapacityError(\"Max groups exceeded\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  int32_t size_ = 1;\n+  ArrayVector dictionaries_;\n+};\n+\n+Result<std::shared_ptr<StructArray>> MakeGroupings(const StructArray& keys) {\n+  if (keys.num_fields() == 0) {\n+    return Status::Invalid(\"Grouping with no keys\");\n+  }\n+\n+  if (keys.null_count() != 0) {\n+    return Status::Invalid(\"Grouping with null keys\");\n+  }\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused, StructDictionary::Encode(keys.fields()));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto sort_indices, compute::SortIndices(*fused.indices));\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted, compute::Take(fused.indices, *sort_indices));\n+  fused.indices = checked_pointer_cast<Int32Array>(sorted.make_array());\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused_counts_and_values,\n+                        compute::ValueCounts(fused.indices));\n+  fused.indices.reset();\n+\n+  auto unique_fused_indices =\n+      checked_pointer_cast<Int32Array>(fused_counts_and_values->GetFieldByName(\"values\"));\n+  ARROW_ASSIGN_OR_RAISE(\n+      auto unique_rows,\n+      fused.dictionary->Decode(std::move(unique_fused_indices), keys.type()->fields()));\n+\n+  auto counts =\n+      checked_pointer_cast<Int64Array>(fused_counts_and_values->GetFieldByName(\"counts\"));\n+  ARROW_ASSIGN_OR_RAISE(auto offsets, CountsToOffsets(std::move(counts)));\n+\n+  auto grouped_sort_indices =\n+      std::make_shared<ListArray>(list(sort_indices->type()), unique_rows->length(),\n+                                  std::move(offsets), std::move(sort_indices));\n+\n+  return StructArray::Make(\n+      ArrayVector{std::move(unique_rows), std::move(grouped_sort_indices)},\n+      std::vector<std::string>{\"values\", \"groupings\"});\n+}\n+\n+Result<std::shared_ptr<ListArray>> ApplyGroupings(const ListArray& groupings,\n+                                                  const Array& array) {\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted,\n+                        compute::Take(array, groupings.data()->child_data[0]));\n+\n+  return std::make_shared<ListArray>(list(array.type()), groupings.length(),\n+                                     groupings.value_offsets(), sorted.make_array());\n+}\n+\n+struct ScalarVectorToArray {\n+  template <typename T, typename AppendScalar,\n+            typename BuilderType = typename TypeTraits<T>::BuilderType,\n+            typename ScalarType = typename TypeTraits<T>::ScalarType>\n+  Status UseBuilder(const AppendScalar& append) {\n+    BuilderType builder(type(), default_memory_pool());\n+    for (const auto& s : scalars_) {\n+      if (s->is_valid) {\n+        RETURN_NOT_OK(append(checked_cast<const ScalarType&>(*s), &builder));\n+      } else {\n+        RETURN_NOT_OK(builder.AppendNull());\n+      }\n+    }\n+    return builder.FinishInternal(&data_);\n+  }\n+\n+  struct AppendValue {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      return builder->Append(s.value);\n+    }\n+  };\n+\n+  struct AppendBuffer {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      const Buffer& buffer = *s.value;\n+      return builder->Append(util::string_view{buffer});\n+    }\n+  };\n+\n+  template <typename T>\n+  enable_if_primitive_ctype<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendValue{});\n+  }\n+\n+  template <typename T>\n+  enable_if_has_string_view<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendBuffer{});\n+  }\n+\n+  Status Visit(const DataType& type) {\n+    return Status::NotImplemented(\"ScalarVectorToArray for type \", type);\n+  }\n+\n+  Result<Datum> Convert(ScalarVector scalars) && {\n+    if (scalars.size() == 0) {\n+      return Status::NotImplemented(\"ScalarVectorToArray with no scalars\");\n+    }\n+    scalars_ = std::move(scalars);\n+    RETURN_NOT_OK(VisitTypeInline(*type(), this));\n+    return Datum(std::move(data_));\n+  }\n+\n+  const std::shared_ptr<DataType>& type() { return scalars_[0]->type; }\n+\n+  ScalarVector scalars_;\n+  std::shared_ptr<ArrayData> data_;\n+};\n+\n+Result<Datum> NaiveGroupBy(std::vector<Datum> aggregands, std::vector<Datum> keys,\n+                           GroupByOptions options) {\n+  ArrayVector keys_arrays;\n+  for (const Datum& key : keys) keys_arrays.push_back(key.make_array());\n+  std::vector<std::string> key_names(keys_arrays.size(), \"\");\n+  ARROW_ASSIGN_OR_RAISE(auto keys_struct,\n+                        StructArray::Make(std::move(keys_arrays), std::move(key_names)));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto groupings_and_values, MakeGroupings(*keys_struct));\n+\n+  auto groupings =\n+      checked_pointer_cast<ListArray>(groupings_and_values->GetFieldByName(\"groupings\"));\n+\n+  int64_t n_groups = groupings->length();\n+\n+  ArrayVector out_columns;\n+\n+  for (size_t i_agg = 0; i_agg < aggregands.size(); ++i_agg) {\n+    const Datum& aggregand = aggregands[i_agg];\n+    const std::string& function = options.aggregates[i_agg].function;\n+\n+    ScalarVector aggregated_scalars;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto grouped_aggregand,\n+                          ApplyGroupings(*groupings, *aggregand.make_array()));\n+\n+    for (int64_t i_group = 0; i_group < n_groups; ++i_group) {\n+      ARROW_ASSIGN_OR_RAISE(\n+          Datum d, CallFunction(function, {grouped_aggregand->value_slice(i_group)}));\n+      aggregated_scalars.push_back(d.scalar());\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(Datum aggregated_column,\n+                          ScalarVectorToArray{}.Convert(std::move(aggregated_scalars)));\n+    out_columns.push_back(aggregated_column.make_array());\n+  }\n+\n+  keys_struct =\n+      checked_pointer_cast<StructArray>(groupings_and_values->GetFieldByName(\"values\"));\n+  for (size_t i_key = 0; i_key < aggregands.size(); ++i_key) {\n+    out_columns.push_back(keys_struct->field(i_key));\n+  }\n+\n+  std::vector<std::string> out_names(out_columns.size(), \"\");\n+  return StructArray::Make(std::move(out_columns), std::move(out_names));\n+}\n+\n+void ValidateGroupBy(GroupByOptions options, std::vector<Datum> aggregands,\n+                     std::vector<Datum> keys) {\n+  ASSERT_OK_AND_ASSIGN(Datum expected,\n+                       group_helpers::NaiveGroupBy(aggregands, keys, options));\n+\n+  ASSERT_OK_AND_ASSIGN(Datum actual, GroupBy(aggregands, keys, options));\n+\n+  // Ordering of groups is not important, so sort by key columns to ensure the comparison\n+  // doesn't fail spuriously\n+\n+  for (Datum* out : {&expected, &actual}) {\n+    auto out_columns = out->array_as<StructArray>()->fields();\n+\n+    SortOptions sort_options;\n+    FieldVector key_fields;\n+    ArrayVector key_columns;\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      auto name = std::to_string(i);\n+      sort_options.sort_keys.emplace_back(name);\n+      key_fields.push_back(field(name, out_columns[0]->type()));\n+      key_columns.push_back(out_columns[0]);\n+    }\n+    auto key_batch = RecordBatch::Make(schema(std::move(key_fields)), out->length(),\n+                                       std::move(key_columns));\n+\n+    ASSERT_OK_AND_ASSIGN(Datum sort_indices, SortIndices(key_batch, sort_options));\n+    ASSERT_OK_AND_ASSIGN(*out, Take(*out, sort_indices, TakeOptions::NoBoundsCheck()));\n+  }\n+\n+  AssertDatumsEqual(expected, actual, /*verbose=*/true);\n+}\n+\n+}  // namespace\n+}  // namespace group_helpers\n+\n+TEST(GroupBy, SumOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n\nReview comment:\n       will do\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:20:45.294+0000",
                    "updated": "2021-03-11T15:20:45.294+0000",
                    "started": "2021-03-11T15:20:45.293+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564666",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564667",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592450949\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_test.cc\n##########\n@@ -27,24 +27,531 @@\n #include \"arrow/array.h\"\n #include \"arrow/chunked_array.h\"\n #include \"arrow/compute/api_aggregate.h\"\n+#include \"arrow/compute/api_scalar.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/kernels/aggregate_internal.h\"\n #include \"arrow/compute/kernels/test_util.h\"\n+#include \"arrow/compute/registry.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bitmap_reader.h\"\n #include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/int_util_internal.h\"\n \n #include \"arrow/testing/gtest_common.h\"\n #include \"arrow/testing/gtest_util.h\"\n #include \"arrow/testing/random.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n+using internal::BitmapReader;\n using internal::checked_cast;\n using internal::checked_pointer_cast;\n \n namespace compute {\n \n+// Copy-pasta from partition.cc\n+//\n+// In the finished product this will only be a test helper for group_by\n+// and partition.cc will rely on a no-aggregate call to group_by.\n+namespace group_helpers {\n+namespace {\n+\n+// Transform an array of counts to offsets which will divide a ListArray\n+// into an equal number of slices with corresponding lengths.\n+Result<std::shared_ptr<Buffer>> CountsToOffsets(std::shared_ptr<Int64Array> counts) {\n+  TypedBufferBuilder<int32_t> offset_builder;\n+  RETURN_NOT_OK(offset_builder.Resize(counts->length() + 1));\n+\n+  int32_t current_offset = 0;\n+  offset_builder.UnsafeAppend(current_offset);\n+\n+  for (int64_t i = 0; i < counts->length(); ++i) {\n+    DCHECK_NE(counts->Value(i), 0);\n+    current_offset += static_cast<int32_t>(counts->Value(i));\n+    offset_builder.UnsafeAppend(current_offset);\n+  }\n+\n+  std::shared_ptr<Buffer> offsets;\n+  RETURN_NOT_OK(offset_builder.Finish(&offsets));\n+  return offsets;\n+}\n+\n+class StructDictionary {\n+ public:\n+  struct Encoded {\n+    std::shared_ptr<Int32Array> indices;\n+    std::shared_ptr<StructDictionary> dictionary;\n+  };\n+\n+  static Result<Encoded> Encode(const ArrayVector& columns) {\n+    Encoded out{nullptr, std::make_shared<StructDictionary>()};\n+\n+    for (const auto& column : columns) {\n+      if (column->null_count() != 0) {\n+        return Status::NotImplemented(\"Grouping on a field with nulls\");\n+      }\n+\n+      RETURN_NOT_OK(out.dictionary->AddOne(column, &out.indices));\n+    }\n+\n+    return out;\n+  }\n+\n+  Result<std::shared_ptr<StructArray>> Decode(std::shared_ptr<Int32Array> fused_indices,\n+                                              FieldVector fields) {\n+    std::vector<Int32Builder> builders(dictionaries_.size());\n+    for (Int32Builder& b : builders) {\n+      RETURN_NOT_OK(b.Resize(fused_indices->length()));\n+    }\n+\n+    std::vector<int32_t> codes(dictionaries_.size());\n+    for (int64_t i = 0; i < fused_indices->length(); ++i) {\n+      Expand(fused_indices->Value(i), codes.data());\n+\n+      auto builder_it = builders.begin();\n+      for (int32_t index : codes) {\n+        builder_it++->UnsafeAppend(index);\n+      }\n+    }\n+\n+    ArrayVector columns(dictionaries_.size());\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      std::shared_ptr<ArrayData> indices;\n+      RETURN_NOT_OK(builders[i].FinishInternal(&indices));\n+\n+      ARROW_ASSIGN_OR_RAISE(Datum column, compute::Take(dictionaries_[i], indices));\n+\n+      if (fields[i]->type()->id() == Type::DICTIONARY) {\n+        RETURN_NOT_OK(RestoreDictionaryEncoding(\n+            checked_pointer_cast<DictionaryType>(fields[i]->type()), &column));\n+      }\n+\n+      columns[i] = column.make_array();\n+    }\n+\n+    return StructArray::Make(std::move(columns), std::move(fields));\n+  }\n+\n+ private:\n+  Status AddOne(Datum column, std::shared_ptr<Int32Array>* fused_indices) {\n+    if (column.type()->id() != Type::DICTIONARY) {\n+      ARROW_ASSIGN_OR_RAISE(column, compute::DictionaryEncode(std::move(column)));\n+    }\n+\n+    auto dict_column = column.array_as<DictionaryArray>();\n+    dictionaries_.push_back(dict_column->dictionary());\n+    ARROW_ASSIGN_OR_RAISE(auto indices, compute::Cast(*dict_column->indices(), int32()));\n+\n+    if (*fused_indices == nullptr) {\n+      *fused_indices = checked_pointer_cast<Int32Array>(std::move(indices));\n+      return IncreaseSize();\n+    }\n+\n+    // It's useful to think about the case where each of dictionaries_ has size 10.\n+    // In this case the decimal digit in the ones place is the code in dictionaries_[0],\n+    // the tens place corresponds to the code in dictionaries_[1], etc.\n+    // The incumbent indices must be shifted to the hundreds place so as not to collide.\n+    ARROW_ASSIGN_OR_RAISE(Datum new_fused_indices,\n+                          compute::Multiply(indices, MakeScalar(size_)));\n+\n+    ARROW_ASSIGN_OR_RAISE(new_fused_indices,\n+                          compute::Add(new_fused_indices, *fused_indices));\n+\n+    *fused_indices = checked_pointer_cast<Int32Array>(new_fused_indices.make_array());\n+    return IncreaseSize();\n+  }\n+\n+  // expand a fused code into component dict codes, order is in order of addition\n+  void Expand(int32_t fused_code, int32_t* codes) {\n+    for (size_t i = 0; i < dictionaries_.size(); ++i) {\n+      auto dictionary_size = static_cast<int32_t>(dictionaries_[i]->length());\n+      codes[i] = fused_code % dictionary_size;\n+      fused_code /= dictionary_size;\n+    }\n+  }\n+\n+  Status RestoreDictionaryEncoding(std::shared_ptr<DictionaryType> expected_type,\n+                                   Datum* column) {\n+    DCHECK_NE(column->type()->id(), Type::DICTIONARY);\n+    ARROW_ASSIGN_OR_RAISE(*column, compute::DictionaryEncode(std::move(*column)));\n+\n+    if (expected_type->index_type()->id() == Type::INT32) {\n+      // dictionary_encode has already yielded the expected index_type\n+      return Status::OK();\n+    }\n+\n+    // cast the indices to the expected index type\n+    auto dictionary = std::move(column->mutable_array()->dictionary);\n+    column->mutable_array()->type = int32();\n+\n+    ARROW_ASSIGN_OR_RAISE(*column,\n+                          compute::Cast(std::move(*column), expected_type->index_type()));\n+\n+    column->mutable_array()->dictionary = std::move(dictionary);\n+    column->mutable_array()->type = expected_type;\n+    return Status::OK();\n+  }\n+\n+  Status IncreaseSize() {\n+    auto factor = static_cast<int32_t>(dictionaries_.back()->length());\n+\n+    if (arrow::internal::MultiplyWithOverflow(size_, factor, &size_)) {\n+      return Status::CapacityError(\"Max groups exceeded\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  int32_t size_ = 1;\n+  ArrayVector dictionaries_;\n+};\n+\n+Result<std::shared_ptr<StructArray>> MakeGroupings(const StructArray& keys) {\n+  if (keys.num_fields() == 0) {\n+    return Status::Invalid(\"Grouping with no keys\");\n+  }\n+\n+  if (keys.null_count() != 0) {\n+    return Status::Invalid(\"Grouping with null keys\");\n+  }\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused, StructDictionary::Encode(keys.fields()));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto sort_indices, compute::SortIndices(*fused.indices));\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted, compute::Take(fused.indices, *sort_indices));\n+  fused.indices = checked_pointer_cast<Int32Array>(sorted.make_array());\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fused_counts_and_values,\n+                        compute::ValueCounts(fused.indices));\n+  fused.indices.reset();\n+\n+  auto unique_fused_indices =\n+      checked_pointer_cast<Int32Array>(fused_counts_and_values->GetFieldByName(\"values\"));\n+  ARROW_ASSIGN_OR_RAISE(\n+      auto unique_rows,\n+      fused.dictionary->Decode(std::move(unique_fused_indices), keys.type()->fields()));\n+\n+  auto counts =\n+      checked_pointer_cast<Int64Array>(fused_counts_and_values->GetFieldByName(\"counts\"));\n+  ARROW_ASSIGN_OR_RAISE(auto offsets, CountsToOffsets(std::move(counts)));\n+\n+  auto grouped_sort_indices =\n+      std::make_shared<ListArray>(list(sort_indices->type()), unique_rows->length(),\n+                                  std::move(offsets), std::move(sort_indices));\n+\n+  return StructArray::Make(\n+      ArrayVector{std::move(unique_rows), std::move(grouped_sort_indices)},\n+      std::vector<std::string>{\"values\", \"groupings\"});\n+}\n+\n+Result<std::shared_ptr<ListArray>> ApplyGroupings(const ListArray& groupings,\n+                                                  const Array& array) {\n+  ARROW_ASSIGN_OR_RAISE(Datum sorted,\n+                        compute::Take(array, groupings.data()->child_data[0]));\n+\n+  return std::make_shared<ListArray>(list(array.type()), groupings.length(),\n+                                     groupings.value_offsets(), sorted.make_array());\n+}\n+\n+struct ScalarVectorToArray {\n+  template <typename T, typename AppendScalar,\n+            typename BuilderType = typename TypeTraits<T>::BuilderType,\n+            typename ScalarType = typename TypeTraits<T>::ScalarType>\n+  Status UseBuilder(const AppendScalar& append) {\n+    BuilderType builder(type(), default_memory_pool());\n+    for (const auto& s : scalars_) {\n+      if (s->is_valid) {\n+        RETURN_NOT_OK(append(checked_cast<const ScalarType&>(*s), &builder));\n+      } else {\n+        RETURN_NOT_OK(builder.AppendNull());\n+      }\n+    }\n+    return builder.FinishInternal(&data_);\n+  }\n+\n+  struct AppendValue {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      return builder->Append(s.value);\n+    }\n+  };\n+\n+  struct AppendBuffer {\n+    template <typename BuilderType, typename ScalarType>\n+    Status operator()(const ScalarType& s, BuilderType* builder) const {\n+      const Buffer& buffer = *s.value;\n+      return builder->Append(util::string_view{buffer});\n+    }\n+  };\n+\n+  template <typename T>\n+  enable_if_primitive_ctype<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendValue{});\n+  }\n+\n+  template <typename T>\n+  enable_if_has_string_view<T, Status> Visit(const T&) {\n+    return UseBuilder<T>(AppendBuffer{});\n+  }\n+\n+  Status Visit(const DataType& type) {\n+    return Status::NotImplemented(\"ScalarVectorToArray for type \", type);\n+  }\n+\n+  Result<Datum> Convert(ScalarVector scalars) && {\n+    if (scalars.size() == 0) {\n+      return Status::NotImplemented(\"ScalarVectorToArray with no scalars\");\n+    }\n+    scalars_ = std::move(scalars);\n+    RETURN_NOT_OK(VisitTypeInline(*type(), this));\n+    return Datum(std::move(data_));\n+  }\n+\n+  const std::shared_ptr<DataType>& type() { return scalars_[0]->type; }\n+\n+  ScalarVector scalars_;\n+  std::shared_ptr<ArrayData> data_;\n+};\n+\n+Result<Datum> NaiveGroupBy(std::vector<Datum> aggregands, std::vector<Datum> keys,\n+                           GroupByOptions options) {\n+  ArrayVector keys_arrays;\n+  for (const Datum& key : keys) keys_arrays.push_back(key.make_array());\n+  std::vector<std::string> key_names(keys_arrays.size(), \"\");\n+  ARROW_ASSIGN_OR_RAISE(auto keys_struct,\n+                        StructArray::Make(std::move(keys_arrays), std::move(key_names)));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto groupings_and_values, MakeGroupings(*keys_struct));\n+\n+  auto groupings =\n+      checked_pointer_cast<ListArray>(groupings_and_values->GetFieldByName(\"groupings\"));\n+\n+  int64_t n_groups = groupings->length();\n+\n+  ArrayVector out_columns;\n+\n+  for (size_t i_agg = 0; i_agg < aggregands.size(); ++i_agg) {\n+    const Datum& aggregand = aggregands[i_agg];\n+    const std::string& function = options.aggregates[i_agg].function;\n+\n+    ScalarVector aggregated_scalars;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto grouped_aggregand,\n+                          ApplyGroupings(*groupings, *aggregand.make_array()));\n+\n+    for (int64_t i_group = 0; i_group < n_groups; ++i_group) {\n+      ARROW_ASSIGN_OR_RAISE(\n+          Datum d, CallFunction(function, {grouped_aggregand->value_slice(i_group)}));\n+      aggregated_scalars.push_back(d.scalar());\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(Datum aggregated_column,\n+                          ScalarVectorToArray{}.Convert(std::move(aggregated_scalars)));\n+    out_columns.push_back(aggregated_column.make_array());\n+  }\n+\n+  keys_struct =\n+      checked_pointer_cast<StructArray>(groupings_and_values->GetFieldByName(\"values\"));\n+  for (size_t i_key = 0; i_key < aggregands.size(); ++i_key) {\n+    out_columns.push_back(keys_struct->field(i_key));\n+  }\n+\n+  std::vector<std::string> out_names(out_columns.size(), \"\");\n+  return StructArray::Make(std::move(out_columns), std::move(out_names));\n+}\n+\n+void ValidateGroupBy(GroupByOptions options, std::vector<Datum> aggregands,\n+                     std::vector<Datum> keys) {\n+  ASSERT_OK_AND_ASSIGN(Datum expected,\n+                       group_helpers::NaiveGroupBy(aggregands, keys, options));\n+\n+  ASSERT_OK_AND_ASSIGN(Datum actual, GroupBy(aggregands, keys, options));\n+\n+  // Ordering of groups is not important, so sort by key columns to ensure the comparison\n+  // doesn't fail spuriously\n+\n+  for (Datum* out : {&expected, &actual}) {\n+    auto out_columns = out->array_as<StructArray>()->fields();\n+\n+    SortOptions sort_options;\n+    FieldVector key_fields;\n+    ArrayVector key_columns;\n+    for (size_t i = 0; i < keys.size(); ++i) {\n+      auto name = std::to_string(i);\n+      sort_options.sort_keys.emplace_back(name);\n+      key_fields.push_back(field(name, out_columns[0]->type()));\n+      key_columns.push_back(out_columns[0]);\n+    }\n+    auto key_batch = RecordBatch::Make(schema(std::move(key_fields)), out->length(),\n+                                       std::move(key_columns));\n+\n+    ASSERT_OK_AND_ASSIGN(Datum sort_indices, SortIndices(key_batch, sort_options));\n+    ASSERT_OK_AND_ASSIGN(*out, Take(*out, sort_indices, TakeOptions::NoBoundsCheck()));\n+  }\n+\n+  AssertDatumsEqual(expected, actual, /*verbose=*/true);\n+}\n+\n+}  // namespace\n+}  // namespace group_helpers\n+\n+TEST(GroupBy, SumOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped, GroupBy({aggregand}, {key},\n+                                                             GroupByOptions{\n+                                                                 {\"sum\", nullptr},\n+                                                             }));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", float64()),\n+                                      field(\"\", int64()),\n+                                  }),\n+                                  R\"([\n+    [4.25,   1],\n+    [-0.125, 2],\n+    [null,   3],\n+    [0.75,   null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, MinMaxOnly) {\n+  auto aggregand = ArrayFromJSON(float64(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 3, 1, 2, 2, null]\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped, GroupBy({aggregand}, {key},\n+                                                             GroupByOptions{\n+                                                                 {\"min_max\", nullptr},\n+                                                             }));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", struct_({\n+                                                    field(\"min\", float64()),\n+                                                    field(\"max\", float64()),\n+                                                })),\n+                                      field(\"\", int64()),\n+                                  }),\n+                                  R\"([\n+    [{\"min\": 1.0,   \"max\": 3.25},  1],\n+    [{\"min\": -0.25, \"max\": 0.125}, 2],\n+    [{\"min\": null,  \"max\": null},  3],\n+    [{\"min\": 0.75,  \"max\": 0.75},  null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, CountAndSum) {\n+  auto aggregand = ArrayFromJSON(float32(), \"[1.0, 0.0, null, 3.25, 0.125, -0.25, 0.75]\");\n+  auto key = ArrayFromJSON(int64(), \"[1, 2, 1, 3, 2, 3, null]\");\n+\n+  CountOptions count_options;\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      Datum aggregated_and_grouped,\n+      // NB: passing an aggregand twice or also using it as a key is legal\n+      GroupBy({aggregand, aggregand, key}, {key},\n+              GroupByOptions{\n+                  {\"count\", &count_options},\n+                  {\"sum\", nullptr},\n+                  {\"sum\", nullptr},\n+              }));\n+\n+  AssertDatumsEqual(\n+      ArrayFromJSON(struct_({\n+                        field(\"\", int64()),\n+                        // NB: summing a float32 array results in float64 sums\n+                        field(\"\", float64()),\n+                        field(\"\", int64()),\n+                        field(\"\", int64()),\n+                    }),\n+                    R\"([\n+    [1, 1.0,   2,    1],\n+    [2, 0.125, 4,    2],\n+    [2, 3.0,   6,    3],\n+    [1, 0.75,  null, null]\n+  ])\"),\n+      aggregated_and_grouped,\n+      /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, StringKey) {\n+  auto aggregand = ArrayFromJSON(int64(), \"[10, 5, 4, 2, 12, 9]\");\n+  auto key = ArrayFromJSON(utf8(), R\"([\"alfa\", \"beta\", \"gamma\", \"gamma\", null, \"beta\"])\");\n+\n+  ASSERT_OK_AND_ASSIGN(Datum aggregated_and_grouped,\n+                       GroupBy({aggregand}, {key}, GroupByOptions{{\"sum\", nullptr}}));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", int64()),\n+                                      field(\"\", utf8()),\n+                                  }),\n+                                  R\"([\n+    [10,   \"alfa\"],\n+    [14,   \"beta\"],\n+    [6,    \"gamma\"],\n+    [12,   null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, MultipleKeys) {\n+  auto aggregand = ArrayFromJSON(float32(), \"[0.125, 0.5, -0.75, 8, 1.0, 2.0]\");\n+  auto int_key = ArrayFromJSON(int32(), \"[0, 1, 0, 1, 0, 1]\");\n+  auto str_key =\n+      ArrayFromJSON(utf8(), R\"([\"beta\", \"beta\", \"gamma\", \"gamma\", null, \"beta\"])\");\n+\n+  ASSERT_OK_AND_ASSIGN(\n+      Datum aggregated_and_grouped,\n+      GroupBy({aggregand}, {int_key, str_key}, GroupByOptions{{\"sum\", nullptr}}));\n+\n+  AssertDatumsEqual(ArrayFromJSON(struct_({\n+                                      field(\"\", float64()),\n+                                      field(\"\", int32()),\n+                                      field(\"\", utf8()),\n+                                  }),\n+                                  R\"([\n+    [0.125, 0, \"beta\"],\n+    [2.5,   1, \"beta\"],\n+    [-0.75, 0, \"gamma\"],\n+    [8,     1, \"gamma\"],\n+    [1.0,   0, null]\n+  ])\"),\n+                    aggregated_and_grouped,\n+                    /*verbose=*/true);\n+}\n+\n+TEST(GroupBy, ConcreteCaseWithValidateGroupBy) {\n\nReview comment:\n       wlll do\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:21:08.647+0000",
                    "updated": "2021-03-11T15:21:08.647+0000",
                    "started": "2021-03-11T15:21:08.646+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564667",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564670",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592454275\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n\nReview comment:\n       This is indeed a typo. Will correct to `uint64()`\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:24:31.904+0000",
                    "updated": "2021-03-11T15:24:31.904+0000",
                    "started": "2021-03-11T15:24:31.904+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564670",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564671",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592455189\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n\nReview comment:\n       Eventually this kernel will be reused (as in the scalar aggregate equivalent) to compute means, which is why I'm maintaining a full count. I'll add a comment\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:25:35.501+0000",
                    "updated": "2021-03-11T15:25:35.501+0000",
                    "started": "2021-03-11T15:25:35.501+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564671",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564672",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592455705\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n+\n+      if (null_bitmap == nullptr) {\n+        KERNEL_ASSIGN_OR_RAISE(null_bitmap, ctx, ctx->AllocateBitmap(num_groups()));\n+        BitUtil::SetBitsTo(null_bitmap->mutable_data(), 0, num_groups(), true);\n+      }\n+\n+      null_count += 1;\n+      BitUtil::SetBitTo(null_bitmap->mutable_data(), i, false);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type_), num_groups(),\n+                           {std::move(null_bitmap), std::move(sums_)}, null_count);\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return out_type_; }\n+\n+  std::shared_ptr<ResizableBuffer> sums_, counts_;\n+  std::shared_ptr<DataType> out_type_;\n+  ConsumeImpl consume_impl_;\n+};\n+\n+struct GroupedMinMaxImpl : public GroupedAggregator {\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, BufferVector*)>;\n+\n+  using ResizeImpl = std::function<Status(Buffer*, int64_t)>;\n+\n+  struct GetImpl {\n+    template <typename T, typename CType = typename TypeTraits<T>::CType>\n+    enable_if_number<T, Status> Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, BufferVector* buffers) {\n+        auto raw_inputs = reinterpret_cast<const CType*>(input->buffers[1]->data());\n+\n+        auto raw_mins = reinterpret_cast<CType*>(buffers->at(0)->mutable_data());\n+        auto raw_maxes = reinterpret_cast<CType*>(buffers->at(1)->mutable_data());\n+\n+        auto raw_has_nulls = buffers->at(2)->mutable_data();\n+        auto raw_has_values = buffers->at(3)->mutable_data();\n+\n+        for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+          auto g = group_ids[i];\n+          bool is_valid = BitUtil::GetBit(input->buffers[0]->data(), input_i);\n\nReview comment:\n       Will do\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:26:08.546+0000",
                    "updated": "2021-03-11T15:26:08.546+0000",
                    "started": "2021-03-11T15:26:08.546+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564672",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564673",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592456204\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n+\n+      if (null_bitmap == nullptr) {\n+        KERNEL_ASSIGN_OR_RAISE(null_bitmap, ctx, ctx->AllocateBitmap(num_groups()));\n+        BitUtil::SetBitsTo(null_bitmap->mutable_data(), 0, num_groups(), true);\n+      }\n+\n+      null_count += 1;\n+      BitUtil::SetBitTo(null_bitmap->mutable_data(), i, false);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type_), num_groups(),\n+                           {std::move(null_bitmap), std::move(sums_)}, null_count);\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return out_type_; }\n+\n+  std::shared_ptr<ResizableBuffer> sums_, counts_;\n+  std::shared_ptr<DataType> out_type_;\n+  ConsumeImpl consume_impl_;\n+};\n+\n+struct GroupedMinMaxImpl : public GroupedAggregator {\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, BufferVector*)>;\n+\n+  using ResizeImpl = std::function<Status(Buffer*, int64_t)>;\n+\n+  struct GetImpl {\n+    template <typename T, typename CType = typename TypeTraits<T>::CType>\n+    enable_if_number<T, Status> Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, BufferVector* buffers) {\n+        auto raw_inputs = reinterpret_cast<const CType*>(input->buffers[1]->data());\n+\n+        auto raw_mins = reinterpret_cast<CType*>(buffers->at(0)->mutable_data());\n+        auto raw_maxes = reinterpret_cast<CType*>(buffers->at(1)->mutable_data());\n+\n+        auto raw_has_nulls = buffers->at(2)->mutable_data();\n+        auto raw_has_values = buffers->at(3)->mutable_data();\n+\n+        for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+          auto g = group_ids[i];\n+          bool is_valid = BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+          if (is_valid) {\n+            raw_maxes[g] = std::max(raw_maxes[g], raw_inputs[input_i]);\n+            raw_mins[g] = std::min(raw_mins[g], raw_inputs[input_i]);\n+            BitUtil::SetBit(raw_has_values, g);\n+          } else {\n+            BitUtil::SetBit(raw_has_nulls, g);\n+          }\n+        }\n+      };\n+\n+      for (auto pair :\n+           {std::make_pair(&resize_min_impl, std::numeric_limits<CType>::max()),\n+            std::make_pair(&resize_max_impl, std::numeric_limits<CType>::min())}) {\n+        *pair.first = [pair](Buffer* vals, int64_t new_num_groups) {\n\nReview comment:\n       That's great, thanks!\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:26:42.142+0000",
                    "updated": "2021-03-11T15:26:42.142+0000",
                    "started": "2021-03-11T15:26:42.142+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564673",
                    "issueId": "13358015"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/worklog/564676",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #9621:\nURL: https://github.com/apache/arrow/pull/9621#discussion_r592463457\n\n\n\n##########\nFile path: cpp/src/arrow/compute/kernels/aggregate_basic.cc\n##########\n@@ -91,6 +95,377 @@ struct CountImpl : public ScalarAggregator {\n   int64_t nulls = 0;\n };\n \n+struct GroupedAggregator {\n+  virtual ~GroupedAggregator() = default;\n+\n+  virtual void Consume(KernelContext*, const Datum& aggregand,\n+                       const uint32_t* group_ids) = 0;\n+\n+  virtual void Finalize(KernelContext* ctx, Datum* out) = 0;\n+\n+  virtual void Resize(KernelContext* ctx, int64_t new_num_groups) = 0;\n+\n+  virtual int64_t num_groups() const = 0;\n+\n+  void MaybeResize(KernelContext* ctx, int64_t length, const uint32_t* group_ids) {\n+    if (length == 0) return;\n+\n+    // maybe a batch of group_ids should include the min/max group id\n+    int64_t max_group = *std::max_element(group_ids, group_ids + length);\n+    auto old_size = num_groups();\n+\n+    if (max_group >= old_size) {\n+      auto new_size = BufferBuilder::GrowByFactor(old_size, max_group + 1);\n+      Resize(ctx, new_size);\n+    }\n+  }\n+\n+  virtual std::shared_ptr<DataType> out_type() const = 0;\n+};\n+\n+struct GroupedCountImpl : public GroupedAggregator {\n+  static std::unique_ptr<GroupedCountImpl> Make(KernelContext* ctx,\n+                                                const std::shared_ptr<DataType>&,\n+                                                const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedCountImpl>();\n+    out->options_ = checked_cast<const CountOptions&>(*options);\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups();\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->TypedResize<int64_t>(new_num_groups));\n+    auto new_size = num_groups();\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+    for (auto i = old_size; i < new_size; ++i) {\n+      raw_counts[i] = 0;\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+\n+    auto raw_counts = reinterpret_cast<int64_t*>(counts_->mutable_data());\n+\n+    const auto& input = aggregand.array();\n+\n+    if (options_.count_mode == CountOptions::COUNT_NULL) {\n+      for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+        auto g = group_ids[i];\n+        raw_counts[g] += !BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+      }\n+      return;\n+    }\n+\n+    arrow::internal::VisitSetBitRunsVoid(\n+        input->buffers[0], input->offset, input->length,\n+        [&](int64_t begin, int64_t length) {\n+          for (int64_t input_i = begin, i = begin - input->offset;\n+               input_i < begin + length; ++input_i, ++i) {\n+            auto g = group_ids[i];\n+            raw_counts[g] += 1;\n+          }\n+        });\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    auto length = num_groups();\n+    *out = std::make_shared<Int64Array>(length, std::move(counts_));\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return int64(); }\n+\n+  CountOptions options_;\n+  std::shared_ptr<ResizableBuffer> counts_;\n+};\n+\n+struct GroupedSumImpl : public GroupedAggregator {\n+  // NB: whether we are accumulating into double, int64_t, or uint64_t\n+  // we always have 64 bits per group in the sums buffer.\n+  static constexpr size_t kSumSize = sizeof(int64_t);\n+\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, Buffer*, Buffer*)>;\n+\n+  struct GetConsumeImpl {\n+    template <typename T,\n+              typename AccumulatorType = typename FindAccumulatorType<T>::Type>\n+    Status Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = reinterpret_cast<const typename TypeTraits<T>::CType*>(\n+            input->buffers[1]->data());\n+        auto raw_sums = reinterpret_cast<typename TypeTraits<AccumulatorType>::CType*>(\n+            sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i, ++i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += raw_input[input_i];\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = TypeTraits<AccumulatorType>::type_singleton();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, Buffer* sums, Buffer* counts) {\n+        auto raw_input = input->buffers[1]->data();\n+        auto raw_sums = reinterpret_cast<uint64_t*>(sums->mutable_data());\n+        auto raw_counts = reinterpret_cast<int64_t*>(counts->mutable_data());\n+\n+        arrow::internal::VisitSetBitRunsVoid(\n+            input->buffers[0], input->offset, input->length,\n+            [&](int64_t begin, int64_t length) {\n+              for (int64_t input_i = begin, i = begin - input->offset;\n+                   input_i < begin + length; ++input_i) {\n+                auto g = group_ids[i];\n+                raw_sums[g] += BitUtil::GetBit(raw_input, input_i);\n+                raw_counts[g] += 1;\n+              }\n+            });\n+      };\n+      out_type = boolean();\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Summing data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    std::shared_ptr<DataType> out_type;\n+  };\n+\n+  static std::unique_ptr<GroupedSumImpl> Make(KernelContext* ctx,\n+                                              const std::shared_ptr<DataType>& input_type,\n+                                              const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedSumImpl>();\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->sums_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    ctx->SetStatus(ctx->Allocate(0).Value(&out->counts_));\n+    if (ctx->HasError()) return nullptr;\n+\n+    GetConsumeImpl get_consume_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_consume_impl));\n+\n+    out->consume_impl_ = std::move(get_consume_impl.consume_impl);\n+    out->out_type_ = std::move(get_consume_impl.out_type);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_size = num_groups() * kSumSize;\n+    KERNEL_RETURN_IF_ERROR(ctx, sums_->Resize(new_num_groups * kSumSize));\n+    KERNEL_RETURN_IF_ERROR(ctx, counts_->Resize(new_num_groups * sizeof(int64_t)));\n+    auto new_size = num_groups() * kSumSize;\n+    std::memset(sums_->mutable_data() + old_size, 0, new_size - old_size);\n+    std::memset(counts_->mutable_data() + old_size, 0, new_size - old_size);\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, sums_.get(), counts_.get());\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    std::shared_ptr<Buffer> null_bitmap;\n+    int64_t null_count = 0;\n+\n+    for (int64_t i = 0; i < num_groups(); ++i) {\n+      if (reinterpret_cast<const int64_t*>(counts_->data())[i] > 0) continue;\n+\n+      if (null_bitmap == nullptr) {\n+        KERNEL_ASSIGN_OR_RAISE(null_bitmap, ctx, ctx->AllocateBitmap(num_groups()));\n+        BitUtil::SetBitsTo(null_bitmap->mutable_data(), 0, num_groups(), true);\n+      }\n+\n+      null_count += 1;\n+      BitUtil::SetBitTo(null_bitmap->mutable_data(), i, false);\n+    }\n+\n+    *out = ArrayData::Make(std::move(out_type_), num_groups(),\n+                           {std::move(null_bitmap), std::move(sums_)}, null_count);\n+  }\n+\n+  int64_t num_groups() const override { return counts_->size() / sizeof(int64_t); }\n+\n+  std::shared_ptr<DataType> out_type() const override { return out_type_; }\n+\n+  std::shared_ptr<ResizableBuffer> sums_, counts_;\n+  std::shared_ptr<DataType> out_type_;\n+  ConsumeImpl consume_impl_;\n+};\n+\n+struct GroupedMinMaxImpl : public GroupedAggregator {\n+  using ConsumeImpl = std::function<void(const std::shared_ptr<ArrayData>&,\n+                                         const uint32_t*, BufferVector*)>;\n+\n+  using ResizeImpl = std::function<Status(Buffer*, int64_t)>;\n+\n+  struct GetImpl {\n+    template <typename T, typename CType = typename TypeTraits<T>::CType>\n+    enable_if_number<T, Status> Visit(const T&) {\n+      consume_impl = [](const std::shared_ptr<ArrayData>& input,\n+                        const uint32_t* group_ids, BufferVector* buffers) {\n+        auto raw_inputs = reinterpret_cast<const CType*>(input->buffers[1]->data());\n+\n+        auto raw_mins = reinterpret_cast<CType*>(buffers->at(0)->mutable_data());\n+        auto raw_maxes = reinterpret_cast<CType*>(buffers->at(1)->mutable_data());\n+\n+        auto raw_has_nulls = buffers->at(2)->mutable_data();\n+        auto raw_has_values = buffers->at(3)->mutable_data();\n+\n+        for (int64_t i = 0, input_i = input->offset; i < input->length; ++i, ++input_i) {\n+          auto g = group_ids[i];\n+          bool is_valid = BitUtil::GetBit(input->buffers[0]->data(), input_i);\n+          if (is_valid) {\n+            raw_maxes[g] = std::max(raw_maxes[g], raw_inputs[input_i]);\n+            raw_mins[g] = std::min(raw_mins[g], raw_inputs[input_i]);\n+            BitUtil::SetBit(raw_has_values, g);\n+          } else {\n+            BitUtil::SetBit(raw_has_nulls, g);\n+          }\n+        }\n+      };\n+\n+      for (auto pair :\n+           {std::make_pair(&resize_min_impl, std::numeric_limits<CType>::max()),\n+            std::make_pair(&resize_max_impl, std::numeric_limits<CType>::min())}) {\n+        *pair.first = [pair](Buffer* vals, int64_t new_num_groups) {\n+          int64_t old_num_groups = vals->size() / sizeof(CType);\n+\n+          int64_t new_size = new_num_groups * sizeof(CType);\n+          RETURN_NOT_OK(checked_cast<ResizableBuffer*>(vals)->Resize(new_size));\n+\n+          auto raw_vals = reinterpret_cast<CType*>(vals->mutable_data());\n+          for (int64_t i = old_num_groups; i != new_num_groups; ++i) {\n+            raw_vals[i] = pair.second;\n+          }\n+          return Status::OK();\n+        };\n+      }\n+\n+      return Status::OK();\n+    }\n+\n+    Status Visit(const BooleanType& type) {\n+      return Status::NotImplemented(\"Grouped MinMax data of type \", type);\n+    }\n+\n+    Status Visit(const HalfFloatType& type) {\n+      return Status::NotImplemented(\"Grouped MinMax data of type \", type);\n+    }\n+\n+    Status Visit(const DataType& type) {\n+      return Status::NotImplemented(\"Grouped MinMax data of type \", type);\n+    }\n+\n+    ConsumeImpl consume_impl;\n+    ResizeImpl resize_min_impl, resize_max_impl;\n+  };\n+\n+  static std::unique_ptr<GroupedMinMaxImpl> Make(\n+      KernelContext* ctx, const std::shared_ptr<DataType>& input_type,\n+      const FunctionOptions* options) {\n+    auto out = ::arrow::internal::make_unique<GroupedMinMaxImpl>();\n+    out->options_ = *checked_cast<const MinMaxOptions*>(options);\n+    out->type_ = input_type;\n+\n+    out->buffers_.resize(4);\n+    for (auto& buf : out->buffers_) {\n+      ctx->SetStatus(ctx->Allocate(0).Value(&buf));\n+      if (ctx->HasError()) return nullptr;\n+    }\n+\n+    GetImpl get_impl;\n+    ctx->SetStatus(VisitTypeInline(*input_type, &get_impl));\n+\n+    out->consume_impl_ = std::move(get_impl.consume_impl);\n+    out->resize_min_impl_ = std::move(get_impl.resize_min_impl);\n+    out->resize_max_impl_ = std::move(get_impl.resize_max_impl);\n+    return out;\n+  }\n+\n+  void Resize(KernelContext* ctx, int64_t new_num_groups) override {\n+    auto old_num_groups = num_groups_;\n+    num_groups_ = new_num_groups;\n+\n+    KERNEL_RETURN_IF_ERROR(ctx, resize_min_impl_(buffers_[0].get(), new_num_groups));\n+    KERNEL_RETURN_IF_ERROR(ctx, resize_max_impl_(buffers_[1].get(), new_num_groups));\n+\n+    for (auto buffer : {buffers_[2].get(), buffers_[3].get()}) {\n+      KERNEL_RETURN_IF_ERROR(ctx, checked_cast<ResizableBuffer*>(buffer)->Resize(\n+                                      BitUtil::BytesForBits(new_num_groups)));\n+      BitUtil::SetBitsTo(buffer->mutable_data(), old_num_groups, new_num_groups, false);\n+    }\n+  }\n+\n+  void Consume(KernelContext* ctx, const Datum& aggregand,\n+               const uint32_t* group_ids) override {\n+    MaybeResize(ctx, aggregand.length(), group_ids);\n+    if (ctx->HasError()) return;\n+    consume_impl_(aggregand.array(), group_ids, &buffers_);\n+  }\n+\n+  void Finalize(KernelContext* ctx, Datum* out) override {\n+    // aggregation for group is valid if there was at least one value in that group\n+    std::shared_ptr<Buffer> null_bitmap = std::move(buffers_[3]);\n+\n+    if (options_.null_handling == MinMaxOptions::EMIT_NULL) {\n+      // ... and there were no nulls in that group\n\nReview comment:\n       At the time of writing this I wasn't immediately confident we could use a buffer as both an input and output to UnalignedBitmapOp. After looking through BitmapWordWriter I think this would be safe\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-11T15:34:58.073+0000",
                    "updated": "2021-03-11T15:34:58.073+0000",
                    "started": "2021-03-11T15:34:58.073+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "564676",
                    "issueId": "13358015"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 31800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@385c9311[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@13a5b5e2[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@31e26484[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@fd6ddff[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2fa56789[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@17d697f2[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@35b207c8[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@cb21da4[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@8afaa94[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@2be5d574[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@ecc0b87[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@120f68c7[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 31800,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Mar 23 20:06:40 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-03-23T20:06:40.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11591/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2021-02-10T19:48:38.000+0000",
        "updated": "2021-03-23T20:06:50.000+0000",
        "timeoriginalestimate": null,
        "description": null,
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "8h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 31800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Compute] Prototype version of hash aggregation",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13358015/comment/17307390",
                    "id": "17307390",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bkietz",
                        "name": "bkietz",
                        "key": "bkietz",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bkietz&avatarId=37277",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bkietz&avatarId=37277",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bkietz&avatarId=37277",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bkietz&avatarId=37277"
                        },
                        "displayName": "Ben Kietzman",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 9621\n[https://github.com/apache/arrow/pull/9621]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bkietz",
                        "name": "bkietz",
                        "key": "bkietz",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bkietz&avatarId=37277",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bkietz&avatarId=37277",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bkietz&avatarId=37277",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bkietz&avatarId=37277"
                        },
                        "displayName": "Ben Kietzman",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2021-03-23T20:06:40.462+0000",
                    "updated": "2021-03-23T20:06:40.462+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0nko0:",
        "customfield_12314139": null
    }
}