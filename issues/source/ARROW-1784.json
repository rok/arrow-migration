{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13117346",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346",
    "key": "ARROW-1784",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12341352",
                "id": "12341352",
                "name": "0.8.0",
                "archived": false,
                "released": true,
                "releaseDate": "2017-12-18"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": null,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12519697",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12519697",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13117344",
                    "key": "ARROW-1783",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117344",
                    "fields": {
                        "summary": "[Python] Convert SerializedPyObject to/from sequence of component buffers with minimal memory allocation / copying",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            },
            {
                "id": "12520860",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12520860",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13120677",
                    "key": "ARROW-1854",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13120677",
                    "fields": {
                        "summary": "[Python] Improve performance of serializing object dtype ndarrays",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12521595",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12521595",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13123002",
                    "key": "ARROW-1887",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13123002",
                    "fields": {
                        "summary": "[Python] More efficient serialization of pandas Index types in custom serialization from ARROW-1784",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": null,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1784/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 0,
            "worklogs": []
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": null,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@1c620321[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@52225320[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7234d35b[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@32cf356e[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@467d04a2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@25b8ce30[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@71c26fd7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@3aed50c4[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@149b0b2f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@77b19ec2[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@15effb34[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@4554f23d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": null,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Dec 06 19:10:33 UTC 2017",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2017-12-06T19:10:30.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1784/watchers",
            "watchCount": 6,
            "isWatching": false
        },
        "created": "2017-11-09T17:35:21.000+0000",
        "updated": "2017-12-06T19:10:33.000+0000",
        "timeoriginalestimate": null,
        "description": "See discussion in https://github.com/dask/distributed/pull/931\r\n\r\nThis will permit zero-copy reads for DataFrames not containing Python objects. In the event of an {{ObjectBlock}} these arrays will not be worse than pickle to reconstruct on the receiving side",
        "customfield_10010": null,
        "timetracking": {},
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python] Read and write pandas.DataFrame in pyarrow.serialize by decomposing the BlockManager rather than coercing to Arrow format",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16246134",
                    "id": "16246134",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mrocklin",
                        "name": "mrocklin",
                        "key": "mrocklin",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Matthew Rocklin",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Note that matching the BlockManager itself is not important for us.  I could imagine doing things columnwise as well if that feels cleaner semantically or more future-proof.  ",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mrocklin",
                        "name": "mrocklin",
                        "key": "mrocklin",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Matthew Rocklin",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-09T17:44:33.753+0000",
                    "updated": "2017-11-09T17:44:33.753+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16246309",
                    "id": "16246309",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "It's hard to prevent a memory doubling on receipt if you go column-wise (e.g. {{pd.DataFrame(data)}} where data is a dict of columns will double memory). So I think as long as we avoid memory doubling we are good",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-11-09T19:11:39.540+0000",
                    "updated": "2017-11-09T19:11:39.540+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16277922",
                    "id": "16277922",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm opened a new pull request #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390\n \n \n   This patch adds a serialization path for pandas.DataFrame (and Series) that decomposes the internal BlockManager into a dictionary structure that can be serialized to the zero-copy component representation from ARROW-1783, and then reconstructed similarly. \r\n   \r\n   The impact of this is that when a DataFrame has no data that requires pickling, the reconstruction is zero-copy. I will post some benchmarks to illustrate the impact of this. The performance improvements are pretty remarkable, nearly 1000x speedup on a large DataFrame.\r\n   \r\n   As some follow-up work, we will need to do more efficient serialization of the different pandas Index types. We should create a new JIRA for this\r\n   \r\n   cc @jreback @mrocklin @pitrou @cpcloud \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T02:52:52.972+0000",
                    "updated": "2017-12-05T02:52:52.972+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16277927",
                    "id": "16277927",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349179326\n \n \n   Here's an example of a DataFrame that zero-copies. Serialization time goes from 200ms to 160 microseconds. Deserialization time from 56ms to about the same. This serialization code path is going to Arrow representation as an intermediary -- vanilla pickle is 126ms in (faster because I deliberately created a large column with a lot of duplicated strings), 60ms out. \r\n   \r\n   ![serialize_no_strings](https://user-images.githubusercontent.com/329591/33587774-a2227fca-d93d-11e7-8435-ff368b48ab6b.png)\r\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T02:56:08.831+0000",
                    "updated": "2017-12-05T02:56:08.831+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16277928",
                    "id": "16277928",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349179326\n \n \n   Here's an example of a DataFrame that zero-copies. Serialization time goes from 200ms to 160 microseconds. Deserialization time from 56ms to about the same. This serialization code path is going to Arrow representation as an intermediary -- vanilla pickle is 126ms in, 60ms out. \r\n   \r\n   ![serialize_no_strings](https://user-images.githubusercontent.com/329591/33587774-a2227fca-d93d-11e7-8435-ff368b48ab6b.png)\r\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T02:56:29.609+0000",
                    "updated": "2017-12-05T02:56:29.609+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16277931",
                    "id": "16277931",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349179650\n \n \n   Here's the same thing with a bunch of strings. \r\n   \r\n   * serialize with Arrow table as intermediary: 1.64s in, 1.44s out\r\n   * serialize using pickle: 623ms in, 489ms out\r\n   * serialize using component method: 554ms in, 408ms out\r\n   \r\n   ![serialize_with_strings](https://user-images.githubusercontent.com/329591/33587846-01d61b5c-d93e-11e7-9b42-981f41d78b52.png)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T02:58:20.712+0000",
                    "updated": "2017-12-05T02:58:20.712+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16277933",
                    "id": "16277933",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349179770\n \n \n   Most importantly for consumers like Dask, whenever there is an internal block where a copy can be avoided, it is avoided. This will avoid excess memory use on serialization (no additional copies) and extra memory use on receive (no copies)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T02:59:09.190+0000",
                    "updated": "2017-12-05T02:59:09.190+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16277936",
                    "id": "16277936",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "mrocklin commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349180433\n \n \n   Thank you for putting this together.  I look forward to trying this out with Dask and seeing if it relieves the memory pressure we're seeing when sending dataframes.  What does the current dev-build process look like?  I think I read that you all had set up nightly builds on the twosigma channel?\r\n   \r\n   > The impact of this is that when a DataFrame has no data that requires pickling, the reconstruction is zero-copy. I will post some benchmarks to illustrate the impact of this. The performance improvements are pretty remarkable, nearly 1000x speedup on a large DataFrame.\r\n   \r\n   This is to be expected, right?  \r\n   \r\n   > serialize with Arrow table as intermediary: 1.64s in, 1.44s out\r\n   > serialize using pickle: 623ms in, 489ms out\r\n   > serialize using component method: 554ms in, 408ms out\r\n   \r\n   That's surprisingly nice.  Do you have a sense for what is going on here?  100ms in copying memory?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T03:03:33.404+0000",
                    "updated": "2017-12-05T03:03:33.404+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16277940",
                    "id": "16277940",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#discussion_r154836339\n \n \n\n ##########\n File path: python/pyarrow/pandas_compat.py\n ##########\n @@ -348,25 +349,85 @@ def get_datetimetz_type(values, dtype, type_):\n \n     return values, type_\n \n+# ----------------------------------------------------------------------\n+# Converting pandas.DataFrame to a dict containing only NumPy arrays or other\n+# objects friendly to pyarrow.serialize\n \n-def make_datetimetz(tz):\n+\n+def dataframe_to_serialized_dict(frame):\n \n Review comment:\n   @jreback let me know if I missed anything on these functions\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T03:08:41.216+0000",
                    "updated": "2017-12-05T03:08:41.216+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16277943",
                    "id": "16277943",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349181695\n \n \n   > I think I read that you all had set up nightly builds on the twosigma channel?\r\n   \r\n   yes, as soon as this is merged, it should show up in the next nightly https://anaconda.org/twosigma/pyarrow/files. Though we are having a small problem with the version numbers in the nightlies (https://issues.apache.org/jira/browse/ARROW-1881) that needs to get fixed in the next day or two (cc @xhochy)\r\n   \r\n   > This is to be expected, right?\r\n   \r\n   Yes, it's a nice confirmation that pandas definitely is not making any unexpected memory copies (it can be quite zealous about copying stuff)\r\n   \r\n   > That's surprisingly nice. Do you have a sense for what is going on here? 100ms in copying memory?\r\n   \r\n   Yes, I think this is strictly from copying the internal numeric ndarrays. The memory use vs. pickle will also be less by whatever the total pickled footprint of those numeric arrays that are being copied\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T03:11:40.792+0000",
                    "updated": "2017-12-05T03:11:40.792+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16278394",
                    "id": "16278394",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "pitrou commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349268777\n \n \n   Shouldn't `dataframe_to_serialized_dict` and `serialized_dict_to_dataframe` actually be exposed by Pandas? They seem generally useful (and touch internal details of dataframes).\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T10:54:08.111+0000",
                    "updated": "2017-12-05T10:54:08.111+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16278410",
                    "id": "16278410",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "jreback commented on a change in pull request #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#discussion_r154838798\n \n \n\n ##########\n File path: python/pyarrow/pandas_compat.py\n ##########\n @@ -348,25 +349,85 @@ def get_datetimetz_type(values, dtype, type_):\n \n     return values, type_\n \n+# ----------------------------------------------------------------------\n+# Converting pandas.DataFrame to a dict containing only NumPy arrays or other\n+# objects friendly to pyarrow.serialize\n \n-def make_datetimetz(tz):\n+\n+def dataframe_to_serialized_dict(frame):\n+    block_manager = frame._data\n+\n+    blocks = []\n+    axes = [ax for ax in block_manager.axes]\n+\n+    for block in block_manager.blocks:\n+        values = block.values\n+        block_data = {}\n+\n+        if isinstance(block, _int.DatetimeTZBlock):\n+            block_data['timezone'] = values.tz.zone\n+            values = values.values\n+        elif isinstance(block, _int.CategoricalBlock):\n+            block_data.update(dictionary=values.categories,\n+                              ordered=values.ordered)\n+            values = values.codes\n+\n+        block_data.update(\n+            placement=block.mgr_locs.as_array,\n+            block=values\n+        )\n+        blocks.append(block_data)\n+\n+    return {\n+        'blocks': blocks,\n+        'axes': axes\n+    }\n+\n+\n+def serialized_dict_to_dataframe(data):\n+    reconstructed_blocks = [_reconstruct_block(block)\n+                            for block in data['blocks']]\n+\n+    block_mgr = _int.BlockManager(reconstructed_blocks, data['axes'])\n+    return pd.DataFrame(block_mgr)\n+\n+\n+def _reconstruct_block(item):\n+    # Construct the individual blocks converting dictionary types to pandas\n+    # categorical types and Timestamps-with-timezones types to the proper\n+    # pandas Blocks\n+\n+    block_arr = item['block']\n+    placement = item['placement']\n+    if 'dictionary' in item:\n+        cat = pd.Categorical(block_arr,\n \n Review comment:\n   should be ``.from_codes`` as going to deprecate ``fastpath=`` soon\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T11:12:16.236+0000",
                    "updated": "2017-12-05T11:12:16.236+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16278429",
                    "id": "16278429",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "jreback commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349279650\n \n \n   @pitrou the internal conversion functions could / should be exposed in pandas\r\n   but should also live here until pyarrow drops support for < 0.22 (a while)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T11:40:36.422+0000",
                    "updated": "2017-12-05T11:40:36.422+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16279229",
                    "id": "16279229",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349451053\n \n \n   Done, and added docs. Will merge once the build passes\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-05T21:42:36.366+0000",
                    "updated": "2017-12-05T21:42:36.366+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16280515",
                    "id": "16280515",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349706781\n \n \n   Seems there is some problem with the manylinux1 build, will dig in\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-06T17:05:09.354+0000",
                    "updated": "2017-12-06T17:05:09.354+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16280572",
                    "id": "16280572",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390#issuecomment-349720241\n \n \n   @jreback seems there is some pickling issue with IntervalIndex in pandas 0.20.x, I was that something changed or fixed in 0.21? See https://github.com/apache/arrow/pull/1390/commits/21adbe7d41db18a139f64c4939412142208946ae\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-06T17:48:48.888+0000",
                    "updated": "2017-12-06T17:48:48.888+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16280701",
                    "id": "16280701",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 1390\n[https://github.com/apache/arrow/pull/1390]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-12-06T19:10:30.637+0000",
                    "updated": "2017-12-06T19:10:30.637+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117346/comment/16280702",
                    "id": "16280702",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm closed pull request #1390: ARROW-1784: [Python] Enable zero-copy serialization, deserialization of pandas.DataFrame via components\nURL: https://github.com/apache/arrow/pull/1390\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/python/doc/source/index.rst b/python/doc/source/index.rst\nindex b933d2359..c35f20be8 100644\n--- a/python/doc/source/index.rst\n+++ b/python/doc/source/index.rst\n@@ -18,10 +18,14 @@\n Apache Arrow (Python)\n =====================\n \n-Arrow is a columnar in-memory analytics layer designed to accelerate big data.\n-It houses a set of canonical in-memory representations of flat and hierarchical\n-data along with multiple language-bindings for structure manipulation. It also\n-provides IPC and common algorithm implementations.\n+Apache Arrow is a cross-language development platform for in-memory data. It\n+specifies a standardized language-independent columnar memory format for flat\n+and hierarchical data, organized for efficient analytic operations on modern\n+hardware. It also provides computational libraries and zero-copy streaming\n+messaging and interprocess communication.\n+\n+The Arrow Python bindings have first-class integration with NumPy, pandas, and\n+built-in Python objects.\n \n This is the documentation of the Python API of Apache Arrow. For more details\n on the format and other language bindings see\ndiff --git a/python/doc/source/ipc.rst b/python/doc/source/ipc.rst\nindex 17fe84e03..6842cb5be 100644\n--- a/python/doc/source/ipc.rst\n+++ b/python/doc/source/ipc.rst\n@@ -256,6 +256,83 @@ Lastly, we use this context as an additioanl argument to ``pyarrow.serialize``:\n    buf = pa.serialize(val, context=context).to_buffer()\n    restored_val = pa.deserialize(buf, context=context)\n \n+The ``SerializationContext`` also has convenience methods ``serialize`` and\n+``deserialize``, so these are equivalent statements:\n+\n+.. code-block:: python\n+\n+   buf = context.serialize(val).to_buffer()\n+   restored_val = context.deserialize(buf)\n+\n+Component-based Serialization\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+For serializing Python objects containing some number of NumPy arrays, Arrow\n+buffers, or other data types, it may be desirable to transport their serialized\n+representation without having to produce an intermediate copy using the\n+``to_buffer`` method. To motivate this, support we have a list of NumPy arrays:\n+\n+.. ipython:: python\n+\n+   import numpy as np\n+   data = [np.random.randn(10, 10) for i in range(5)]\n+\n+The call ``pa.serialize(data)`` does not copy the memory inside each of these\n+NumPy arrays. This serialized representation can be then decomposed into a\n+dictionary containing a sequence of ``pyarrow.Buffer`` objects containing\n+metadata for each array and references to the memory inside the arrays. To do\n+this, use the ``to_components`` method:\n+\n+.. ipython:: python\n+\n+   serialized = pa.serialize(data)\n+   components = serialized.to_components()\n+\n+The particular details of the output of ``to_components`` are not too\n+important. The objects in the ``'data'`` field are ``pyarrow.Buffer`` objects,\n+which are zero-copy convertible to Python ``memoryview`` objects:\n+\n+.. ipython:: python\n+\n+   memoryview(components['data'][0])\n+\n+A memoryview can be converted back to a ``Buffer`` with ``pyarrow.frombuffer``:\n+\n+.. ipython:: python\n+\n+   mv = memoryview(components['data'][0])\n+   buf = pa.frombuffer(mv)\n+\n+An object can be reconstructed from its component-based representation using\n+``deserialize_components``:\n+\n+.. ipython:: python\n+\n+   restored_data = pa.deserialize_components(components)\n+   restored_data[0]\n+\n+``deserialize_components`` is also available as a method on\n+``SerializationContext`` objects.\n+\n+Serializing pandas Objects\n+--------------------------\n+\n+We provide a serialization context that has optimized handling of pandas\n+objects like ``DataFrame`` and ``Series``. This is the\n+``pyarrow.pandas_serialization_context`` member. Combined with component-based\n+serialization above, this enables zero-copy transport of pandas DataFrame\n+objects not containing any Python objects:\n+\n+.. ipython:: python\n+\n+   import pandas as pd\n+   df = pd.DataFrame({'a': [1, 2, 3, 4, 5]})\n+   context = pa.pandas_serialization_context\n+   serialized_df = context.serialize(df)\n+   df_components = serialized_df.to_components()\n+   original_df = context.deserialize_components(df_components)\n+   original_df\n+\n Feather Format\n --------------\n \ndiff --git a/python/manylinux1/README.md b/python/manylinux1/README.md\nindex a74f7a27b..3d462ff2f 100644\n--- a/python/manylinux1/README.md\n+++ b/python/manylinux1/README.md\n@@ -37,7 +37,7 @@ git clone ../../ arrow\n # Build the native baseimage\n docker build -t arrow-base-x86_64 -f Dockerfile-x86_64 .\n # Build the python packages\n-docker run --rm -t -i -v $PWD:/io arrow-base-x86_64 /io/build_arrow.sh\n+docker run --shm-size=2g --rm -t -i -v $PWD:/io arrow-base-x86_64 /io/build_arrow.sh\n # Now the new packages are located in the dist/ folder\n ls -l dist/\n ```\ndiff --git a/python/pyarrow/pandas_compat.py b/python/pyarrow/pandas_compat.py\nindex a50ef96e7..8459ec31b 100644\n--- a/python/pyarrow/pandas_compat.py\n+++ b/python/pyarrow/pandas_compat.py\n@@ -20,6 +20,7 @@\n import json\n import re\n \n+import pandas.core.internals as _int\n import numpy as np\n import pandas as pd\n \n@@ -348,25 +349,85 @@ def get_datetimetz_type(values, dtype, type_):\n \n     return values, type_\n \n+# ----------------------------------------------------------------------\n+# Converting pandas.DataFrame to a dict containing only NumPy arrays or other\n+# objects friendly to pyarrow.serialize\n \n-def make_datetimetz(tz):\n+\n+def dataframe_to_serialized_dict(frame):\n+    block_manager = frame._data\n+\n+    blocks = []\n+    axes = [ax for ax in block_manager.axes]\n+\n+    for block in block_manager.blocks:\n+        values = block.values\n+        block_data = {}\n+\n+        if isinstance(block, _int.DatetimeTZBlock):\n+            block_data['timezone'] = values.tz.zone\n+            values = values.values\n+        elif isinstance(block, _int.CategoricalBlock):\n+            block_data.update(dictionary=values.categories,\n+                              ordered=values.ordered)\n+            values = values.codes\n+\n+        block_data.update(\n+            placement=block.mgr_locs.as_array,\n+            block=values\n+        )\n+        blocks.append(block_data)\n+\n+    return {\n+        'blocks': blocks,\n+        'axes': axes\n+    }\n+\n+\n+def serialized_dict_to_dataframe(data):\n+    reconstructed_blocks = [_reconstruct_block(block)\n+                            for block in data['blocks']]\n+\n+    block_mgr = _int.BlockManager(reconstructed_blocks, data['axes'])\n+    return pd.DataFrame(block_mgr)\n+\n+\n+def _reconstruct_block(item):\n+    # Construct the individual blocks converting dictionary types to pandas\n+    # categorical types and Timestamps-with-timezones types to the proper\n+    # pandas Blocks\n+\n+    block_arr = item['block']\n+    placement = item['placement']\n+    if 'dictionary' in item:\n+        cat = pd.Categorical.from_codes(block_arr,\n+                                        categories=item['dictionary'],\n+                                        ordered=item['ordered'])\n+        block = _int.make_block(cat, placement=placement,\n+                                klass=_int.CategoricalBlock,\n+                                fastpath=True)\n+    elif 'timezone' in item:\n+        dtype = _make_datetimetz(item['timezone'])\n+        block = _int.make_block(block_arr, placement=placement,\n+                                klass=_int.DatetimeTZBlock,\n+                                dtype=dtype, fastpath=True)\n+    else:\n+        block = _int.make_block(block_arr, placement=placement)\n+\n+    return block\n+\n+\n+def _make_datetimetz(tz):\n     from pyarrow.compat import DatetimeTZDtype\n     return DatetimeTZDtype('ns', tz=tz)\n \n \n-def backwards_compatible_index_name(raw_name, logical_name):\n-    pattern = r'^__index_level_\\d+__$'\n-    if raw_name == logical_name and re.match(pattern, raw_name) is not None:\n-        return None\n-    else:\n-        return logical_name\n+# ----------------------------------------------------------------------\n+# Converting pyarrow.Table efficiently to pandas.DataFrame\n \n \n def table_to_blockmanager(options, table, memory_pool, nthreads=1,\n                           categoricals=None):\n-    import pandas.core.internals as _int\n-    import pyarrow.lib as lib\n-\n     index_columns = []\n     columns = []\n     column_indexes = []\n@@ -405,37 +466,13 @@ def table_to_blockmanager(options, table, memory_pool, nthreads=1,\n \n             index_arrays.append(pd.Series(values, dtype=col_pandas.dtype))\n             index_names.append(\n-                backwards_compatible_index_name(raw_name, logical_name)\n+                _backwards_compatible_index_name(raw_name, logical_name)\n             )\n             block_table = block_table.remove_column(\n                 block_table.schema.get_field_index(raw_name)\n             )\n \n-    # Convert an arrow table to Block from the internal pandas API\n-    result = lib.table_to_blocks(options, block_table, nthreads, memory_pool)\n-\n-    # Construct the individual blocks converting dictionary types to pandas\n-    # categorical types and Timestamps-with-timezones types to the proper\n-    # pandas Blocks\n-    blocks = []\n-    for item in result:\n-        block_arr = item['block']\n-        placement = item['placement']\n-        if 'dictionary' in item:\n-            cat = pd.Categorical(block_arr,\n-                                 categories=item['dictionary'],\n-                                 ordered=item['ordered'], fastpath=True)\n-            block = _int.make_block(cat, placement=placement,\n-                                    klass=_int.CategoricalBlock,\n-                                    fastpath=True)\n-        elif 'timezone' in item:\n-            dtype = make_datetimetz(item['timezone'])\n-            block = _int.make_block(block_arr, placement=placement,\n-                                    klass=_int.DatetimeTZBlock,\n-                                    dtype=dtype, fastpath=True)\n-        else:\n-            block = _int.make_block(block_arr, placement=placement)\n-        blocks.append(block)\n+    blocks = _table_to_blocks(options, block_table, nthreads, memory_pool)\n \n     # Construct the row index\n     if len(index_arrays) > 1:\n@@ -477,31 +514,7 @@ def table_to_blockmanager(options, table, memory_pool, nthreads=1,\n \n     # if we're reconstructing the index\n     if has_pandas_metadata:\n-\n-        # Get levels and labels, and provide sane defaults if the index has a\n-        # single level to avoid if/else spaghetti.\n-        levels = getattr(columns, 'levels', None) or [columns]\n-        labels = getattr(columns, 'labels', None) or [\n-            pd.RangeIndex(len(level)) for level in levels\n-        ]\n-\n-        # Convert each level to the dtype provided in the metadata\n-        levels_dtypes = [\n-            (level, col_index.get('numpy_type', level.dtype))\n-            for level, col_index in zip_longest(\n-                levels, column_indexes, fillvalue={}\n-            )\n-        ]\n-        new_levels = [\n-            _level if _level.dtype == _dtype else _level.astype(_dtype)\n-            for _level, _dtype in levels_dtypes\n-        ]\n-\n-        columns = pd.MultiIndex(\n-            levels=new_levels,\n-            labels=labels,\n-            names=columns.names\n-        )\n+        columns = _reconstruct_columns_from_metadata(columns, column_indexes)\n \n     # ARROW-1751: flatten a single level column MultiIndex for pandas 0.21.0\n     columns = _flatten_single_level_multiindex(columns)\n@@ -510,6 +523,55 @@ def table_to_blockmanager(options, table, memory_pool, nthreads=1,\n     return _int.BlockManager(blocks, axes)\n \n \n+def _backwards_compatible_index_name(raw_name, logical_name):\n+    # Part of table_to_blockmanager\n+    pattern = r'^__index_level_\\d+__$'\n+    if raw_name == logical_name and re.match(pattern, raw_name) is not None:\n+        return None\n+    else:\n+        return logical_name\n+\n+\n+def _reconstruct_columns_from_metadata(columns, column_indexes):\n+    # Part of table_to_blockmanager\n+\n+    # Get levels and labels, and provide sane defaults if the index has a\n+    # single level to avoid if/else spaghetti.\n+    levels = getattr(columns, 'levels', None) or [columns]\n+    labels = getattr(columns, 'labels', None) or [\n+        pd.RangeIndex(len(level)) for level in levels\n+    ]\n+\n+    # Convert each level to the dtype provided in the metadata\n+    levels_dtypes = [\n+        (level, col_index.get('numpy_type', level.dtype))\n+        for level, col_index in zip_longest(\n+            levels, column_indexes, fillvalue={}\n+        )\n+    ]\n+    new_levels = [\n+        _level if _level.dtype == _dtype else _level.astype(_dtype)\n+        for _level, _dtype in levels_dtypes\n+    ]\n+\n+    return pd.MultiIndex(\n+        levels=new_levels,\n+        labels=labels,\n+        names=columns.names\n+    )\n+\n+\n+def _table_to_blocks(options, block_table, nthreads, memory_pool):\n+    # Part of table_to_blockmanager\n+\n+    # Convert an arrow table to Block from the internal pandas API\n+    result = pa.lib.table_to_blocks(options, block_table, nthreads,\n+                                    memory_pool)\n+\n+    # Defined above\n+    return [_reconstruct_block(item) for item in result]\n+\n+\n def _flatten_single_level_multiindex(index):\n     if isinstance(index, pd.MultiIndex) and index.nlevels == 1:\n         levels, = index.levels\ndiff --git a/python/pyarrow/serialization.pxi b/python/pyarrow/serialization.pxi\nindex faf164b3e..cbc5e3b84 100644\n--- a/python/pyarrow/serialization.pxi\n+++ b/python/pyarrow/serialization.pxi\n@@ -152,6 +152,30 @@ cdef class SerializationContext:\n                     obj.__dict__.update(serialized_obj)\n         return obj\n \n+    def serialize(self, obj):\n+        \"\"\"\n+        Call pyarrow.serialize and pass this SerializationContext\n+        \"\"\"\n+        return serialize(obj, context=self)\n+\n+    def serialize_to(self, object value, sink):\n+        \"\"\"\n+        Call pyarrow.serialize_to and pass this SerializationContext\n+        \"\"\"\n+        return serialize_to(value, sink, context=self)\n+\n+    def deserialize(self, what):\n+        \"\"\"\n+        Call pyarrow.deserialize and pass this SerializationContext\n+        \"\"\"\n+        return deserialize(what, context=self)\n+\n+    def deserialize_components(self, what):\n+        \"\"\"\n+        Call pyarrow.deserialize_components and pass this SerializationContext\n+        \"\"\"\n+        return deserialize_components(what, context=self)\n+\n \n _default_serialization_context = SerializationContext()\n \ndiff --git a/python/pyarrow/serialization.py b/python/pyarrow/serialization.py\nindex 08e6cce75..b6d2b0258 100644\n--- a/python/pyarrow/serialization.py\n+++ b/python/pyarrow/serialization.py\n@@ -43,15 +43,19 @@ def _deserialize_numpy_array_list(data):\n     return np.array(data[0], dtype=np.dtype(data[1]))\n \n \n-def _serialize_numpy_array_pickle(obj):\n-    pickled = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)\n+def _pickle_to_buffer(x):\n+    pickled = pickle.dumps(x, protocol=pickle.HIGHEST_PROTOCOL)\n     return frombuffer(pickled)\n \n \n-def _deserialize_numpy_array_pickle(data):\n+def _load_pickle_from_buffer(data):\n     return pickle.loads(memoryview(data))\n \n \n+_serialize_numpy_array_pickle = _pickle_to_buffer\n+_deserialize_numpy_array_pickle = _load_pickle_from_buffer\n+\n+\n def register_default_serialization_handlers(serialization_context):\n \n     # ----------------------------------------------------------------------\n@@ -107,38 +111,6 @@ def _deserialize_default_dict(data):\n         custom_serializer=_serialize_numpy_array_list,\n         custom_deserializer=_deserialize_numpy_array_list)\n \n-    # ----------------------------------------------------------------------\n-    # Set up serialization for pandas Series and DataFrame\n-\n-    try:\n-        import pandas as pd\n-\n-        def _serialize_pandas_series(obj):\n-            return serialize_pandas(pd.DataFrame({obj.name: obj}))\n-\n-        def _deserialize_pandas_series(data):\n-            deserialized = deserialize_pandas(data)\n-            return deserialized[deserialized.columns[0]]\n-\n-        def _serialize_pandas_dataframe(obj):\n-            return serialize_pandas(obj)\n-\n-        def _deserialize_pandas_dataframe(data):\n-            return deserialize_pandas(data)\n-\n-        serialization_context.register_type(\n-            pd.Series, 'pd.Series',\n-            custom_serializer=_serialize_pandas_series,\n-            custom_deserializer=_deserialize_pandas_series)\n-\n-        serialization_context.register_type(\n-            pd.DataFrame, 'pd.DataFrame',\n-            custom_serializer=_serialize_pandas_dataframe,\n-            custom_deserializer=_deserialize_pandas_dataframe)\n-    except ImportError:\n-        # no pandas\n-        pass\n-\n     # ----------------------------------------------------------------------\n     # Set up serialization for pytorch tensors\n \n@@ -165,8 +137,87 @@ def _deserialize_torch_tensor(data):\n \n register_default_serialization_handlers(_default_serialization_context)\n \n+\n+# ----------------------------------------------------------------------\n+# pandas-specific serialization matters\n+\n+\n pandas_serialization_context = _default_serialization_context.clone()\n \n+\n+def _register_pandas_arrow_handlers(context):\n+    try:\n+        import pandas as pd\n+    except ImportError:\n+        return\n+\n+    def _serialize_pandas_series(obj):\n+        return serialize_pandas(pd.DataFrame({obj.name: obj}))\n+\n+    def _deserialize_pandas_series(data):\n+        deserialized = deserialize_pandas(data)\n+        return deserialized[deserialized.columns[0]]\n+\n+    def _serialize_pandas_dataframe(obj):\n+        return serialize_pandas(obj)\n+\n+    def _deserialize_pandas_dataframe(data):\n+        return deserialize_pandas(data)\n+\n+    context.register_type(\n+        pd.Series, 'pd.Series',\n+        custom_serializer=_serialize_pandas_series,\n+        custom_deserializer=_deserialize_pandas_series)\n+\n+    context.register_type(\n+        pd.DataFrame, 'pd.DataFrame',\n+        custom_serializer=_serialize_pandas_dataframe,\n+        custom_deserializer=_deserialize_pandas_dataframe)\n+\n+\n+def _register_custom_pandas_handlers(context):\n+    # ARROW-1784, faster path for pandas-only visibility\n+\n+    try:\n+        import pandas as pd\n+    except ImportError:\n+        return\n+\n+    import pyarrow.pandas_compat as pdcompat\n+\n+    def _serialize_pandas_dataframe(obj):\n+        return pdcompat.dataframe_to_serialized_dict(obj)\n+\n+    def _deserialize_pandas_dataframe(data):\n+        return pdcompat.serialized_dict_to_dataframe(data)\n+\n+    def _serialize_pandas_series(obj):\n+        return _serialize_pandas_dataframe(pd.DataFrame({obj.name: obj}))\n+\n+    def _deserialize_pandas_series(data):\n+        deserialized = _deserialize_pandas_dataframe(data)\n+        return deserialized[deserialized.columns[0]]\n+\n+    context.register_type(\n+        pd.Series, 'pd.Series',\n+        custom_serializer=_serialize_pandas_series,\n+        custom_deserializer=_deserialize_pandas_series)\n+\n+    context.register_type(\n+        pd.Index, 'pd.Index',\n+        custom_serializer=_pickle_to_buffer,\n+        custom_deserializer=_load_pickle_from_buffer)\n+\n+    context.register_type(\n+        pd.DataFrame, 'pd.DataFrame',\n+        custom_serializer=_serialize_pandas_dataframe,\n+        custom_deserializer=_deserialize_pandas_dataframe)\n+\n+\n+_register_pandas_arrow_handlers(_default_serialization_context)\n+_register_custom_pandas_handlers(pandas_serialization_context)\n+\n+\n pandas_serialization_context.register_type(\n     np.ndarray, 'np.array',\n     custom_serializer=_serialize_numpy_array_pickle,\ndiff --git a/python/pyarrow/tests/test_convert_pandas.py b/python/pyarrow/tests/test_convert_pandas.py\nindex a2a1d6b69..960ea5263 100644\n--- a/python/pyarrow/tests/test_convert_pandas.py\n+++ b/python/pyarrow/tests/test_convert_pandas.py\n@@ -60,70 +60,73 @@ def _alltypes_example(size=100):\n     })\n \n \n-class TestPandasConversion(object):\n+def _check_pandas_roundtrip(df, expected=None, nthreads=1,\n+                            expected_schema=None,\n+                            check_dtype=True, schema=None,\n+                            preserve_index=False,\n+                            as_batch=False):\n+    klass = pa.RecordBatch if as_batch else pa.Table\n+    table = klass.from_pandas(df, schema=schema,\n+                              preserve_index=preserve_index,\n+                              nthreads=nthreads)\n+\n+    result = table.to_pandas(nthreads=nthreads)\n+    if expected_schema:\n+        assert table.schema.equals(expected_schema)\n+    if expected is None:\n+        expected = df\n+    tm.assert_frame_equal(result, expected, check_dtype=check_dtype,\n+                          check_index_type=('equiv' if preserve_index\n+                                            else False))\n+\n+\n+def _check_series_roundtrip(s, type_=None):\n+    arr = pa.array(s, from_pandas=True, type=type_)\n+\n+    result = pd.Series(arr.to_pandas(), name=s.name)\n+    if patypes.is_timestamp(arr.type) and arr.type.tz is not None:\n+        result = (result.dt.tz_localize('utc')\n+                  .dt.tz_convert(arr.type.tz))\n+\n+    tm.assert_series_equal(s, result)\n+\n+\n+def _check_array_roundtrip(values, expected=None, mask=None,\n+                           type=None):\n+    arr = pa.array(values, from_pandas=True, mask=mask, type=type)\n+    result = arr.to_pandas()\n+\n+    values_nulls = pd.isnull(values)\n+    if mask is None:\n+        assert arr.null_count == values_nulls.sum()\n+    else:\n+        assert arr.null_count == (mask | values_nulls).sum()\n+\n+    if mask is None:\n+        tm.assert_series_equal(pd.Series(result), pd.Series(values),\n+                               check_names=False)\n+    else:\n+        expected = pd.Series(np.ma.masked_array(values, mask=mask))\n+        tm.assert_series_equal(pd.Series(result), expected,\n+                               check_names=False)\n+\n+\n+def _check_array_from_pandas_roundtrip(np_array):\n+    arr = pa.array(np_array, from_pandas=True)\n+    result = arr.to_pandas()\n+    npt.assert_array_equal(result, np_array)\n \n-    def setUp(self):\n-        pass\n-\n-    def tearDown(self):\n-        pass\n-\n-    def _check_pandas_roundtrip(self, df, expected=None, nthreads=1,\n-                                expected_schema=None,\n-                                check_dtype=True, schema=None,\n-                                preserve_index=False,\n-                                as_batch=False):\n-        klass = pa.RecordBatch if as_batch else pa.Table\n-        table = klass.from_pandas(df, schema=schema,\n-                                  preserve_index=preserve_index,\n-                                  nthreads=nthreads)\n-\n-        result = table.to_pandas(nthreads=nthreads)\n-        if expected_schema:\n-            assert table.schema.equals(expected_schema)\n-        if expected is None:\n-            expected = df\n-        tm.assert_frame_equal(result, expected, check_dtype=check_dtype,\n-                              check_index_type=('equiv' if preserve_index\n-                                                else False))\n-\n-    def _check_series_roundtrip(self, s, type_=None):\n-        arr = pa.array(s, from_pandas=True, type=type_)\n-\n-        result = pd.Series(arr.to_pandas(), name=s.name)\n-        if patypes.is_timestamp(arr.type) and arr.type.tz is not None:\n-            result = (result.dt.tz_localize('utc')\n-                      .dt.tz_convert(arr.type.tz))\n-\n-        tm.assert_series_equal(s, result)\n-\n-    def _check_array_roundtrip(self, values, expected=None, mask=None,\n-                               type=None):\n-        arr = pa.array(values, from_pandas=True, mask=mask, type=type)\n-        result = arr.to_pandas()\n-\n-        values_nulls = pd.isnull(values)\n-        if mask is None:\n-            assert arr.null_count == values_nulls.sum()\n-        else:\n-            assert arr.null_count == (mask | values_nulls).sum()\n-\n-        if mask is None:\n-            tm.assert_series_equal(pd.Series(result), pd.Series(values),\n-                                   check_names=False)\n-        else:\n-            expected = pd.Series(np.ma.masked_array(values, mask=mask))\n-            tm.assert_series_equal(pd.Series(result), expected,\n-                                   check_names=False)\n+\n+class TestPandasConversion(object):\n \n     def test_all_none_objects(self):\n         df = pd.DataFrame({'a': [None, None, None]})\n-        self._check_pandas_roundtrip(df)\n+        _check_pandas_roundtrip(df)\n \n     def test_all_none_category(self):\n         df = pd.DataFrame({'a': [None, None, None]})\n         df['a'] = df['a'].astype('category')\n-        self._check_pandas_roundtrip(df)\n+        _check_pandas_roundtrip(df)\n \n     def test_non_string_columns(self):\n         df = pd.DataFrame({0: [1, 2, 3]})\n@@ -133,14 +136,14 @@ def test_non_string_columns(self):\n     def test_column_index_names_are_preserved(self):\n         df = pd.DataFrame({'data': [1, 2, 3]})\n         df.columns.names = ['a']\n-        self._check_pandas_roundtrip(df, preserve_index=True)\n+        _check_pandas_roundtrip(df, preserve_index=True)\n \n     def test_multiindex_columns(self):\n         columns = pd.MultiIndex.from_arrays([\n             ['one', 'two'], ['X', 'Y']\n         ])\n         df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n-        self._check_pandas_roundtrip(df, preserve_index=True)\n+        _check_pandas_roundtrip(df, preserve_index=True)\n \n     def test_multiindex_columns_with_dtypes(self):\n         columns = pd.MultiIndex.from_arrays(\n@@ -151,11 +154,11 @@ def test_multiindex_columns_with_dtypes(self):\n             names=['level_1', 'level_2'],\n         )\n         df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')], columns=columns)\n-        self._check_pandas_roundtrip(df, preserve_index=True)\n+        _check_pandas_roundtrip(df, preserve_index=True)\n \n     def test_integer_index_column(self):\n         df = pd.DataFrame([(1, 'a'), (2, 'b'), (3, 'c')])\n-        self._check_pandas_roundtrip(df, preserve_index=True)\n+        _check_pandas_roundtrip(df, preserve_index=True)\n \n     def test_categorical_column_index(self):\n         # I *really* hope no one uses category dtypes for single level column\n@@ -203,7 +206,7 @@ def test_categorical_row_index(self):\n         df['a'] = df.a.astype('category')\n         df = df.set_index('a')\n \n-        self._check_pandas_roundtrip(df, preserve_index=True)\n+        _check_pandas_roundtrip(df, preserve_index=True)\n \n     def test_float_no_nulls(self):\n         data = {}\n@@ -218,7 +221,7 @@ def test_float_no_nulls(self):\n \n         df = pd.DataFrame(data)\n         schema = pa.schema(fields)\n-        self._check_pandas_roundtrip(df, expected_schema=schema)\n+        _check_pandas_roundtrip(df, expected_schema=schema)\n \n     def test_zero_copy_success(self):\n         result = pa.array([0, 1, 2]).to_pandas(zero_copy_only=True)\n@@ -312,8 +315,8 @@ def test_float_object_nulls(self):\n         expected = pd.DataFrame({'floats': pd.to_numeric(arr)})\n         field = pa.field('floats', pa.float64())\n         schema = pa.schema([field])\n-        self._check_pandas_roundtrip(df, expected=expected,\n-                                     expected_schema=schema)\n+        _check_pandas_roundtrip(df, expected=expected,\n+                                expected_schema=schema)\n \n     def test_int_object_nulls(self):\n         arr = np.array([None, 1, np.int64(3)] * 5, dtype=object)\n@@ -321,8 +324,8 @@ def test_int_object_nulls(self):\n         expected = pd.DataFrame({'ints': pd.to_numeric(arr)})\n         field = pa.field('ints', pa.int64())\n         schema = pa.schema([field])\n-        self._check_pandas_roundtrip(df, expected=expected,\n-                                     expected_schema=schema)\n+        _check_pandas_roundtrip(df, expected=expected,\n+                                expected_schema=schema)\n \n     def test_integer_no_nulls(self):\n         data = OrderedDict()\n@@ -347,7 +350,7 @@ def test_integer_no_nulls(self):\n \n         df = pd.DataFrame(data)\n         schema = pa.schema(fields)\n-        self._check_pandas_roundtrip(df, expected_schema=schema)\n+        _check_pandas_roundtrip(df, expected_schema=schema)\n \n     def test_integer_with_nulls(self):\n         # pandas requires upcast to float dtype\n@@ -395,7 +398,7 @@ def test_boolean_no_nulls(self):\n         df = pd.DataFrame({'bools': np.random.randn(num_values) > 0})\n         field = pa.field('bools', pa.bool_())\n         schema = pa.schema([field])\n-        self._check_pandas_roundtrip(df, expected_schema=schema)\n+        _check_pandas_roundtrip(df, expected_schema=schema)\n \n     def test_boolean_nulls(self):\n         # pandas requires upcast to object dtype\n@@ -425,7 +428,7 @@ def test_boolean_object_nulls(self):\n         df = pd.DataFrame({'bools': arr})\n         field = pa.field('bools', pa.bool_())\n         schema = pa.schema([field])\n-        self._check_pandas_roundtrip(df, expected_schema=schema)\n+        _check_pandas_roundtrip(df, expected_schema=schema)\n \n     def test_all_nulls_cast_numeric(self):\n         arr = np.array([None], dtype=object)\n@@ -445,7 +448,7 @@ def test_unicode(self):\n         field = pa.field('strings', pa.string())\n         schema = pa.schema([field])\n \n-        self._check_pandas_roundtrip(df, expected_schema=schema)\n+        _check_pandas_roundtrip(df, expected_schema=schema)\n \n     def test_bytes_to_binary(self):\n         values = [u('qux'), b'foo', None, 'bar', 'qux', np.nan]\n@@ -456,7 +459,7 @@ def test_bytes_to_binary(self):\n \n         values2 = [b'qux', b'foo', None, b'bar', b'qux', np.nan]\n         expected = pd.DataFrame({'strings': values2})\n-        self._check_pandas_roundtrip(df, expected)\n+        _check_pandas_roundtrip(df, expected)\n \n     @pytest.mark.large_memory\n     def test_bytes_exceed_2gb(self):\n@@ -499,7 +502,7 @@ def test_timestamps_notimezone_no_nulls(self):\n         })\n         field = pa.field('datetime64', pa.timestamp('ns'))\n         schema = pa.schema([field])\n-        self._check_pandas_roundtrip(\n+        _check_pandas_roundtrip(\n             df,\n             expected_schema=schema,\n         )\n@@ -514,7 +517,7 @@ def test_timestamps_notimezone_nulls(self):\n         })\n         field = pa.field('datetime64', pa.timestamp('ns'))\n         schema = pa.schema([field])\n-        self._check_pandas_roundtrip(\n+        _check_pandas_roundtrip(\n             df,\n             expected_schema=schema,\n         )\n@@ -529,9 +532,9 @@ def test_timestamps_with_timezone(self):\n         })\n         df['datetime64'] = (df['datetime64'].dt.tz_localize('US/Eastern')\n                             .to_frame())\n-        self._check_pandas_roundtrip(df)\n+        _check_pandas_roundtrip(df)\n \n-        self._check_series_roundtrip(df['datetime64'])\n+        _check_series_roundtrip(df['datetime64'])\n \n         # drop-in a null and ns instead of ms\n         df = pd.DataFrame({\n@@ -545,7 +548,7 @@ def test_timestamps_with_timezone(self):\n         df['datetime64'] = (df['datetime64'].dt.tz_localize('US/Eastern')\n                             .to_frame())\n \n-        self._check_pandas_roundtrip(df)\n+        _check_pandas_roundtrip(df)\n \n     def test_datetime64_to_date32(self):\n         # ARROW-1718\n@@ -638,13 +641,13 @@ def test_timedelta(self):\n \n     def test_column_of_arrays(self):\n         df, schema = dataframe_with_arrays()\n-        self._check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n+        _check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n         table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n         assert table.schema.equals(schema)\n \n         for column in df.columns:\n             field = schema.field_by_name(column)\n-            self._check_array_roundtrip(df[column], type=field.type)\n+            _check_array_roundtrip(df[column], type=field.type)\n \n     def test_column_of_arrays_to_py(self):\n         # Test regression in ARROW-1199 not caught in above test\n@@ -665,13 +668,13 @@ def test_column_of_arrays_to_py(self):\n \n     def test_column_of_lists(self):\n         df, schema = dataframe_with_lists()\n-        self._check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n+        _check_pandas_roundtrip(df, schema=schema, expected_schema=schema)\n         table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n         assert table.schema.equals(schema)\n \n         for column in df.columns:\n             field = schema.field_by_name(column)\n-            self._check_array_roundtrip(df[column], type=field.type)\n+            _check_array_roundtrip(df[column], type=field.type)\n \n     def test_column_of_lists_chunked(self):\n         # ARROW-1357\n@@ -723,7 +726,7 @@ def test_column_of_lists_strided(self):\n         arr = df['int64'].values[::3]\n         assert arr.strides[0] != 8\n \n-        self._check_array_roundtrip(arr)\n+        _check_array_roundtrip(arr)\n \n     def test_nested_lists_all_none(self):\n         data = np.array([[None, None], None], dtype=object)\n@@ -742,8 +745,8 @@ def test_nested_lists_all_none(self):\n \n     def test_threaded_conversion(self):\n         df = _alltypes_example()\n-        self._check_pandas_roundtrip(df, nthreads=2)\n-        self._check_pandas_roundtrip(df, nthreads=2, as_batch=True)\n+        _check_pandas_roundtrip(df, nthreads=2)\n+        _check_pandas_roundtrip(df, nthreads=2, as_batch=True)\n \n     def test_category(self):\n         repeats = 5\n@@ -761,7 +764,7 @@ def test_category(self):\n                            'strings': v1 * repeats,\n                            'strings2': v1 * repeats,\n                            'strings3': v3 * repeats})\n-        self._check_pandas_roundtrip(df)\n+        _check_pandas_roundtrip(df)\n \n         arrays = [\n             pd.Categorical(v1 * repeats),\n@@ -769,7 +772,7 @@ def test_category(self):\n             pd.Categorical(v3 * repeats)\n         ]\n         for values in arrays:\n-            self._check_array_roundtrip(values)\n+            _check_array_roundtrip(values)\n \n     def test_mixed_types_fails(self):\n         data = pd.DataFrame({'a': ['a', 1, 2.0]})\n@@ -816,9 +819,9 @@ def test_strided_data_import(self):\n             df = pd.DataFrame(case, columns=columns)\n             col = df['a']\n \n-            self._check_pandas_roundtrip(df)\n-            self._check_array_roundtrip(col)\n-            self._check_array_roundtrip(col, mask=strided_mask)\n+            _check_pandas_roundtrip(df)\n+            _check_array_roundtrip(col)\n+            _check_array_roundtrip(col, mask=strided_mask)\n \n     def test_decimal_32_from_pandas(self):\n         expected = pd.DataFrame({\n@@ -978,11 +981,6 @@ def test_arrow_time_to_pandas(self):\n \n         tm.assert_frame_equal(df, expected_df)\n \n-    def _check_array_from_pandas_roundtrip(self, np_array):\n-        arr = pa.array(np_array, from_pandas=True)\n-        result = arr.to_pandas()\n-        npt.assert_array_equal(result, np_array)\n-\n     def test_numpy_datetime64_columns(self):\n         datetime64_ns = np.array([\n                 '2007-07-13T01:23:34.123456789',\n@@ -990,7 +988,7 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-13T12:34:56.432539784',\n                 '2010-08-13T05:46:57.437699912'],\n                 dtype='datetime64[ns]')\n-        self._check_array_from_pandas_roundtrip(datetime64_ns)\n+        _check_array_from_pandas_roundtrip(datetime64_ns)\n \n         datetime64_us = np.array([\n                 '2007-07-13T01:23:34.123456',\n@@ -998,7 +996,7 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-13T12:34:56.432539',\n                 '2010-08-13T05:46:57.437699'],\n                 dtype='datetime64[us]')\n-        self._check_array_from_pandas_roundtrip(datetime64_us)\n+        _check_array_from_pandas_roundtrip(datetime64_us)\n \n         datetime64_ms = np.array([\n                 '2007-07-13T01:23:34.123',\n@@ -1006,7 +1004,7 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-13T12:34:56.432',\n                 '2010-08-13T05:46:57.437'],\n                 dtype='datetime64[ms]')\n-        self._check_array_from_pandas_roundtrip(datetime64_ms)\n+        _check_array_from_pandas_roundtrip(datetime64_ms)\n \n         datetime64_s = np.array([\n                 '2007-07-13T01:23:34',\n@@ -1014,7 +1012,7 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-13T12:34:56',\n                 '2010-08-13T05:46:57'],\n                 dtype='datetime64[s]')\n-        self._check_array_from_pandas_roundtrip(datetime64_s)\n+        _check_array_from_pandas_roundtrip(datetime64_s)\n \n     def test_numpy_datetime64_day_unit(self):\n         datetime64_d = np.array([\n@@ -1023,7 +1021,7 @@ def test_numpy_datetime64_day_unit(self):\n                 '2006-01-15',\n                 '2010-08-19'],\n                 dtype='datetime64[D]')\n-        self._check_array_from_pandas_roundtrip(datetime64_d)\n+        _check_array_from_pandas_roundtrip(datetime64_d)\n \n     def test_all_nones(self):\n         def _check_series(s):\n@@ -1070,8 +1068,8 @@ def test_partial_schema(self):\n             pa.field('c', pa.int64())\n         ])\n \n-        self._check_pandas_roundtrip(df, schema=partial_schema,\n-                                     expected_schema=expected_schema)\n+        _check_pandas_roundtrip(df, schema=partial_schema,\n+                                expected_schema=expected_schema)\n \n     def test_structarray(self):\n         ints = pa.array([None, 2, 3], type=pa.int64())\n@@ -1106,7 +1104,7 @@ def test_infer_lists(self):\n             pa.field('nested_strs', pa.list_(pa.list_(pa.string())))\n         ])\n \n-        self._check_pandas_roundtrip(df, expected_schema=expected_schema)\n+        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n \n     def test_infer_numpy_array(self):\n         data = OrderedDict([\n@@ -1120,7 +1118,7 @@ def test_infer_numpy_array(self):\n             pa.field('ints', pa.list_(pa.int64()))\n         ])\n \n-        self._check_pandas_roundtrip(df, expected_schema=expected_schema)\n+        _check_pandas_roundtrip(df, expected_schema=expected_schema)\n \n     def test_metadata_with_mixed_types(self):\n         df = pd.DataFrame({'data': [b'some_bytes', u'some_unicode']})\n@@ -1175,12 +1173,12 @@ def test_table_str_to_categorical(self):\n \n     def test_table_batch_empty_dataframe(self):\n         df = pd.DataFrame({})\n-        self._check_pandas_roundtrip(df)\n-        self._check_pandas_roundtrip(df, as_batch=True)\n+        _check_pandas_roundtrip(df)\n+        _check_pandas_roundtrip(df, as_batch=True)\n \n         df2 = pd.DataFrame({}, index=[0, 1, 2])\n-        self._check_pandas_roundtrip(df2, preserve_index=True)\n-        self._check_pandas_roundtrip(df2, as_batch=True, preserve_index=True)\n+        _check_pandas_roundtrip(df2, preserve_index=True)\n+        _check_pandas_roundtrip(df2, as_batch=True, preserve_index=True)\n \n     def test_array_from_pandas_date_with_mask(self):\n         m = np.array([True, False, True])\n@@ -1222,6 +1220,51 @@ def test_array_from_pandas_typed_array_with_mask(self, t, data, expected):\n                                     type=pa.list_(t())).equals(result)\n \n \n+def _fully_loaded_dataframe_example():\n+    from distutils.version import LooseVersion\n+\n+    index = pd.MultiIndex.from_arrays([\n+        pd.date_range('2000-01-01', periods=5).repeat(2),\n+        np.tile(np.array(['foo', 'bar'], dtype=object), 5)\n+    ])\n+\n+    c1 = pd.date_range('2000-01-01', periods=10)\n+    data = {\n+        0: c1,\n+        1: c1.tz_localize('utc'),\n+        2: c1.tz_localize('US/Eastern'),\n+        3: c1[::2].tz_localize('utc').repeat(2).astype('category'),\n+        4: ['foo', 'bar'] * 5,\n+        5: pd.Series(['foo', 'bar'] * 5).astype('category').values,\n+        6: [True, False] * 5,\n+        7: np.random.randn(10),\n+        8: np.random.randint(0, 100, size=10),\n+        9: pd.period_range('2013', periods=10, freq='M')\n+    }\n+\n+    if LooseVersion(pd.__version__) >= '0.21':\n+        # There is an issue with pickling IntervalIndex in pandas 0.20.x\n+        data[10] = pd.interval_range(start=1, freq=1, periods=10)\n+\n+    return pd.DataFrame(data, index=index)\n+\n+\n+def _check_serialize_components_roundtrip(df):\n+    ctx = pa.pandas_serialization_context\n+\n+    components = ctx.serialize(df).to_components()\n+    deserialized = ctx.deserialize_components(components)\n+\n+    tm.assert_frame_equal(df, deserialized)\n+\n+\n+def test_serialize_deserialize_pandas():\n+    # ARROW-1784, serialize and deserialize DataFrame by decomposing\n+    # BlockManager\n+    df = _fully_loaded_dataframe_example()\n+    _check_serialize_components_roundtrip(df)\n+\n+\n def _pytime_from_micros(val):\n     microseconds = val % 1000000\n     val //= 1000000\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-12-06T19:10:33.676+0000",
                    "updated": "2017-12-06T19:10:33.676+0000"
                }
            ],
            "maxResults": 18,
            "total": 18,
            "startAt": 0
        },
        "customfield_12311820": "0|i3mltz:",
        "customfield_12314139": null
    }
}