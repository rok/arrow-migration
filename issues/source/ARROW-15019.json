{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13415935",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935",
    "key": "ARROW-15019",
    "fields": {
        "parent": {
            "id": "13396785",
            "key": "ARROW-13703",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13396785",
            "fields": {
                "summary": "[Python][R] Add bindings for new dataset writing options",
                "status": {
                    "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                    "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                    "name": "Resolved",
                    "id": "5",
                    "statusCategory": {
                        "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                        "id": 3,
                        "key": "done",
                        "colorName": "green",
                        "name": "Done"
                    }
                },
                "priority": {
                    "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                    "name": "Major",
                    "id": "3"
                },
                "issuetype": {
                    "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                    "id": "4",
                    "description": "An improvement or enhancement to an existing feature or task.",
                    "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                    "name": "Improvement",
                    "subtask": false,
                    "avatarId": 21140
                }
            }
        },
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350591",
                "id": "12350591",
                "description": "",
                "name": "7.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-02-03"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "dataset",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12629176",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12629176",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13418918",
                    "key": "ARROW-15183",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918",
                    "fields": {
                        "summary": "[Python][Docs] Add Missing Dataset Write Options ",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
            "name": "vibhatha",
            "key": "vibhatha",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
            },
            "displayName": "Vibhatha Lakmal Abeykoon",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
            "name": "vibhatha",
            "key": "vibhatha",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
            },
            "displayName": "Vibhatha Lakmal Abeykoon",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
            "name": "vibhatha",
            "key": "vibhatha",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
            },
            "displayName": "Vibhatha Lakmal Abeykoon",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 49800,
            "total": 49800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 49800,
            "total": 49800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15019/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 83,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/692995",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha opened a new pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911\n\n\n   Adding `max_open_files` and `max_rows_per_files` to python bindings of dataset API. \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-09T06:21:41.925+0000",
                    "updated": "2021-12-09T06:21:41.925+0000",
                    "started": "2021-12-09T06:21:41.925+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "692995",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/692996",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#issuecomment-989551417\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-09T06:21:55.013+0000",
                    "updated": "2021-12-09T06:21:55.013+0000",
                    "started": "2021-12-09T06:21:55.012+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "692996",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/693726",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#issuecomment-990642643\n\n\n   > Thanks for the PR!\r\n   > Can you add some tests for the new keywords?\r\n   \r\n   Yes of course, I am working on it. Kept it in the draft mode till it's done :) \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-10T06:02:39.603+0000",
                    "updated": "2021-12-10T06:02:39.603+0000",
                    "started": "2021-12-10T06:02:39.603+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "693726",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/696541",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#issuecomment-994708170\n\n\n   In the same PR, added a few test cases for `max_rows_per_group` and `min_rows_per_group`. Not exactly sure about the test case written for the `min_rows_per_group`. Would like some comments about this. \r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-15T11:39:16.580+0000",
                    "updated": "2021-12-15T11:39:16.580+0000",
                    "started": "2021-12-15T11:39:16.580+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "696541",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/696828",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r769934178\n\n\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -798,6 +800,20 @@ def write_dataset(data, base_dir, basename_template=None, format=None,\n         used determined by the number of available CPU cores.\n     max_partitions : int, default 1024\n         Maximum number of partitions any batch may be written into.\n+    max_open_files : int, default 1024\n+        Maximum number of number of files can be opened\n+    max_rows_per_file : int, default 0\n+        Maximum number of rows per file\n\nReview comment:\n       Same here, helpful to have the extra guidance:\r\n   \r\n   ```suggestion\r\n           Maximum number of rows per file. If greater than 0 then this will \r\n           limit how many rows are placed in any single file. Otherwise there \r\n           will be no limit and one file will be created in each output directory \r\n           unless files need to be closed to respect max_open_files\r\n   ```\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -798,6 +800,20 @@ def write_dataset(data, base_dir, basename_template=None, format=None,\n         used determined by the number of available CPU cores.\n     max_partitions : int, default 1024\n         Maximum number of partitions any batch may be written into.\n+    max_open_files : int, default 1024\n+        Maximum number of number of files can be opened\n+    max_rows_per_file : int, default 0\n+        Maximum number of rows per file\n+    min_rows_per_group : int, default 0\n+        Minimum number of rows per group. When the value is greater than 0,\n+        the dataset writer will batch incoming data and only write the row\n+        groups to the disk when sufficient rows have accumulated.\n+    max_rows_per_group : int, default 1 << 20\n\nReview comment:\n       Could we instead write the default like `1024 * 1024`? I find that easier to think about and I don't think I'm alone in that.\r\n   \r\n   ```suggestion\r\n       max_rows_per_group : int, default 1024 * 1024\r\n   ```\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -798,6 +800,20 @@ def write_dataset(data, base_dir, basename_template=None, format=None,\n         used determined by the number of available CPU cores.\n     max_partitions : int, default 1024\n         Maximum number of partitions any batch may be written into.\n+    max_open_files : int, default 1024\n+        Maximum number of number of files can be opened\n\nReview comment:\n       I found this a little confusing. Could we add the full docstring from the C++ docs?\r\n   \r\n   ```suggestion\r\n           Maximum number of number of files that can be open at a time. \r\n           If an attempt is made to open too many files then the least recently \r\n           used file will be closed. If this setting is set too low you may end up\r\n           fragmenting your data into many small files.\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-15T19:21:15.440+0000",
                    "updated": "2021-12-15T19:21:15.440+0000",
                    "started": "2021-12-15T19:21:15.440+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "696828",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/696881",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770011466\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n\nReview comment:\n       Why do we care about num unique rows? It's not clear to me that it is necessary.\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n\nReview comment:\n       nit: if you don't need the index, then you shouldn't need the `enumerate()`\r\n   \r\n   ```suggestion\r\n       for file_dir in file_dirs:\r\n   ```\r\n   (Also if you want to generalize this function to handle no partitioning or deeper partitioning, consider using `os.walk()`)\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n\nReview comment:\n       I think with a little refactoring, these tests could be combined into one that uses `pytest.mark.parametrize()` to test different scenarios. What do you think of that?\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n\nReview comment:\n       Is `records_per_row` just the number of rows here? I'm confused by this name.\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n\nReview comment:\n       The `min_rows_per_file` and `min_rows_per_group` are a little harder harder to test for all cases though, since they have exceptions when there aren't enough rows.\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n+\n+    ds.write_dataset(record_batch, sub_directory, format=\"parquet\",\n+                     max_rows_per_file=max_rows_per_file,\n+                     max_rows_per_group=max_rows_per_group)\n+\n+    files_in_dir = os.listdir(sub_directory)\n+\n+    # number of partitions with max_rows and the partition with the remainder\n+    expected_partitions = len(data[0]) // max_rows_per_file + 1\n+    expected_row_combination = [max_rows_per_file\n+                                for i in range(expected_partitions - 1)] \\\n+        + [len(data[0]) - ((expected_partitions - 1) * max_rows_per_file)]\n+\n+    # test whether the expected amount of files are written\n+    assert len(files_in_dir) == expected_partitions\n+\n+    # compute the number of rows per each file written\n+    result_row_combination = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = sub_directory / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        result_row_combination.append(dataset.to_table().shape[0])\n+\n+    # test whether the generated files have the expected number of rows\n+    assert len(expected_row_combination) == len(result_row_combination)\n+    assert sum(expected_row_combination) == sum(result_row_combination)\n+\n+\n+def test_write_dataset_min_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    min_rows_per_group = 10\n+    max_rows_per_group = 20\n+    num_of_columns = 2\n+    records_per_row = 49\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"min_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     min_rows_per_group=min_rows_per_group,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = data_source / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        table = dataset.to_table()\n+        batches = table.to_batches()\n+        for batch in batches:\n+            batched_data.append(batch.num_rows)\n+\n+    assert batched_data[0] > min_rows_per_group and \\\n+        batched_data[0] <= max_rows_per_group\n+    assert batched_data[1] > min_rows_per_group and \\\n+        batched_data[1] <= max_rows_per_group\n+    assert batched_data[2] <= max_rows_per_group\n+\n+\n+def test_write_dataset_max_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_group = 18\n+    num_of_columns = 2\n+    records_per_row = 30\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"max_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n\nReview comment:\n       nit: don't need `_` or `enumerate()` here.\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n+\n+    ds.write_dataset(record_batch, sub_directory, format=\"parquet\",\n+                     max_rows_per_file=max_rows_per_file,\n+                     max_rows_per_group=max_rows_per_group)\n+\n+    files_in_dir = os.listdir(sub_directory)\n+\n+    # number of partitions with max_rows and the partition with the remainder\n+    expected_partitions = len(data[0]) // max_rows_per_file + 1\n+    expected_row_combination = [max_rows_per_file\n+                                for i in range(expected_partitions - 1)] \\\n+        + [len(data[0]) - ((expected_partitions - 1) * max_rows_per_file)]\n+\n+    # test whether the expected amount of files are written\n+    assert len(files_in_dir) == expected_partitions\n+\n+    # compute the number of rows per each file written\n+    result_row_combination = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = sub_directory / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        result_row_combination.append(dataset.to_table().shape[0])\n+\n+    # test whether the generated files have the expected number of rows\n+    assert len(expected_row_combination) == len(result_row_combination)\n+    assert sum(expected_row_combination) == sum(result_row_combination)\n+\n+\n+def test_write_dataset_min_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    min_rows_per_group = 10\n+    max_rows_per_group = 20\n+    num_of_columns = 2\n+    records_per_row = 49\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"min_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     min_rows_per_group=min_rows_per_group,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = data_source / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        table = dataset.to_table()\n+        batches = table.to_batches()\n+        for batch in batches:\n+            batched_data.append(batch.num_rows)\n+\n+    assert batched_data[0] > min_rows_per_group and \\\n+        batched_data[0] <= max_rows_per_group\n+    assert batched_data[1] > min_rows_per_group and \\\n+        batched_data[1] <= max_rows_per_group\n+    assert batched_data[2] <= max_rows_per_group\n+\n+\n+def test_write_dataset_max_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_group = 18\n+    num_of_columns = 2\n+    records_per_row = 30\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"max_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = data_source / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        table = dataset.to_table()\n+        batches = table.to_batches()\n+        for batch in batches:\n+            batched_data.append(batch.num_rows)\n+\n+    assert batched_data == [18, 12]\n+\n+\n+def test_write_dataset_max_open_files(tempdir):\n+    # TODO: INCOMPLETE TEST CASE WIP\n+    directory = tempdir / 'ds'\n+    print(\"Directory : \", directory)\n+\n+    record_batch_1 = pa.record_batch(data=[[1, 2, 3, 4, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_2 = pa.record_batch(data=[[5, 6, 7, 8, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_3 = pa.record_batch(data=[[9, 10, 11, 12, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_4 = pa.record_batch(data=[[13, 14, 15, 16, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+\n+    table = pa.Table.from_batches([record_batch_1, record_batch_2,\n+                                   record_batch_3, record_batch_4])\n+\n+    partitioning = ds.partitioning(\n+        pa.schema([(\"c2\", pa.string())]), flavor=\"hive\")\n+\n+    data_source_1 = directory / \"default\"\n+\n+    ds.write_dataset(data=table, base_dir=data_source_1,\n+                     partitioning=partitioning, format=\"parquet\")\n+\n+    # CASE 1: when max_open_files=default & max_open_files >= num_of_partitions\n+    #         the number of unique rows must be equal to\n+    #         the number of files generated\n\nReview comment:\n       I don't think that's true, right? We wouldn't want to create a file per rows just because `max_open_files` is high. \n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n\nReview comment:\n       For example, you could have tests like:\r\n   \r\n   ```python\r\n   assert all(file.nrows <= max_rows_per_file for file in dataset_file)\r\n   assert all(group.nrows <= max_rows_per_file for file in dataset_file for group in file.groups)\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-15T21:03:26.320+0000",
                    "updated": "2021-12-15T21:03:26.320+0000",
                    "started": "2021-12-15T21:03:26.320+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "696881",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/696922",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r769920049\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n\nReview comment:\n       Maybe...\r\n   ```\r\n   files = list(pathlib.Path(base_directory).glob('**/*.parquet'))\r\n   return len(files)\r\n   ```\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n\nReview comment:\n       Nit: It's not obvious to me from the name what `_get_compare_pair` does.  Since it is only used in one test right now maybe move the definition of this function inside the test.\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n+\n+    ds.write_dataset(record_batch, sub_directory, format=\"parquet\",\n+                     max_rows_per_file=max_rows_per_file,\n+                     max_rows_per_group=max_rows_per_group)\n+\n+    files_in_dir = os.listdir(sub_directory)\n+\n+    # number of partitions with max_rows and the partition with the remainder\n+    expected_partitions = len(data[0]) // max_rows_per_file + 1\n+    expected_row_combination = [max_rows_per_file\n+                                for i in range(expected_partitions - 1)] \\\n+        + [len(data[0]) - ((expected_partitions - 1) * max_rows_per_file)]\n+\n+    # test whether the expected amount of files are written\n+    assert len(files_in_dir) == expected_partitions\n+\n+    # compute the number of rows per each file written\n+    result_row_combination = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = sub_directory / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        result_row_combination.append(dataset.to_table().shape[0])\n+\n+    # test whether the generated files have the expected number of rows\n+    assert len(expected_row_combination) == len(result_row_combination)\n+    assert sum(expected_row_combination) == sum(result_row_combination)\n+\n+\n+def test_write_dataset_min_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    min_rows_per_group = 10\n+    max_rows_per_group = 20\n+    num_of_columns = 2\n+    records_per_row = 49\n+    unique_records = 5\n\nReview comment:\n       I think a better test for `min_rows_per_group` would be writing multiple batches where `records_per_row` is less than `min_rows_per_group`.  For example, if `min_rows_per_group = 10` and `max_rows_per_group = 20` and `records_per_row = 4` then:\r\n   \r\n   Writing 2 batches -> 1 file 1 row group\r\n   Writing 3 batches -> 1 file 2 row groups\r\n   \n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n\nReview comment:\n       Nit: `records_per_row` is a confusing name.  It's maybe `records_per_column` but perhaps just `num_rows`?\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n+\n+    ds.write_dataset(record_batch, sub_directory, format=\"parquet\",\n+                     max_rows_per_file=max_rows_per_file,\n+                     max_rows_per_group=max_rows_per_group)\n+\n+    files_in_dir = os.listdir(sub_directory)\n+\n+    # number of partitions with max_rows and the partition with the remainder\n+    expected_partitions = len(data[0]) // max_rows_per_file + 1\n\nReview comment:\n       Instead of `len(data[0])` you could reuse `records_per_row` (although num_rows as mentioned elsewhere) and it would be a bit clearer.\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n\nReview comment:\n       Nit: You could just use `directory` (or even `tempdir`).  Each test will get its own temporary directory so these subdirs aren't really required.\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n+\n+    ds.write_dataset(record_batch, sub_directory, format=\"parquet\",\n+                     max_rows_per_file=max_rows_per_file,\n+                     max_rows_per_group=max_rows_per_group)\n+\n+    files_in_dir = os.listdir(sub_directory)\n+\n+    # number of partitions with max_rows and the partition with the remainder\n+    expected_partitions = len(data[0]) // max_rows_per_file + 1\n+    expected_row_combination = [max_rows_per_file\n+                                for i in range(expected_partitions - 1)] \\\n+        + [len(data[0]) - ((expected_partitions - 1) * max_rows_per_file)]\n\nReview comment:\n       There is some complexity here to calculate `expected_row_combination` but at the end of the day all you use is `len(expected_row_combination)` which is `expected_partitions` and `sum(expected_row_combination)` which is `records_per_row`.  Maybe you don't need `expected_row_combination`?\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n\nReview comment:\n       Is the number of `unique_records` relevant for these tests?\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n\nReview comment:\n       Perhaps return a record batch?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-15T22:38:40.551+0000",
                    "updated": "2021-12-15T22:38:40.551+0000",
                    "started": "2021-12-15T22:38:40.551+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "696922",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697010",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770171487\n\n\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -798,6 +800,20 @@ def write_dataset(data, base_dir, basename_template=None, format=None,\n         used determined by the number of available CPU cores.\n     max_partitions : int, default 1024\n         Maximum number of partitions any batch may be written into.\n+    max_open_files : int, default 1024\n+        Maximum number of number of files can be opened\n+    max_rows_per_file : int, default 0\n+        Maximum number of rows per file\n+    min_rows_per_group : int, default 0\n+        Minimum number of rows per group. When the value is greater than 0,\n+        the dataset writer will batch incoming data and only write the row\n+        groups to the disk when sufficient rows have accumulated.\n+    max_rows_per_group : int, default 1 << 20\n\nReview comment:\n       I agree, it is more readable. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T02:06:05.495+0000",
                    "updated": "2021-12-16T02:06:05.495+0000",
                    "started": "2021-12-16T02:06:05.494+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697010",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697015",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770174085\n\n\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -798,6 +800,20 @@ def write_dataset(data, base_dir, basename_template=None, format=None,\n         used determined by the number of available CPU cores.\n     max_partitions : int, default 1024\n         Maximum number of partitions any batch may be written into.\n+    max_open_files : int, default 1024\n+        Maximum number of number of files can be opened\n\nReview comment:\n       Yes, I understand. Added the C++ full docstrings. \r\n   \r\n   > Went through the tests. I had a few questions on some parts that I didn't understand, but I think they generally seem right.\r\n   > \r\n   > If you are up for the task, I think it may be worth consolidating them so it's simpler to read through.\r\n   \r\n   Of course, I will improve the test cases. Thank you for the detailed review comments. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T02:13:45.100+0000",
                    "updated": "2021-12-16T02:13:45.100+0000",
                    "started": "2021-12-16T02:13:45.099+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697015",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697019",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770178062\n\n\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -798,6 +800,20 @@ def write_dataset(data, base_dir, basename_template=None, format=None,\n         used determined by the number of available CPU cores.\n     max_partitions : int, default 1024\n         Maximum number of partitions any batch may be written into.\n+    max_open_files : int, default 1024\n+        Maximum number of number of files can be opened\n+    max_rows_per_file : int, default 0\n+        Maximum number of rows per file\n\nReview comment:\n       Added the full C++ docstrings. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T02:25:42.785+0000",
                    "updated": "2021-12-16T02:25:42.785+0000",
                    "started": "2021-12-16T02:25:42.784+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697019",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697022",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770181260\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n\nReview comment:\n       I used the enumerate with the idea of it being faster. But, I see your point. Just curious, if the list is too long, is it wise to use enumerate?\r\n   \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T02:35:05.247+0000",
                    "updated": "2021-12-16T02:35:05.247+0000",
                    "started": "2021-12-16T02:35:05.247+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697022",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697027",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770190430\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n\nReview comment:\n       `os.walk` is a nice suggestion. Updated the code. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T03:02:24.838+0000",
                    "updated": "2021-12-16T03:02:24.838+0000",
                    "started": "2021-12-16T03:02:24.838+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697027",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697029",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770190430\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n\nReview comment:\n       `os.walk` is a nice suggestion. looking into comment from @westonpace on the same part, since we are only doing a non-nested partitioning `list(pathlib.Path(base_directory).glob('**/*.parquet'))` seems to be simpler and match this case. what do you think? \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T03:10:11.604+0000",
                    "updated": "2021-12-16T03:10:11.604+0000",
                    "started": "2021-12-16T03:10:11.604+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697029",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697041",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770210014\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n\nReview comment:\n       Here we partition by the row labels in the column[1] of record_batch (I have used the wrong column - typo). So in case we have duplicate row labels, we take the unique to determine the number of unique partitions created. Although, not sure if this over complicates the test case. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T04:09:22.764+0000",
                    "updated": "2021-12-16T04:09:22.764+0000",
                    "started": "2021-12-16T04:09:22.763+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697041",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697046",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770212451\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n\nReview comment:\n       > I think with a little refactoring, these tests could be combined into one that uses `pytest.mark.parametrize()` to test different scenarios. What do you think of that?\r\n   \r\n   Are you referring to the `test_write_dataset_max_rows_per_file` or `test_write_dataset_max_open_files` ? \r\n   \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T04:16:49.422+0000",
                    "updated": "2021-12-16T04:16:49.422+0000",
                    "started": "2021-12-16T04:16:49.421+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697046",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697047",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770213177\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n\nReview comment:\n       Exactly, this is a wrong variable name selection. This must be `number_of_records` (suits best) or `records_per_column`.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T04:19:13.888+0000",
                    "updated": "2021-12-16T04:19:13.888+0000",
                    "started": "2021-12-16T04:19:13.888+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697047",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697048",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770214661\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n+\n+    ds.write_dataset(record_batch, sub_directory, format=\"parquet\",\n+                     max_rows_per_file=max_rows_per_file,\n+                     max_rows_per_group=max_rows_per_group)\n+\n+    files_in_dir = os.listdir(sub_directory)\n+\n+    # number of partitions with max_rows and the partition with the remainder\n+    expected_partitions = len(data[0]) // max_rows_per_file + 1\n+    expected_row_combination = [max_rows_per_file\n+                                for i in range(expected_partitions - 1)] \\\n+        + [len(data[0]) - ((expected_partitions - 1) * max_rows_per_file)]\n+\n+    # test whether the expected amount of files are written\n+    assert len(files_in_dir) == expected_partitions\n+\n+    # compute the number of rows per each file written\n+    result_row_combination = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = sub_directory / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        result_row_combination.append(dataset.to_table().shape[0])\n+\n+    # test whether the generated files have the expected number of rows\n+    assert len(expected_row_combination) == len(result_row_combination)\n+    assert sum(expected_row_combination) == sum(result_row_combination)\n+\n+\n+def test_write_dataset_min_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    min_rows_per_group = 10\n+    max_rows_per_group = 20\n+    num_of_columns = 2\n+    records_per_row = 49\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"min_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     min_rows_per_group=min_rows_per_group,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = data_source / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        table = dataset.to_table()\n+        batches = table.to_batches()\n+        for batch in batches:\n+            batched_data.append(batch.num_rows)\n+\n+    assert batched_data[0] > min_rows_per_group and \\\n+        batched_data[0] <= max_rows_per_group\n+    assert batched_data[1] > min_rows_per_group and \\\n+        batched_data[1] <= max_rows_per_group\n+    assert batched_data[2] <= max_rows_per_group\n+\n+\n+def test_write_dataset_max_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_group = 18\n+    num_of_columns = 2\n+    records_per_row = 30\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"max_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = data_source / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        table = dataset.to_table()\n+        batches = table.to_batches()\n+        for batch in batches:\n+            batched_data.append(batch.num_rows)\n+\n+    assert batched_data == [18, 12]\n+\n+\n+def test_write_dataset_max_open_files(tempdir):\n+    # TODO: INCOMPLETE TEST CASE WIP\n+    directory = tempdir / 'ds'\n+    print(\"Directory : \", directory)\n+\n+    record_batch_1 = pa.record_batch(data=[[1, 2, 3, 4, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_2 = pa.record_batch(data=[[5, 6, 7, 8, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_3 = pa.record_batch(data=[[9, 10, 11, 12, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_4 = pa.record_batch(data=[[13, 14, 15, 16, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+\n+    table = pa.Table.from_batches([record_batch_1, record_batch_2,\n+                                   record_batch_3, record_batch_4])\n+\n+    partitioning = ds.partitioning(\n+        pa.schema([(\"c2\", pa.string())]), flavor=\"hive\")\n+\n+    data_source_1 = directory / \"default\"\n+\n+    ds.write_dataset(data=table, base_dir=data_source_1,\n+                     partitioning=partitioning, format=\"parquet\")\n+\n+    # CASE 1: when max_open_files=default & max_open_files >= num_of_partitions\n+    #         the number of unique rows must be equal to\n+    #         the number of files generated\n\nReview comment:\n       I think there is a misinterpretation in the comment. It must be \r\n   \r\n   ```python\r\n        # CASE 1: when max_open_files=default & max_open_files >= num_of_partitions\r\n        # In case of a writing to disk via partitioning based on a particular column (considering row labels in that column), \r\n        # the number of unique rows must be equal to the number of files generated\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T04:24:21.440+0000",
                    "updated": "2021-12-16T04:24:21.440+0000",
                    "started": "2021-12-16T04:24:21.439+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697048",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697049",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770214661\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n+\n+    ds.write_dataset(record_batch, sub_directory, format=\"parquet\",\n+                     max_rows_per_file=max_rows_per_file,\n+                     max_rows_per_group=max_rows_per_group)\n+\n+    files_in_dir = os.listdir(sub_directory)\n+\n+    # number of partitions with max_rows and the partition with the remainder\n+    expected_partitions = len(data[0]) // max_rows_per_file + 1\n+    expected_row_combination = [max_rows_per_file\n+                                for i in range(expected_partitions - 1)] \\\n+        + [len(data[0]) - ((expected_partitions - 1) * max_rows_per_file)]\n+\n+    # test whether the expected amount of files are written\n+    assert len(files_in_dir) == expected_partitions\n+\n+    # compute the number of rows per each file written\n+    result_row_combination = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = sub_directory / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        result_row_combination.append(dataset.to_table().shape[0])\n+\n+    # test whether the generated files have the expected number of rows\n+    assert len(expected_row_combination) == len(result_row_combination)\n+    assert sum(expected_row_combination) == sum(result_row_combination)\n+\n+\n+def test_write_dataset_min_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    min_rows_per_group = 10\n+    max_rows_per_group = 20\n+    num_of_columns = 2\n+    records_per_row = 49\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"min_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     min_rows_per_group=min_rows_per_group,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = data_source / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        table = dataset.to_table()\n+        batches = table.to_batches()\n+        for batch in batches:\n+            batched_data.append(batch.num_rows)\n+\n+    assert batched_data[0] > min_rows_per_group and \\\n+        batched_data[0] <= max_rows_per_group\n+    assert batched_data[1] > min_rows_per_group and \\\n+        batched_data[1] <= max_rows_per_group\n+    assert batched_data[2] <= max_rows_per_group\n+\n+\n+def test_write_dataset_max_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_group = 18\n+    num_of_columns = 2\n+    records_per_row = 30\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"max_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = data_source / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        table = dataset.to_table()\n+        batches = table.to_batches()\n+        for batch in batches:\n+            batched_data.append(batch.num_rows)\n+\n+    assert batched_data == [18, 12]\n+\n+\n+def test_write_dataset_max_open_files(tempdir):\n+    # TODO: INCOMPLETE TEST CASE WIP\n+    directory = tempdir / 'ds'\n+    print(\"Directory : \", directory)\n+\n+    record_batch_1 = pa.record_batch(data=[[1, 2, 3, 4, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_2 = pa.record_batch(data=[[5, 6, 7, 8, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_3 = pa.record_batch(data=[[9, 10, 11, 12, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+    record_batch_4 = pa.record_batch(data=[[13, 14, 15, 16, 0],\n+                                     ['a', 'b', 'c', 'd', 'e']],\n+                                     names=['c1', 'c2'])\n+\n+    table = pa.Table.from_batches([record_batch_1, record_batch_2,\n+                                   record_batch_3, record_batch_4])\n+\n+    partitioning = ds.partitioning(\n+        pa.schema([(\"c2\", pa.string())]), flavor=\"hive\")\n+\n+    data_source_1 = directory / \"default\"\n+\n+    ds.write_dataset(data=table, base_dir=data_source_1,\n+                     partitioning=partitioning, format=\"parquet\")\n+\n+    # CASE 1: when max_open_files=default & max_open_files >= num_of_partitions\n+    #         the number of unique rows must be equal to\n+    #         the number of files generated\n\nReview comment:\n       I think there is a misinterpretation in the comment and I haven't completely documented the statement. It must be \r\n   \r\n   ```python\r\n        # CASE 1: when max_open_files=default & max_open_files >= num_of_partitions\r\n        # In case of a writing to disk via partitioning based on a particular column (considering row labels in that column), \r\n        # the number of unique rows must be equal to the number of files generated\r\n   ```\r\n   \r\n   Do you think this is appropriate? \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T04:24:56.505+0000",
                    "updated": "2021-12-16T04:24:56.505+0000",
                    "started": "2021-12-16T04:24:56.505+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697049",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697054",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770218395\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n+    data = []\n+    column_names = []\n+    if unique_records is None:\n+        unique_records = records_per_row\n+    for i in range(num_of_columns):\n+        data.append(_generate_random_int_array(size=records_per_row,\n+                                               min=1,\n+                                               max=unique_records))\n+        column_names.append(\"c\" + str(i))\n+    return data, column_names\n+\n+\n+def _get_num_of_files_generated(base_directory):\n+    file_dirs = os.listdir(base_directory)\n+    number_of_files = 0\n+    for _, file_dir in enumerate(file_dirs):\n+        sub_dir_path = base_directory / file_dir\n+        number_of_files += len(os.listdir(sub_dir_path))\n+    return number_of_files\n+\n+\n+def _get_compare_pair(data_source, record_batch):\n+    num_of_files_generated = _get_num_of_files_generated(\n+        base_directory=data_source)\n+    number_of_unique_rows = len(pa.compute.unique(record_batch[0]))\n+    return num_of_files_generated, number_of_unique_rows\n+\n+\n+def test_write_dataset_max_rows_per_file(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_file = 10\n+    max_rows_per_group = 10\n+    num_of_columns = 2\n+    records_per_row = 35\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    sub_directory = directory / 'onewrite'\n+\n+    ds.write_dataset(record_batch, sub_directory, format=\"parquet\",\n+                     max_rows_per_file=max_rows_per_file,\n+                     max_rows_per_group=max_rows_per_group)\n+\n+    files_in_dir = os.listdir(sub_directory)\n+\n+    # number of partitions with max_rows and the partition with the remainder\n+    expected_partitions = len(data[0]) // max_rows_per_file + 1\n+    expected_row_combination = [max_rows_per_file\n+                                for i in range(expected_partitions - 1)] \\\n+        + [len(data[0]) - ((expected_partitions - 1) * max_rows_per_file)]\n+\n+    # test whether the expected amount of files are written\n+    assert len(files_in_dir) == expected_partitions\n+\n+    # compute the number of rows per each file written\n+    result_row_combination = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = sub_directory / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        result_row_combination.append(dataset.to_table().shape[0])\n+\n+    # test whether the generated files have the expected number of rows\n+    assert len(expected_row_combination) == len(result_row_combination)\n+    assert sum(expected_row_combination) == sum(result_row_combination)\n+\n+\n+def test_write_dataset_min_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    min_rows_per_group = 10\n+    max_rows_per_group = 20\n+    num_of_columns = 2\n+    records_per_row = 49\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"min_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     min_rows_per_group=min_rows_per_group,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n+        f_path = data_source / str(f_file)\n+        dataset = ds.dataset(f_path, format=\"parquet\")\n+        table = dataset.to_table()\n+        batches = table.to_batches()\n+        for batch in batches:\n+            batched_data.append(batch.num_rows)\n+\n+    assert batched_data[0] > min_rows_per_group and \\\n+        batched_data[0] <= max_rows_per_group\n+    assert batched_data[1] > min_rows_per_group and \\\n+        batched_data[1] <= max_rows_per_group\n+    assert batched_data[2] <= max_rows_per_group\n+\n+\n+def test_write_dataset_max_rows_per_group(tempdir):\n+    directory = tempdir / 'ds'\n+    max_rows_per_group = 18\n+    num_of_columns = 2\n+    records_per_row = 30\n+    unique_records = 5\n+\n+    data, column_names = _generate_data_and_columns(num_of_columns,\n+                                                    records_per_row,\n+                                                    unique_records)\n+\n+    record_batch = pa.record_batch(data=data, names=column_names)\n+\n+    data_source = directory / \"max_rows_group\"\n+\n+    ds.write_dataset(data=record_batch, base_dir=data_source,\n+                     max_rows_per_group=max_rows_per_group,\n+                     format=\"parquet\")\n+\n+    files_in_dir = os.listdir(data_source)\n+    batched_data = []\n+    for _, f_file in enumerate(files_in_dir):\n\nReview comment:\n       I agree, updated the code. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T04:36:13.278+0000",
                    "updated": "2021-12-16T04:36:13.278+0000",
                    "started": "2021-12-16T04:36:13.278+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697054",
                    "issueId": "13415935"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/worklog/697059",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #11911:\nURL: https://github.com/apache/arrow/pull/11911#discussion_r770222550\n\n\n\n##########\nFile path: python/pyarrow/tests/test_dataset.py\n##########\n@@ -3621,6 +3621,204 @@ def compare_tables_ignoring_order(t1, t2):\n     assert not extra_file.exists()\n \n \n+def _generate_random_int_array(size=4, min=1, max=10):\n+    return np.random.randint(min, max, size)\n+\n+\n+def _generate_data_and_columns(num_of_columns, records_per_row,\n+                               unique_records=None):\n\nReview comment:\n       I think you're correct here, for the test case this is not required, I added this to decide the duplication factor of rows. But it is not required here. Removed it. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-16T04:50:10.464+0000",
                    "updated": "2021-12-16T04:50:10.464+0000",
                    "started": "2021-12-16T04:50:10.464+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "697059",
                    "issueId": "13415935"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
            "id": "7",
            "description": "The sub-task of the issue",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
            "name": "Sub-task",
            "subtask": true,
            "avatarId": 21146
        },
        "timespent": 49800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@547fdf92[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@18cbfe59[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3f1828bb[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@78e63037[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3b03ce0b[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@211ea8cd[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5f3d11a5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@1613e5fd[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@33c512a2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@67c98cd7[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@504e09b2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@597b324b[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 49800,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Dec 20 19:02:31 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-12-20T19:02:31.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15019/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2021-12-08T01:35:42.000+0000",
        "updated": "2022-01-04T23:07:16.000+0000",
        "timeoriginalestimate": null,
        "description": "Added a subtask to separate the work on Python bindings",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "13h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 49800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python] Add bindings for new dataset writing options",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/comment/17455221",
                    "id": "17455221",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
                        "name": "jorisvandenbossche",
                        "key": "jorisvandenbossche",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Joris Van den Bossche",
                        "active": true,
                        "timeZone": "Europe/Brussels"
                    },
                    "body": "{{existing_data_behavior}} was already added in ARROW-14620, but the other options are indeed not yet added",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
                        "name": "jorisvandenbossche",
                        "key": "jorisvandenbossche",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Joris Van den Bossche",
                        "active": true,
                        "timeZone": "Europe/Brussels"
                    },
                    "created": "2021-12-08T12:52:53.676+0000",
                    "updated": "2021-12-08T12:52:53.676+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/comment/17458834",
                    "id": "17458834",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Thanks for the heads up\u00a0[~jorisvandenbossche]. I am\u00a0only working on the `max_open_files` and `max_rows_per_file` options.\u00a0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2021-12-14T00:56:50.284+0000",
                    "updated": "2021-12-14T00:56:50.284+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/comment/17458863",
                    "id": "17458863",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "[~westonpace]\u00a0Added a test case for max_rows_per_file. Not exactly sure about the test case required for max_open_files. Should we write a similar one to the corresponding in C++ test case?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2021-12-14T02:21:46.004+0000",
                    "updated": "2021-12-14T02:21:46.004+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/comment/17458885",
                    "id": "17458885",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Also it seems `min_rows_per_group` and `max_rows_per_group` would be required to write these test cases. The C++ test case shows that. Without this parameter we may run into issues like\u00a0\r\n\r\n`pyarrow.lib.ArrowInvalid: max_rows_per_group must be less than or equal to max_rows_per_file{*}`{*}\r\n\r\nCPP Test: https://github.com/apache/arrow/blob/cf8d81d9fcbc43ce57b8a0d36c05f8b4273a5fa3/cpp/src/arrow/dataset/dataset_writer_test.cc#L210\r\n\r\nShould we add a separate PR to expose these attributes to Python or should we do within the same PR?\u00a0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2021-12-14T04:01:38.743+0000",
                    "updated": "2021-12-14T04:03:27.176+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/comment/17458976",
                    "id": "17458976",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Discussion with [~westonpace]: decided to include the row_group params to this PR.\u00a0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2021-12-14T08:22:33.278+0000",
                    "updated": "2021-12-14T08:22:33.278+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935/comment/17462798",
                    "id": "17462798",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Issue resolved by pull request 11911\n[https://github.com/apache/arrow/pull/11911]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2021-12-20T19:02:31.690+0000",
                    "updated": "2021-12-20T19:02:31.690+0000"
                }
            ],
            "maxResults": 6,
            "total": 6,
            "startAt": 0
        },
        "customfield_12311820": "0|z0xg3s:",
        "customfield_12314139": null
    }
}