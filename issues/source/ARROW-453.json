{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13031560",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560",
    "key": "ARROW-453",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12345978",
                "id": "12345978",
                "description": "",
                "name": "0.15.0",
                "archived": false,
                "released": true,
                "releaseDate": "2019-10-05"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "filesystem",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12568238",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12568238",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13079797",
                    "key": "ARROW-1119",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13079797",
                    "fields": {
                        "summary": "[Python/C++] Implement NativeFile interfaces for Amazon S3",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            },
            {
                "id": "12568239",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12568239",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13099488",
                    "key": "ARROW-1456",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13099488",
                    "fields": {
                        "summary": "[Python] Run s3fs unit tests in Travis CI",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12568258",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12568258",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13208534",
                    "key": "ARROW-4208",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13208534",
                    "fields": {
                        "summary": "[CI/Python] Have automatized tests for S3",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12574641",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12574641",
                "type": {
                    "id": "12310051",
                    "name": "Supercedes",
                    "inward": "is superceded by",
                    "outward": "supercedes",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310051"
                },
                "outwardIssue": {
                    "id": "13079797",
                    "key": "ARROW-1119",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13079797",
                    "fields": {
                        "summary": "[Python/C++] Implement NativeFile interfaces for Amazon S3",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
            "name": "apitrou",
            "key": "pitrou",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
            },
            "displayName": "Antoine Pitrou",
            "active": true,
            "timeZone": "Europe/Paris"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 42600,
            "total": 42600,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 42600,
            "total": 42600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-453/votes",
            "votes": 1,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 88,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/299482",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #5167: ARROW-453: [C++] Filesystem implementation for S3\nURL: https://github.com/apache/arrow/pull/5167\n \n \n   Unit testing is done using the Minio standalone S3 server.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-22T14:49:27.447+0000",
                    "updated": "2019-08-22T14:49:27.447+0000",
                    "started": "2019-08-22T14:49:27.447+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "299482",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/299496",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on issue #5167: [WIP] ARROW-453: [C++] Filesystem implementation for S3\nURL: https://github.com/apache/arrow/pull/5167#issuecomment-523951493\n \n \n   s3fs test passed on Travis-CI: https://travis-ci.org/pitrou/arrow/jobs/575384780#L5243\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-22T15:18:59.337+0000",
                    "updated": "2019-08-22T15:18:59.337+0000",
                    "started": "2019-08-22T15:18:59.336+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "299496",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/299571",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on issue #5167: [WIP] ARROW-453: [C++] Filesystem implementation for S3\nURL: https://github.com/apache/arrow/pull/5167#issuecomment-523990213\n \n \n   s3fs test passed on AppVeyor: https://ci.appveyor.com/project/pitrou/arrow/build/job/o639sped92wndo5f#L1740\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-22T16:59:38.077+0000",
                    "updated": "2019-08-22T16:59:38.077+0000",
                    "started": "2019-08-22T16:59:38.076+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "299571",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/299805",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io commented on issue #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#issuecomment-524109572\n \n \n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/5167?src=pr&el=h1) Report\n   > Merging [#5167](https://codecov.io/gh/apache/arrow/pull/5167?src=pr&el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/3c9236d198b93ae3f59d39ce958909a1909078f5?src=pr&el=desc) will **increase** coverage by `1.09%`.\n   > The diff coverage is `94.59%`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/5167/graphs/tree.svg?width=650&token=LpTCFbqVT1&height=150&src=pr)](https://codecov.io/gh/apache/arrow/pull/5167?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #5167      +/-   ##\n   ==========================================\n   + Coverage   87.63%   88.73%   +1.09%     \n   ==========================================\n     Files        1022      935      -87     \n     Lines      146258   121759   -24499     \n     Branches     1437     1437              \n   ==========================================\n   - Hits       128174   108041   -20133     \n   + Misses      17722    13356    -4366     \n     Partials      362      362\n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/5167?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [cpp/src/arrow/testing/util.h](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy90ZXN0aW5nL3V0aWwuaA==) | `94.44% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [cpp/src/arrow/filesystem/filesystem.h](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9maWxlc3lzdGVtL2ZpbGVzeXN0ZW0uaA==) | `100% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [cpp/src/arrow/filesystem/filesystem.cc](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9maWxlc3lzdGVtL2ZpbGVzeXN0ZW0uY2M=) | `74.38% <0%> (-12.16%)` | :arrow_down: |\n   | [cpp/src/arrow/filesystem/test\\_util.cc](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9maWxlc3lzdGVtL3Rlc3RfdXRpbC5jYw==) | `99.38% <100%> (+0.04%)` | :arrow_up: |\n   | [cpp/src/arrow/filesystem/path\\_util.h](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9maWxlc3lzdGVtL3BhdGhfdXRpbC5o) | `100% <100%> (\u00f8)` | :arrow_up: |\n   | [cpp/src/arrow/filesystem/test\\_util.h](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9maWxlc3lzdGVtL3Rlc3RfdXRpbC5o) | `100% <100%> (\u00f8)` | :arrow_up: |\n   | [cpp/src/arrow/filesystem/s3\\_internal.h](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9maWxlc3lzdGVtL3MzX2ludGVybmFsLmg=) | `100% <100%> (\u00f8)` | |\n   | [cpp/src/arrow/filesystem/s3fs.h](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9maWxlc3lzdGVtL3MzZnMuaA==) | `100% <100%> (\u00f8)` | |\n   | [cpp/src/arrow/testing/util.cc](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy90ZXN0aW5nL3V0aWwuY2M=) | `95.5% <100%> (+0.26%)` | :arrow_up: |\n   | [cpp/src/arrow/filesystem/s3fs.cc](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9maWxlc3lzdGVtL3MzZnMuY2M=) | `92.93% <92.93%> (\u00f8)` | |\n   | ... and [100 more](https://codecov.io/gh/apache/arrow/pull/5167/diff?src=pr&el=tree-more) | |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/5167?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/5167?src=pr&el=footer). Last update [3c9236d...a0d5f28](https://codecov.io/gh/apache/arrow/pull/5167?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-22T23:02:27.371+0000",
                    "updated": "2019-08-22T23:02:27.371+0000",
                    "started": "2019-08-22T23:02:27.371+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "299805",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/299821",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#issuecomment-524116492\n \n \n   Will endeavor to get a review to you in the next couple working days\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-22T23:32:08.800+0000",
                    "updated": "2019-08-22T23:32:08.800+0000",
                    "started": "2019-08-22T23:32:08.800+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "299821",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300556",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317291217\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.h\n ##########\n @@ -0,0 +1,84 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/util/macros.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+struct ARROW_EXPORT S3Options {\n+  // AWS region to connect to\n+  std::string region = \"us-east-1\";\n+\n+  // XXX perhaps instead take a URL like \"http://localhost:9000\"?\n+  // If non-empty, override region with a connect string such as \"localhost:9000\"\n+  std::string endpoint_override;\n+  // Default \"https\"\n+  std::string scheme = \"https\";\n+\n+  std::string access_key;\n+  std::string secret_key;\n+};\n+\n+class ARROW_EXPORT S3FileSystem : public FileSystem {\n \n Review comment:\n   To follow on with our discussion about PIMPL vs. virtual interfaces, you could simply have a function `CreateS3FileSystem(options, &fs)` and skip the PIMPL. \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:21.453+0000",
                    "updated": "2019-08-23T22:37:21.453+0000",
                    "started": "2019-08-23T22:37:21.453+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300556",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300557",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317318086\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.cc\n ##########\n @@ -0,0 +1,1100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <algorithm>\n+#include <sstream>\n+#include <utility>\n+\n+#ifdef _WIN32\n+// Undefine preprocessor macros that interfere with AWS function / method names\n+#ifdef GetMessage\n+#undef GetMessage\n+#endif\n+#ifdef GetObject\n+#undef GetObject\n+#endif\n+#endif\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/logging/ConsoleLogSystem.h>\n+#include <aws/core/utils/stream/PreallocatedStreamBuf.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/AbortMultipartUploadRequest.h>\n+#include <aws/s3/model/CompleteMultipartUploadRequest.h>\n+#include <aws/s3/model/CompletedMultipartUpload.h>\n+#include <aws/s3/model/CompletedPart.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/CreateMultipartUploadRequest.h>\n+#include <aws/s3/model/DeleteBucketRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectsRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListBucketsResult.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+#include <aws/s3/model/UploadPartRequest.h>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::Aws::Client::AWSError;\n+using ::Aws::S3::S3Errors;\n+namespace S3Model = Aws::S3::Model;\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::ErrorToStatus;\n+using ::arrow::fs::internal::FromAwsString;\n+using ::arrow::fs::internal::IsAlreadyExists;\n+using ::arrow::fs::internal::IsNotFound;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+using ::arrow::fs::internal::ToTimePoint;\n+using ::arrow::fs::internal::ToURLEncodedAwsString;\n+\n+static const char kSep = '/';\n+\n+namespace {\n+\n+util::string_view RemoveTrailingSeparator(util::string_view key) {\n+  // Remove trailing separator\n+  if (!key.empty() && key.back() == kSep) {\n+    key.remove_suffix(1);\n+  }\n+  return key;\n+}\n+\n+// XXX Sanitize paths by removing leading slash?\n+\n+struct S3Path {\n+  std::string full_path;\n+  std::string bucket;\n+  std::string key;\n+  std::vector<std::string> key_parts;\n+\n+  static Status FromString(const std::string& s, S3Path* out) {\n+    auto first_sep = s.find_first_of(kSep);\n+    if (first_sep == 0) {\n+      return Status::Invalid(\"Path cannot start with a separator ('\", s, \"')\");\n+    }\n+    if (first_sep == std::string::npos) {\n+      *out = {s, s, \"\", {}};\n+      return Status::OK();\n+    }\n+    out->full_path = s;\n+    out->bucket = s.substr(0, first_sep);\n+    out->key = s.substr(first_sep + 1);\n+    out->key_parts = internal::SplitAbstractPath(out->key);\n+    return internal::ValidateAbstractPathParts(out->key_parts);\n+  }\n+\n+  Aws::String ToURLEncodedAwsString() const {\n+    // URL-encode individual parts, not the '/' separator\n+    Aws::String res;\n+    res += internal::ToURLEncodedAwsString(bucket);\n+    for (const auto& part : key_parts) {\n+      res += kSep;\n+      res += internal::ToURLEncodedAwsString(part);\n+    }\n+    return res;\n+  }\n+\n+  S3Path parent() const {\n+    DCHECK(!key_parts.empty());\n+    auto parent = S3Path{\"\", bucket, \"\", key_parts};\n+    parent.key_parts.pop_back();\n+    parent.key = internal::JoinAbstractPath(parent.key_parts);\n+    parent.full_path = parent.bucket + kSep + parent.key;\n+    return parent;\n+  }\n+\n+  bool has_parent() const { return !key.empty(); }\n+\n+  bool empty() const { return bucket.empty() && key.empty(); }\n+\n+  bool operator==(const S3Path& other) const {\n+    return bucket == other.bucket && key == other.key;\n+  }\n+};\n+\n+// XXX return in OutcomeToStatus instead?\n+Status PathNotFound(const S3Path& path) {\n+  return Status::IOError(\"Path does not exist '\", path.full_path, \"'\");\n+}\n+\n+Status PathNotFound(const std::string& bucket, const std::string& key) {\n+  return Status::IOError(\"Path does not exist '\", bucket, kSep, key, \"'\");\n+}\n+\n+// Status NotADir(const S3Path& path) {\n+//   return Status::IOError(\"Not a directory: '\", path.full_path, \"'\");\n+// }\n+\n+Status NotAFile(const S3Path& path) {\n+  return Status::IOError(\"Not a regular file: '\", path.full_path, \"'\");\n+}\n+\n+Status ValidateFilePath(const S3Path& path) {\n+  if (path.bucket.empty() || path.key.empty()) {\n+    return NotAFile(path);\n+  }\n+  return Status::OK();\n+}\n+\n+std::string FormatRange(int64_t start, int64_t length) {\n+  // Format a HTTP range header value\n+  std::stringstream ss;\n+  ss << \"bytes=\" << start << \"-\" << start + length - 1;\n+  return ss.str();\n+}\n+\n+Status GetObjectRange(Aws::S3::S3Client* client, const S3Path& path, int64_t start,\n+                      int64_t length, S3Model::GetObjectResult* out) {\n+  S3Model::GetObjectRequest req;\n+  req.SetBucket(ToAwsString(path.bucket));\n+  req.SetKey(ToAwsString(path.key));\n+  req.SetRange(ToAwsString(FormatRange(start, length)));\n+  ARROW_AWS_ASSIGN_OR_RAISE(*out, client->GetObject(req));\n+  return Status::OK();\n+}\n+\n+// A RandomAccessFile that reads from a S3 object\n+class ObjectInputFile : public io::RandomAccessFile {\n+ public:\n+  ObjectInputFile(Aws::S3::S3Client* client, const S3Path& path)\n+      : client_(client), path_(path) {}\n+\n+  ~ObjectInputFile() override {}\n+\n+  Status Init() {\n+    // Issue a HEAD Object to get the content-length and ensure any\n+    // errors (e.g. file not found) don't wait until the first Read() call.\n+    S3Model::HeadObjectRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+\n+    auto outcome = client_->HeadObject(req);\n+    if (!outcome.IsSuccess()) {\n+      if (IsNotFound(outcome.GetError())) {\n+        return PathNotFound(path_);\n+      } else {\n+        return ErrorToStatus(outcome.GetError());\n+      }\n+    }\n+    content_length_ = outcome.GetResult().GetContentLength();\n+    return Status::OK();\n+  }\n+\n+  Status CheckClosed() const {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  // RandomAccessFile APIs\n+\n+  Status Close() override {\n+    closed_ = true;\n+    return Status::OK();\n+  }\n+\n+  bool closed() const override { return closed_; }\n+\n+  Status Tell(int64_t* position) const override {\n+    RETURN_NOT_OK(CheckClosed());\n+\n+    *position = pos_;\n+    return Status::OK();\n+  }\n+\n+  Status GetSize(int64_t* size) override {\n+    RETURN_NOT_OK(CheckClosed());\n+\n+    *size = content_length_;\n+    return Status::OK();\n+  }\n+\n+  Status Seek(int64_t position) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return Status::OK();\n+  }\n+\n+  Status ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                void* out) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // Read the desired range of bytes\n+    S3Model::GetObjectResult result;\n+    RETURN_NOT_OK(GetObjectRange(client_, path_, position, nbytes, &result));\n+\n+    auto& stream = result.GetBody();\n+    stream.read(reinterpret_cast<char*>(out), nbytes);\n+    // NOTE: the stream is a stringstream by default, there is no actual error\n+    // to check for.  However, stream.fail() may return true if EOF is reached.\n+    *bytes_read = stream.gcount();\n+    return Status::OK();\n+  }\n+\n+  Status ReadAt(int64_t position, int64_t nbytes, std::shared_ptr<Buffer>* out) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    std::shared_ptr<ResizableBuffer> buf;\n+    int64_t bytes_read;\n+    RETURN_NOT_OK(AllocateResizableBuffer(nbytes, &buf));\n+    if (nbytes > 0) {\n+      RETURN_NOT_OK(ReadAt(position, nbytes, &bytes_read, buf->mutable_data()));\n+      DCHECK_LE(bytes_read, nbytes);\n+      RETURN_NOT_OK(buf->Resize(bytes_read));\n+    }\n+    *out = std::move(buf);\n+    return Status::OK();\n+  }\n+\n+  Status Read(int64_t nbytes, int64_t* bytes_read, void* out) override {\n+    RETURN_NOT_OK(ReadAt(pos_, nbytes, bytes_read, out));\n+    pos_ += *bytes_read;\n+    return Status::OK();\n+  }\n+\n+  Status Read(int64_t nbytes, std::shared_ptr<Buffer>* out) override {\n+    RETURN_NOT_OK(ReadAt(pos_, nbytes, out));\n+    pos_ += (*out)->size();\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Aws::S3::S3Client* client_;\n+  S3Path path_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+};\n+\n+// A non-copying istream.\n+// See https://stackoverflow.com/questions/35322033/aws-c-sdk-uploadpart-times-out\n+// https://stackoverflow.com/questions/13059091/creating-an-input-stream-from-constant-memory\n+\n+class StringViewStream : Aws::Utils::Stream::PreallocatedStreamBuf, public std::iostream {\n+ public:\n+  StringViewStream(const void* data, int64_t nbytes)\n+      : Aws::Utils::Stream::PreallocatedStreamBuf(\n+            reinterpret_cast<unsigned char*>(const_cast<void*>(data)),\n+            static_cast<size_t>(nbytes)),\n+        std::iostream(this) {}\n+};\n+\n+// Minimum size for each part of a multipart upload, except for the last part.\n+// AWS doc says \"5 MB\" but it's not clear whether those are MB or MiB,\n+// so I chose the safer value.\n+// (see https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)\n+static constexpr int64_t kMinimumPartUpload = 5 * 1024 * 1024;\n+\n+// An OutputStream that writes to a S3 object\n+class ObjectOutputStream : public io::OutputStream {\n+ public:\n+  ObjectOutputStream(Aws::S3::S3Client* client, const S3Path& path)\n+      : client_(client), path_(path) {}\n+\n+  ~ObjectOutputStream() override {\n+    if (!closed_) {\n+      auto st = Abort();\n+      if (!st.ok()) {\n+        ARROW_LOG(ERROR) << \"Could not abort multipart upload: \" << st;\n+      }\n+    }\n+  }\n+\n+  Status Init() {\n+    // Initiate the multi-part upload\n+    S3Model::CreateMultipartUploadRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+\n+    auto outcome = client_->CreateMultipartUpload(req);\n+    if (!outcome.IsSuccess()) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+    upload_id_ = outcome.GetResult().GetUploadId();\n+    closed_ = false;\n+    return Status::OK();\n+  }\n+\n+  Status Abort() {\n+    S3Model::AbortMultipartUploadRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+    req.SetUploadId(upload_id_);\n+\n+    auto outcome = client_->AbortMultipartUpload(req);\n+    if (!outcome.IsSuccess()) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+    current_part_.reset();\n+    closed_ = true;\n+    return Status::OK();\n+  }\n+\n+  // OutputStream interface\n+\n+  Status Close() override {\n+    if (closed_) {\n+      return Status::OK();\n+    }\n+\n+    if (current_part_) {\n+      // Upload last part\n+      RETURN_NOT_OK(CommitCurrentPart());\n+    }\n+\n+    // S3 mandates at least one part, upload an empty one if necessary\n+    if (completed_upload_.GetParts().empty()) {\n+      RETURN_NOT_OK(UploadPart(\"\", 0));\n+    }\n+    DCHECK(completed_upload_.PartsHasBeenSet());\n+\n+    S3Model::CompleteMultipartUploadRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+    req.SetUploadId(upload_id_);\n+    req.SetMultipartUpload(completed_upload_);\n+\n+    auto outcome = client_->CompleteMultipartUpload(req);\n+    if (!outcome.IsSuccess()) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+\n+    closed_ = true;\n+    return Status::OK();\n+  }\n+\n+  bool closed() const override { return closed_; }\n+\n+  Status Tell(int64_t* position) const override {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+    *position = pos_;\n+    return Status::OK();\n+  }\n+\n+  Status Write(const void* data, int64_t nbytes) override {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+\n+    // With up to 10000 parts in an upload (S3 limit), a stream writing chunks\n+    // of exactly 5MB would be limited to 50GB total.  To avoid that, we bump\n+    // the upload threshold every 100 parts.  So the pattern is:\n+    // - part 1 to 99: 5MB threshold\n+    // - part 100 to 199: 10MB threshold\n+    // - part 200 to 299: 15MB threshold\n+    // ...\n+    // - part 9900 to 9999: 500MB threshold\n+    // So the total size limit is 2475000MB or ~2.4TB, while keeping manageable\n+    // chunk sizes and avoiding too much buffering in the common case of a small-ish\n+    // stream.  If the limit's not enough, we can revisit.\n+    if (part_number_ % 100 == 0) {\n+      part_upload_threshold_ += kMinimumPartUpload;\n+    }\n+\n+    if (!current_part_ && nbytes >= part_upload_threshold_) {\n+      // No current part and data large enough, upload it directly without copying\n+      RETURN_NOT_OK(UploadPart(data, nbytes));\n+      pos_ += nbytes;\n+      return Status::OK();\n+    }\n+    // Can't upload data on its own, need to buffer it\n+    if (!current_part_) {\n+      RETURN_NOT_OK(io::BufferOutputStream::Create(\n+          part_upload_threshold_, default_memory_pool(), &current_part_));\n+      current_part_size_ = 0;\n+    }\n+    RETURN_NOT_OK(current_part_->Write(data, nbytes));\n+    pos_ += nbytes;\n+    current_part_size_ += nbytes;\n+\n+    if (current_part_size_ >= part_upload_threshold_) {\n+      // Current part large enough, upload it\n+      RETURN_NOT_OK(CommitCurrentPart());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  Status Flush() override {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status CommitCurrentPart() {\n+    std::shared_ptr<Buffer> buf;\n+    RETURN_NOT_OK(current_part_->Finish(&buf));\n+    current_part_.reset();\n+    current_part_size_ = 0;\n+    return UploadPart(buf->data(), buf->size());\n+  }\n+\n+  Status UploadPart(const void* data, int64_t nbytes) {\n+    S3Model::UploadPartRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+    req.SetUploadId(upload_id_);\n+    req.SetPartNumber(part_number_);\n+    req.SetContentLength(nbytes);\n+    req.SetBody(std::make_shared<StringViewStream>(data, nbytes));\n+\n+    auto outcome = client_->UploadPart(req);\n+    if (!outcome.IsSuccess()) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+    // Append ETag and part number for this uploaded part\n+    // (will be needed for upload completion in Close())\n+    S3Model::CompletedPart part;\n+    part.SetPartNumber(part_number_);\n+    part.SetETag(outcome.GetResult().GetETag());\n+    completed_upload_.AddParts(std::move(part));\n+    ++part_number_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Aws::S3::S3Client* client_;\n+  S3Path path_;\n+  Aws::String upload_id_;\n+  S3Model::CompletedMultipartUpload completed_upload_;\n+  bool closed_ = true;\n+  int64_t pos_ = 0;\n+  int32_t part_number_ = 1;\n+  std::shared_ptr<io::BufferOutputStream> current_part_;\n+  int64_t current_part_size_ = 0;\n+  int64_t part_upload_threshold_ = kMinimumPartUpload;\n+};\n+\n+// This function assumes st->path() is already set\n+Status FileObjectToStats(const S3Model::HeadObjectResult& obj, FileStats* st) {\n+  st->set_type(FileType::File);\n+  st->set_size(static_cast<int64_t>(obj.GetContentLength()));\n+  st->set_mtime(ToTimePoint(obj.GetLastModified()));\n+  return Status::OK();\n+}\n+\n+Status FileObjectToStats(const S3Model::Object& obj, FileStats* st) {\n+  st->set_type(FileType::File);\n+  st->set_size(static_cast<int64_t>(obj.GetSize()));\n+  st->set_mtime(ToTimePoint(obj.GetLastModified()));\n+  return Status::OK();\n+}\n+\n+}  // namespace\n+\n+class S3FileSystem::Impl {\n+ public:\n+  S3Options options_;\n+  Aws::Client::ClientConfiguration client_config_;\n+  Aws::Auth::AWSCredentials credentials_;\n+  std::unique_ptr<Aws::S3::S3Client> client_;\n+\n+  void Init() {\n+    credentials_ = {ToAwsString(options_.access_key), ToAwsString(options_.secret_key)};\n+    client_config_.endpointOverride = ToAwsString(options_.endpoint_override);\n+    // FIXME propagate / expose other options\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    bool use_virtual_addressing = options_.endpoint_override.empty();\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+  }\n+\n+  // Create a bucket.  Successful if bucket already exists.\n+  Status CreateBucket(const std::string& bucket) {\n+    S3Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(bucket));\n+\n+    auto outcome = client_->CreateBucket(req);\n+    if (!outcome.IsSuccess() && !IsAlreadyExists(outcome.GetError())) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Create an object with empty contents.  Successful if object already exists.\n+  Status CreateEmptyObject(const std::string& bucket, const std::string& key) {\n+    S3Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(bucket));\n+    req.SetKey(ToAwsString(key));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  Status CreateEmptyDir(const std::string& bucket, const std::string& key) {\n+    DCHECK(!key.empty());\n+    return CreateEmptyObject(bucket, key + kSep);\n+  }\n+\n+  Status DeleteObject(const std::string& bucket, const std::string& key) {\n+    S3Model::DeleteObjectRequest req;\n+    req.SetBucket(ToAwsString(bucket));\n+    req.SetKey(ToAwsString(key));\n+    return OutcomeToStatus(client_->DeleteObject(req));\n+  }\n+\n+  Status CopyObject(const S3Path& src_path, const S3Path& dest_path) {\n+    S3Model::CopyObjectRequest req;\n+    req.SetBucket(ToAwsString(dest_path.bucket));\n+    req.SetKey(ToAwsString(dest_path.key));\n+    // Copy source \"Must be URL-encoded\" according to AWS SDK docs.\n+    req.SetCopySource(src_path.ToURLEncodedAwsString());\n+    return OutcomeToStatus(client_->CopyObject(req));\n+  }\n+\n+  // On Minio, an empty \"directory\" doesn't satisfy the same API requests as\n+  // a non-empty \"directory\".  This is a Minio-specific quirk, but we need\n+  // to handle it for unit testing.\n+\n+  Status IsEmptyDirectory(const std::string& bucket, const std::string& key, bool* out) {\n+    S3Model::HeadObjectRequest req;\n+    req.SetBucket(ToAwsString(bucket));\n+    req.SetKey(ToAwsString(key) + kSep);\n+\n+    auto outcome = client_->HeadObject(req);\n+    if (outcome.IsSuccess()) {\n+      *out = true;\n+      return Status::OK();\n+    }\n+    if (IsNotFound(outcome.GetError())) {\n+      *out = false;\n+      return Status::OK();\n+    }\n+    return ErrorToStatus(outcome.GetError());\n+  }\n+\n+  Status IsEmptyDirectory(const S3Path& path, bool* out) {\n+    return IsEmptyDirectory(path.bucket, path.key, out);\n+  }\n+\n+  Status IsNonEmptyDirectory(const S3Path& path, bool* out) {\n+    S3Model::ListObjectsV2Request req;\n+    req.SetBucket(ToAwsString(path.bucket));\n+    req.SetPrefix(ToAwsString(path.key) + kSep);\n+    req.SetDelimiter(Aws::String() + kSep);\n+    req.SetMaxKeys(1);\n+    auto outcome = client_->ListObjectsV2(req);\n+    if (outcome.IsSuccess()) {\n+      *out = outcome.GetResult().GetKeyCount() > 0;\n+      return Status::OK();\n+    }\n+    if (IsNotFound(outcome.GetError())) {\n+      *out = false;\n+      return Status::OK();\n+    }\n+    return ErrorToStatus(outcome.GetError());\n+  }\n+\n+  // List objects under a given prefix, issuing continuation requests if necessary\n+  template <typename ResultCallable, typename ErrorCallable>\n+  Status ListObjectsV2(const std::string& bucket, const std::string& prefix,\n+                       ResultCallable&& result_callable, ErrorCallable&& error_callable) {\n+    S3Model::ListObjectsV2Request req;\n+    req.SetBucket(ToAwsString(bucket));\n+    if (!prefix.empty()) {\n+      req.SetPrefix(ToAwsString(prefix) + kSep);\n+    }\n+    req.SetDelimiter(Aws::String() + kSep);\n+    req.SetMaxKeys(1000);\n+\n+    while (true) {\n+      auto outcome = client_->ListObjectsV2(req);\n+      if (!outcome.IsSuccess()) {\n+        return error_callable(outcome.GetError());\n+      }\n+      const auto& result = outcome.GetResult();\n+      RETURN_NOT_OK(result_callable(result));\n+      // Was the result limited by max-keys? If so, use the continuation token\n+      // to fetch further results.\n+      if (!result.GetIsTruncated()) {\n+        break;\n+      }\n+      DCHECK(!result.GetNextContinuationToken().empty());\n+      req.SetContinuationToken(result.GetNextContinuationToken());\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status Walk(const Selector& select, const std::string& bucket, const std::string& key,\n+              std::vector<FileStats>* out) {\n+    bool is_empty = true;\n+    std::vector<std::string> child_keys;\n+\n+    auto handle_results = [&](const S3Model::ListObjectsV2Result& result) -> Status {\n+      // Walk \"files\"\n+      for (const auto& obj : result.GetContents()) {\n+        is_empty = false;\n+        FileStats st;\n+        std::stringstream child_key;\n+        child_key << bucket << kSep << obj.GetKey();\n+        st.set_path(child_key.str());\n+        RETURN_NOT_OK(FileObjectToStats(obj, &st));\n+        out->push_back(std::move(st));\n+      }\n+      // Walk \"directories\"\n+      for (const auto& prefix : result.GetCommonPrefixes()) {\n+        is_empty = false;\n+        const auto child_key = RemoveTrailingSeparator(FromAwsString(prefix.GetPrefix()));\n+        std::stringstream ss;\n+        ss << bucket << kSep << child_key;\n+        FileStats st;\n+        st.set_path(ss.str());\n+        st.set_type(FileType::Directory);\n+        out->push_back(std::move(st));\n+        if (select.recursive) {\n+          child_keys.emplace_back(child_key);\n+        }\n+      }\n+      return Status::OK();\n+    };\n+\n+    auto handle_error = [&](const AWSError<S3Errors>& error) -> Status {\n+      if (select.allow_non_existent && IsNotFound(error)) {\n+        return Status::OK();\n+      }\n+      return ErrorToStatus(error);\n+    };\n+\n+    RETURN_NOT_OK(\n+        ListObjectsV2(bucket, key, std::move(handle_results), std::move(handle_error)));\n+\n+    // Recurse\n+    for (const auto& child_key : child_keys) {\n+      RETURN_NOT_OK(Walk(select, bucket, child_key, out));\n+    }\n+\n+    // If no contents were found, perhaps it's an empty \"directory\",\n+    // or perhaps it's a non-existent entry.  Check.\n+    if (is_empty && !select.allow_non_existent) {\n+      RETURN_NOT_OK(IsEmptyDirectory(bucket, key, &is_empty));\n+      if (!is_empty) {\n+        return PathNotFound(bucket, key);\n+      }\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status WalkForDeleteDir(const std::string& bucket, const std::string& key,\n+                          std::vector<std::string>* file_keys,\n+                          std::vector<std::string>* dir_keys) {\n+    std::vector<std::string> child_keys;\n+\n+    auto handle_results = [&](const S3Model::ListObjectsV2Result& result) -> Status {\n+      // Walk \"files\"\n+      for (const auto& obj : result.GetContents()) {\n+        file_keys->emplace_back(FromAwsString(obj.GetKey()));\n+      }\n+      // Walk \"directories\"\n+      for (const auto& prefix : result.GetCommonPrefixes()) {\n+        auto child_key = FromAwsString(prefix.GetPrefix());\n+        dir_keys->emplace_back(child_key);\n+        child_keys.emplace_back(RemoveTrailingSeparator(child_key));\n+      }\n+      return Status::OK();\n+    };\n+\n+    auto handle_error = [&](const AWSError<S3Errors>& error) -> Status {\n+      return ErrorToStatus(error);\n+    };\n+\n+    RETURN_NOT_OK(\n+        ListObjectsV2(bucket, key, std::move(handle_results), std::move(handle_error)));\n+\n+    // Recurse\n+    for (const auto& child_key : child_keys) {\n+      RETURN_NOT_OK(WalkForDeleteDir(bucket, child_key, file_keys, dir_keys));\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Delete multiple objects at once\n+  Status DeleteObjects(const std::string& bucket, const std::vector<std::string>& keys) {\n+    // At most 1000 keys per multiple-delete request\n+    constexpr size_t chunk_size = 1000;\n+    for (size_t start = 0; start < keys.size(); start += chunk_size) {\n+      S3Model::DeleteObjectsRequest req;\n+      S3Model::Delete del;\n+      for (size_t i = start; i < std::min(keys.size(), chunk_size); ++i) {\n+        del.AddObjects(S3Model::ObjectIdentifier().WithKey(ToAwsString(keys[i])));\n+      }\n+      req.SetBucket(ToAwsString(bucket));\n+      req.SetDelete(std::move(del));\n+      auto outcome = client_->DeleteObjects(req);\n+      if (!outcome.IsSuccess()) {\n+        return ErrorToStatus(outcome.GetError());\n+      }\n+      // Also need to check per-key errors, even on successful outcome\n+      // See\n+      // https://docs.aws.amazon.com/fr_fr/AmazonS3/latest/API/multiobjectdeleteapi.html\n+      const auto& errors = outcome.GetResult().GetErrors();\n+      if (!errors.empty()) {\n+        std::stringstream ss;\n+        ss << \"Got the following \" << errors.size()\n+           << \" errors when deleting objects in S3 bucket '\" << bucket << \"':\\n\";\n+        for (const auto& error : errors) {\n+          ss << \"- key '\" << error.GetKey() << \"': \" << error.GetMessage() << \"\\n\";\n+        }\n+        return Status::IOError(ss.str());\n+      }\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status DeleteDir(const std::string& bucket, const std::string& key) {\n+    std::vector<std::string> file_keys;\n+    std::vector<std::string> dir_keys;\n+    RETURN_NOT_OK(WalkForDeleteDir(bucket, key, &file_keys, &dir_keys));\n+    if (file_keys.empty() && dir_keys.empty() && !key.empty()) {\n+      // No contents found, is it an empty directory?\n+      bool exists = false;\n+      RETURN_NOT_OK(IsEmptyDirectory(bucket, key, &exists));\n+      if (!exists) {\n+        return PathNotFound(bucket, key);\n+      }\n+    }\n+    // First delete all \"files\", then delete all child \"directories\"\n+    RETURN_NOT_OK(DeleteObjects(bucket, file_keys));\n+    // XXX This doesn't seem necessary on Minio\n+    RETURN_NOT_OK(DeleteObjects(bucket, dir_keys));\n+    // Finally, delete the base dir itself\n+    if (!key.empty()) {\n+      RETURN_NOT_OK(DeleteObject(bucket, key + kSep));\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status EnsureParentExists(const S3Path& path) {\n+    if (path.has_parent()) {\n+      // Parent may be implicitly deleted if it became empty, recreate it\n+      S3Path parent = path.parent();\n+      if (!parent.key.empty()) {\n+        return CreateEmptyDir(parent.bucket, parent.key);\n+      }\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status ListBuckets(std::vector<std::string>* out) {\n+    out->clear();\n+    auto outcome = client_->ListBuckets();\n+    if (!outcome.IsSuccess()) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+    for (const auto& bucket : outcome.GetResult().GetBuckets()) {\n+      out->emplace_back(FromAwsString(bucket.GetName()));\n+    }\n+    return Status::OK();\n+  }\n+};\n+\n+S3FileSystem::S3FileSystem(const S3Options& options) : impl_(new Impl{options}) {\n+  impl_->Init();\n+}\n+\n+S3FileSystem::~S3FileSystem() {}\n+\n+Status S3FileSystem::GetTargetStats(const std::string& s, FileStats* out) {\n+  S3Path path;\n+  RETURN_NOT_OK(S3Path::FromString(s, &path));\n+  FileStats st;\n+  st.set_path(s);\n+\n+  if (path.empty()) {\n+    // It's the root path \"\"\n+    st.set_type(FileType::Directory);\n+    *out = st;\n+    return Status::OK();\n+  } else if (path.key.empty()) {\n+    // It's a bucket\n+    S3Model::HeadBucketRequest req;\n+    req.SetBucket(ToAwsString(path.bucket));\n+\n+    auto outcome = impl_->client_->HeadBucket(req);\n+    if (!outcome.IsSuccess()) {\n+      if (!IsNotFound(outcome.GetError())) {\n+        return ErrorToStatus(outcome.GetError());\n+      }\n+      st.set_type(FileType::NonExistent);\n+      *out = st;\n+      return Status::OK();\n+    }\n+    // NOTE: S3 doesn't have a bucket modification time.  Only a creation\n+    // time is available, and you have to list all buckets to get it.\n+    st.set_type(FileType::Directory);\n+    *out = st;\n+    return Status::OK();\n+  } else {\n+    // It's an object\n+    S3Model::HeadObjectRequest req;\n+    req.SetBucket(ToAwsString(path.bucket));\n+    req.SetKey(ToAwsString(path.key));\n+\n+    auto outcome = impl_->client_->HeadObject(req);\n+    if (outcome.IsSuccess()) {\n+      // \"File\" object found\n+      *out = st;\n+      return FileObjectToStats(outcome.GetResult(), out);\n+    }\n+    if (!IsNotFound(outcome.GetError())) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+    // Not found => perhaps it's an empty \"directory\"\n+    bool is_dir = false;\n+    RETURN_NOT_OK(impl_->IsEmptyDirectory(path, &is_dir));\n+    if (is_dir) {\n+      st.set_type(FileType::Directory);\n+      *out = st;\n+      return Status::OK();\n+    }\n+    // Not found => perhaps it's a non-empty \"directory\"\n+    RETURN_NOT_OK(impl_->IsNonEmptyDirectory(path, &is_dir));\n+    if (is_dir) {\n+      st.set_type(FileType::Directory);\n+    } else {\n+      st.set_type(FileType::NonExistent);\n+    }\n+    *out = st;\n+    return Status::OK();\n+  }\n+}\n+\n+Status S3FileSystem::GetTargetStats(const Selector& select, std::vector<FileStats>* out) {\n+  S3Path base_path;\n+  RETURN_NOT_OK(S3Path::FromString(select.base_dir, &base_path));\n+  out->clear();\n+\n+  if (base_path.empty()) {\n+    // List all buckets\n+    std::vector<std::string> buckets;\n+    RETURN_NOT_OK(impl_->ListBuckets(&buckets));\n+    for (const auto& bucket : buckets) {\n+      FileStats st;\n+      st.set_path(bucket);\n+      st.set_type(FileType::Directory);\n+      out->push_back(std::move(st));\n+      if (select.recursive) {\n+        RETURN_NOT_OK(impl_->Walk(select, bucket, \"\", out));\n+      }\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Nominal case -> walk a single bucket\n+  return impl_->Walk(select, base_path.bucket, base_path.key, out);\n+}\n+\n+Status S3FileSystem::CreateDir(const std::string& s, bool recursive) {\n+  S3Path path;\n+  RETURN_NOT_OK(S3Path::FromString(s, &path));\n+\n+  if (path.key.empty()) {\n+    // Create bucket\n+    return impl_->CreateBucket(path.bucket);\n+  }\n+\n+  // Create object\n+  if (recursive) {\n+    // Ensure bucket exists\n+    RETURN_NOT_OK(impl_->CreateBucket(path.bucket));\n+    // Ensure that all parents exist, then the directory itself\n+    std::string parent_key;\n+    for (auto it = path.key_parts.begin(); it != path.key_parts.end(); ++it) {\n+      parent_key += *it;\n+      parent_key += kSep;\n+      RETURN_NOT_OK(impl_->CreateEmptyObject(path.bucket, parent_key));\n+    }\n+    return Status::OK();\n+  } else {\n+    // Check parent dir exists\n+    if (path.has_parent()) {\n+      S3Path parent_path = path.parent();\n+      bool exists;\n+      RETURN_NOT_OK(impl_->IsNonEmptyDirectory(parent_path, &exists));\n+      if (!exists) {\n+        RETURN_NOT_OK(impl_->IsEmptyDirectory(parent_path, &exists));\n+      }\n+      if (!exists) {\n+        return Status::IOError(\"Cannot create directory '\", path.full_path,\n+                               \"': parent directory does not exist\");\n+      }\n+    }\n+\n+    // XXX Should we check that no non-directory entry exists?\n+    // Minio does it for us, not sure about other S3 implementations.\n+    return impl_->CreateEmptyDir(path.bucket, path.key);\n+  }\n+}\n+\n+Status S3FileSystem::DeleteDir(const std::string& s) {\n+  S3Path path;\n+  RETURN_NOT_OK(S3Path::FromString(s, &path));\n+\n+  if (path.empty()) {\n+    return Status::NotImplemented(\"Cannot delete all S3 buckets\");\n+  }\n+  RETURN_NOT_OK(impl_->DeleteDir(path.bucket, path.key));\n+  if (path.key.empty()) {\n+    // Also delete bucket\n+    S3Model::DeleteBucketRequest req;\n+    req.SetBucket(ToAwsString(path.bucket));\n+    return OutcomeToStatus(impl_->client_->DeleteBucket(req));\n+  }\n \n Review comment:\n   Unclear if we want to allow this function to delete buckets (since these are the \"heaviest\" of S3 objects) by default. It might be better to error when trying to delete a bucket and instead offset a `DeleteBucket` API\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:21.578+0000",
                    "updated": "2019-08-23T22:37:21.578+0000",
                    "started": "2019-08-23T22:37:21.578+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300557",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300558",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317290607\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.h\n ##########\n @@ -0,0 +1,84 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/util/macros.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+struct ARROW_EXPORT S3Options {\n+  // AWS region to connect to\n+  std::string region = \"us-east-1\";\n+\n+  // XXX perhaps instead take a URL like \"http://localhost:9000\"?\n+  // If non-empty, override region with a connect string such as \"localhost:9000\"\n+  std::string endpoint_override;\n+  // Default \"https\"\n+  std::string scheme = \"https\";\n+\n+  std::string access_key;\n+  std::string secret_key;\n \n Review comment:\n   Q: Do we want to provide for anonymous access?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:21.813+0000",
                    "updated": "2019-08-23T22:37:21.813+0000",
                    "started": "2019-08-23T22:37:21.813+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300558",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300559",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317298634\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.cc\n ##########\n @@ -0,0 +1,1100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <algorithm>\n+#include <sstream>\n+#include <utility>\n+\n+#ifdef _WIN32\n+// Undefine preprocessor macros that interfere with AWS function / method names\n+#ifdef GetMessage\n+#undef GetMessage\n+#endif\n+#ifdef GetObject\n+#undef GetObject\n+#endif\n+#endif\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/logging/ConsoleLogSystem.h>\n+#include <aws/core/utils/stream/PreallocatedStreamBuf.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/AbortMultipartUploadRequest.h>\n+#include <aws/s3/model/CompleteMultipartUploadRequest.h>\n+#include <aws/s3/model/CompletedMultipartUpload.h>\n+#include <aws/s3/model/CompletedPart.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/CreateMultipartUploadRequest.h>\n+#include <aws/s3/model/DeleteBucketRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectsRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListBucketsResult.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+#include <aws/s3/model/UploadPartRequest.h>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n \n Review comment:\n   Move this include to the top of the file\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.190+0000",
                    "updated": "2019-08-23T22:37:22.190+0000",
                    "started": "2019-08-23T22:37:22.190+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300559",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300560",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317290496\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.h\n ##########\n @@ -0,0 +1,84 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/util/macros.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+struct ARROW_EXPORT S3Options {\n+  // AWS region to connect to\n+  std::string region = \"us-east-1\";\n \n Review comment:\n   Do you know how boto/boto3, Python s3fs, or other S3 libraries handle setting the region? Do they automatically determine the region for a bucket? \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.196+0000",
                    "updated": "2019-08-23T22:37:22.196+0000",
                    "started": "2019-08-23T22:37:22.196+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300560",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300561",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317285096\n \n \n\n ##########\n File path: .travis.yml\n ##########\n @@ -75,6 +75,7 @@ matrix:\n     - ARROW_TRAVIS_ORC=1\n     - ARROW_TRAVIS_PARQUET=1\n     - ARROW_TRAVIS_PLASMA=1\n+    - ARROW_TRAVIS_S3FS=1\n \n Review comment:\n   Nit: I'd be OK just using \"S3\" in places. but this is okay too\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.200+0000",
                    "updated": "2019-08-23T22:37:22.200+0000",
                    "started": "2019-08-23T22:37:22.200+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300561",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300562",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317289373\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3_internal.h\n ##########\n @@ -0,0 +1,142 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <string>\n+#include <utility>\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/DateTime.h>\n+#include <aws/core/utils/StringUtils.h>\n+\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/string_view.h\"\n+\n+namespace arrow {\n+namespace fs {\n+namespace internal {\n+\n+#define ARROW_AWS_ASSIGN_OR_RAISE_IMPL(outcome_name, lhs, rexpr) \\\n+  auto outcome_name = (rexpr);                                   \\\n+  if (!outcome_name.IsSuccess()) {                               \\\n+    return ErrorToStatus(outcome_name.GetError());               \\\n+  }                                                              \\\n+  lhs = std::move(outcome_name).GetResultWithOwnership();\n+\n+#define ARROW_AWS_ASSIGN_OR_RAISE_NAME(x, y) ARROW_CONCAT(x, y)\n+\n+#define ARROW_AWS_ASSIGN_OR_RAISE(lhs, rexpr) \\\n+  ARROW_AWS_ASSIGN_OR_RAISE_IMPL(             \\\n+      ARROW_AWS_ASSIGN_OR_RAISE_NAME(_aws_error_or_value, __COUNTER__), lhs, rexpr);\n+\n+template <typename Error>\n+inline bool IsConnectError(const Aws::Client::AWSError<Error>& error) {\n+  if (error.GetErrorType() == Aws::Client::CoreErrors::NETWORK_CONNECTION) {\n+    return true;\n+  }\n+  // Sometimes Minio may fail with a 503 error\n+  // (exception name: XMinioServerNotInitialized,\n+  //  message: \"Server not initialized, please try again\")\n+  auto http_code = static_cast<int>(error.GetResponseCode());\n+  switch (http_code) {\n+    case 502:  // Bad gateway\n+    case 503:  // Service unavailable\n+    case 504:  // Gateway timeout\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+inline bool IsNotFound(const Aws::Client::AWSError<Aws::S3::S3Errors>& error) {\n+  const auto error_type = error.GetErrorType();\n+  return (error_type == Aws::S3::S3Errors::NO_SUCH_BUCKET ||\n+          error_type == Aws::S3::S3Errors::RESOURCE_NOT_FOUND);\n+}\n+\n+inline bool IsAlreadyExists(const Aws::Client::AWSError<Aws::S3::S3Errors>& error) {\n+  const auto error_type = error.GetErrorType();\n+  return (error_type == Aws::S3::S3Errors::BUCKET_ALREADY_EXISTS ||\n+          error_type == Aws::S3::S3Errors::BUCKET_ALREADY_OWNED_BY_YOU);\n+}\n+\n+template <typename ErrorType>\n+inline Status ErrorToStatus(const Aws::Client::AWSError<ErrorType>& error) {\n+  // XXX Handle fine-grained error types\n+  // See\n+  // https://sdk.amazonaws.com/cpp/api/LATEST/namespace_aws_1_1_s3.html#ae3f82f8132b619b6e91c88a9f1bde371\n+  return Status::IOError(\"AWS Error [code \", static_cast<int>(error.GetErrorType()),\n+                         \"]: \", error.GetMessage());\n+}\n+\n+template <typename Result, typename Error>\n+inline Status OutcomeToStatus(const Aws::Utils::Outcome<Result, Error>& outcome) {\n+  if (outcome.IsSuccess()) {\n+    return Status::OK();\n+  } else {\n+    return ErrorToStatus(outcome.GetError());\n+  }\n+}\n+\n+inline Aws::String ToAwsString(const std::string& s) {\n+  // Direct construction of Aws::String from std::string doesn't work because\n+  // it uses a specific Allocator class.\n+  return Aws::String(s.begin(), s.end());\n+}\n+\n+inline util::string_view FromAwsString(const Aws::String& s) {\n+  return {s.data(), s.length()};\n+}\n+\n+inline Aws::String ToURLEncodedAwsString(const std::string& s) {\n+  return Aws::Utils::StringUtils::URLEncode(s.data());\n+}\n+\n+inline TimePoint ToTimePoint(const Aws::Utils::DateTime& dt) {\n+  return std::chrono::time_point_cast<std::chrono::nanoseconds>(dt.UnderlyingTimestamp());\n+}\n+\n+// A connect retry strategy with a controlled max duration.\n+\n+class ConnectRetryStrategy : public Aws::Client::RetryStrategy {\n+ public:\n+  static const int32_t kMaxRetryDuration = 4000; /* milliseconds */\n+  static const int32_t kRetryInterval = 200;     /* milliseconds */\n+\n+  bool ShouldRetry(const Aws::Client::AWSError<Aws::Client::CoreErrors>& error,\n+                   long attemptedRetries) const override {  // NOLINT\n \n Review comment:\n   nit: snake_case\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.258+0000",
                    "updated": "2019-08-23T22:37:22.258+0000",
                    "started": "2019-08-23T22:37:22.257+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300562",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300563",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317299164\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.cc\n ##########\n @@ -0,0 +1,1100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <algorithm>\n+#include <sstream>\n+#include <utility>\n+\n+#ifdef _WIN32\n+// Undefine preprocessor macros that interfere with AWS function / method names\n+#ifdef GetMessage\n+#undef GetMessage\n+#endif\n+#ifdef GetObject\n+#undef GetObject\n+#endif\n+#endif\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/logging/ConsoleLogSystem.h>\n+#include <aws/core/utils/stream/PreallocatedStreamBuf.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/AbortMultipartUploadRequest.h>\n+#include <aws/s3/model/CompleteMultipartUploadRequest.h>\n+#include <aws/s3/model/CompletedMultipartUpload.h>\n+#include <aws/s3/model/CompletedPart.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/CreateMultipartUploadRequest.h>\n+#include <aws/s3/model/DeleteBucketRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectsRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListBucketsResult.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+#include <aws/s3/model/UploadPartRequest.h>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::Aws::Client::AWSError;\n+using ::Aws::S3::S3Errors;\n+namespace S3Model = Aws::S3::Model;\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::ErrorToStatus;\n+using ::arrow::fs::internal::FromAwsString;\n+using ::arrow::fs::internal::IsAlreadyExists;\n+using ::arrow::fs::internal::IsNotFound;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+using ::arrow::fs::internal::ToTimePoint;\n+using ::arrow::fs::internal::ToURLEncodedAwsString;\n+\n+static const char kSep = '/';\n+\n+namespace {\n+\n+util::string_view RemoveTrailingSeparator(util::string_view key) {\n+  // Remove trailing separator\n+  if (!key.empty() && key.back() == kSep) {\n+    key.remove_suffix(1);\n+  }\n+  return key;\n+}\n \n Review comment:\n   Seems like could be reusable utility code (perhaps with the sep as a parameter)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.289+0000",
                    "updated": "2019-08-23T22:37:22.289+0000",
                    "started": "2019-08-23T22:37:22.289+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300563",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300564",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317286074\n \n \n\n ##########\n File path: cpp/cmake_modules/DefineOptions.cmake\n ##########\n @@ -174,6 +174,9 @@ if(\"${CMAKE_SOURCE_DIR}\" STREQUAL \"${CMAKE_CURRENT_SOURCE_DIR}\")\n \n   define_option(ARROW_JSON \"Build Arrow with JSON support (requires RapidJSON)\" ON)\n \n+  define_option(ARROW_S3FS \"Build Arrow with S3FS support (requires the AWS SDK for C++)\"\n \n Review comment:\n   I'm torn on what to use instead of \"S3FS\" in documentation, since this supports any \"Amazon S3-compatible\" filesystem API. For the build option, `ARROW_S3` might be slightly better.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.292+0000",
                    "updated": "2019-08-23T22:37:22.292+0000",
                    "started": "2019-08-23T22:37:22.291+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300564",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300565",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317303005\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.cc\n ##########\n @@ -0,0 +1,1100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <algorithm>\n+#include <sstream>\n+#include <utility>\n+\n+#ifdef _WIN32\n+// Undefine preprocessor macros that interfere with AWS function / method names\n+#ifdef GetMessage\n+#undef GetMessage\n+#endif\n+#ifdef GetObject\n+#undef GetObject\n+#endif\n+#endif\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/logging/ConsoleLogSystem.h>\n+#include <aws/core/utils/stream/PreallocatedStreamBuf.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/AbortMultipartUploadRequest.h>\n+#include <aws/s3/model/CompleteMultipartUploadRequest.h>\n+#include <aws/s3/model/CompletedMultipartUpload.h>\n+#include <aws/s3/model/CompletedPart.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/CreateMultipartUploadRequest.h>\n+#include <aws/s3/model/DeleteBucketRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectsRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListBucketsResult.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+#include <aws/s3/model/UploadPartRequest.h>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::Aws::Client::AWSError;\n+using ::Aws::S3::S3Errors;\n+namespace S3Model = Aws::S3::Model;\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::ErrorToStatus;\n+using ::arrow::fs::internal::FromAwsString;\n+using ::arrow::fs::internal::IsAlreadyExists;\n+using ::arrow::fs::internal::IsNotFound;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+using ::arrow::fs::internal::ToTimePoint;\n+using ::arrow::fs::internal::ToURLEncodedAwsString;\n+\n+static const char kSep = '/';\n+\n+namespace {\n+\n+util::string_view RemoveTrailingSeparator(util::string_view key) {\n+  // Remove trailing separator\n+  if (!key.empty() && key.back() == kSep) {\n+    key.remove_suffix(1);\n+  }\n+  return key;\n+}\n+\n+// XXX Sanitize paths by removing leading slash?\n+\n+struct S3Path {\n+  std::string full_path;\n+  std::string bucket;\n+  std::string key;\n+  std::vector<std::string> key_parts;\n+\n+  static Status FromString(const std::string& s, S3Path* out) {\n+    auto first_sep = s.find_first_of(kSep);\n+    if (first_sep == 0) {\n+      return Status::Invalid(\"Path cannot start with a separator ('\", s, \"')\");\n+    }\n+    if (first_sep == std::string::npos) {\n+      *out = {s, s, \"\", {}};\n+      return Status::OK();\n+    }\n+    out->full_path = s;\n+    out->bucket = s.substr(0, first_sep);\n+    out->key = s.substr(first_sep + 1);\n+    out->key_parts = internal::SplitAbstractPath(out->key);\n+    return internal::ValidateAbstractPathParts(out->key_parts);\n+  }\n+\n+  Aws::String ToURLEncodedAwsString() const {\n+    // URL-encode individual parts, not the '/' separator\n+    Aws::String res;\n+    res += internal::ToURLEncodedAwsString(bucket);\n+    for (const auto& part : key_parts) {\n+      res += kSep;\n+      res += internal::ToURLEncodedAwsString(part);\n+    }\n+    return res;\n+  }\n+\n+  S3Path parent() const {\n+    DCHECK(!key_parts.empty());\n+    auto parent = S3Path{\"\", bucket, \"\", key_parts};\n+    parent.key_parts.pop_back();\n+    parent.key = internal::JoinAbstractPath(parent.key_parts);\n+    parent.full_path = parent.bucket + kSep + parent.key;\n+    return parent;\n+  }\n+\n+  bool has_parent() const { return !key.empty(); }\n+\n+  bool empty() const { return bucket.empty() && key.empty(); }\n+\n+  bool operator==(const S3Path& other) const {\n+    return bucket == other.bucket && key == other.key;\n+  }\n+};\n+\n+// XXX return in OutcomeToStatus instead?\n+Status PathNotFound(const S3Path& path) {\n+  return Status::IOError(\"Path does not exist '\", path.full_path, \"'\");\n+}\n+\n+Status PathNotFound(const std::string& bucket, const std::string& key) {\n+  return Status::IOError(\"Path does not exist '\", bucket, kSep, key, \"'\");\n+}\n+\n+// Status NotADir(const S3Path& path) {\n+//   return Status::IOError(\"Not a directory: '\", path.full_path, \"'\");\n+// }\n+\n+Status NotAFile(const S3Path& path) {\n+  return Status::IOError(\"Not a regular file: '\", path.full_path, \"'\");\n+}\n+\n+Status ValidateFilePath(const S3Path& path) {\n+  if (path.bucket.empty() || path.key.empty()) {\n+    return NotAFile(path);\n+  }\n+  return Status::OK();\n+}\n+\n+std::string FormatRange(int64_t start, int64_t length) {\n+  // Format a HTTP range header value\n+  std::stringstream ss;\n+  ss << \"bytes=\" << start << \"-\" << start + length - 1;\n+  return ss.str();\n+}\n+\n+Status GetObjectRange(Aws::S3::S3Client* client, const S3Path& path, int64_t start,\n+                      int64_t length, S3Model::GetObjectResult* out) {\n+  S3Model::GetObjectRequest req;\n+  req.SetBucket(ToAwsString(path.bucket));\n+  req.SetKey(ToAwsString(path.key));\n+  req.SetRange(ToAwsString(FormatRange(start, length)));\n+  ARROW_AWS_ASSIGN_OR_RAISE(*out, client->GetObject(req));\n+  return Status::OK();\n+}\n+\n+// A RandomAccessFile that reads from a S3 object\n+class ObjectInputFile : public io::RandomAccessFile {\n+ public:\n+  ObjectInputFile(Aws::S3::S3Client* client, const S3Path& path)\n+      : client_(client), path_(path) {}\n+\n+  ~ObjectInputFile() override {}\n \n Review comment:\n   Can this be removed?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.299+0000",
                    "updated": "2019-08-23T22:37:22.299+0000",
                    "started": "2019-08-23T22:37:22.298+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300565",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300566",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317286247\n \n \n\n ##########\n File path: cpp/cmake_modules/ThirdpartyToolchain.cmake\n ##########\n @@ -2374,6 +2384,79 @@ if(ARROW_ORC)\n   message(STATUS \"Found ORC headers: ${ORC_INCLUDE_DIR}\")\n endif()\n \n+# ----------------------------------------------------------------------\n+# AWS SDK for C++\n+\n+macro(build_awssdk)\n+  message(\n+    FATAL_ERROR \"FIXME: Building AWS C++ SDK from source will link with wrong libcrypto\")\n \n Review comment:\n   That's super fun. \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.377+0000",
                    "updated": "2019-08-23T22:37:22.377+0000",
                    "started": "2019-08-23T22:37:22.376+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300566",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300567",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317299813\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.cc\n ##########\n @@ -0,0 +1,1100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <algorithm>\n+#include <sstream>\n+#include <utility>\n+\n+#ifdef _WIN32\n+// Undefine preprocessor macros that interfere with AWS function / method names\n+#ifdef GetMessage\n+#undef GetMessage\n+#endif\n+#ifdef GetObject\n+#undef GetObject\n+#endif\n+#endif\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/logging/ConsoleLogSystem.h>\n+#include <aws/core/utils/stream/PreallocatedStreamBuf.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/AbortMultipartUploadRequest.h>\n+#include <aws/s3/model/CompleteMultipartUploadRequest.h>\n+#include <aws/s3/model/CompletedMultipartUpload.h>\n+#include <aws/s3/model/CompletedPart.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/CreateMultipartUploadRequest.h>\n+#include <aws/s3/model/DeleteBucketRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectsRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListBucketsResult.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+#include <aws/s3/model/UploadPartRequest.h>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::Aws::Client::AWSError;\n+using ::Aws::S3::S3Errors;\n+namespace S3Model = Aws::S3::Model;\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::ErrorToStatus;\n+using ::arrow::fs::internal::FromAwsString;\n+using ::arrow::fs::internal::IsAlreadyExists;\n+using ::arrow::fs::internal::IsNotFound;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+using ::arrow::fs::internal::ToTimePoint;\n+using ::arrow::fs::internal::ToURLEncodedAwsString;\n+\n+static const char kSep = '/';\n+\n+namespace {\n+\n+util::string_view RemoveTrailingSeparator(util::string_view key) {\n+  // Remove trailing separator\n+  if (!key.empty() && key.back() == kSep) {\n+    key.remove_suffix(1);\n+  }\n+  return key;\n+}\n+\n+// XXX Sanitize paths by removing leading slash?\n \n Review comment:\n   Or even removing `s3://` if it happens to be there\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.873+0000",
                    "updated": "2019-08-23T22:37:22.873+0000",
                    "started": "2019-08-23T22:37:22.872+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300567",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300568",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317305242\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.cc\n ##########\n @@ -0,0 +1,1100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <algorithm>\n+#include <sstream>\n+#include <utility>\n+\n+#ifdef _WIN32\n+// Undefine preprocessor macros that interfere with AWS function / method names\n+#ifdef GetMessage\n+#undef GetMessage\n+#endif\n+#ifdef GetObject\n+#undef GetObject\n+#endif\n+#endif\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/logging/ConsoleLogSystem.h>\n+#include <aws/core/utils/stream/PreallocatedStreamBuf.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/AbortMultipartUploadRequest.h>\n+#include <aws/s3/model/CompleteMultipartUploadRequest.h>\n+#include <aws/s3/model/CompletedMultipartUpload.h>\n+#include <aws/s3/model/CompletedPart.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/CreateMultipartUploadRequest.h>\n+#include <aws/s3/model/DeleteBucketRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectsRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListBucketsResult.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+#include <aws/s3/model/UploadPartRequest.h>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::Aws::Client::AWSError;\n+using ::Aws::S3::S3Errors;\n+namespace S3Model = Aws::S3::Model;\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::ErrorToStatus;\n+using ::arrow::fs::internal::FromAwsString;\n+using ::arrow::fs::internal::IsAlreadyExists;\n+using ::arrow::fs::internal::IsNotFound;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+using ::arrow::fs::internal::ToTimePoint;\n+using ::arrow::fs::internal::ToURLEncodedAwsString;\n+\n+static const char kSep = '/';\n+\n+namespace {\n+\n+util::string_view RemoveTrailingSeparator(util::string_view key) {\n+  // Remove trailing separator\n+  if (!key.empty() && key.back() == kSep) {\n+    key.remove_suffix(1);\n+  }\n+  return key;\n+}\n+\n+// XXX Sanitize paths by removing leading slash?\n+\n+struct S3Path {\n+  std::string full_path;\n+  std::string bucket;\n+  std::string key;\n+  std::vector<std::string> key_parts;\n+\n+  static Status FromString(const std::string& s, S3Path* out) {\n+    auto first_sep = s.find_first_of(kSep);\n+    if (first_sep == 0) {\n+      return Status::Invalid(\"Path cannot start with a separator ('\", s, \"')\");\n+    }\n+    if (first_sep == std::string::npos) {\n+      *out = {s, s, \"\", {}};\n+      return Status::OK();\n+    }\n+    out->full_path = s;\n+    out->bucket = s.substr(0, first_sep);\n+    out->key = s.substr(first_sep + 1);\n+    out->key_parts = internal::SplitAbstractPath(out->key);\n+    return internal::ValidateAbstractPathParts(out->key_parts);\n+  }\n+\n+  Aws::String ToURLEncodedAwsString() const {\n+    // URL-encode individual parts, not the '/' separator\n+    Aws::String res;\n+    res += internal::ToURLEncodedAwsString(bucket);\n+    for (const auto& part : key_parts) {\n+      res += kSep;\n+      res += internal::ToURLEncodedAwsString(part);\n+    }\n+    return res;\n+  }\n+\n+  S3Path parent() const {\n+    DCHECK(!key_parts.empty());\n+    auto parent = S3Path{\"\", bucket, \"\", key_parts};\n+    parent.key_parts.pop_back();\n+    parent.key = internal::JoinAbstractPath(parent.key_parts);\n+    parent.full_path = parent.bucket + kSep + parent.key;\n+    return parent;\n+  }\n+\n+  bool has_parent() const { return !key.empty(); }\n+\n+  bool empty() const { return bucket.empty() && key.empty(); }\n+\n+  bool operator==(const S3Path& other) const {\n+    return bucket == other.bucket && key == other.key;\n+  }\n+};\n+\n+// XXX return in OutcomeToStatus instead?\n+Status PathNotFound(const S3Path& path) {\n+  return Status::IOError(\"Path does not exist '\", path.full_path, \"'\");\n+}\n+\n+Status PathNotFound(const std::string& bucket, const std::string& key) {\n+  return Status::IOError(\"Path does not exist '\", bucket, kSep, key, \"'\");\n+}\n+\n+// Status NotADir(const S3Path& path) {\n+//   return Status::IOError(\"Not a directory: '\", path.full_path, \"'\");\n+// }\n+\n+Status NotAFile(const S3Path& path) {\n+  return Status::IOError(\"Not a regular file: '\", path.full_path, \"'\");\n+}\n+\n+Status ValidateFilePath(const S3Path& path) {\n+  if (path.bucket.empty() || path.key.empty()) {\n+    return NotAFile(path);\n+  }\n+  return Status::OK();\n+}\n+\n+std::string FormatRange(int64_t start, int64_t length) {\n+  // Format a HTTP range header value\n+  std::stringstream ss;\n+  ss << \"bytes=\" << start << \"-\" << start + length - 1;\n+  return ss.str();\n+}\n+\n+Status GetObjectRange(Aws::S3::S3Client* client, const S3Path& path, int64_t start,\n+                      int64_t length, S3Model::GetObjectResult* out) {\n+  S3Model::GetObjectRequest req;\n+  req.SetBucket(ToAwsString(path.bucket));\n+  req.SetKey(ToAwsString(path.key));\n+  req.SetRange(ToAwsString(FormatRange(start, length)));\n+  ARROW_AWS_ASSIGN_OR_RAISE(*out, client->GetObject(req));\n+  return Status::OK();\n+}\n+\n+// A RandomAccessFile that reads from a S3 object\n+class ObjectInputFile : public io::RandomAccessFile {\n+ public:\n+  ObjectInputFile(Aws::S3::S3Client* client, const S3Path& path)\n+      : client_(client), path_(path) {}\n+\n+  ~ObjectInputFile() override {}\n+\n+  Status Init() {\n+    // Issue a HEAD Object to get the content-length and ensure any\n+    // errors (e.g. file not found) don't wait until the first Read() call.\n+    S3Model::HeadObjectRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+\n+    auto outcome = client_->HeadObject(req);\n+    if (!outcome.IsSuccess()) {\n+      if (IsNotFound(outcome.GetError())) {\n+        return PathNotFound(path_);\n+      } else {\n+        return ErrorToStatus(outcome.GetError());\n+      }\n+    }\n+    content_length_ = outcome.GetResult().GetContentLength();\n+    return Status::OK();\n+  }\n+\n+  Status CheckClosed() const {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  // RandomAccessFile APIs\n+\n+  Status Close() override {\n+    closed_ = true;\n+    return Status::OK();\n+  }\n+\n+  bool closed() const override { return closed_; }\n+\n+  Status Tell(int64_t* position) const override {\n+    RETURN_NOT_OK(CheckClosed());\n+\n+    *position = pos_;\n+    return Status::OK();\n+  }\n+\n+  Status GetSize(int64_t* size) override {\n+    RETURN_NOT_OK(CheckClosed());\n+\n+    *size = content_length_;\n+    return Status::OK();\n+  }\n+\n+  Status Seek(int64_t position) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return Status::OK();\n+  }\n+\n+  Status ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                void* out) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // Read the desired range of bytes\n+    S3Model::GetObjectResult result;\n+    RETURN_NOT_OK(GetObjectRange(client_, path_, position, nbytes, &result));\n+\n+    auto& stream = result.GetBody();\n+    stream.read(reinterpret_cast<char*>(out), nbytes);\n+    // NOTE: the stream is a stringstream by default, there is no actual error\n+    // to check for.  However, stream.fail() may return true if EOF is reached.\n+    *bytes_read = stream.gcount();\n+    return Status::OK();\n+  }\n+\n+  Status ReadAt(int64_t position, int64_t nbytes, std::shared_ptr<Buffer>* out) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    std::shared_ptr<ResizableBuffer> buf;\n+    int64_t bytes_read;\n+    RETURN_NOT_OK(AllocateResizableBuffer(nbytes, &buf));\n+    if (nbytes > 0) {\n+      RETURN_NOT_OK(ReadAt(position, nbytes, &bytes_read, buf->mutable_data()));\n+      DCHECK_LE(bytes_read, nbytes);\n+      RETURN_NOT_OK(buf->Resize(bytes_read));\n+    }\n+    *out = std::move(buf);\n+    return Status::OK();\n+  }\n+\n+  Status Read(int64_t nbytes, int64_t* bytes_read, void* out) override {\n+    RETURN_NOT_OK(ReadAt(pos_, nbytes, bytes_read, out));\n+    pos_ += *bytes_read;\n+    return Status::OK();\n+  }\n+\n+  Status Read(int64_t nbytes, std::shared_ptr<Buffer>* out) override {\n+    RETURN_NOT_OK(ReadAt(pos_, nbytes, out));\n+    pos_ += (*out)->size();\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Aws::S3::S3Client* client_;\n+  S3Path path_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+};\n+\n+// A non-copying istream.\n+// See https://stackoverflow.com/questions/35322033/aws-c-sdk-uploadpart-times-out\n+// https://stackoverflow.com/questions/13059091/creating-an-input-stream-from-constant-memory\n+\n+class StringViewStream : Aws::Utils::Stream::PreallocatedStreamBuf, public std::iostream {\n+ public:\n+  StringViewStream(const void* data, int64_t nbytes)\n+      : Aws::Utils::Stream::PreallocatedStreamBuf(\n+            reinterpret_cast<unsigned char*>(const_cast<void*>(data)),\n+            static_cast<size_t>(nbytes)),\n+        std::iostream(this) {}\n+};\n+\n+// Minimum size for each part of a multipart upload, except for the last part.\n+// AWS doc says \"5 MB\" but it's not clear whether those are MB or MiB,\n+// so I chose the safer value.\n \n Review comment:\n   I would guess that the 5 * 2^20 interpretation is what they mean\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.878+0000",
                    "updated": "2019-08-23T22:37:22.878+0000",
                    "started": "2019-08-23T22:37:22.876+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300568",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300569",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317306366\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.cc\n ##########\n @@ -0,0 +1,1100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <algorithm>\n+#include <sstream>\n+#include <utility>\n+\n+#ifdef _WIN32\n+// Undefine preprocessor macros that interfere with AWS function / method names\n+#ifdef GetMessage\n+#undef GetMessage\n+#endif\n+#ifdef GetObject\n+#undef GetObject\n+#endif\n+#endif\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/logging/ConsoleLogSystem.h>\n+#include <aws/core/utils/stream/PreallocatedStreamBuf.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/AbortMultipartUploadRequest.h>\n+#include <aws/s3/model/CompleteMultipartUploadRequest.h>\n+#include <aws/s3/model/CompletedMultipartUpload.h>\n+#include <aws/s3/model/CompletedPart.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/CreateMultipartUploadRequest.h>\n+#include <aws/s3/model/DeleteBucketRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectsRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListBucketsResult.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+#include <aws/s3/model/UploadPartRequest.h>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::Aws::Client::AWSError;\n+using ::Aws::S3::S3Errors;\n+namespace S3Model = Aws::S3::Model;\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::ErrorToStatus;\n+using ::arrow::fs::internal::FromAwsString;\n+using ::arrow::fs::internal::IsAlreadyExists;\n+using ::arrow::fs::internal::IsNotFound;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+using ::arrow::fs::internal::ToTimePoint;\n+using ::arrow::fs::internal::ToURLEncodedAwsString;\n+\n+static const char kSep = '/';\n+\n+namespace {\n+\n+util::string_view RemoveTrailingSeparator(util::string_view key) {\n+  // Remove trailing separator\n+  if (!key.empty() && key.back() == kSep) {\n+    key.remove_suffix(1);\n+  }\n+  return key;\n+}\n+\n+// XXX Sanitize paths by removing leading slash?\n+\n+struct S3Path {\n+  std::string full_path;\n+  std::string bucket;\n+  std::string key;\n+  std::vector<std::string> key_parts;\n+\n+  static Status FromString(const std::string& s, S3Path* out) {\n+    auto first_sep = s.find_first_of(kSep);\n+    if (first_sep == 0) {\n+      return Status::Invalid(\"Path cannot start with a separator ('\", s, \"')\");\n+    }\n+    if (first_sep == std::string::npos) {\n+      *out = {s, s, \"\", {}};\n+      return Status::OK();\n+    }\n+    out->full_path = s;\n+    out->bucket = s.substr(0, first_sep);\n+    out->key = s.substr(first_sep + 1);\n+    out->key_parts = internal::SplitAbstractPath(out->key);\n+    return internal::ValidateAbstractPathParts(out->key_parts);\n+  }\n+\n+  Aws::String ToURLEncodedAwsString() const {\n+    // URL-encode individual parts, not the '/' separator\n+    Aws::String res;\n+    res += internal::ToURLEncodedAwsString(bucket);\n+    for (const auto& part : key_parts) {\n+      res += kSep;\n+      res += internal::ToURLEncodedAwsString(part);\n+    }\n+    return res;\n+  }\n+\n+  S3Path parent() const {\n+    DCHECK(!key_parts.empty());\n+    auto parent = S3Path{\"\", bucket, \"\", key_parts};\n+    parent.key_parts.pop_back();\n+    parent.key = internal::JoinAbstractPath(parent.key_parts);\n+    parent.full_path = parent.bucket + kSep + parent.key;\n+    return parent;\n+  }\n+\n+  bool has_parent() const { return !key.empty(); }\n+\n+  bool empty() const { return bucket.empty() && key.empty(); }\n+\n+  bool operator==(const S3Path& other) const {\n+    return bucket == other.bucket && key == other.key;\n+  }\n+};\n+\n+// XXX return in OutcomeToStatus instead?\n+Status PathNotFound(const S3Path& path) {\n+  return Status::IOError(\"Path does not exist '\", path.full_path, \"'\");\n+}\n+\n+Status PathNotFound(const std::string& bucket, const std::string& key) {\n+  return Status::IOError(\"Path does not exist '\", bucket, kSep, key, \"'\");\n+}\n+\n+// Status NotADir(const S3Path& path) {\n+//   return Status::IOError(\"Not a directory: '\", path.full_path, \"'\");\n+// }\n+\n+Status NotAFile(const S3Path& path) {\n+  return Status::IOError(\"Not a regular file: '\", path.full_path, \"'\");\n+}\n+\n+Status ValidateFilePath(const S3Path& path) {\n+  if (path.bucket.empty() || path.key.empty()) {\n+    return NotAFile(path);\n+  }\n+  return Status::OK();\n+}\n+\n+std::string FormatRange(int64_t start, int64_t length) {\n+  // Format a HTTP range header value\n+  std::stringstream ss;\n+  ss << \"bytes=\" << start << \"-\" << start + length - 1;\n+  return ss.str();\n+}\n+\n+Status GetObjectRange(Aws::S3::S3Client* client, const S3Path& path, int64_t start,\n+                      int64_t length, S3Model::GetObjectResult* out) {\n+  S3Model::GetObjectRequest req;\n+  req.SetBucket(ToAwsString(path.bucket));\n+  req.SetKey(ToAwsString(path.key));\n+  req.SetRange(ToAwsString(FormatRange(start, length)));\n+  ARROW_AWS_ASSIGN_OR_RAISE(*out, client->GetObject(req));\n+  return Status::OK();\n+}\n+\n+// A RandomAccessFile that reads from a S3 object\n+class ObjectInputFile : public io::RandomAccessFile {\n+ public:\n+  ObjectInputFile(Aws::S3::S3Client* client, const S3Path& path)\n+      : client_(client), path_(path) {}\n+\n+  ~ObjectInputFile() override {}\n+\n+  Status Init() {\n+    // Issue a HEAD Object to get the content-length and ensure any\n+    // errors (e.g. file not found) don't wait until the first Read() call.\n+    S3Model::HeadObjectRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+\n+    auto outcome = client_->HeadObject(req);\n+    if (!outcome.IsSuccess()) {\n+      if (IsNotFound(outcome.GetError())) {\n+        return PathNotFound(path_);\n+      } else {\n+        return ErrorToStatus(outcome.GetError());\n+      }\n+    }\n+    content_length_ = outcome.GetResult().GetContentLength();\n+    return Status::OK();\n+  }\n+\n+  Status CheckClosed() const {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  // RandomAccessFile APIs\n+\n+  Status Close() override {\n+    closed_ = true;\n+    return Status::OK();\n+  }\n+\n+  bool closed() const override { return closed_; }\n+\n+  Status Tell(int64_t* position) const override {\n+    RETURN_NOT_OK(CheckClosed());\n+\n+    *position = pos_;\n+    return Status::OK();\n+  }\n+\n+  Status GetSize(int64_t* size) override {\n+    RETURN_NOT_OK(CheckClosed());\n+\n+    *size = content_length_;\n+    return Status::OK();\n+  }\n+\n+  Status Seek(int64_t position) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return Status::OK();\n+  }\n+\n+  Status ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                void* out) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // Read the desired range of bytes\n+    S3Model::GetObjectResult result;\n+    RETURN_NOT_OK(GetObjectRange(client_, path_, position, nbytes, &result));\n+\n+    auto& stream = result.GetBody();\n+    stream.read(reinterpret_cast<char*>(out), nbytes);\n+    // NOTE: the stream is a stringstream by default, there is no actual error\n+    // to check for.  However, stream.fail() may return true if EOF is reached.\n+    *bytes_read = stream.gcount();\n+    return Status::OK();\n+  }\n+\n+  Status ReadAt(int64_t position, int64_t nbytes, std::shared_ptr<Buffer>* out) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    std::shared_ptr<ResizableBuffer> buf;\n+    int64_t bytes_read;\n+    RETURN_NOT_OK(AllocateResizableBuffer(nbytes, &buf));\n+    if (nbytes > 0) {\n+      RETURN_NOT_OK(ReadAt(position, nbytes, &bytes_read, buf->mutable_data()));\n+      DCHECK_LE(bytes_read, nbytes);\n+      RETURN_NOT_OK(buf->Resize(bytes_read));\n+    }\n+    *out = std::move(buf);\n+    return Status::OK();\n+  }\n+\n+  Status Read(int64_t nbytes, int64_t* bytes_read, void* out) override {\n+    RETURN_NOT_OK(ReadAt(pos_, nbytes, bytes_read, out));\n+    pos_ += *bytes_read;\n+    return Status::OK();\n+  }\n+\n+  Status Read(int64_t nbytes, std::shared_ptr<Buffer>* out) override {\n+    RETURN_NOT_OK(ReadAt(pos_, nbytes, out));\n+    pos_ += (*out)->size();\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Aws::S3::S3Client* client_;\n+  S3Path path_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+};\n+\n+// A non-copying istream.\n+// See https://stackoverflow.com/questions/35322033/aws-c-sdk-uploadpart-times-out\n+// https://stackoverflow.com/questions/13059091/creating-an-input-stream-from-constant-memory\n+\n+class StringViewStream : Aws::Utils::Stream::PreallocatedStreamBuf, public std::iostream {\n+ public:\n+  StringViewStream(const void* data, int64_t nbytes)\n+      : Aws::Utils::Stream::PreallocatedStreamBuf(\n+            reinterpret_cast<unsigned char*>(const_cast<void*>(data)),\n+            static_cast<size_t>(nbytes)),\n+        std::iostream(this) {}\n+};\n+\n+// Minimum size for each part of a multipart upload, except for the last part.\n+// AWS doc says \"5 MB\" but it's not clear whether those are MB or MiB,\n+// so I chose the safer value.\n+// (see https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)\n+static constexpr int64_t kMinimumPartUpload = 5 * 1024 * 1024;\n+\n+// An OutputStream that writes to a S3 object\n+class ObjectOutputStream : public io::OutputStream {\n+ public:\n+  ObjectOutputStream(Aws::S3::S3Client* client, const S3Path& path)\n+      : client_(client), path_(path) {}\n+\n+  ~ObjectOutputStream() override {\n+    if (!closed_) {\n+      auto st = Abort();\n+      if (!st.ok()) {\n+        ARROW_LOG(ERROR) << \"Could not abort multipart upload: \" << st;\n+      }\n+    }\n+  }\n+\n+  Status Init() {\n+    // Initiate the multi-part upload\n+    S3Model::CreateMultipartUploadRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+\n+    auto outcome = client_->CreateMultipartUpload(req);\n+    if (!outcome.IsSuccess()) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+    upload_id_ = outcome.GetResult().GetUploadId();\n+    closed_ = false;\n+    return Status::OK();\n+  }\n+\n+  Status Abort() {\n+    S3Model::AbortMultipartUploadRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+    req.SetUploadId(upload_id_);\n+\n+    auto outcome = client_->AbortMultipartUpload(req);\n+    if (!outcome.IsSuccess()) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+    current_part_.reset();\n+    closed_ = true;\n+    return Status::OK();\n+  }\n+\n+  // OutputStream interface\n+\n+  Status Close() override {\n+    if (closed_) {\n+      return Status::OK();\n+    }\n+\n+    if (current_part_) {\n+      // Upload last part\n+      RETURN_NOT_OK(CommitCurrentPart());\n+    }\n+\n+    // S3 mandates at least one part, upload an empty one if necessary\n+    if (completed_upload_.GetParts().empty()) {\n+      RETURN_NOT_OK(UploadPart(\"\", 0));\n+    }\n+    DCHECK(completed_upload_.PartsHasBeenSet());\n+\n+    S3Model::CompleteMultipartUploadRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+    req.SetUploadId(upload_id_);\n+    req.SetMultipartUpload(completed_upload_);\n+\n+    auto outcome = client_->CompleteMultipartUpload(req);\n+    if (!outcome.IsSuccess()) {\n+      return ErrorToStatus(outcome.GetError());\n+    }\n+\n+    closed_ = true;\n+    return Status::OK();\n+  }\n+\n+  bool closed() const override { return closed_; }\n+\n+  Status Tell(int64_t* position) const override {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+    *position = pos_;\n+    return Status::OK();\n+  }\n+\n+  Status Write(const void* data, int64_t nbytes) override {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+\n+    // With up to 10000 parts in an upload (S3 limit), a stream writing chunks\n+    // of exactly 5MB would be limited to 50GB total.  To avoid that, we bump\n+    // the upload threshold every 100 parts.  So the pattern is:\n+    // - part 1 to 99: 5MB threshold\n+    // - part 100 to 199: 10MB threshold\n+    // - part 200 to 299: 15MB threshold\n+    // ...\n+    // - part 9900 to 9999: 500MB threshold\n+    // So the total size limit is 2475000MB or ~2.4TB, while keeping manageable\n+    // chunk sizes and avoiding too much buffering in the common case of a small-ish\n+    // stream.  If the limit's not enough, we can revisit.\n+    if (part_number_ % 100 == 0) {\n+      part_upload_threshold_ += kMinimumPartUpload;\n+    }\n+\n+    if (!current_part_ && nbytes >= part_upload_threshold_) {\n+      // No current part and data large enough, upload it directly without copying\n+      RETURN_NOT_OK(UploadPart(data, nbytes));\n+      pos_ += nbytes;\n+      return Status::OK();\n+    }\n+    // Can't upload data on its own, need to buffer it\n+    if (!current_part_) {\n+      RETURN_NOT_OK(io::BufferOutputStream::Create(\n+          part_upload_threshold_, default_memory_pool(), &current_part_));\n+      current_part_size_ = 0;\n+    }\n+    RETURN_NOT_OK(current_part_->Write(data, nbytes));\n+    pos_ += nbytes;\n+    current_part_size_ += nbytes;\n+\n+    if (current_part_size_ >= part_upload_threshold_) {\n+      // Current part large enough, upload it\n+      RETURN_NOT_OK(CommitCurrentPart());\n \n Review comment:\n   Aside: with S3 performance being always a wildcard, we should open some JIRAs about making the part uploads \"less blocking\". When the data is buffered, this is safe to do, while when unbuffered it may not be, but in any case we can discuss more there. Empirical read/write performance with this code will force our hand to tackle this issue eventually, one way or the other\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.881+0000",
                    "updated": "2019-08-23T22:37:22.881+0000",
                    "started": "2019-08-23T22:37:22.881+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300569",
                    "issueId": "13031560"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/worklog/300570",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5167: [WIP] ARROW-453: [C++] Filesystem implementation for Amazon S3\nURL: https://github.com/apache/arrow/pull/5167#discussion_r317303803\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs.cc\n ##########\n @@ -0,0 +1,1100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <algorithm>\n+#include <sstream>\n+#include <utility>\n+\n+#ifdef _WIN32\n+// Undefine preprocessor macros that interfere with AWS function / method names\n+#ifdef GetMessage\n+#undef GetMessage\n+#endif\n+#ifdef GetObject\n+#undef GetObject\n+#endif\n+#endif\n+\n+#include <aws/core/Aws.h>\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/core/client/RetryStrategy.h>\n+#include <aws/core/utils/logging/ConsoleLogSystem.h>\n+#include <aws/core/utils/stream/PreallocatedStreamBuf.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/AbortMultipartUploadRequest.h>\n+#include <aws/s3/model/CompleteMultipartUploadRequest.h>\n+#include <aws/s3/model/CompletedMultipartUpload.h>\n+#include <aws/s3/model/CompletedPart.h>\n+#include <aws/s3/model/CopyObjectRequest.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/CreateMultipartUploadRequest.h>\n+#include <aws/s3/model/DeleteBucketRequest.h>\n+#include <aws/s3/model/DeleteObjectRequest.h>\n+#include <aws/s3/model/DeleteObjectsRequest.h>\n+#include <aws/s3/model/GetObjectRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/HeadObjectRequest.h>\n+#include <aws/s3/model/ListBucketsResult.h>\n+#include <aws/s3/model/ListObjectsV2Request.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+#include <aws/s3/model/UploadPartRequest.h>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::Aws::Client::AWSError;\n+using ::Aws::S3::S3Errors;\n+namespace S3Model = Aws::S3::Model;\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::ErrorToStatus;\n+using ::arrow::fs::internal::FromAwsString;\n+using ::arrow::fs::internal::IsAlreadyExists;\n+using ::arrow::fs::internal::IsNotFound;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+using ::arrow::fs::internal::ToTimePoint;\n+using ::arrow::fs::internal::ToURLEncodedAwsString;\n+\n+static const char kSep = '/';\n+\n+namespace {\n+\n+util::string_view RemoveTrailingSeparator(util::string_view key) {\n+  // Remove trailing separator\n+  if (!key.empty() && key.back() == kSep) {\n+    key.remove_suffix(1);\n+  }\n+  return key;\n+}\n+\n+// XXX Sanitize paths by removing leading slash?\n+\n+struct S3Path {\n+  std::string full_path;\n+  std::string bucket;\n+  std::string key;\n+  std::vector<std::string> key_parts;\n+\n+  static Status FromString(const std::string& s, S3Path* out) {\n+    auto first_sep = s.find_first_of(kSep);\n+    if (first_sep == 0) {\n+      return Status::Invalid(\"Path cannot start with a separator ('\", s, \"')\");\n+    }\n+    if (first_sep == std::string::npos) {\n+      *out = {s, s, \"\", {}};\n+      return Status::OK();\n+    }\n+    out->full_path = s;\n+    out->bucket = s.substr(0, first_sep);\n+    out->key = s.substr(first_sep + 1);\n+    out->key_parts = internal::SplitAbstractPath(out->key);\n+    return internal::ValidateAbstractPathParts(out->key_parts);\n+  }\n+\n+  Aws::String ToURLEncodedAwsString() const {\n+    // URL-encode individual parts, not the '/' separator\n+    Aws::String res;\n+    res += internal::ToURLEncodedAwsString(bucket);\n+    for (const auto& part : key_parts) {\n+      res += kSep;\n+      res += internal::ToURLEncodedAwsString(part);\n+    }\n+    return res;\n+  }\n+\n+  S3Path parent() const {\n+    DCHECK(!key_parts.empty());\n+    auto parent = S3Path{\"\", bucket, \"\", key_parts};\n+    parent.key_parts.pop_back();\n+    parent.key = internal::JoinAbstractPath(parent.key_parts);\n+    parent.full_path = parent.bucket + kSep + parent.key;\n+    return parent;\n+  }\n+\n+  bool has_parent() const { return !key.empty(); }\n+\n+  bool empty() const { return bucket.empty() && key.empty(); }\n+\n+  bool operator==(const S3Path& other) const {\n+    return bucket == other.bucket && key == other.key;\n+  }\n+};\n+\n+// XXX return in OutcomeToStatus instead?\n+Status PathNotFound(const S3Path& path) {\n+  return Status::IOError(\"Path does not exist '\", path.full_path, \"'\");\n+}\n+\n+Status PathNotFound(const std::string& bucket, const std::string& key) {\n+  return Status::IOError(\"Path does not exist '\", bucket, kSep, key, \"'\");\n+}\n+\n+// Status NotADir(const S3Path& path) {\n+//   return Status::IOError(\"Not a directory: '\", path.full_path, \"'\");\n+// }\n+\n+Status NotAFile(const S3Path& path) {\n+  return Status::IOError(\"Not a regular file: '\", path.full_path, \"'\");\n+}\n+\n+Status ValidateFilePath(const S3Path& path) {\n+  if (path.bucket.empty() || path.key.empty()) {\n+    return NotAFile(path);\n+  }\n+  return Status::OK();\n+}\n+\n+std::string FormatRange(int64_t start, int64_t length) {\n+  // Format a HTTP range header value\n+  std::stringstream ss;\n+  ss << \"bytes=\" << start << \"-\" << start + length - 1;\n+  return ss.str();\n+}\n+\n+Status GetObjectRange(Aws::S3::S3Client* client, const S3Path& path, int64_t start,\n+                      int64_t length, S3Model::GetObjectResult* out) {\n+  S3Model::GetObjectRequest req;\n+  req.SetBucket(ToAwsString(path.bucket));\n+  req.SetKey(ToAwsString(path.key));\n+  req.SetRange(ToAwsString(FormatRange(start, length)));\n+  ARROW_AWS_ASSIGN_OR_RAISE(*out, client->GetObject(req));\n+  return Status::OK();\n+}\n+\n+// A RandomAccessFile that reads from a S3 object\n+class ObjectInputFile : public io::RandomAccessFile {\n+ public:\n+  ObjectInputFile(Aws::S3::S3Client* client, const S3Path& path)\n+      : client_(client), path_(path) {}\n+\n+  ~ObjectInputFile() override {}\n+\n+  Status Init() {\n+    // Issue a HEAD Object to get the content-length and ensure any\n+    // errors (e.g. file not found) don't wait until the first Read() call.\n+    S3Model::HeadObjectRequest req;\n+    req.SetBucket(ToAwsString(path_.bucket));\n+    req.SetKey(ToAwsString(path_.key));\n+\n+    auto outcome = client_->HeadObject(req);\n+    if (!outcome.IsSuccess()) {\n+      if (IsNotFound(outcome.GetError())) {\n+        return PathNotFound(path_);\n+      } else {\n+        return ErrorToStatus(outcome.GetError());\n+      }\n+    }\n+    content_length_ = outcome.GetResult().GetContentLength();\n+    return Status::OK();\n+  }\n+\n+  Status CheckClosed() const {\n+    if (closed_) {\n+      return Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return Status::OK();\n+  }\n+\n+  // RandomAccessFile APIs\n+\n+  Status Close() override {\n+    closed_ = true;\n+    return Status::OK();\n+  }\n+\n+  bool closed() const override { return closed_; }\n+\n+  Status Tell(int64_t* position) const override {\n+    RETURN_NOT_OK(CheckClosed());\n+\n+    *position = pos_;\n+    return Status::OK();\n+  }\n+\n+  Status GetSize(int64_t* size) override {\n+    RETURN_NOT_OK(CheckClosed());\n+\n+    *size = content_length_;\n+    return Status::OK();\n+  }\n+\n+  Status Seek(int64_t position) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return Status::OK();\n+  }\n+\n+  Status ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                void* out) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // Read the desired range of bytes\n+    S3Model::GetObjectResult result;\n+    RETURN_NOT_OK(GetObjectRange(client_, path_, position, nbytes, &result));\n+\n+    auto& stream = result.GetBody();\n+    stream.read(reinterpret_cast<char*>(out), nbytes);\n+    // NOTE: the stream is a stringstream by default, there is no actual error\n+    // to check for.  However, stream.fail() may return true if EOF is reached.\n+    *bytes_read = stream.gcount();\n+    return Status::OK();\n+  }\n+\n+  Status ReadAt(int64_t position, int64_t nbytes, std::shared_ptr<Buffer>* out) override {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    std::shared_ptr<ResizableBuffer> buf;\n+    int64_t bytes_read;\n+    RETURN_NOT_OK(AllocateResizableBuffer(nbytes, &buf));\n+    if (nbytes > 0) {\n+      RETURN_NOT_OK(ReadAt(position, nbytes, &bytes_read, buf->mutable_data()));\n+      DCHECK_LE(bytes_read, nbytes);\n+      RETURN_NOT_OK(buf->Resize(bytes_read));\n+    }\n+    *out = std::move(buf);\n+    return Status::OK();\n+  }\n \n Review comment:\n   Aside: seems like a reusable \"default\" `ReadAt` implementation could be put in an internal header somewhere\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-23T22:37:22.912+0000",
                    "updated": "2019-08-23T22:37:22.912+0000",
                    "started": "2019-08-23T22:37:22.912+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "300570",
                    "issueId": "13031560"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 42600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@3e90465b[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@45cb0a04[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@17d4db4d[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@a527899[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@ce153ac[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@7040e5ab[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@417f565d[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@58f0785b[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5e8a3cda[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@2990e71d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@12e691cc[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@490f116b[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 42600,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Aug 29 14:36:04 UTC 2019",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2019-08-29T14:36:04.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-453/watchers",
            "watchCount": 9,
            "isWatching": false
        },
        "created": "2017-01-02T23:27:59.000+0000",
        "updated": "2019-11-20T20:59:39.000+0000",
        "timeoriginalestimate": null,
        "description": "The BSD-licensed C++ code in SFrame (https://github.com/turi-code/SFrame/tree/master/oss_src/fileio) may provide some inspiration. ",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "11h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 42600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Add filesystem implementation for Amazon S3",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/15815990",
                    "id": "15815990",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "We might also look at Impala's C++ S3 implementation",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-01-10T19:51:22.862+0000",
                    "updated": "2017-01-10T19:51:22.862+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16090167",
                    "id": "16090167",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "See also https://github.com/tensorflow/tensorflow/pull/11089",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-07-17T17:50:45.236+0000",
                    "updated": "2017-07-17T17:50:45.236+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907451",
                    "id": "16907451",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "I'm trying to use [MinIO|https://min.io/] for automated tests. It seems to have a rather delicate protocol implementation...",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2019-08-14T17:05:38.013+0000",
                    "updated": "2019-08-14T17:05:38.013+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907458",
                    "id": "16907458",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "General question: should the S3 filesystem be anchored on a specific bucket (meaning \"path/to/file\" is a key inside the bucket) or not (meaning \"bucket/path/to/file\" gets divided into bucket name and key string)?\r\n\r\nBoth are possible, and I notice that the Python projects [s3fs|https://s3fs.readthedocs.io/en/latest/] and [fs-s3fs|https://fs-s3fs.readthedocs.io/en/latest/] took different decisions here.\r\n\r\n[~mdurant] What's your opinion?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2019-08-14T17:08:43.371+0000",
                    "updated": "2019-08-14T17:09:49.465+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907571",
                    "id": "16907571",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "body": "Obviously, I choose to do this one particular way, so you cannot count me as unbiased. To the particular question, I think that a URL like \"s3://bucket/path/file\" is very convenient at pinpointing a particular object without further qualification.\r\n\r\nHowever, fsspec creates pyarrow-compatible instances for any of the implementations it references (including s3fs and gcsfs), so who do you need to do anything? You are talking about C-land?\u00a0I'm not sure I am behind repeating all the work and coming up with new file-system standards yet again. Of course, I am a python-first kind of person.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "created": "2019-08-14T19:38:19.711+0000",
                    "updated": "2019-08-14T19:38:19.711+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907577",
                    "id": "16907577",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "As the title says, this is a C++ issue. We want to be able to read data from remote filesystems directly in C++ without going through Python calls (or even relying on the presence of a Python interpreter), hence why are reimplementing a filesystem layer in C++.\r\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2019-08-14T19:45:39.266+0000",
                    "updated": "2019-08-14T19:45:39.266+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907579",
                    "id": "16907579",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Right, we're implementing things in C++ so that we can use a common implementation in Python, R, Ruby, MATLAB, and any other language that builds bindings to the core Arrow library. ",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-14T19:48:02.413+0000",
                    "updated": "2019-08-14T19:48:02.413+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907582",
                    "id": "16907582",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "body": "Can you *please* follow the fsspec model? I think it's a good model and, although young, centralises and codifies a number of FS-related concepts. If you were to rewrite all the implementations there in C++, that would be all to the good, and shouldn't require too much effort on each individual one.\u00a0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "created": "2019-08-14T19:49:11.912+0000",
                    "updated": "2019-08-14T19:49:11.912+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907587",
                    "id": "16907587",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "Which part of the model do you suggest we follow? The filesystem class API? If I look at https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.spec.AbstractFileSystem , there are a lot of methods there and we don't know which ones we'll need.\r\n\r\nOr are there any particular gotchas or design points that you want to illuminate us about?\r\n\r\nAlso you can find our current abstract API here: https://github.com/apache/arrow/blob/master/cpp/src/arrow/filesystem/filesystem.h#L53",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2019-08-14T19:56:01.502+0000",
                    "updated": "2019-08-14T19:56:01.502+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907589",
                    "id": "16907589",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "body": "Yes, I mean the whole spec, so that you make general-purpose file-system interfaces, rather than just the small part of the API that the internal arrow functions need. That might be part of the arrow mission, to open up data sources local and remote. Otherwise, users would need to use their own language implementations in addition to arrow, when they want to do any management of the remote files (copying, permissions, etc.).",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "created": "2019-08-14T19:59:45.459+0000",
                    "updated": "2019-08-14T19:59:45.459+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907590",
                    "id": "16907590",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "[~pitrou] one consideration is that data might be spread across multiple buckets with the same credentials, am I understanding that right? As some other data points, TensorFlow's implementation seems to treat the bucket as part of the file path\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system_test.cc\r\n\r\nTuri Create (formerly Dato Create) accepts fully qualified URLs including even AWS keys, bucket name, and object name\r\n\r\nhttps://github.com/apple/turicreate/blob/master/src/core/storage/fileio/s3_api.hpp#L177",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-14T20:00:48.441+0000",
                    "updated": "2019-08-14T20:00:57.554+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907591",
                    "id": "16907591",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "[~mdurant] that may be a longer-term aspirational goal but I think we are currently pretty scope-limited to the features we need to be able to read and write datasets in these systems. Maybe later when we have a bigger developer and maintainer community we can fill some of the feature gaps as a one-stop-shop filesystem interface ",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-14T20:02:38.053+0000",
                    "updated": "2019-08-14T20:02:38.053+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907592",
                    "id": "16907592",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "body": ">\u00a0Turi Create (formerly Dato Create) accepts fully qualified URLs including even AWS keys, bucket name, and object name\r\n\r\nfsspec also allows this, like s3://key:secret@bucket/path/file, which is rare for s3, but more common with ssh or hdfs. Users should probably *not* be putting their credentials into URLs in the general case.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "created": "2019-08-14T20:05:27.831+0000",
                    "updated": "2019-08-14T20:05:43.150+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16907594",
                    "id": "16907594",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "To be clear, for now I am not putting a URI layer in the filesystem. This can be added separately.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2019-08-14T20:08:34.269+0000",
                    "updated": "2019-08-14T20:08:34.269+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13031560/comment/16918668",
                    "id": "16918668",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "Issue resolved by pull request 5167\n[https://github.com/apache/arrow/pull/5167]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2019-08-29T14:36:04.645+0000",
                    "updated": "2019-08-29T14:36:04.645+0000"
                }
            ],
            "maxResults": 15,
            "total": 15,
            "startAt": 0
        },
        "customfield_12311820": "0|i386ov:",
        "customfield_12314139": null
    }
}