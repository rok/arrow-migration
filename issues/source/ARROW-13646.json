{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13395709",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709",
    "key": "ARROW-13646",
    "fields": {
        "parent": {
            "id": "13286806",
            "key": "ARROW-7905",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13286806",
            "fields": {
                "summary": "[Go][Parquet] Port the C++ Parquet implementation to Go",
                "status": {
                    "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                    "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                    "name": "Resolved",
                    "id": "5",
                    "statusCategory": {
                        "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                        "id": 3,
                        "key": "done",
                        "colorName": "green",
                        "name": "Done"
                    }
                },
                "priority": {
                    "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                    "name": "Minor",
                    "id": "4"
                },
                "issuetype": {
                    "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                    "id": "2",
                    "description": "A new feature of the product, which has yet to be developed.",
                    "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                    "name": "New Feature",
                    "subtask": false,
                    "avatarId": 21141
                }
            }
        },
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350323",
                "id": "12350323",
                "description": "",
                "name": "6.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-10-26"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
            "name": "Minor",
            "id": "4"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12349983",
                "id": "12349983",
                "description": "",
                "name": "5.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-07-28"
            }
        ],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
            "name": "zeroshade",
            "key": "zeroshade",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
            },
            "displayName": "Matthew Topol",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12333772",
                "id": "12333772",
                "name": "Go"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12337837",
                "id": "12337837",
                "name": "Parquet"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
            "name": "zeroshade",
            "key": "zeroshade",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
            },
            "displayName": "Matthew Topol",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
            "name": "zeroshade",
            "key": "zeroshade",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
            },
            "displayName": "Matthew Topol",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 55200,
            "total": 55200,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 55200,
            "total": 55200,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-13646/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 92,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/638706",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade opened a new pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951\n\n\n   Here's the next chunk of code following the merging of #10716 \r\n   \r\n   Thankfully the metadata package is actually relatively small compared to everything else so far.\r\n   \r\n   @emkornfield @sbinet @nickpoorman for visibility\r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-17T15:27:39.847+0000",
                    "updated": "2021-08-17T15:27:39.847+0000",
                    "started": "2021-08-17T15:27:39.846+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "638706",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/638707",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#issuecomment-900397921\n\n\n   https://issues.apache.org/jira/browse/ARROW-13646\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-17T15:28:03.741+0000",
                    "updated": "2021-08-17T15:28:03.741+0000",
                    "started": "2021-08-17T15:28:03.741+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "638707",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/639111",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nickpoorman commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r691271681\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n\nReview comment:\n       Since there\u2019s always an append here of at least 2 elements can we initialize the array to a capacity of 2 (or 3 possibly if it\u2019s the max)?\n\n##########\nFile path: go/parquet/metadata/file.go\n##########\n@@ -0,0 +1,480 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// DefaultCompressionType is used unless a different compression is specified\n+// in the properties\n+var DefaultCompressionType = compress.Codecs.Uncompressed\n+\n+// FileMetaDataBuilder is a proxy for more easily constructing file metadata\n+// particularly used when writing a file out.\n+type FileMetaDataBuilder struct {\n+\tmetadata       *format.FileMetaData\n+\tprops          *parquet.WriterProperties\n+\tschema         *schema.Schema\n+\trowGroups      []*format.RowGroup\n+\tcurrentRgBldr  *RowGroupMetaDataBuilder\n+\tkvmeta         KeyValueMetadata\n+\tcryptoMetadata *format.FileCryptoMetaData\n+}\n+\n+// NewFileMetadataBuilder will use the default writer properties if nil is passed for\n+// the writer properties and nil is allowable for the key value metadata.\n+func NewFileMetadataBuilder(schema *schema.Schema, props *parquet.WriterProperties, kvmeta KeyValueMetadata) *FileMetaDataBuilder {\n+\tvar crypto *format.FileCryptoMetaData\n+\tif props.FileEncryptionProperties() != nil && props.FileEncryptionProperties().EncryptedFooter() {\n+\t\tcrypto = format.NewFileCryptoMetaData()\n+\t}\n+\treturn &FileMetaDataBuilder{\n+\t\tmetadata:       format.NewFileMetaData(),\n+\t\tprops:          props,\n+\t\tschema:         schema,\n+\t\tkvmeta:         kvmeta,\n+\t\tcryptoMetadata: crypto,\n+\t}\n+}\n+\n+// GetFileCryptoMetaData returns the cryptographic information for encrypting/\n+// decrypting the file.\n+func (f *FileMetaDataBuilder) GetFileCryptoMetaData() *FileCryptoMetadata {\n+\tif f.cryptoMetadata == nil {\n+\t\treturn nil\n+\t}\n+\n+\tprops := f.props.FileEncryptionProperties()\n+\tf.cryptoMetadata.EncryptionAlgorithm = props.Algorithm().ToThrift()\n+\tkeyMetadata := props.FooterKeyMetadata()\n+\tif keyMetadata != \"\" {\n+\t\tf.cryptoMetadata.KeyMetadata = []byte(keyMetadata)\n+\t}\n+\n+\treturn &FileCryptoMetadata{f.cryptoMetadata, 0}\n+}\n+\n+// AppendRowGroup adds a rowgroup to the list and returns a builder\n+// for that row group\n+func (f *FileMetaDataBuilder) AppendRowGroup() *RowGroupMetaDataBuilder {\n+\tif f.rowGroups == nil {\n+\t\tf.rowGroups = make([]*format.RowGroup, 0, 1)\n+\t}\n+\n+\trg := format.NewRowGroup()\n+\tf.rowGroups = append(f.rowGroups, rg)\n+\tf.currentRgBldr = NewRowGroupMetaDataBuilder(f.props, f.schema, rg)\n+\treturn f.currentRgBldr\n+}\n+\n+// Finish will finalize the metadata of the number of rows, row groups,\n+// version etc. This will clear out this filemetadatabuilder so it can\n+// be re-used\n+func (f *FileMetaDataBuilder) Finish() (*FileMetaData, error) {\n+\ttotalRows := int64(0)\n+\tfor _, rg := range f.rowGroups {\n+\t\ttotalRows += rg.NumRows\n+\t}\n+\tf.metadata.NumRows = totalRows\n+\tf.metadata.RowGroups = f.rowGroups\n+\tswitch f.props.Version() {\n+\tcase parquet.V1:\n+\t\tf.metadata.Version = 1\n+\tcase parquet.V2:\n+\t\tf.metadata.Version = 2\n+\tdefault:\n+\t\tf.metadata.Version = 0\n+\t}\n+\tcreatedBy := f.props.CreatedBy()\n+\tf.metadata.CreatedBy = &createdBy\n+\n+\t// Users cannot set the `ColumnOrder` since we donot not have user defined sort order\n+\t// in the spec yet.\n+\t// We always default to `TYPE_DEFINED_ORDER`. We can expose it in\n+\t// the API once we have user defined sort orders in the Parquet format.\n+\t// TypeDefinedOrder implies choose SortOrder based on ConvertedType/PhysicalType\n+\ttypeDefined := format.NewTypeDefinedOrder()\n+\tcolOrder := &format.ColumnOrder{TYPE_ORDER: typeDefined}\n+\tf.metadata.ColumnOrders = make([]*format.ColumnOrder, f.schema.NumColumns())\n+\tfor idx := range f.metadata.ColumnOrders {\n+\t\tf.metadata.ColumnOrders[idx] = colOrder\n+\t}\n+\n+\tfileEncProps := f.props.FileEncryptionProperties()\n+\tif fileEncProps != nil && !fileEncProps.EncryptedFooter() {\n+\t\tvar signingAlgo parquet.Algorithm\n+\t\talgo := fileEncProps.Algorithm()\n+\t\tsigningAlgo.Aad.AadFileUnique = algo.Aad.AadFileUnique\n+\t\tsigningAlgo.Aad.SupplyAadPrefix = algo.Aad.SupplyAadPrefix\n+\t\tif !algo.Aad.SupplyAadPrefix {\n+\t\t\tsigningAlgo.Aad.AadPrefix = algo.Aad.AadPrefix\n+\t\t}\n+\t\tsigningAlgo.Algo = parquet.AesGcm\n+\t\tf.metadata.EncryptionAlgorithm = signingAlgo.ToThrift()\n+\t\tfooterSigningMetadata := f.props.FileEncryptionProperties().FooterKeyMetadata()\n+\t\tif footerSigningMetadata != \"\" {\n+\t\t\tf.metadata.FooterSigningKeyMetadata = []byte(footerSigningMetadata)\n+\t\t}\n+\t}\n+\n+\tf.metadata.Schema = schema.ToThrift(f.schema.Root())\n+\tf.metadata.KeyValueMetadata = f.kvmeta\n+\n+\tout := &FileMetaData{\n+\t\tFileMetaData: f.metadata,\n+\t\tversion:      NewAppVersion(f.metadata.GetCreatedBy()),\n+\t}\n+\tif err := out.initSchema(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\tout.initColumnOrders()\n+\n+\tf.metadata = format.NewFileMetaData()\n+\tf.rowGroups = nil\n+\treturn out, nil\n+}\n+\n+// KeyValueMetadata is an alias for a slice of thrift keyvalue pairs\n+type KeyValueMetadata []*format.KeyValue\n+\n+// NewKeyValueMetadata is equivalent to make(KeyValueMetadata, 0)\n+func NewKeyValueMetadata() KeyValueMetadata {\n+\treturn make(KeyValueMetadata, 0)\n+}\n+\n+func (k *KeyValueMetadata) Append(key, value string) {\n+\t*k = append(*k, &format.KeyValue{Key: key, Value: &value})\n+}\n+\n+func (k KeyValueMetadata) Len() int { return len(k) }\n+\n+// Equals compares all of the metadata keys and values to check they are equal\n+func (k KeyValueMetadata) Equals(other KeyValueMetadata) bool {\n+\treturn reflect.DeepEqual(k, other)\n+}\n+\n+func (k KeyValueMetadata) Keys() (ret []string) {\n+\tret = make([]string, len(k))\n+\tfor idx, v := range k {\n+\t\tret[idx] = v.GetKey()\n+\t}\n+\treturn\n+}\n+\n+func (k KeyValueMetadata) Values() (ret []string) {\n+\tret = make([]string, len(k))\n+\tfor idx, v := range k {\n+\t\tret[idx] = v.GetValue()\n+\t}\n+\treturn\n+}\n+\n+func (k KeyValueMetadata) FindValue(key string) *string {\n+\tfor _, v := range k {\n+\t\tif v.Key == key {\n+\t\t\treturn v.Value\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// FileMetaData is a proxy around the underlying thrift FileMetaData object\n+// to make it easier to use and interact with.\n+type FileMetaData struct {\n+\t*format.FileMetaData\n+\tversion       *AppVersion\n+\tSchema        *schema.Schema\n+\tFileDecryptor encryption.FileDecryptor\n+\tmetadataLen   int\n+}\n+\n+// NewFileMetaData takes in the raw bytes of the serialized metadata to deserialize\n+// and will attempt to decrypt the footer if a decryptor is provided.\n+func NewFileMetaData(data []byte, fileDecryptor encryption.FileDecryptor) (*FileMetaData, error) {\n+\tmeta := format.NewFileMetaData()\n+\tif fileDecryptor != nil {\n+\t\tfooterDecryptor := fileDecryptor.GetFooterDecryptor()\n+\t\tdata = footerDecryptor.Decrypt(data)\n+\t}\n+\n+\tremain, err := thrift.DeserializeThrift(meta, data)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tf := &FileMetaData{\n+\t\tFileMetaData:  meta,\n+\t\tversion:       NewAppVersion(meta.GetCreatedBy()),\n+\t\tmetadataLen:   len(data) - int(remain),\n+\t\tFileDecryptor: fileDecryptor,\n+\t}\n+\n+\tf.initSchema()\n+\tf.initColumnOrders()\n+\n+\t// init keyvalue metadata\n\nReview comment:\n       What\u2019s this comment supposed to be for?\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n+\tif hasDict {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryIndexEncoding()))\n+\t\tif c.props.Version() == parquet.V1 {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t\t} else {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryPageEncoding()))\n+\t\t}\n+\t} else { // no dictionary\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.EncodingFor(c.column.Path())))\n+\t}\n+\n+\tthriftEncodings = append(thriftEncodings, format.Encoding(parquet.Encodings.RLE))\n+\t// Only PLAIN encoding is supported for fallback in V1\n+\t// TODO(zeroshade): Use user specified encoding for V2\n+\tif dictFallback {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t}\n+\tc.chunk.MetaData.Encodings = thriftEncodings\n+\n+\tthriftEncodingStats := make([]*format.PageEncodingStats, 0, len(dictEncodingStats)+len(dataEncodingStats))\n+\tfor k, v := range dictEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DICTIONARY_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tfor k, v := range dataEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DATA_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tc.chunk.MetaData.EncodingStats = thriftEncodingStats\n+\n+\tencryptProps := c.props.ColumnEncryptionProperties(c.column.Path())\n+\tif encryptProps != nil && encryptProps.IsEncrypted() {\n+\t\tccmd := format.NewColumnCryptoMetaData()\n+\t\tif encryptProps.IsEncryptedWithFooterKey() {\n+\t\t\tccmd.ENCRYPTION_WITH_FOOTER_KEY = format.NewEncryptionWithFooterKey()\n+\t\t} else {\n+\t\t\tccmd.ENCRYPTION_WITH_COLUMN_KEY = &format.EncryptionWithColumnKey{\n+\t\t\t\tKeyMetadata:  []byte(encryptProps.KeyMetadata()),\n+\t\t\t\tPathInSchema: c.column.ColumnPath(),\n+\t\t\t}\n+\t\t}\n+\t\tc.chunk.CryptoMetadata = ccmd\n+\n+\t\tencryptedFooter := c.props.FileEncryptionProperties().EncryptedFooter()\n+\t\tencryptMetadata := !encryptedFooter || !encryptProps.IsEncryptedWithFooterKey()\n+\t\tif encryptMetadata {\n+\t\t\t// Serialize and encrypt ColumnMetadata separately\n+\t\t\t// Thrift-serialize the ColumnMetaData structure,\n+\t\t\t// encrypt it with the column key, and write to encrypted_column_metadata\n+\t\t\tserializer := thrift.NewThriftSerializer()\n+\t\t\tdata, err := serializer.Write(context.Background(), c.chunk.MetaData)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tvar buf bytes.Buffer\n+\t\t\tmetaEncryptor.Encrypt(&buf, data)\n+\t\t\tc.chunk.EncryptedColumnMetadata = buf.Bytes()\n+\n+\t\t\tif encryptedFooter {\n+\t\t\t\tc.compressedSize = c.chunk.MetaData.GetTotalCompressedSize()\n+\t\t\t\tc.chunk.MetaData = nil\n+\t\t\t} else {\n+\t\t\t\t// Keep redacted metadata version for old readers\n+\t\t\t\tc.chunk.MetaData.Statistics = nil\n+\t\t\t\tc.chunk.MetaData.EncodingStats = nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// WriteTo will always return 0 as the int64 since the thrift writer library\n\nReview comment:\n       Hmm, I wonder if we should keep track of the bytes or explicit return a -1. Is the president for this elsewhere?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T14:19:52.502+0000",
                    "updated": "2021-08-18T14:19:52.502+0000",
                    "started": "2021-08-18T14:19:52.502+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639111",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/639112",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nickpoorman commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r691277427\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n+\tif hasDict {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryIndexEncoding()))\n+\t\tif c.props.Version() == parquet.V1 {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t\t} else {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryPageEncoding()))\n+\t\t}\n+\t} else { // no dictionary\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.EncodingFor(c.column.Path())))\n+\t}\n+\n+\tthriftEncodings = append(thriftEncodings, format.Encoding(parquet.Encodings.RLE))\n+\t// Only PLAIN encoding is supported for fallback in V1\n+\t// TODO(zeroshade): Use user specified encoding for V2\n+\tif dictFallback {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t}\n+\tc.chunk.MetaData.Encodings = thriftEncodings\n+\n+\tthriftEncodingStats := make([]*format.PageEncodingStats, 0, len(dictEncodingStats)+len(dataEncodingStats))\n+\tfor k, v := range dictEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DICTIONARY_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tfor k, v := range dataEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DATA_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tc.chunk.MetaData.EncodingStats = thriftEncodingStats\n+\n+\tencryptProps := c.props.ColumnEncryptionProperties(c.column.Path())\n+\tif encryptProps != nil && encryptProps.IsEncrypted() {\n+\t\tccmd := format.NewColumnCryptoMetaData()\n+\t\tif encryptProps.IsEncryptedWithFooterKey() {\n+\t\t\tccmd.ENCRYPTION_WITH_FOOTER_KEY = format.NewEncryptionWithFooterKey()\n+\t\t} else {\n+\t\t\tccmd.ENCRYPTION_WITH_COLUMN_KEY = &format.EncryptionWithColumnKey{\n+\t\t\t\tKeyMetadata:  []byte(encryptProps.KeyMetadata()),\n+\t\t\t\tPathInSchema: c.column.ColumnPath(),\n+\t\t\t}\n+\t\t}\n+\t\tc.chunk.CryptoMetadata = ccmd\n+\n+\t\tencryptedFooter := c.props.FileEncryptionProperties().EncryptedFooter()\n+\t\tencryptMetadata := !encryptedFooter || !encryptProps.IsEncryptedWithFooterKey()\n+\t\tif encryptMetadata {\n+\t\t\t// Serialize and encrypt ColumnMetadata separately\n+\t\t\t// Thrift-serialize the ColumnMetaData structure,\n+\t\t\t// encrypt it with the column key, and write to encrypted_column_metadata\n+\t\t\tserializer := thrift.NewThriftSerializer()\n+\t\t\tdata, err := serializer.Write(context.Background(), c.chunk.MetaData)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tvar buf bytes.Buffer\n+\t\t\tmetaEncryptor.Encrypt(&buf, data)\n+\t\t\tc.chunk.EncryptedColumnMetadata = buf.Bytes()\n+\n+\t\t\tif encryptedFooter {\n+\t\t\t\tc.compressedSize = c.chunk.MetaData.GetTotalCompressedSize()\n+\t\t\t\tc.chunk.MetaData = nil\n+\t\t\t} else {\n+\t\t\t\t// Keep redacted metadata version for old readers\n+\t\t\t\tc.chunk.MetaData.Statistics = nil\n+\t\t\t\tc.chunk.MetaData.EncodingStats = nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// WriteTo will always return 0 as the int64 since the thrift writer library\n\nReview comment:\n       Hmm, I wonder if we should keep track of the bytes or explicit return a -1. Is the precedent for this elsewhere?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T14:20:19.473+0000",
                    "updated": "2021-08-18T14:20:19.473+0000",
                    "started": "2021-08-18T14:20:19.473+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639112",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/639113",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nickpoorman commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r691277427\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n+\tif hasDict {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryIndexEncoding()))\n+\t\tif c.props.Version() == parquet.V1 {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t\t} else {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryPageEncoding()))\n+\t\t}\n+\t} else { // no dictionary\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.EncodingFor(c.column.Path())))\n+\t}\n+\n+\tthriftEncodings = append(thriftEncodings, format.Encoding(parquet.Encodings.RLE))\n+\t// Only PLAIN encoding is supported for fallback in V1\n+\t// TODO(zeroshade): Use user specified encoding for V2\n+\tif dictFallback {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t}\n+\tc.chunk.MetaData.Encodings = thriftEncodings\n+\n+\tthriftEncodingStats := make([]*format.PageEncodingStats, 0, len(dictEncodingStats)+len(dataEncodingStats))\n+\tfor k, v := range dictEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DICTIONARY_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tfor k, v := range dataEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DATA_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tc.chunk.MetaData.EncodingStats = thriftEncodingStats\n+\n+\tencryptProps := c.props.ColumnEncryptionProperties(c.column.Path())\n+\tif encryptProps != nil && encryptProps.IsEncrypted() {\n+\t\tccmd := format.NewColumnCryptoMetaData()\n+\t\tif encryptProps.IsEncryptedWithFooterKey() {\n+\t\t\tccmd.ENCRYPTION_WITH_FOOTER_KEY = format.NewEncryptionWithFooterKey()\n+\t\t} else {\n+\t\t\tccmd.ENCRYPTION_WITH_COLUMN_KEY = &format.EncryptionWithColumnKey{\n+\t\t\t\tKeyMetadata:  []byte(encryptProps.KeyMetadata()),\n+\t\t\t\tPathInSchema: c.column.ColumnPath(),\n+\t\t\t}\n+\t\t}\n+\t\tc.chunk.CryptoMetadata = ccmd\n+\n+\t\tencryptedFooter := c.props.FileEncryptionProperties().EncryptedFooter()\n+\t\tencryptMetadata := !encryptedFooter || !encryptProps.IsEncryptedWithFooterKey()\n+\t\tif encryptMetadata {\n+\t\t\t// Serialize and encrypt ColumnMetadata separately\n+\t\t\t// Thrift-serialize the ColumnMetaData structure,\n+\t\t\t// encrypt it with the column key, and write to encrypted_column_metadata\n+\t\t\tserializer := thrift.NewThriftSerializer()\n+\t\t\tdata, err := serializer.Write(context.Background(), c.chunk.MetaData)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tvar buf bytes.Buffer\n+\t\t\tmetaEncryptor.Encrypt(&buf, data)\n+\t\t\tc.chunk.EncryptedColumnMetadata = buf.Bytes()\n+\n+\t\t\tif encryptedFooter {\n+\t\t\t\tc.compressedSize = c.chunk.MetaData.GetTotalCompressedSize()\n+\t\t\t\tc.chunk.MetaData = nil\n+\t\t\t} else {\n+\t\t\t\t// Keep redacted metadata version for old readers\n+\t\t\t\tc.chunk.MetaData.Statistics = nil\n+\t\t\t\tc.chunk.MetaData.EncodingStats = nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// WriteTo will always return 0 as the int64 since the thrift writer library\n\nReview comment:\n       Hmm, I wonder if we should keep track of the bytes or explicit return a -1. Is there precedent for this elsewhere?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T14:20:35.154+0000",
                    "updated": "2021-08-18T14:20:35.154+0000",
                    "started": "2021-08-18T14:20:35.153+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639113",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/639490",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nickpoorman commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r691271681\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n\nReview comment:\n       Since there\u2019s always an append here of at least 2 elements can we initialize the array to a capacity of 2 (or 3 possibly if it\u2019s the max)?\n\n##########\nFile path: go/parquet/metadata/file.go\n##########\n@@ -0,0 +1,480 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// DefaultCompressionType is used unless a different compression is specified\n+// in the properties\n+var DefaultCompressionType = compress.Codecs.Uncompressed\n+\n+// FileMetaDataBuilder is a proxy for more easily constructing file metadata\n+// particularly used when writing a file out.\n+type FileMetaDataBuilder struct {\n+\tmetadata       *format.FileMetaData\n+\tprops          *parquet.WriterProperties\n+\tschema         *schema.Schema\n+\trowGroups      []*format.RowGroup\n+\tcurrentRgBldr  *RowGroupMetaDataBuilder\n+\tkvmeta         KeyValueMetadata\n+\tcryptoMetadata *format.FileCryptoMetaData\n+}\n+\n+// NewFileMetadataBuilder will use the default writer properties if nil is passed for\n+// the writer properties and nil is allowable for the key value metadata.\n+func NewFileMetadataBuilder(schema *schema.Schema, props *parquet.WriterProperties, kvmeta KeyValueMetadata) *FileMetaDataBuilder {\n+\tvar crypto *format.FileCryptoMetaData\n+\tif props.FileEncryptionProperties() != nil && props.FileEncryptionProperties().EncryptedFooter() {\n+\t\tcrypto = format.NewFileCryptoMetaData()\n+\t}\n+\treturn &FileMetaDataBuilder{\n+\t\tmetadata:       format.NewFileMetaData(),\n+\t\tprops:          props,\n+\t\tschema:         schema,\n+\t\tkvmeta:         kvmeta,\n+\t\tcryptoMetadata: crypto,\n+\t}\n+}\n+\n+// GetFileCryptoMetaData returns the cryptographic information for encrypting/\n+// decrypting the file.\n+func (f *FileMetaDataBuilder) GetFileCryptoMetaData() *FileCryptoMetadata {\n+\tif f.cryptoMetadata == nil {\n+\t\treturn nil\n+\t}\n+\n+\tprops := f.props.FileEncryptionProperties()\n+\tf.cryptoMetadata.EncryptionAlgorithm = props.Algorithm().ToThrift()\n+\tkeyMetadata := props.FooterKeyMetadata()\n+\tif keyMetadata != \"\" {\n+\t\tf.cryptoMetadata.KeyMetadata = []byte(keyMetadata)\n+\t}\n+\n+\treturn &FileCryptoMetadata{f.cryptoMetadata, 0}\n+}\n+\n+// AppendRowGroup adds a rowgroup to the list and returns a builder\n+// for that row group\n+func (f *FileMetaDataBuilder) AppendRowGroup() *RowGroupMetaDataBuilder {\n+\tif f.rowGroups == nil {\n+\t\tf.rowGroups = make([]*format.RowGroup, 0, 1)\n+\t}\n+\n+\trg := format.NewRowGroup()\n+\tf.rowGroups = append(f.rowGroups, rg)\n+\tf.currentRgBldr = NewRowGroupMetaDataBuilder(f.props, f.schema, rg)\n+\treturn f.currentRgBldr\n+}\n+\n+// Finish will finalize the metadata of the number of rows, row groups,\n+// version etc. This will clear out this filemetadatabuilder so it can\n+// be re-used\n+func (f *FileMetaDataBuilder) Finish() (*FileMetaData, error) {\n+\ttotalRows := int64(0)\n+\tfor _, rg := range f.rowGroups {\n+\t\ttotalRows += rg.NumRows\n+\t}\n+\tf.metadata.NumRows = totalRows\n+\tf.metadata.RowGroups = f.rowGroups\n+\tswitch f.props.Version() {\n+\tcase parquet.V1:\n+\t\tf.metadata.Version = 1\n+\tcase parquet.V2:\n+\t\tf.metadata.Version = 2\n+\tdefault:\n+\t\tf.metadata.Version = 0\n+\t}\n+\tcreatedBy := f.props.CreatedBy()\n+\tf.metadata.CreatedBy = &createdBy\n+\n+\t// Users cannot set the `ColumnOrder` since we donot not have user defined sort order\n+\t// in the spec yet.\n+\t// We always default to `TYPE_DEFINED_ORDER`. We can expose it in\n+\t// the API once we have user defined sort orders in the Parquet format.\n+\t// TypeDefinedOrder implies choose SortOrder based on ConvertedType/PhysicalType\n+\ttypeDefined := format.NewTypeDefinedOrder()\n+\tcolOrder := &format.ColumnOrder{TYPE_ORDER: typeDefined}\n+\tf.metadata.ColumnOrders = make([]*format.ColumnOrder, f.schema.NumColumns())\n+\tfor idx := range f.metadata.ColumnOrders {\n+\t\tf.metadata.ColumnOrders[idx] = colOrder\n+\t}\n+\n+\tfileEncProps := f.props.FileEncryptionProperties()\n+\tif fileEncProps != nil && !fileEncProps.EncryptedFooter() {\n+\t\tvar signingAlgo parquet.Algorithm\n+\t\talgo := fileEncProps.Algorithm()\n+\t\tsigningAlgo.Aad.AadFileUnique = algo.Aad.AadFileUnique\n+\t\tsigningAlgo.Aad.SupplyAadPrefix = algo.Aad.SupplyAadPrefix\n+\t\tif !algo.Aad.SupplyAadPrefix {\n+\t\t\tsigningAlgo.Aad.AadPrefix = algo.Aad.AadPrefix\n+\t\t}\n+\t\tsigningAlgo.Algo = parquet.AesGcm\n+\t\tf.metadata.EncryptionAlgorithm = signingAlgo.ToThrift()\n+\t\tfooterSigningMetadata := f.props.FileEncryptionProperties().FooterKeyMetadata()\n+\t\tif footerSigningMetadata != \"\" {\n+\t\t\tf.metadata.FooterSigningKeyMetadata = []byte(footerSigningMetadata)\n+\t\t}\n+\t}\n+\n+\tf.metadata.Schema = schema.ToThrift(f.schema.Root())\n+\tf.metadata.KeyValueMetadata = f.kvmeta\n+\n+\tout := &FileMetaData{\n+\t\tFileMetaData: f.metadata,\n+\t\tversion:      NewAppVersion(f.metadata.GetCreatedBy()),\n+\t}\n+\tif err := out.initSchema(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\tout.initColumnOrders()\n+\n+\tf.metadata = format.NewFileMetaData()\n+\tf.rowGroups = nil\n+\treturn out, nil\n+}\n+\n+// KeyValueMetadata is an alias for a slice of thrift keyvalue pairs\n+type KeyValueMetadata []*format.KeyValue\n+\n+// NewKeyValueMetadata is equivalent to make(KeyValueMetadata, 0)\n+func NewKeyValueMetadata() KeyValueMetadata {\n+\treturn make(KeyValueMetadata, 0)\n+}\n+\n+func (k *KeyValueMetadata) Append(key, value string) {\n+\t*k = append(*k, &format.KeyValue{Key: key, Value: &value})\n+}\n+\n+func (k KeyValueMetadata) Len() int { return len(k) }\n+\n+// Equals compares all of the metadata keys and values to check they are equal\n+func (k KeyValueMetadata) Equals(other KeyValueMetadata) bool {\n+\treturn reflect.DeepEqual(k, other)\n+}\n+\n+func (k KeyValueMetadata) Keys() (ret []string) {\n+\tret = make([]string, len(k))\n+\tfor idx, v := range k {\n+\t\tret[idx] = v.GetKey()\n+\t}\n+\treturn\n+}\n+\n+func (k KeyValueMetadata) Values() (ret []string) {\n+\tret = make([]string, len(k))\n+\tfor idx, v := range k {\n+\t\tret[idx] = v.GetValue()\n+\t}\n+\treturn\n+}\n+\n+func (k KeyValueMetadata) FindValue(key string) *string {\n+\tfor _, v := range k {\n+\t\tif v.Key == key {\n+\t\t\treturn v.Value\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// FileMetaData is a proxy around the underlying thrift FileMetaData object\n+// to make it easier to use and interact with.\n+type FileMetaData struct {\n+\t*format.FileMetaData\n+\tversion       *AppVersion\n+\tSchema        *schema.Schema\n+\tFileDecryptor encryption.FileDecryptor\n+\tmetadataLen   int\n+}\n+\n+// NewFileMetaData takes in the raw bytes of the serialized metadata to deserialize\n+// and will attempt to decrypt the footer if a decryptor is provided.\n+func NewFileMetaData(data []byte, fileDecryptor encryption.FileDecryptor) (*FileMetaData, error) {\n+\tmeta := format.NewFileMetaData()\n+\tif fileDecryptor != nil {\n+\t\tfooterDecryptor := fileDecryptor.GetFooterDecryptor()\n+\t\tdata = footerDecryptor.Decrypt(data)\n+\t}\n+\n+\tremain, err := thrift.DeserializeThrift(meta, data)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tf := &FileMetaData{\n+\t\tFileMetaData:  meta,\n+\t\tversion:       NewAppVersion(meta.GetCreatedBy()),\n+\t\tmetadataLen:   len(data) - int(remain),\n+\t\tFileDecryptor: fileDecryptor,\n+\t}\n+\n+\tf.initSchema()\n+\tf.initColumnOrders()\n+\n+\t// init keyvalue metadata\n\nReview comment:\n       What\u2019s this comment supposed to be for?\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n+\tif hasDict {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryIndexEncoding()))\n+\t\tif c.props.Version() == parquet.V1 {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t\t} else {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryPageEncoding()))\n+\t\t}\n+\t} else { // no dictionary\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.EncodingFor(c.column.Path())))\n+\t}\n+\n+\tthriftEncodings = append(thriftEncodings, format.Encoding(parquet.Encodings.RLE))\n+\t// Only PLAIN encoding is supported for fallback in V1\n+\t// TODO(zeroshade): Use user specified encoding for V2\n+\tif dictFallback {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t}\n+\tc.chunk.MetaData.Encodings = thriftEncodings\n+\n+\tthriftEncodingStats := make([]*format.PageEncodingStats, 0, len(dictEncodingStats)+len(dataEncodingStats))\n+\tfor k, v := range dictEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DICTIONARY_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tfor k, v := range dataEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DATA_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tc.chunk.MetaData.EncodingStats = thriftEncodingStats\n+\n+\tencryptProps := c.props.ColumnEncryptionProperties(c.column.Path())\n+\tif encryptProps != nil && encryptProps.IsEncrypted() {\n+\t\tccmd := format.NewColumnCryptoMetaData()\n+\t\tif encryptProps.IsEncryptedWithFooterKey() {\n+\t\t\tccmd.ENCRYPTION_WITH_FOOTER_KEY = format.NewEncryptionWithFooterKey()\n+\t\t} else {\n+\t\t\tccmd.ENCRYPTION_WITH_COLUMN_KEY = &format.EncryptionWithColumnKey{\n+\t\t\t\tKeyMetadata:  []byte(encryptProps.KeyMetadata()),\n+\t\t\t\tPathInSchema: c.column.ColumnPath(),\n+\t\t\t}\n+\t\t}\n+\t\tc.chunk.CryptoMetadata = ccmd\n+\n+\t\tencryptedFooter := c.props.FileEncryptionProperties().EncryptedFooter()\n+\t\tencryptMetadata := !encryptedFooter || !encryptProps.IsEncryptedWithFooterKey()\n+\t\tif encryptMetadata {\n+\t\t\t// Serialize and encrypt ColumnMetadata separately\n+\t\t\t// Thrift-serialize the ColumnMetaData structure,\n+\t\t\t// encrypt it with the column key, and write to encrypted_column_metadata\n+\t\t\tserializer := thrift.NewThriftSerializer()\n+\t\t\tdata, err := serializer.Write(context.Background(), c.chunk.MetaData)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tvar buf bytes.Buffer\n+\t\t\tmetaEncryptor.Encrypt(&buf, data)\n+\t\t\tc.chunk.EncryptedColumnMetadata = buf.Bytes()\n+\n+\t\t\tif encryptedFooter {\n+\t\t\t\tc.compressedSize = c.chunk.MetaData.GetTotalCompressedSize()\n+\t\t\t\tc.chunk.MetaData = nil\n+\t\t\t} else {\n+\t\t\t\t// Keep redacted metadata version for old readers\n+\t\t\t\tc.chunk.MetaData.Statistics = nil\n+\t\t\t\tc.chunk.MetaData.EncodingStats = nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// WriteTo will always return 0 as the int64 since the thrift writer library\n\nReview comment:\n       Hmm, I wonder if we should keep track of the bytes or explicit return a -1. Is the president for this elsewhere?\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n+\tif hasDict {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryIndexEncoding()))\n+\t\tif c.props.Version() == parquet.V1 {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t\t} else {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryPageEncoding()))\n+\t\t}\n+\t} else { // no dictionary\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.EncodingFor(c.column.Path())))\n+\t}\n+\n+\tthriftEncodings = append(thriftEncodings, format.Encoding(parquet.Encodings.RLE))\n+\t// Only PLAIN encoding is supported for fallback in V1\n+\t// TODO(zeroshade): Use user specified encoding for V2\n+\tif dictFallback {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t}\n+\tc.chunk.MetaData.Encodings = thriftEncodings\n+\n+\tthriftEncodingStats := make([]*format.PageEncodingStats, 0, len(dictEncodingStats)+len(dataEncodingStats))\n+\tfor k, v := range dictEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DICTIONARY_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tfor k, v := range dataEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DATA_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tc.chunk.MetaData.EncodingStats = thriftEncodingStats\n+\n+\tencryptProps := c.props.ColumnEncryptionProperties(c.column.Path())\n+\tif encryptProps != nil && encryptProps.IsEncrypted() {\n+\t\tccmd := format.NewColumnCryptoMetaData()\n+\t\tif encryptProps.IsEncryptedWithFooterKey() {\n+\t\t\tccmd.ENCRYPTION_WITH_FOOTER_KEY = format.NewEncryptionWithFooterKey()\n+\t\t} else {\n+\t\t\tccmd.ENCRYPTION_WITH_COLUMN_KEY = &format.EncryptionWithColumnKey{\n+\t\t\t\tKeyMetadata:  []byte(encryptProps.KeyMetadata()),\n+\t\t\t\tPathInSchema: c.column.ColumnPath(),\n+\t\t\t}\n+\t\t}\n+\t\tc.chunk.CryptoMetadata = ccmd\n+\n+\t\tencryptedFooter := c.props.FileEncryptionProperties().EncryptedFooter()\n+\t\tencryptMetadata := !encryptedFooter || !encryptProps.IsEncryptedWithFooterKey()\n+\t\tif encryptMetadata {\n+\t\t\t// Serialize and encrypt ColumnMetadata separately\n+\t\t\t// Thrift-serialize the ColumnMetaData structure,\n+\t\t\t// encrypt it with the column key, and write to encrypted_column_metadata\n+\t\t\tserializer := thrift.NewThriftSerializer()\n+\t\t\tdata, err := serializer.Write(context.Background(), c.chunk.MetaData)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tvar buf bytes.Buffer\n+\t\t\tmetaEncryptor.Encrypt(&buf, data)\n+\t\t\tc.chunk.EncryptedColumnMetadata = buf.Bytes()\n+\n+\t\t\tif encryptedFooter {\n+\t\t\t\tc.compressedSize = c.chunk.MetaData.GetTotalCompressedSize()\n+\t\t\t\tc.chunk.MetaData = nil\n+\t\t\t} else {\n+\t\t\t\t// Keep redacted metadata version for old readers\n+\t\t\t\tc.chunk.MetaData.Statistics = nil\n+\t\t\t\tc.chunk.MetaData.EncodingStats = nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// WriteTo will always return 0 as the int64 since the thrift writer library\n\nReview comment:\n       Hmm, I wonder if we should keep track of the bytes or explicit return a -1. Is the precedent for this elsewhere?\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n+\tif hasDict {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryIndexEncoding()))\n+\t\tif c.props.Version() == parquet.V1 {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t\t} else {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryPageEncoding()))\n+\t\t}\n+\t} else { // no dictionary\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.EncodingFor(c.column.Path())))\n+\t}\n+\n+\tthriftEncodings = append(thriftEncodings, format.Encoding(parquet.Encodings.RLE))\n+\t// Only PLAIN encoding is supported for fallback in V1\n+\t// TODO(zeroshade): Use user specified encoding for V2\n+\tif dictFallback {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t}\n+\tc.chunk.MetaData.Encodings = thriftEncodings\n+\n+\tthriftEncodingStats := make([]*format.PageEncodingStats, 0, len(dictEncodingStats)+len(dataEncodingStats))\n+\tfor k, v := range dictEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DICTIONARY_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tfor k, v := range dataEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DATA_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tc.chunk.MetaData.EncodingStats = thriftEncodingStats\n+\n+\tencryptProps := c.props.ColumnEncryptionProperties(c.column.Path())\n+\tif encryptProps != nil && encryptProps.IsEncrypted() {\n+\t\tccmd := format.NewColumnCryptoMetaData()\n+\t\tif encryptProps.IsEncryptedWithFooterKey() {\n+\t\t\tccmd.ENCRYPTION_WITH_FOOTER_KEY = format.NewEncryptionWithFooterKey()\n+\t\t} else {\n+\t\t\tccmd.ENCRYPTION_WITH_COLUMN_KEY = &format.EncryptionWithColumnKey{\n+\t\t\t\tKeyMetadata:  []byte(encryptProps.KeyMetadata()),\n+\t\t\t\tPathInSchema: c.column.ColumnPath(),\n+\t\t\t}\n+\t\t}\n+\t\tc.chunk.CryptoMetadata = ccmd\n+\n+\t\tencryptedFooter := c.props.FileEncryptionProperties().EncryptedFooter()\n+\t\tencryptMetadata := !encryptedFooter || !encryptProps.IsEncryptedWithFooterKey()\n+\t\tif encryptMetadata {\n+\t\t\t// Serialize and encrypt ColumnMetadata separately\n+\t\t\t// Thrift-serialize the ColumnMetaData structure,\n+\t\t\t// encrypt it with the column key, and write to encrypted_column_metadata\n+\t\t\tserializer := thrift.NewThriftSerializer()\n+\t\t\tdata, err := serializer.Write(context.Background(), c.chunk.MetaData)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tvar buf bytes.Buffer\n+\t\t\tmetaEncryptor.Encrypt(&buf, data)\n+\t\t\tc.chunk.EncryptedColumnMetadata = buf.Bytes()\n+\n+\t\t\tif encryptedFooter {\n+\t\t\t\tc.compressedSize = c.chunk.MetaData.GetTotalCompressedSize()\n+\t\t\t\tc.chunk.MetaData = nil\n+\t\t\t} else {\n+\t\t\t\t// Keep redacted metadata version for old readers\n+\t\t\t\tc.chunk.MetaData.Statistics = nil\n+\t\t\t\tc.chunk.MetaData.EncodingStats = nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// WriteTo will always return 0 as the int64 since the thrift writer library\n\nReview comment:\n       Hmm, I wonder if we should keep track of the bytes or explicit return a -1. Is there precedent for this elsewhere?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T17:03:35.082+0000",
                    "updated": "2021-08-18T17:03:35.082+0000",
                    "started": "2021-08-18T17:03:35.082+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639490",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/640294",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r693062384\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n\nReview comment:\n       Sure, that makes sense. I'll add that.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-20T16:10:49.428+0000",
                    "updated": "2021-08-20T16:10:49.428+0000",
                    "started": "2021-08-20T16:10:49.428+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "640294",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/640299",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r693068969\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n+\tif dictPageOffset > 0 {\n+\t\tc.chunk.MetaData.DictionaryPageOffset = &dictPageOffset\n+\t\tc.chunk.FileOffset = dictPageOffset + compressed\n+\t} else {\n+\t\tc.chunk.FileOffset = dataPageOffset + compressed\n+\t}\n+\n+\tc.chunk.MetaData.NumValues = nvalues\n+\tif indexPageOffset >= 0 {\n+\t\tc.chunk.MetaData.IndexPageOffset = &indexPageOffset\n+\t}\n+\n+\tc.chunk.MetaData.DataPageOffset = dataPageOffset\n+\tc.chunk.MetaData.TotalUncompressedSize = uncompressed\n+\tc.chunk.MetaData.TotalCompressedSize = compressed\n+\n+\tthriftEncodings := make([]format.Encoding, 0)\n+\tif hasDict {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryIndexEncoding()))\n+\t\tif c.props.Version() == parquet.V1 {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t\t} else {\n+\t\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.DictionaryPageEncoding()))\n+\t\t}\n+\t} else { // no dictionary\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding(c.props.EncodingFor(c.column.Path())))\n+\t}\n+\n+\tthriftEncodings = append(thriftEncodings, format.Encoding(parquet.Encodings.RLE))\n+\t// Only PLAIN encoding is supported for fallback in V1\n+\t// TODO(zeroshade): Use user specified encoding for V2\n+\tif dictFallback {\n+\t\tthriftEncodings = append(thriftEncodings, format.Encoding_PLAIN)\n+\t}\n+\tc.chunk.MetaData.Encodings = thriftEncodings\n+\n+\tthriftEncodingStats := make([]*format.PageEncodingStats, 0, len(dictEncodingStats)+len(dataEncodingStats))\n+\tfor k, v := range dictEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DICTIONARY_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tfor k, v := range dataEncodingStats {\n+\t\tthriftEncodingStats = append(thriftEncodingStats, &format.PageEncodingStats{\n+\t\t\tPageType: format.PageType_DATA_PAGE,\n+\t\t\tEncoding: format.Encoding(k),\n+\t\t\tCount:    v,\n+\t\t})\n+\t}\n+\tc.chunk.MetaData.EncodingStats = thriftEncodingStats\n+\n+\tencryptProps := c.props.ColumnEncryptionProperties(c.column.Path())\n+\tif encryptProps != nil && encryptProps.IsEncrypted() {\n+\t\tccmd := format.NewColumnCryptoMetaData()\n+\t\tif encryptProps.IsEncryptedWithFooterKey() {\n+\t\t\tccmd.ENCRYPTION_WITH_FOOTER_KEY = format.NewEncryptionWithFooterKey()\n+\t\t} else {\n+\t\t\tccmd.ENCRYPTION_WITH_COLUMN_KEY = &format.EncryptionWithColumnKey{\n+\t\t\t\tKeyMetadata:  []byte(encryptProps.KeyMetadata()),\n+\t\t\t\tPathInSchema: c.column.ColumnPath(),\n+\t\t\t}\n+\t\t}\n+\t\tc.chunk.CryptoMetadata = ccmd\n+\n+\t\tencryptedFooter := c.props.FileEncryptionProperties().EncryptedFooter()\n+\t\tencryptMetadata := !encryptedFooter || !encryptProps.IsEncryptedWithFooterKey()\n+\t\tif encryptMetadata {\n+\t\t\t// Serialize and encrypt ColumnMetadata separately\n+\t\t\t// Thrift-serialize the ColumnMetaData structure,\n+\t\t\t// encrypt it with the column key, and write to encrypted_column_metadata\n+\t\t\tserializer := thrift.NewThriftSerializer()\n+\t\t\tdata, err := serializer.Write(context.Background(), c.chunk.MetaData)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tvar buf bytes.Buffer\n+\t\t\tmetaEncryptor.Encrypt(&buf, data)\n+\t\t\tc.chunk.EncryptedColumnMetadata = buf.Bytes()\n+\n+\t\t\tif encryptedFooter {\n+\t\t\t\tc.compressedSize = c.chunk.MetaData.GetTotalCompressedSize()\n+\t\t\t\tc.chunk.MetaData = nil\n+\t\t\t} else {\n+\t\t\t\t// Keep redacted metadata version for old readers\n+\t\t\t\tc.chunk.MetaData.Statistics = nil\n+\t\t\t\tc.chunk.MetaData.EncodingStats = nil\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// WriteTo will always return 0 as the int64 since the thrift writer library\n\nReview comment:\n       So, in internal/utils/write_utils.go I do have what i called a `TellWrapper` which wrapped an io.Writer in order to keep track of the bytes written so that it could add a `Tell()` function to tell the current position which I use elsewhere. \r\n   \r\n   The main reason why I didn't do something like that here was that I didn't want every call to `WriteTo` to create an object wrapping the io.Writer in order to count the bytes written. It's just unfortunate that the current thrift interface for a `TStruct` has a `Write` function but doesn't return the number of bytes it wrote.\r\n   \r\n   I don't remember there being anywhere else that I do this so you might be right that it makes sense to explicitly return a -1 here instead of a 0. Personally as long as the comment documents what is being returned, I don't much mind whether it's a 0 or a -1 as long as we're explicit about it.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-20T16:21:41.043+0000",
                    "updated": "2021-08-20T16:21:41.043+0000",
                    "started": "2021-08-20T16:21:41.043+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "640299",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/640301",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r693070620\n\n\n\n##########\nFile path: go/parquet/metadata/file.go\n##########\n@@ -0,0 +1,480 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// DefaultCompressionType is used unless a different compression is specified\n+// in the properties\n+var DefaultCompressionType = compress.Codecs.Uncompressed\n+\n+// FileMetaDataBuilder is a proxy for more easily constructing file metadata\n+// particularly used when writing a file out.\n+type FileMetaDataBuilder struct {\n+\tmetadata       *format.FileMetaData\n+\tprops          *parquet.WriterProperties\n+\tschema         *schema.Schema\n+\trowGroups      []*format.RowGroup\n+\tcurrentRgBldr  *RowGroupMetaDataBuilder\n+\tkvmeta         KeyValueMetadata\n+\tcryptoMetadata *format.FileCryptoMetaData\n+}\n+\n+// NewFileMetadataBuilder will use the default writer properties if nil is passed for\n+// the writer properties and nil is allowable for the key value metadata.\n+func NewFileMetadataBuilder(schema *schema.Schema, props *parquet.WriterProperties, kvmeta KeyValueMetadata) *FileMetaDataBuilder {\n+\tvar crypto *format.FileCryptoMetaData\n+\tif props.FileEncryptionProperties() != nil && props.FileEncryptionProperties().EncryptedFooter() {\n+\t\tcrypto = format.NewFileCryptoMetaData()\n+\t}\n+\treturn &FileMetaDataBuilder{\n+\t\tmetadata:       format.NewFileMetaData(),\n+\t\tprops:          props,\n+\t\tschema:         schema,\n+\t\tkvmeta:         kvmeta,\n+\t\tcryptoMetadata: crypto,\n+\t}\n+}\n+\n+// GetFileCryptoMetaData returns the cryptographic information for encrypting/\n+// decrypting the file.\n+func (f *FileMetaDataBuilder) GetFileCryptoMetaData() *FileCryptoMetadata {\n+\tif f.cryptoMetadata == nil {\n+\t\treturn nil\n+\t}\n+\n+\tprops := f.props.FileEncryptionProperties()\n+\tf.cryptoMetadata.EncryptionAlgorithm = props.Algorithm().ToThrift()\n+\tkeyMetadata := props.FooterKeyMetadata()\n+\tif keyMetadata != \"\" {\n+\t\tf.cryptoMetadata.KeyMetadata = []byte(keyMetadata)\n+\t}\n+\n+\treturn &FileCryptoMetadata{f.cryptoMetadata, 0}\n+}\n+\n+// AppendRowGroup adds a rowgroup to the list and returns a builder\n+// for that row group\n+func (f *FileMetaDataBuilder) AppendRowGroup() *RowGroupMetaDataBuilder {\n+\tif f.rowGroups == nil {\n+\t\tf.rowGroups = make([]*format.RowGroup, 0, 1)\n+\t}\n+\n+\trg := format.NewRowGroup()\n+\tf.rowGroups = append(f.rowGroups, rg)\n+\tf.currentRgBldr = NewRowGroupMetaDataBuilder(f.props, f.schema, rg)\n+\treturn f.currentRgBldr\n+}\n+\n+// Finish will finalize the metadata of the number of rows, row groups,\n+// version etc. This will clear out this filemetadatabuilder so it can\n+// be re-used\n+func (f *FileMetaDataBuilder) Finish() (*FileMetaData, error) {\n+\ttotalRows := int64(0)\n+\tfor _, rg := range f.rowGroups {\n+\t\ttotalRows += rg.NumRows\n+\t}\n+\tf.metadata.NumRows = totalRows\n+\tf.metadata.RowGroups = f.rowGroups\n+\tswitch f.props.Version() {\n+\tcase parquet.V1:\n+\t\tf.metadata.Version = 1\n+\tcase parquet.V2:\n+\t\tf.metadata.Version = 2\n+\tdefault:\n+\t\tf.metadata.Version = 0\n+\t}\n+\tcreatedBy := f.props.CreatedBy()\n+\tf.metadata.CreatedBy = &createdBy\n+\n+\t// Users cannot set the `ColumnOrder` since we donot not have user defined sort order\n+\t// in the spec yet.\n+\t// We always default to `TYPE_DEFINED_ORDER`. We can expose it in\n+\t// the API once we have user defined sort orders in the Parquet format.\n+\t// TypeDefinedOrder implies choose SortOrder based on ConvertedType/PhysicalType\n+\ttypeDefined := format.NewTypeDefinedOrder()\n+\tcolOrder := &format.ColumnOrder{TYPE_ORDER: typeDefined}\n+\tf.metadata.ColumnOrders = make([]*format.ColumnOrder, f.schema.NumColumns())\n+\tfor idx := range f.metadata.ColumnOrders {\n+\t\tf.metadata.ColumnOrders[idx] = colOrder\n+\t}\n+\n+\tfileEncProps := f.props.FileEncryptionProperties()\n+\tif fileEncProps != nil && !fileEncProps.EncryptedFooter() {\n+\t\tvar signingAlgo parquet.Algorithm\n+\t\talgo := fileEncProps.Algorithm()\n+\t\tsigningAlgo.Aad.AadFileUnique = algo.Aad.AadFileUnique\n+\t\tsigningAlgo.Aad.SupplyAadPrefix = algo.Aad.SupplyAadPrefix\n+\t\tif !algo.Aad.SupplyAadPrefix {\n+\t\t\tsigningAlgo.Aad.AadPrefix = algo.Aad.AadPrefix\n+\t\t}\n+\t\tsigningAlgo.Algo = parquet.AesGcm\n+\t\tf.metadata.EncryptionAlgorithm = signingAlgo.ToThrift()\n+\t\tfooterSigningMetadata := f.props.FileEncryptionProperties().FooterKeyMetadata()\n+\t\tif footerSigningMetadata != \"\" {\n+\t\t\tf.metadata.FooterSigningKeyMetadata = []byte(footerSigningMetadata)\n+\t\t}\n+\t}\n+\n+\tf.metadata.Schema = schema.ToThrift(f.schema.Root())\n+\tf.metadata.KeyValueMetadata = f.kvmeta\n+\n+\tout := &FileMetaData{\n+\t\tFileMetaData: f.metadata,\n+\t\tversion:      NewAppVersion(f.metadata.GetCreatedBy()),\n+\t}\n+\tif err := out.initSchema(); err != nil {\n+\t\treturn nil, err\n+\t}\n+\tout.initColumnOrders()\n+\n+\tf.metadata = format.NewFileMetaData()\n+\tf.rowGroups = nil\n+\treturn out, nil\n+}\n+\n+// KeyValueMetadata is an alias for a slice of thrift keyvalue pairs\n+type KeyValueMetadata []*format.KeyValue\n+\n+// NewKeyValueMetadata is equivalent to make(KeyValueMetadata, 0)\n+func NewKeyValueMetadata() KeyValueMetadata {\n+\treturn make(KeyValueMetadata, 0)\n+}\n+\n+func (k *KeyValueMetadata) Append(key, value string) {\n+\t*k = append(*k, &format.KeyValue{Key: key, Value: &value})\n+}\n+\n+func (k KeyValueMetadata) Len() int { return len(k) }\n+\n+// Equals compares all of the metadata keys and values to check they are equal\n+func (k KeyValueMetadata) Equals(other KeyValueMetadata) bool {\n+\treturn reflect.DeepEqual(k, other)\n+}\n+\n+func (k KeyValueMetadata) Keys() (ret []string) {\n+\tret = make([]string, len(k))\n+\tfor idx, v := range k {\n+\t\tret[idx] = v.GetKey()\n+\t}\n+\treturn\n+}\n+\n+func (k KeyValueMetadata) Values() (ret []string) {\n+\tret = make([]string, len(k))\n+\tfor idx, v := range k {\n+\t\tret[idx] = v.GetValue()\n+\t}\n+\treturn\n+}\n+\n+func (k KeyValueMetadata) FindValue(key string) *string {\n+\tfor _, v := range k {\n+\t\tif v.Key == key {\n+\t\t\treturn v.Value\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// FileMetaData is a proxy around the underlying thrift FileMetaData object\n+// to make it easier to use and interact with.\n+type FileMetaData struct {\n+\t*format.FileMetaData\n+\tversion       *AppVersion\n+\tSchema        *schema.Schema\n+\tFileDecryptor encryption.FileDecryptor\n+\tmetadataLen   int\n+}\n+\n+// NewFileMetaData takes in the raw bytes of the serialized metadata to deserialize\n+// and will attempt to decrypt the footer if a decryptor is provided.\n+func NewFileMetaData(data []byte, fileDecryptor encryption.FileDecryptor) (*FileMetaData, error) {\n+\tmeta := format.NewFileMetaData()\n+\tif fileDecryptor != nil {\n+\t\tfooterDecryptor := fileDecryptor.GetFooterDecryptor()\n+\t\tdata = footerDecryptor.Decrypt(data)\n+\t}\n+\n+\tremain, err := thrift.DeserializeThrift(meta, data)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tf := &FileMetaData{\n+\t\tFileMetaData:  meta,\n+\t\tversion:       NewAppVersion(meta.GetCreatedBy()),\n+\t\tmetadataLen:   len(data) - int(remain),\n+\t\tFileDecryptor: fileDecryptor,\n+\t}\n+\n+\tf.initSchema()\n+\tf.initColumnOrders()\n+\n+\t// init keyvalue metadata\n\nReview comment:\n       Ah, that was a todo from an earlier version before i created that `KeyValueMetadata` type and the `NewKeyValueMetadata` function which initialized one. It can be removed, good catch.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-20T16:24:20.192+0000",
                    "updated": "2021-08-20T16:24:20.192+0000",
                    "started": "2021-08-20T16:24:20.191+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "640301",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/640715",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#issuecomment-903812078\n\n\n   Bump @emkornfield when you get a chance.\r\n   \r\n   Thanks!\r\n   \r\n   Also, @nickpoorman I've implemented the suggested comment, removed the old comment, and replied to your question when you have a chance. I'm curious your thoughts on my response. Thanks!\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-23T14:13:41.256+0000",
                    "updated": "2021-08-23T14:13:41.256+0000",
                    "started": "2021-08-23T14:13:41.256+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "640715",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/643496",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#issuecomment-908402814\n\n\n   Bump @emkornfield @nickpoorman Just trying to get eyes on this so i can get it merged and finally get the File package up here :)\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-30T14:44:04.383+0000",
                    "updated": "2021-08-30T14:44:04.383+0000",
                    "started": "2021-08-30T14:44:04.383+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "643496",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/643825",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#issuecomment-908869922\n\n\n   Sorry for the delay this is on my todo list for this week.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-31T03:32:59.997+0000",
                    "updated": "2021-08-31T03:32:59.997+0000",
                    "started": "2021-08-31T03:32:59.996+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "643825",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/644216",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#issuecomment-908402814\n\n\n   Bump @emkornfield @nickpoorman Just trying to get eyes on this so i can get it merged and finally get the File package up here :)\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-31T15:00:00.271+0000",
                    "updated": "2021-08-31T15:00:00.271+0000",
                    "started": "2021-08-31T15:00:00.271+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "644216",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/644307",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#issuecomment-908869922\n\n\n   Sorry for the delay this is on my todo list for this week.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-31T15:09:34.274+0000",
                    "updated": "2021-08-31T15:09:34.274+0000",
                    "started": "2021-08-31T15:09:34.274+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "644307",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/644718",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r699813228\n\n\n\n##########\nFile path: go/parquet/metadata/app_version.go\n##########\n@@ -0,0 +1,172 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"regexp\"\n+\t\"strconv\"\n+\t\"strings\"\n+\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+var (\n+\t// Regular expression for the version format\n+\t// major . minor . patch unknown - prerelease.x + build info\n+\t// Eg: 1.5.0ab-cdh5.5.0+cd\n+\tversionRx = regexp.MustCompile(`^(\\d+)\\.(\\d+)\\.(\\d+)([^-+]*)?(?:-([^+]*))?(?:\\+(.*))?$`)\n+\t// Regular expression for the application format\n+\t// application_name version VERSION_FORMAT (build build_name)\n+\t// Eg: parquet-cpp version 1.5.0ab-xyz5.5.0+cd (build abcd)\n+\tapplicationRx = regexp.MustCompile(`^(.*?)\\s*(?:(version\\s*(?:([^(]*?)\\s*(?:\\(\\s*build\\s*([^)]*?)\\s*\\))?)?)?)$`)\n+\n+\t// Parquet816FixedVersion is the version used for fixing PARQUET-816\n+\t// that changed the padding calculations for dictionary headers on row groups.\n+\tParquet816FixedVersion      = NewAppVersionExplicit(\"parquet-mr\", 1, 2, 9)\n+\tparquet251FixedVersion      = NewAppVersionExplicit(\"parquet-mr\", 1, 8, 0)\n+\tparquetCPPFixedStatsVersion = NewAppVersionExplicit(\"parquet-cpp\", 1, 3, 0)\n+\tparquetMRFixedStatsVersion  = NewAppVersionExplicit(\"parquet-mr\", 1, 10, 0)\n+)\n+\n+// AppVersion represents a specific application version either read from\n+// or written to a parquet file.\n+type AppVersion struct {\n+\tApp     string\n+\tBuild   string\n+\tVersion struct {\n+\t\tMajor      int\n+\t\tMinor      int\n+\t\tPatch      int\n+\t\tUnknown    string\n+\t\tPreRelease string\n+\t\tBuildInfo  string\n+\t}\n+}\n+\n+// NewAppVersionExplicit is a convenience function to construct a specific\n+// application version from the given app string and version\n+func NewAppVersionExplicit(app string, major, minor, patch int) *AppVersion {\n+\tv := &AppVersion{App: app}\n+\tv.Version.Major = major\n+\tv.Version.Minor = minor\n+\tv.Version.Patch = patch\n+\treturn v\n+}\n+\n+// NewAppVersion parses a \"created by\" string such as \"parquet-go 1.0.0\".\n+//\n+// It also supports handling pre-releases and build info such as\n+// \tparquet-cpp version 1.5.0ab-xyz5.5.0+cd (build abcd)\n+func NewAppVersion(createdby string) *AppVersion {\n+\tv := &AppVersion{}\n+\n+\tvar ver []string\n+\n+\tm := applicationRx.FindStringSubmatch(strings.ToLower(createdby))\n+\tif len(m) >= 4 {\n+\t\tv.App = m[1]\n+\t\tv.Build = m[4]\n+\t\tver = versionRx.FindStringSubmatch(m[3])\n+\t} else {\n+\t\tv.App = \"unknown\"\n+\t}\n+\n+\tif len(ver) >= 7 {\n+\t\tv.Version.Major, _ = strconv.Atoi(ver[1])\n+\t\tv.Version.Minor, _ = strconv.Atoi(ver[2])\n+\t\tv.Version.Patch, _ = strconv.Atoi(ver[3])\n+\t\tv.Version.Unknown = ver[4]\n+\t\tv.Version.PreRelease = ver[5]\n+\t\tv.Version.BuildInfo = ver[6]\n+\t}\n+\treturn v\n+}\n+\n+// LessThan compares the app versions and returns true if this version\n+// is \"less than\" the passed version.\n+//\n+// If the apps don't match, this always returns false. Otherwise it compares\n\nReview comment:\n       should this be ternary?  the contract seems strange?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-09-01T03:17:33.496+0000",
                    "updated": "2021-09-01T03:17:33.496+0000",
                    "started": "2021-09-01T03:17:33.495+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "644718",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/644719",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r699813598\n\n\n\n##########\nFile path: go/parquet/metadata/app_version.go\n##########\n@@ -0,0 +1,172 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"regexp\"\n+\t\"strconv\"\n+\t\"strings\"\n+\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+var (\n+\t// Regular expression for the version format\n+\t// major . minor . patch unknown - prerelease.x + build info\n+\t// Eg: 1.5.0ab-cdh5.5.0+cd\n+\tversionRx = regexp.MustCompile(`^(\\d+)\\.(\\d+)\\.(\\d+)([^-+]*)?(?:-([^+]*))?(?:\\+(.*))?$`)\n+\t// Regular expression for the application format\n+\t// application_name version VERSION_FORMAT (build build_name)\n+\t// Eg: parquet-cpp version 1.5.0ab-xyz5.5.0+cd (build abcd)\n+\tapplicationRx = regexp.MustCompile(`^(.*?)\\s*(?:(version\\s*(?:([^(]*?)\\s*(?:\\(\\s*build\\s*([^)]*?)\\s*\\))?)?)?)$`)\n+\n+\t// Parquet816FixedVersion is the version used for fixing PARQUET-816\n+\t// that changed the padding calculations for dictionary headers on row groups.\n+\tParquet816FixedVersion      = NewAppVersionExplicit(\"parquet-mr\", 1, 2, 9)\n+\tparquet251FixedVersion      = NewAppVersionExplicit(\"parquet-mr\", 1, 8, 0)\n+\tparquetCPPFixedStatsVersion = NewAppVersionExplicit(\"parquet-cpp\", 1, 3, 0)\n+\tparquetMRFixedStatsVersion  = NewAppVersionExplicit(\"parquet-mr\", 1, 10, 0)\n+)\n+\n+// AppVersion represents a specific application version either read from\n+// or written to a parquet file.\n+type AppVersion struct {\n+\tApp     string\n+\tBuild   string\n+\tVersion struct {\n+\t\tMajor      int\n+\t\tMinor      int\n+\t\tPatch      int\n+\t\tUnknown    string\n+\t\tPreRelease string\n+\t\tBuildInfo  string\n+\t}\n+}\n+\n+// NewAppVersionExplicit is a convenience function to construct a specific\n+// application version from the given app string and version\n+func NewAppVersionExplicit(app string, major, minor, patch int) *AppVersion {\n+\tv := &AppVersion{App: app}\n+\tv.Version.Major = major\n+\tv.Version.Minor = minor\n+\tv.Version.Patch = patch\n+\treturn v\n+}\n+\n+// NewAppVersion parses a \"created by\" string such as \"parquet-go 1.0.0\".\n+//\n+// It also supports handling pre-releases and build info such as\n+// \tparquet-cpp version 1.5.0ab-xyz5.5.0+cd (build abcd)\n+func NewAppVersion(createdby string) *AppVersion {\n+\tv := &AppVersion{}\n+\n+\tvar ver []string\n+\n+\tm := applicationRx.FindStringSubmatch(strings.ToLower(createdby))\n+\tif len(m) >= 4 {\n+\t\tv.App = m[1]\n+\t\tv.Build = m[4]\n+\t\tver = versionRx.FindStringSubmatch(m[3])\n+\t} else {\n+\t\tv.App = \"unknown\"\n+\t}\n+\n+\tif len(ver) >= 7 {\n+\t\tv.Version.Major, _ = strconv.Atoi(ver[1])\n+\t\tv.Version.Minor, _ = strconv.Atoi(ver[2])\n+\t\tv.Version.Patch, _ = strconv.Atoi(ver[3])\n+\t\tv.Version.Unknown = ver[4]\n+\t\tv.Version.PreRelease = ver[5]\n+\t\tv.Version.BuildInfo = ver[6]\n+\t}\n+\treturn v\n+}\n+\n+// LessThan compares the app versions and returns true if this version\n+// is \"less than\" the passed version.\n+//\n+// If the apps don't match, this always returns false. Otherwise it compares\n+// the major versions first, then the minor versions, and finally the patch\n+// versions.\n+//\n+// Pre-release and build info are not considered.\n+func (v AppVersion) LessThan(other *AppVersion) bool {\n+\tswitch {\n+\tcase v.App != other.App:\n+\t\treturn false\n+\tcase v.Version.Major < other.Version.Major:\n+\t\treturn true\n+\tcase v.Version.Major > other.Version.Major:\n+\t\treturn false\n+\tcase v.Version.Minor < other.Version.Minor:\n+\t\treturn true\n+\tcase v.Version.Minor > other.Version.Minor:\n+\t\treturn false\n+\t}\n+\n+\treturn v.Version.Patch < other.Version.Patch\n+}\n+\n+// Equal only compares the Application and major/minor/patch versions.\n+//\n+// Pre-release and build info are not considered.\n+func (v AppVersion) Equal(other *AppVersion) bool {\n+\treturn v.App == other.App &&\n+\t\tv.Version.Major == other.Version.Major &&\n+\t\tv.Version.Minor == other.Version.Minor &&\n+\t\tv.Version.Patch == other.Version.Patch\n+}\n+\n+// HasCorrectStatistics checks whether or not the statistics are valid to be used\n+// based on the primitive type and the version since previous versions had issues with\n+// properly computing stats.\n+//\n+// Reference: parquet-cpp/src/parquet/metadata.cc\n+//\n+// PARQUET-686 has more discussion on statistics\n\nReview comment:\n       We haven't done this elsewhere but it isn't the only statistics bug from he C++ perspective.  Decimal logical types comparisons for C++ where broken until recently and null statistics for list types are still broken. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-09-01T03:18:47.079+0000",
                    "updated": "2021-09-01T03:18:47.079+0000",
                    "started": "2021-09-01T03:18:47.079+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "644719",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/644720",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r699813907\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n\nReview comment:\n       is there anyway in go to get  abetter stent that these aren't misalligned?  builder pattern?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-09-01T03:19:51.698+0000",
                    "updated": "2021-09-01T03:19:51.698+0000",
                    "started": "2021-09-01T03:19:51.698+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "644720",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/644721",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r699814498\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n\nReview comment:\n       can we be more specific.  Does this number include nulls? \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-09-01T03:21:36.387+0000",
                    "updated": "2021-09-01T03:21:36.387+0000",
                    "started": "2021-09-01T03:21:36.387+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "644721",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/644722",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r699814982\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n\nReview comment:\n       nit: should there be bloom filter info here as well?\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n\nReview comment:\n       nit: should there be bloom filter info in this file as well?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-09-01T03:23:28.134+0000",
                    "updated": "2021-09-01T03:23:28.134+0000",
                    "started": "2021-09-01T03:23:28.134+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "644722",
                    "issueId": "13395709"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/worklog/644723",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #10951:\nURL: https://github.com/apache/arrow/pull/10951#discussion_r699815567\n\n\n\n##########\nFile path: go/parquet/metadata/column_chunk.go\n##########\n@@ -0,0 +1,385 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package metadata\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"io\"\n+\t\"reflect\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/compress\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\tformat \"github.com/apache/arrow/go/parquet/internal/gen-go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/thrift\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+\t\"golang.org/x/xerrors\"\n+)\n+\n+// PageEncodingStats is used for counting the number of pages of specific\n+// types with the given internal encoding.\n+type PageEncodingStats struct {\n+\tEncoding parquet.Encoding\n+\tPageType format.PageType\n+}\n+\n+func makeColumnStats(metadata *format.ColumnMetaData, descr *schema.Column, mem memory.Allocator) TypedStatistics {\n+\tif descr.ColumnOrder() == parquet.ColumnOrders.TypeDefinedOrder {\n+\t\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.MinValue, metadata.Statistics.MaxValue,\n+\t\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\t\tmetadata.Statistics.IsSetMaxValue() || metadata.Statistics.IsSetMinValue(),\n+\t\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+\t}\n+\treturn NewStatisticsFromEncoded(descr, mem, metadata.Statistics.Min, metadata.Statistics.Max,\n+\t\tmetadata.NumValues-metadata.Statistics.GetNullCount(),\n+\t\tmetadata.Statistics.GetNullCount(), metadata.Statistics.GetDistinctCount(),\n+\t\tmetadata.Statistics.IsSetMax() || metadata.Statistics.IsSetMin(),\n+\t\tmetadata.Statistics.IsSetNullCount(), metadata.Statistics.IsSetDistinctCount())\n+}\n+\n+// ColumnChunkMetaData is a proxy around format.ColumnChunkMetaData\n+// containing all of the information and metadata for a given column chunk\n+// and it's associated Column\n+type ColumnChunkMetaData struct {\n+\tcolumn        *format.ColumnChunk\n+\tcolumnMeta    *format.ColumnMetaData\n+\tdecryptedMeta format.ColumnMetaData\n+\tdescr         *schema.Column\n+\twriterVersion *AppVersion\n+\tencodings     []parquet.Encoding\n+\tencodingStats []format.PageEncodingStats\n+\tpossibleStats TypedStatistics\n+\tmem           memory.Allocator\n+}\n+\n+// NewColumnChunkMetaData creates an instance of the metadata from a column chunk and descriptor\n+//\n+// this is primarily used internally or between the subpackages. ColumnChunkMetaDataBuilder should\n+// be used by consumers instead of using this directly.\n+func NewColumnChunkMetaData(column *format.ColumnChunk, descr *schema.Column, writerVersion *AppVersion, rowGroupOrdinal, columnOrdinal int16, fileDecryptor encryption.FileDecryptor) (*ColumnChunkMetaData, error) {\n+\tc := &ColumnChunkMetaData{\n+\t\tcolumn:        column,\n+\t\tcolumnMeta:    column.GetMetaData(),\n+\t\tdescr:         descr,\n+\t\twriterVersion: writerVersion,\n+\t\tmem:           memory.DefaultAllocator,\n+\t}\n+\tif column.IsSetCryptoMetadata() {\n+\t\tccmd := column.CryptoMetadata\n+\n+\t\tif ccmd.IsSetENCRYPTION_WITH_COLUMN_KEY() {\n+\t\t\tif fileDecryptor != nil && fileDecryptor.Properties() != nil {\n+\t\t\t\t// should decrypt metadata\n+\t\t\t\tpath := parquet.ColumnPath(ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetPathInSchema())\n+\t\t\t\tkeyMetadata := ccmd.ENCRYPTION_WITH_COLUMN_KEY.GetKeyMetadata()\n+\t\t\t\taadColumnMetadata := encryption.CreateModuleAad(fileDecryptor.FileAad(), encryption.ColumnMetaModule, rowGroupOrdinal, columnOrdinal, -1)\n+\t\t\t\tdecryptor := fileDecryptor.GetColumnMetaDecryptor(path.String(), string(keyMetadata), aadColumnMetadata)\n+\t\t\t\tthrift.DeserializeThrift(&c.decryptedMeta, decryptor.Decrypt(column.GetEncryptedColumnMetadata()))\n+\t\t\t\tc.columnMeta = &c.decryptedMeta\n+\t\t\t} else {\n+\t\t\t\treturn nil, xerrors.New(\"cannot decrypt column metadata. file decryption not setup correctly\")\n+\t\t\t}\n+\t\t}\n+\t}\n+\tfor _, enc := range c.columnMeta.Encodings {\n+\t\tc.encodings = append(c.encodings, parquet.Encoding(enc))\n+\t}\n+\tfor _, enc := range c.columnMeta.EncodingStats {\n+\t\tc.encodingStats = append(c.encodingStats, *enc)\n+\t}\n+\treturn c, nil\n+}\n+\n+// CryptoMetadata returns the cryptographic metadata for how this column was\n+// encrypted and how to decrypt it.\n+func (c *ColumnChunkMetaData) CryptoMetadata() *format.ColumnCryptoMetaData {\n+\treturn c.column.GetCryptoMetadata()\n+}\n+\n+// FileOffset is the location in the file where the column data begins\n+func (c *ColumnChunkMetaData) FileOffset() int64 { return c.column.FileOffset }\n+\n+// FilePath gives the name of the parquet file if provided in the metadata\n+func (c *ColumnChunkMetaData) FilePath() string { return c.column.GetFilePath() }\n+\n+// Type is the physical storage type used in the parquet file for this column chunk.\n+func (c *ColumnChunkMetaData) Type() parquet.Type { return parquet.Type(c.columnMeta.Type) }\n+\n+// NumValues is the number of values stored in just this chunk\n+func (c *ColumnChunkMetaData) NumValues() int64 { return c.columnMeta.NumValues }\n+\n+// PathInSchema is the full path to this column from the root of the schema including\n+// any nested columns\n+func (c *ColumnChunkMetaData) PathInSchema() parquet.ColumnPath {\n+\treturn c.columnMeta.GetPathInSchema()\n+}\n+\n+// Compression provides the type of compression used for this particular chunk.\n+func (c *ColumnChunkMetaData) Compression() compress.Compression {\n+\treturn compress.Compression(c.columnMeta.Codec)\n+}\n+\n+// Encodings returns the list of different encodings used in this chunk\n+func (c *ColumnChunkMetaData) Encodings() []parquet.Encoding { return c.encodings }\n+\n+// EncodingStats connects the order of encodings based on the list of pages and types\n+func (c *ColumnChunkMetaData) EncodingStats() []PageEncodingStats {\n+\tret := make([]PageEncodingStats, len(c.encodingStats))\n+\tfor idx := range ret {\n+\t\tret[idx].Encoding = parquet.Encoding(c.encodingStats[idx].Encoding)\n+\t\tret[idx].PageType = c.encodingStats[idx].PageType\n+\t}\n+\treturn ret\n+}\n+\n+// HasDictionaryPage returns true if there is a dictionary page offset set in\n+// this metadata.\n+func (c *ColumnChunkMetaData) HasDictionaryPage() bool {\n+\treturn c.columnMeta.IsSetDictionaryPageOffset()\n+}\n+\n+// DictionaryPageOffset returns the location in the file where the dictionary page starts\n+func (c *ColumnChunkMetaData) DictionaryPageOffset() int64 {\n+\treturn c.columnMeta.GetDictionaryPageOffset()\n+}\n+\n+// DataPageOffset returns the location in the file where the data pages begin for this column\n+func (c *ColumnChunkMetaData) DataPageOffset() int64 { return c.columnMeta.GetDataPageOffset() }\n+\n+// HasIndexPage returns true if the offset for the index page is set in the metadata\n+func (c *ColumnChunkMetaData) HasIndexPage() bool { return c.columnMeta.IsSetIndexPageOffset() }\n+\n+// IndexPageOffset is the location in the file where the index page starts.\n+func (c *ColumnChunkMetaData) IndexPageOffset() int64 { return c.columnMeta.GetIndexPageOffset() }\n+\n+// TotalCompressedSize will be equal to TotalUncompressedSize if the data is not compressed.\n+// Otherwise this will be the size of the actual data in the file.\n+func (c *ColumnChunkMetaData) TotalCompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalCompressedSize()\n+}\n+\n+// TotalUncompressedSize is the total size of the raw data after uncompressing the chunk\n+func (c *ColumnChunkMetaData) TotalUncompressedSize() int64 {\n+\treturn c.columnMeta.GetTotalUncompressedSize()\n+}\n+\n+// StatsSet returns true only if there are statistics set in the metadata and the column\n+// descriptor has a sort order that is not SortUnknown\n+//\n+// It also checks the writer version to ensure that it was not written by a version\n+// of parquet which is known to have incorrect stat computations.\n+func (c *ColumnChunkMetaData) StatsSet() (bool, error) {\n+\tif !c.columnMeta.IsSetStatistics() || c.descr.SortOrder() == schema.SortUNKNOWN {\n+\t\treturn false, nil\n+\t}\n+\n+\tif c.possibleStats == nil {\n+\t\tc.possibleStats = makeColumnStats(c.columnMeta, c.descr, c.mem)\n+\t}\n+\n+\tencoded, err := c.possibleStats.Encode()\n+\tif err != nil {\n+\t\treturn false, err\n+\t}\n+\treturn c.writerVersion.HasCorrectStatistics(c.Type(), encoded, c.descr.SortOrder()), nil\n+}\n+\n+func (c *ColumnChunkMetaData) Equals(other *ColumnChunkMetaData) bool {\n+\treturn reflect.DeepEqual(c.columnMeta, other.columnMeta)\n+}\n+\n+// Statistics can return nil if there are no stats in this metadata\n+func (c *ColumnChunkMetaData) Statistics() (TypedStatistics, error) {\n+\tok, err := c.StatsSet()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif ok {\n+\t\treturn c.possibleStats, nil\n+\t}\n+\treturn nil, nil\n+}\n+\n+// ColumnChunkMetaDataBuilder is used during writing to construct metadata\n+// for a given column chunk while writing, providing a proxy around constructing\n+// the actual thrift object.\n+type ColumnChunkMetaDataBuilder struct {\n+\tchunk  *format.ColumnChunk\n+\tprops  *parquet.WriterProperties\n+\tcolumn *schema.Column\n+\n+\tcompressedSize int64\n+}\n+\n+func NewColumnChunkMetaDataBuilder(props *parquet.WriterProperties, column *schema.Column) *ColumnChunkMetaDataBuilder {\n+\treturn NewColumnChunkMetaDataBuilderWithContents(props, column, format.NewColumnChunk())\n+}\n+\n+// NewColumnChunkMetaDataBuilderWithContents will construct a builder and start it with the provided\n+// column chunk information rather than with an empty column chunk.\n+func NewColumnChunkMetaDataBuilderWithContents(props *parquet.WriterProperties, column *schema.Column, chunk *format.ColumnChunk) *ColumnChunkMetaDataBuilder {\n+\tb := &ColumnChunkMetaDataBuilder{\n+\t\tprops:  props,\n+\t\tcolumn: column,\n+\t\tchunk:  chunk,\n+\t}\n+\tb.init(chunk)\n+\treturn b\n+}\n+\n+// Contents returns the underlying thrift ColumnChunk object so that it can be used\n+// for constructing or duplicating column metadata\n+func (c *ColumnChunkMetaDataBuilder) Contents() *format.ColumnChunk { return c.chunk }\n+\n+func (c *ColumnChunkMetaDataBuilder) init(chunk *format.ColumnChunk) {\n+\tc.chunk = chunk\n+\tif !c.chunk.IsSetMetaData() {\n+\t\tc.chunk.MetaData = format.NewColumnMetaData()\n+\t}\n+\tc.chunk.MetaData.Type = format.Type(c.column.PhysicalType())\n+\tc.chunk.MetaData.PathInSchema = schema.ColumnPathFromNode(c.column.SchemaNode())\n+\tc.chunk.MetaData.Codec = format.CompressionCodec(c.props.CompressionFor(c.column.Path()))\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetFilePath(val string) {\n+\tc.chunk.FilePath = &val\n+}\n+\n+// Descr returns the associated column descriptor for this column chunk\n+func (c *ColumnChunkMetaDataBuilder) Descr() *schema.Column { return c.column }\n+\n+func (c *ColumnChunkMetaDataBuilder) TotalCompressedSize() int64 {\n+\t// if this column is encrypted, after Finish is called, the MetaData\n+\t// field is set to nil and we store the compressed size so return that\n+\tif c.chunk.MetaData == nil {\n+\t\treturn c.compressedSize\n+\t}\n+\treturn c.chunk.MetaData.GetTotalCompressedSize()\n+}\n+\n+func (c *ColumnChunkMetaDataBuilder) SetStats(val EncodedStatistics) {\n+\tc.chunk.MetaData.Statistics = val.ToThrift()\n+}\n+\n+// Finish finalizes the metadata with the given offsets,\n+// flushes any compression that needs to be done, and performs\n+// any encryption if an encryptor is provided.\n+func (c *ColumnChunkMetaDataBuilder) Finish(nvalues, dictPageOffset, indexPageOffset, dataPageOffset, compressed, uncompressed int64, hasDict, dictFallback bool, dictEncodingStats, dataEncodingStats map[parquet.Encoding]int32, metaEncryptor encryption.Encryptor) error {\n\nReview comment:\n       does it make sense to make a structure here to pass through instead of all these parameters?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-09-01T03:25:10.337+0000",
                    "updated": "2021-09-01T03:25:10.337+0000",
                    "started": "2021-09-01T03:25:10.337+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "644723",
                    "issueId": "13395709"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
            "id": "7",
            "description": "The sub-task of the issue",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
            "name": "Sub-task",
            "subtask": true,
            "avatarId": 21146
        },
        "timespent": 55200,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@4051b1c6[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@630806bb[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6022682[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@251cc527[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@8d6af24[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@40037a40[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@572262f4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@14d4b0ca[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@29c692fb[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@72cc2cf9[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@363dd596[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@7de717b[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 55200,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Sep 13 14:32:32 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-09-13T14:32:32.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-13646/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2021-08-17T15:25:59.000+0000",
        "updated": "2021-09-13T14:32:33.000+0000",
        "timeoriginalestimate": null,
        "description": "The next chunk of code for the native Go implementation of Parquet: the metadata package which includes all of the types and functionality for handling Parquet metadata and statistics.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "15h 20m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 55200
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Go][Parquet] Add Metadata Package",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395709/comment/17414238",
                    "id": "17414238",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
                        "name": "zeroshade",
                        "key": "zeroshade",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
                        },
                        "displayName": "Matthew Topol",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 10951\n[https://github.com/apache/arrow/pull/10951]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
                        "name": "zeroshade",
                        "key": "zeroshade",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
                        },
                        "displayName": "Matthew Topol",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2021-09-13T14:32:32.997+0000",
                    "updated": "2021-09-13T14:32:32.997+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0tziw:",
        "customfield_12314139": null
    }
}