{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13103395",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395",
    "key": "ARROW-1559",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12341352",
                "id": "12341352",
                "name": "0.8.0",
                "archived": false,
                "released": true,
                "releaseDate": "2017-12-18"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "Analytics",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": null,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": null,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1559/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 0,
            "worklogs": []
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": null,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@4947f964[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@36846ac8[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@783616b1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@6d71e155[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@23d2d0ba[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@4f64e4ff[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@72ff03c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@6536d2c5[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@50d499d9[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@38ffcc20[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@40a97513[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@135a3739[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": null,
        "customfield_12312520": null,
        "customfield_12312521": "Sat Nov 18 21:37:59 UTC 2017",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2017-11-21T23:55:12.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1559/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2017-09-19T21:50:10.000+0000",
        "updated": "2017-11-21T23:55:12.000+0000",
        "timeoriginalestimate": null,
        "description": null,
        "customfield_10010": null,
        "timetracking": {},
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Kernel implementations for \"unique\" (compute distinct elements of array)",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16220896",
                    "id": "16220896",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "All hash table related functions will need an option to address how nulls are handled. For example, null could be included or excluded from the unique values depending on the needs of the application (sometimes null is data, other times it is to be ignored). ",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-10-26T17:53:36.764+0000",
                    "updated": "2017-10-26T17:53:36.764+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16223771",
                    "id": "16223771",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy opened a new pull request #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266\n \n \n   Only intended to implement selective categorical conversion in `to_pandas()` but it seems that there is a lot missing to do this in a clean fashion.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-28T23:09:10.574+0000",
                    "updated": "2017-10-28T23:09:10.574+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16223776",
                    "id": "16223776",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-340226336\n \n \n   @wesm I'm struggling a bit here on how to continue. The next step would be to implement `X->Dictionary` casts for columns. This can get a bit tricky as we first need to fill a `UniqueBuilder` and then go over all chunks and build the resulting arrays. This then sadly means that we have a single stateful cast kernel instance.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-28T23:23:12.118+0000",
                    "updated": "2017-10-28T23:23:12.118+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16224260",
                    "id": "16224260",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-340315454\n \n \n   Let me spend some time looking at this and I'll let you know my thoughts, worst case by EOD tomorrow (Monday)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-30T00:22:32.789+0000",
                    "updated": "2017-10-30T00:22:32.789+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16225491",
                    "id": "16225491",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r147791018\n \n \n\n ##########\n File path: cpp/src/arrow/builder.cc\n ##########\n @@ -1110,20 +1034,202 @@ Status DictionaryBuilder<T>::AppendDictionary(const Scalar& value) {\n   }                                                                                 \\\n                                                                                     \\\n   template <>                                                                       \\\n-  int DictionaryBuilder<Type>::HashValue(const WrappedBinary& value) {              \\\n+  WrappedBinary UniqueBuilder<Type>::GetDictionaryValue(int64_t index) {            \\\n+    int32_t v_len;                                                                  \\\n+    const uint8_t* v = dict_builder_.GetValue(static_cast<int64_t>(index), &v_len); \\\n+    return WrappedBinary(v, v_len);                                                 \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  template <>                                                                       \\\n+  Status UniqueBuilder<Type>::AppendDictionary(const WrappedBinary& value) {        \\\n+    return dict_builder_.Append(value.ptr_, value.length_);                         \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  template <>                                                                       \\\n+  int UniqueBuilder<Type>::HashValue(const WrappedBinary& value) {                  \\\n     return HashUtil::Hash(value.ptr_, value.length_, 0);                            \\\n   }                                                                                 \\\n                                                                                     \\\n   template <>                                                                       \\\n-  bool DictionaryBuilder<Type>::SlotDifferent(hash_slot_t index,                    \\\n-                                              const WrappedBinary& value) {         \\\n+  bool UniqueBuilder<Type>::SlotDifferent(hash_slot_t index,                        \\\n+                                          const WrappedBinary& value) {             \\\n     int32_t other_length;                                                           \\\n     const uint8_t* other_value =                                                    \\\n         dict_builder_.GetValue(static_cast<int64_t>(index), &other_length);         \\\n     return !(other_length == value.length_ &&                                       \\\n              0 == memcmp(other_value, value.ptr_, value.length_));                  \\\n   }\n \n+BINARY_UNIQUE_SPECIALIZATIONS(StringType);\n+BINARY_UNIQUE_SPECIALIZATIONS(BinaryType);\n+\n+template <typename T>\n+Status UniqueBuilder<T>::FinishInternal(std::shared_ptr<ArrayData>* out) {\n+  return dict_builder_.FinishInternal(out);\n+}\n+\n+template class UniqueBuilder<UInt8Type>;\n+template class UniqueBuilder<UInt16Type>;\n+template class UniqueBuilder<UInt32Type>;\n+template class UniqueBuilder<UInt64Type>;\n+template class UniqueBuilder<Int8Type>;\n+template class UniqueBuilder<Int16Type>;\n+template class UniqueBuilder<Int32Type>;\n+template class UniqueBuilder<Int64Type>;\n+template class UniqueBuilder<Date32Type>;\n+template class UniqueBuilder<Date64Type>;\n+template class UniqueBuilder<Time32Type>;\n+template class UniqueBuilder<Time64Type>;\n+template class UniqueBuilder<TimestampType>;\n+template class UniqueBuilder<FloatType>;\n+template class UniqueBuilder<DoubleType>;\n+template class UniqueBuilder<FixedSizeBinaryType>;\n+template class UniqueBuilder<BinaryType>;\n+template class UniqueBuilder<StringType>;\n+\n+// ----------------------------------------------------------------------\n+// DictionaryBuilder\n+\n+template <typename T>\n+DictionaryBuilder<T>::DictionaryBuilder(const std::shared_ptr<DataType>& type,\n+                                        MemoryPool* pool)\n+    : ArrayBuilder(type, pool), unique_builder_(type, pool), values_builder_(pool) {}\n+\n+DictionaryBuilder<NullType>::DictionaryBuilder(const std::shared_ptr<DataType>& type,\n+                                               MemoryPool* pool)\n+    : ArrayBuilder(type, pool), values_builder_(pool) {}\n+\n+DictionaryBuilder<NullType>::~DictionaryBuilder() {}\n+\n+template <>\n+DictionaryBuilder<FixedSizeBinaryType>::DictionaryBuilder(\n+    const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+    : ArrayBuilder(type, pool), unique_builder_(type, pool), values_builder_(pool) {}\n+\n+template <typename T>\n+Status DictionaryBuilder<T>::Init(int64_t elements) {\n+  RETURN_NOT_OK(ArrayBuilder::Init(elements));\n+  RETURN_NOT_OK(unique_builder_.Init(elements));\n+  return values_builder_.Init(elements);\n+}\n+\n+Status DictionaryBuilder<NullType>::Init(int64_t elements) {\n+  RETURN_NOT_OK(ArrayBuilder::Init(elements));\n+  return values_builder_.Init(elements);\n+}\n+\n+template <typename T>\n+Status DictionaryBuilder<T>::Resize(int64_t capacity) {\n+  if (capacity < kMinBuilderCapacity) {\n+    capacity = kMinBuilderCapacity;\n+  }\n+\n+  if (capacity_ == 0) {\n+    return Init(capacity);\n+  } else {\n+    RETURN_NOT_OK(unique_builder_.Resize(capacity));\n+    return ArrayBuilder::Resize(capacity);\n+  }\n+}\n+\n+Status DictionaryBuilder<NullType>::Resize(int64_t capacity) {\n+  if (capacity < kMinBuilderCapacity) {\n+    capacity = kMinBuilderCapacity;\n+  }\n+\n+  if (capacity_ == 0) {\n+    return Init(capacity);\n+  } else {\n+    return ArrayBuilder::Resize(capacity);\n+  }\n+}\n+\n+template <typename T>\n+Status DictionaryBuilder<T>::FinishInternal(std::shared_ptr<ArrayData>* out) {\n+  std::shared_ptr<Array> dictionary;\n+  RETURN_NOT_OK(unique_builder_.Finish(&dictionary));\n+\n+  RETURN_NOT_OK(values_builder_.FinishInternal(out));\n+  (*out)->type = std::make_shared<DictionaryType>((*out)->type, dictionary);\n+  return Status::OK();\n+}\n+\n+Status DictionaryBuilder<NullType>::FinishInternal(std::shared_ptr<ArrayData>* out) {\n+  std::shared_ptr<Array> dictionary = std::make_shared<NullArray>(0);\n+\n+  RETURN_NOT_OK(values_builder_.FinishInternal(out));\n+  (*out)->type = std::make_shared<DictionaryType>((*out)->type, dictionary);\n+  return Status::OK();\n+}\n+\n+template <typename T>\n+Status DictionaryBuilder<T>::Append(const Scalar& value) {\n+  RETURN_NOT_OK(Reserve(1));\n+  int32_t index;\n+  RETURN_NOT_OK(unique_builder_.Append(value, &index));\n \n Review comment:\n   In my experience with implementing these things in the past, we are probably going to want to create a base hash table pass class that accepts a functor whose action can be inlined in the innermost loop of the hash table pass. This functor would accumulate the results of the hash table pass, whether it's one of:\r\n   \r\n   * isin (values_to_test, value_set) -> boolean array\r\n   * match (values_to_test, value_set) -> integer array (positional version of isin)\r\n   * unique (values) -> unique values\r\n   * dictionary-encode / factorize (values) -> unique values + category indices\r\n   * value-counts(values) -> unique values + counts\r\n   \r\n   We should write functors for each of these \"actions\" and then instantiate kernels for each input type and functor.\r\n   \r\n   On this note, probably all of this code (including the DictionaryBuilder code -- such that it does not disrupt parquet-cpp) should move to src/arrow/compute.\r\n   \r\n   I also question whether we need to preserve a scalar append API when instead we could do hash table passes in batches in the places where incremental dictionary-encoding is important\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-30T18:27:31.581+0000",
                    "updated": "2017-10-30T18:27:31.581+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16225492",
                    "id": "16225492",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r147787872\n \n \n\n ##########\n File path: cpp/src/arrow/CMakeLists.txt\n ##########\n @@ -52,6 +52,7 @@ if (ARROW_COMPUTE)\n   set(ARROW_SRCS ${ARROW_SRCS}\n     compute/cast.cc\n     compute/context.cc\n+    compute/hash_kernels.cc\n \n Review comment:\n   In a subsequent PR we should create the `src/arrow/compute/kernels` directory for cleaner organization of code\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-30T18:27:31.582+0000",
                    "updated": "2017-10-30T18:27:31.582+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16225493",
                    "id": "16225493",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r147788617\n \n \n\n ##########\n File path: cpp/src/arrow/builder.cc\n ##########\n @@ -815,48 +815,35 @@ Status BooleanBuilder::Append(const std::vector<bool>& values) {\n }\n \n // ----------------------------------------------------------------------\n-// DictionaryBuilder\n+// UniqueBuilder\n \n template <typename T>\n-DictionaryBuilder<T>::DictionaryBuilder(const std::shared_ptr<DataType>& type,\n-                                        MemoryPool* pool)\n+UniqueBuilder<T>::UniqueBuilder(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n     : ArrayBuilder(type, pool),\n \n Review comment:\n   We may reach a point where it doesn't make sense to inherit from ArrayBuilder, and rather compose it (or not use it at all)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-30T18:27:31.719+0000",
                    "updated": "2017-10-30T18:27:31.719+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16225494",
                    "id": "16225494",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r147792444\n \n \n\n ##########\n File path: cpp/src/arrow/compute/hash_kernels.cc\n ##########\n @@ -0,0 +1,131 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/hash_kernels.h\"\n+\n+#include <sstream>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+template <typename DataType>\n+class UniqueKernel : public UnaryKernel {\n+  Status Call(FunctionContext* ctx, const Array& input,\n+              std::shared_ptr<ArrayData>* out) override {\n+    if (!unique_builder) {\n+      unique_builder =\n+          std::make_shared<UniqueBuilder<DataType>>(input.type(), ctx->memory_pool());\n+    }\n+\n+    RETURN_NOT_OK(unique_builder->AppendArray(input));\n+\n+    if (out) {\n+      RETURN_NOT_OK(unique_builder->FinishInternal(out));\n+      unique_builder.reset();\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+ private:\n+  std::shared_ptr<UniqueBuilder<DataType>> unique_builder;\n+};\n+}\n+\n+#define UNIQUE_FUNCTION_CASE(InType)                                    \\\n+  case InType::type_id:                                                 \\\n+    *kernel = std::unique_ptr<UnaryKernel>(new UniqueKernel<InType>()); \\\n+    break\n+\n+Status GetUniqueFunction(const DataType& in_type, std::unique_ptr<UnaryKernel>* kernel) {\n+  switch (in_type.id()) {\n+    // UNIQUE_FUNCTION_CASE(NullType);\n+    // UNIQUE_FUNCTION_CASE(BooleanType);\n+    UNIQUE_FUNCTION_CASE(UInt8Type);\n+    UNIQUE_FUNCTION_CASE(Int8Type);\n+    UNIQUE_FUNCTION_CASE(UInt16Type);\n+    UNIQUE_FUNCTION_CASE(Int16Type);\n+    UNIQUE_FUNCTION_CASE(UInt32Type);\n+    UNIQUE_FUNCTION_CASE(Int32Type);\n+    UNIQUE_FUNCTION_CASE(UInt64Type);\n+    UNIQUE_FUNCTION_CASE(Int64Type);\n+    UNIQUE_FUNCTION_CASE(FloatType);\n+    UNIQUE_FUNCTION_CASE(DoubleType);\n+    UNIQUE_FUNCTION_CASE(Date32Type);\n+    UNIQUE_FUNCTION_CASE(Date64Type);\n+    UNIQUE_FUNCTION_CASE(Time32Type);\n+    UNIQUE_FUNCTION_CASE(Time64Type);\n+    UNIQUE_FUNCTION_CASE(TimestampType);\n+    UNIQUE_FUNCTION_CASE(BinaryType);\n+    UNIQUE_FUNCTION_CASE(StringType);\n+    UNIQUE_FUNCTION_CASE(FixedSizeBinaryType);\n+    default:\n+      break;\n+  }\n+\n+  if (*kernel == nullptr) {\n+    std::stringstream ss;\n+    ss << \"No unique implemented for \" << in_type.ToString();\n+    return Status::NotImplemented(ss.str());\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* ctx, const Array& array, std::shared_ptr<Array>* out) {\n+  // Dynamic dispatch to obtain right cast function\n+  std::unique_ptr<UnaryKernel> func;\n+  RETURN_NOT_OK(GetUniqueFunction(*array.type(), &func));\n+\n+  std::shared_ptr<ArrayData> out_data;\n+  RETURN_NOT_OK(func->Call(ctx, array, &out_data));\n+  *out = MakeArray(out_data);\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* ctx, const ChunkedArray& array,\n+              std::shared_ptr<Array>* out) {\n+  // Dynamic dispatch to obtain right cast function\n+  std::unique_ptr<UnaryKernel> func;\n+  RETURN_NOT_OK(GetUniqueFunction(*array.type(), &func));\n+\n+  // Call the kernel without out_data on all but the last chunk\n+  for (int i = 0; i < (array.num_chunks() - 1); i++) {\n+    RETURN_NOT_OK(func->Call(ctx, *array.chunk(i), nullptr));\n+  }\n+\n+  std::shared_ptr<ArrayData> out_data;\n+  // The array has a large chunk, call the kernel and retrieve the result.\n+  RETURN_NOT_OK(func->Call(ctx, *array.chunk(array.num_chunks() - 1), &out_data));\n+  *out = MakeArray(out_data);\n+\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* context, const Column& array,\n+              std::shared_ptr<Array>* out) {\n+  return Unique(context, *array.data(), out);\n+}\n \n Review comment:\n   I'm not sure about having `Unique` wrappers for ChunkedArray and Column. What do we think about implementing generic `UnaryInvoke` methods that accept a generic `UnaryKernel` that applies to these data structures?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-30T18:27:31.743+0000",
                    "updated": "2017-10-30T18:27:31.743+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16225497",
                    "id": "16225497",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r147792444\n \n \n\n ##########\n File path: cpp/src/arrow/compute/hash_kernels.cc\n ##########\n @@ -0,0 +1,131 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/hash_kernels.h\"\n+\n+#include <sstream>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+template <typename DataType>\n+class UniqueKernel : public UnaryKernel {\n+  Status Call(FunctionContext* ctx, const Array& input,\n+              std::shared_ptr<ArrayData>* out) override {\n+    if (!unique_builder) {\n+      unique_builder =\n+          std::make_shared<UniqueBuilder<DataType>>(input.type(), ctx->memory_pool());\n+    }\n+\n+    RETURN_NOT_OK(unique_builder->AppendArray(input));\n+\n+    if (out) {\n+      RETURN_NOT_OK(unique_builder->FinishInternal(out));\n+      unique_builder.reset();\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+ private:\n+  std::shared_ptr<UniqueBuilder<DataType>> unique_builder;\n+};\n+}\n+\n+#define UNIQUE_FUNCTION_CASE(InType)                                    \\\n+  case InType::type_id:                                                 \\\n+    *kernel = std::unique_ptr<UnaryKernel>(new UniqueKernel<InType>()); \\\n+    break\n+\n+Status GetUniqueFunction(const DataType& in_type, std::unique_ptr<UnaryKernel>* kernel) {\n+  switch (in_type.id()) {\n+    // UNIQUE_FUNCTION_CASE(NullType);\n+    // UNIQUE_FUNCTION_CASE(BooleanType);\n+    UNIQUE_FUNCTION_CASE(UInt8Type);\n+    UNIQUE_FUNCTION_CASE(Int8Type);\n+    UNIQUE_FUNCTION_CASE(UInt16Type);\n+    UNIQUE_FUNCTION_CASE(Int16Type);\n+    UNIQUE_FUNCTION_CASE(UInt32Type);\n+    UNIQUE_FUNCTION_CASE(Int32Type);\n+    UNIQUE_FUNCTION_CASE(UInt64Type);\n+    UNIQUE_FUNCTION_CASE(Int64Type);\n+    UNIQUE_FUNCTION_CASE(FloatType);\n+    UNIQUE_FUNCTION_CASE(DoubleType);\n+    UNIQUE_FUNCTION_CASE(Date32Type);\n+    UNIQUE_FUNCTION_CASE(Date64Type);\n+    UNIQUE_FUNCTION_CASE(Time32Type);\n+    UNIQUE_FUNCTION_CASE(Time64Type);\n+    UNIQUE_FUNCTION_CASE(TimestampType);\n+    UNIQUE_FUNCTION_CASE(BinaryType);\n+    UNIQUE_FUNCTION_CASE(StringType);\n+    UNIQUE_FUNCTION_CASE(FixedSizeBinaryType);\n+    default:\n+      break;\n+  }\n+\n+  if (*kernel == nullptr) {\n+    std::stringstream ss;\n+    ss << \"No unique implemented for \" << in_type.ToString();\n+    return Status::NotImplemented(ss.str());\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* ctx, const Array& array, std::shared_ptr<Array>* out) {\n+  // Dynamic dispatch to obtain right cast function\n+  std::unique_ptr<UnaryKernel> func;\n+  RETURN_NOT_OK(GetUniqueFunction(*array.type(), &func));\n+\n+  std::shared_ptr<ArrayData> out_data;\n+  RETURN_NOT_OK(func->Call(ctx, array, &out_data));\n+  *out = MakeArray(out_data);\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* ctx, const ChunkedArray& array,\n+              std::shared_ptr<Array>* out) {\n+  // Dynamic dispatch to obtain right cast function\n+  std::unique_ptr<UnaryKernel> func;\n+  RETURN_NOT_OK(GetUniqueFunction(*array.type(), &func));\n+\n+  // Call the kernel without out_data on all but the last chunk\n+  for (int i = 0; i < (array.num_chunks() - 1); i++) {\n+    RETURN_NOT_OK(func->Call(ctx, *array.chunk(i), nullptr));\n+  }\n+\n+  std::shared_ptr<ArrayData> out_data;\n+  // The array has a large chunk, call the kernel and retrieve the result.\n+  RETURN_NOT_OK(func->Call(ctx, *array.chunk(array.num_chunks() - 1), &out_data));\n+  *out = MakeArray(out_data);\n+\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* context, const Column& array,\n+              std::shared_ptr<Array>* out) {\n+  return Unique(context, *array.data(), out);\n+}\n \n Review comment:\n   I'm not sure about having `Unique` wrappers for ChunkedArray and Column. What do we think about implementing generic `InvokeUnary` methods that accept a generic `UnaryKernel` that applies to these data structures?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-30T18:28:38.698+0000",
                    "updated": "2017-10-30T18:28:38.698+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16225663",
                    "id": "16225663",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r147820948\n \n \n\n ##########\n File path: cpp/src/arrow/builder.cc\n ##########\n @@ -1110,20 +1034,202 @@ Status DictionaryBuilder<T>::AppendDictionary(const Scalar& value) {\n   }                                                                                 \\\n                                                                                     \\\n   template <>                                                                       \\\n-  int DictionaryBuilder<Type>::HashValue(const WrappedBinary& value) {              \\\n+  WrappedBinary UniqueBuilder<Type>::GetDictionaryValue(int64_t index) {            \\\n+    int32_t v_len;                                                                  \\\n+    const uint8_t* v = dict_builder_.GetValue(static_cast<int64_t>(index), &v_len); \\\n+    return WrappedBinary(v, v_len);                                                 \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  template <>                                                                       \\\n+  Status UniqueBuilder<Type>::AppendDictionary(const WrappedBinary& value) {        \\\n+    return dict_builder_.Append(value.ptr_, value.length_);                         \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  template <>                                                                       \\\n+  int UniqueBuilder<Type>::HashValue(const WrappedBinary& value) {                  \\\n     return HashUtil::Hash(value.ptr_, value.length_, 0);                            \\\n   }                                                                                 \\\n                                                                                     \\\n   template <>                                                                       \\\n-  bool DictionaryBuilder<Type>::SlotDifferent(hash_slot_t index,                    \\\n-                                              const WrappedBinary& value) {         \\\n+  bool UniqueBuilder<Type>::SlotDifferent(hash_slot_t index,                        \\\n+                                          const WrappedBinary& value) {             \\\n     int32_t other_length;                                                           \\\n     const uint8_t* other_value =                                                    \\\n         dict_builder_.GetValue(static_cast<int64_t>(index), &other_length);         \\\n     return !(other_length == value.length_ &&                                       \\\n              0 == memcmp(other_value, value.ptr_, value.length_));                  \\\n   }\n \n+BINARY_UNIQUE_SPECIALIZATIONS(StringType);\n+BINARY_UNIQUE_SPECIALIZATIONS(BinaryType);\n+\n+template <typename T>\n+Status UniqueBuilder<T>::FinishInternal(std::shared_ptr<ArrayData>* out) {\n+  return dict_builder_.FinishInternal(out);\n+}\n+\n+template class UniqueBuilder<UInt8Type>;\n+template class UniqueBuilder<UInt16Type>;\n+template class UniqueBuilder<UInt32Type>;\n+template class UniqueBuilder<UInt64Type>;\n+template class UniqueBuilder<Int8Type>;\n+template class UniqueBuilder<Int16Type>;\n+template class UniqueBuilder<Int32Type>;\n+template class UniqueBuilder<Int64Type>;\n+template class UniqueBuilder<Date32Type>;\n+template class UniqueBuilder<Date64Type>;\n+template class UniqueBuilder<Time32Type>;\n+template class UniqueBuilder<Time64Type>;\n+template class UniqueBuilder<TimestampType>;\n+template class UniqueBuilder<FloatType>;\n+template class UniqueBuilder<DoubleType>;\n+template class UniqueBuilder<FixedSizeBinaryType>;\n+template class UniqueBuilder<BinaryType>;\n+template class UniqueBuilder<StringType>;\n+\n+// ----------------------------------------------------------------------\n+// DictionaryBuilder\n+\n+template <typename T>\n+DictionaryBuilder<T>::DictionaryBuilder(const std::shared_ptr<DataType>& type,\n+                                        MemoryPool* pool)\n+    : ArrayBuilder(type, pool), unique_builder_(type, pool), values_builder_(pool) {}\n+\n+DictionaryBuilder<NullType>::DictionaryBuilder(const std::shared_ptr<DataType>& type,\n+                                               MemoryPool* pool)\n+    : ArrayBuilder(type, pool), values_builder_(pool) {}\n+\n+DictionaryBuilder<NullType>::~DictionaryBuilder() {}\n+\n+template <>\n+DictionaryBuilder<FixedSizeBinaryType>::DictionaryBuilder(\n+    const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+    : ArrayBuilder(type, pool), unique_builder_(type, pool), values_builder_(pool) {}\n+\n+template <typename T>\n+Status DictionaryBuilder<T>::Init(int64_t elements) {\n+  RETURN_NOT_OK(ArrayBuilder::Init(elements));\n+  RETURN_NOT_OK(unique_builder_.Init(elements));\n+  return values_builder_.Init(elements);\n+}\n+\n+Status DictionaryBuilder<NullType>::Init(int64_t elements) {\n+  RETURN_NOT_OK(ArrayBuilder::Init(elements));\n+  return values_builder_.Init(elements);\n+}\n+\n+template <typename T>\n+Status DictionaryBuilder<T>::Resize(int64_t capacity) {\n+  if (capacity < kMinBuilderCapacity) {\n+    capacity = kMinBuilderCapacity;\n+  }\n+\n+  if (capacity_ == 0) {\n+    return Init(capacity);\n+  } else {\n+    RETURN_NOT_OK(unique_builder_.Resize(capacity));\n+    return ArrayBuilder::Resize(capacity);\n+  }\n+}\n+\n+Status DictionaryBuilder<NullType>::Resize(int64_t capacity) {\n+  if (capacity < kMinBuilderCapacity) {\n+    capacity = kMinBuilderCapacity;\n+  }\n+\n+  if (capacity_ == 0) {\n+    return Init(capacity);\n+  } else {\n+    return ArrayBuilder::Resize(capacity);\n+  }\n+}\n+\n+template <typename T>\n+Status DictionaryBuilder<T>::FinishInternal(std::shared_ptr<ArrayData>* out) {\n+  std::shared_ptr<Array> dictionary;\n+  RETURN_NOT_OK(unique_builder_.Finish(&dictionary));\n+\n+  RETURN_NOT_OK(values_builder_.FinishInternal(out));\n+  (*out)->type = std::make_shared<DictionaryType>((*out)->type, dictionary);\n+  return Status::OK();\n+}\n+\n+Status DictionaryBuilder<NullType>::FinishInternal(std::shared_ptr<ArrayData>* out) {\n+  std::shared_ptr<Array> dictionary = std::make_shared<NullArray>(0);\n+\n+  RETURN_NOT_OK(values_builder_.FinishInternal(out));\n+  (*out)->type = std::make_shared<DictionaryType>((*out)->type, dictionary);\n+  return Status::OK();\n+}\n+\n+template <typename T>\n+Status DictionaryBuilder<T>::Append(const Scalar& value) {\n+  RETURN_NOT_OK(Reserve(1));\n+  int32_t index;\n+  RETURN_NOT_OK(unique_builder_.Append(value, &index));\n \n Review comment:\n   As part of this we need to also define a stateful API so that we can keep pushing more data through the kernel for the ones that need to maintain state. Some of the \"modes\" of the hash table will write their results into output memory, where others (e.g. unique) will keep accumulating the result\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-30T20:13:55.059+0000",
                    "updated": "2017-10-30T20:13:55.059+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16226988",
                    "id": "16226988",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-340800603\n \n \n   Do you mind if I take some time to work on this today/tomorrow? I can push to this branch so we can collaborate here\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-31T15:32:21.806+0000",
                    "updated": "2017-10-31T15:32:21.806+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16234449",
                    "id": "16234449",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-341180952\n \n \n   Feel free to push here, I'm open for the suggested changes. This is probably going to be a bit bigger as it lays some more foundations for the compute library.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-01T17:37:35.615+0000",
                    "updated": "2017-11-01T17:37:35.615+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16234450",
                    "id": "16234450",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on a change in pull request #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r148330119\n \n \n\n ##########\n File path: cpp/src/arrow/compute/hash_kernels.cc\n ##########\n @@ -0,0 +1,131 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/hash_kernels.h\"\n+\n+#include <sstream>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+template <typename DataType>\n+class UniqueKernel : public UnaryKernel {\n+  Status Call(FunctionContext* ctx, const Array& input,\n+              std::shared_ptr<ArrayData>* out) override {\n+    if (!unique_builder) {\n+      unique_builder =\n+          std::make_shared<UniqueBuilder<DataType>>(input.type(), ctx->memory_pool());\n+    }\n+\n+    RETURN_NOT_OK(unique_builder->AppendArray(input));\n+\n+    if (out) {\n+      RETURN_NOT_OK(unique_builder->FinishInternal(out));\n+      unique_builder.reset();\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+ private:\n+  std::shared_ptr<UniqueBuilder<DataType>> unique_builder;\n+};\n+}\n+\n+#define UNIQUE_FUNCTION_CASE(InType)                                    \\\n+  case InType::type_id:                                                 \\\n+    *kernel = std::unique_ptr<UnaryKernel>(new UniqueKernel<InType>()); \\\n+    break\n+\n+Status GetUniqueFunction(const DataType& in_type, std::unique_ptr<UnaryKernel>* kernel) {\n+  switch (in_type.id()) {\n+    // UNIQUE_FUNCTION_CASE(NullType);\n+    // UNIQUE_FUNCTION_CASE(BooleanType);\n+    UNIQUE_FUNCTION_CASE(UInt8Type);\n+    UNIQUE_FUNCTION_CASE(Int8Type);\n+    UNIQUE_FUNCTION_CASE(UInt16Type);\n+    UNIQUE_FUNCTION_CASE(Int16Type);\n+    UNIQUE_FUNCTION_CASE(UInt32Type);\n+    UNIQUE_FUNCTION_CASE(Int32Type);\n+    UNIQUE_FUNCTION_CASE(UInt64Type);\n+    UNIQUE_FUNCTION_CASE(Int64Type);\n+    UNIQUE_FUNCTION_CASE(FloatType);\n+    UNIQUE_FUNCTION_CASE(DoubleType);\n+    UNIQUE_FUNCTION_CASE(Date32Type);\n+    UNIQUE_FUNCTION_CASE(Date64Type);\n+    UNIQUE_FUNCTION_CASE(Time32Type);\n+    UNIQUE_FUNCTION_CASE(Time64Type);\n+    UNIQUE_FUNCTION_CASE(TimestampType);\n+    UNIQUE_FUNCTION_CASE(BinaryType);\n+    UNIQUE_FUNCTION_CASE(StringType);\n+    UNIQUE_FUNCTION_CASE(FixedSizeBinaryType);\n+    default:\n+      break;\n+  }\n+\n+  if (*kernel == nullptr) {\n+    std::stringstream ss;\n+    ss << \"No unique implemented for \" << in_type.ToString();\n+    return Status::NotImplemented(ss.str());\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* ctx, const Array& array, std::shared_ptr<Array>* out) {\n+  // Dynamic dispatch to obtain right cast function\n+  std::unique_ptr<UnaryKernel> func;\n+  RETURN_NOT_OK(GetUniqueFunction(*array.type(), &func));\n+\n+  std::shared_ptr<ArrayData> out_data;\n+  RETURN_NOT_OK(func->Call(ctx, array, &out_data));\n+  *out = MakeArray(out_data);\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* ctx, const ChunkedArray& array,\n+              std::shared_ptr<Array>* out) {\n+  // Dynamic dispatch to obtain right cast function\n+  std::unique_ptr<UnaryKernel> func;\n+  RETURN_NOT_OK(GetUniqueFunction(*array.type(), &func));\n+\n+  // Call the kernel without out_data on all but the last chunk\n+  for (int i = 0; i < (array.num_chunks() - 1); i++) {\n+    RETURN_NOT_OK(func->Call(ctx, *array.chunk(i), nullptr));\n+  }\n+\n+  std::shared_ptr<ArrayData> out_data;\n+  // The array has a large chunk, call the kernel and retrieve the result.\n+  RETURN_NOT_OK(func->Call(ctx, *array.chunk(array.num_chunks() - 1), &out_data));\n+  *out = MakeArray(out_data);\n+\n+  return Status::OK();\n+}\n+\n+Status Unique(FunctionContext* context, const Column& array,\n+              std::shared_ptr<Array>* out) {\n+  return Unique(context, *array.data(), out);\n+}\n \n Review comment:\n   This also sounds fine to me. In the end, we would need to have these three levels for all kernels (Array, ChunkedArray, Column) which would mean a lot of pass-through methods.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-01T17:38:44.363+0000",
                    "updated": "2017-11-01T17:38:44.363+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16234464",
                    "id": "16234464",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-341184022\n \n \n   At the moment, I also tend to step a bit back and first have a look at this again in a design document. There are several issues where I have no clear opinion yet but that would probably require some thinking:\r\n   \r\n    * Do we need kernel call methods for each level of Array/ChunkedArray/Column? Having them instead of a generic `InvokeUnary` on each of the three data structures might lead to a lot of code duplication or simple pass-through functions. Otherwise having an `InvokeUnary` method would prohibit us from doing some optimizations in the case that we pass over several arrays in a column and could do some operations only once.\r\n    * My use case here is to selective categorical conversion, my initial approach was to implement `unique(column)` and then use this to create a `DictionaryType` instance that would then be fed to all underlying arrays to make the categorical conversion. This might not be the best solution as the `DictionaryType` instance doesn't contain the hash map anymore and would have to reconstruct it.\r\n   \r\n   Also, do we in general have a design document for the kernels? We need to think about state, parallelisation, .. in general. I might have missed this but I think having it integrated into the Arrow documentation will ease entry for future contributors (and myself).\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-01T17:46:12.515+0000",
                    "updated": "2017-11-01T17:46:12.515+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16236111",
                    "id": "16236111",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-341486675\n \n \n   I would be happy to write some design documents, but in exchange I need some more help moving along routine development and maintenance of the project. The honest truth is that I simply haven't had enough time to really think through all of the details of what the kernel APIs need to look like -- for example, we need to define an object model to accommodate scalar values (for example, adding a scalar to an array, or casting a scalar from one type to another). The code for the cast kernels could be a lot cleaner than it is. I have a high tolerance for this kind of uncertainty and don't mind doing a lot of refactoring as we figure out the right general shape of the kernel-operator API. I've also been looking at libraries like Dremio and TensorFlow a bit for inspiration. \r\n   \r\n   At a high level, the purpose of the kernels is to be able to perform analytics on chunked arrays. Depending on the operator, the kernels may need to be stateful (e.g. reductions, hash-table based analytics) or stateless (elementwise functions, NumPy ufunc-like math, etc.). Operators will need to be able to accommodate dispatch to different variants of kernels (e.g. SIMD-enabled, non-SIMD, GPU)\r\n   \r\n   I'd like to spend the majority of my time working solely on kernels and analytics, but I can't do that at the expense of all the other small things that would fall through the cracks otherwise. Out of the 0.8.0 milestone so far, I have resolved 64 JIRAs -- the rest of the Arrow community has done 67. So my effective burden of moving along the project only looking at JIRAs is around 50%. When you add release management and PR maintenance, the number goes above 50% for sure. \r\n   \r\n   This is not a complaint, just pointing out with things as they are I am not sure I can do more than I'm already doing. I will be more than happy do more design and architecture work as soon as the community starts sharing more of the development workload. At the moment, to let the development work drop to write more documentation and design docs seems like an unacceptable compromise to me. Getting the project to a format stable 1.0.0 release and to make it suitable for production use for data interchange in Apache Spark and elsewhere is the most important thing for me right now, and engineering work in support of that is going to take priority over design docs\r\n   \r\n   I'm on a plane right now so I'm going to hack on these hash kernels for several hours and see how far I can get\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-02T16:52:42.982+0000",
                    "updated": "2017-11-02T16:52:42.982+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16236124",
                    "id": "16236124",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-341489847\n \n \n   @wesm I don't expect you to write the documents. This is just a general remark that we need those. As I'm probably more interested in API stability and moving forward with the casting, I should spend time on it. \r\n   \r\n   As my time is also quite severely limited at the moment, I will have a look at writing more documentation. This is not really an easy task but I think necessary for us to get more community members on board.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-02T17:02:15.607+0000",
                    "updated": "2017-11-02T17:02:15.607+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16244149",
                    "id": "16244149",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-342852623\n \n \n   I'm still working on this. There's quite a bit of work to do to refactor the `DictionaryBuilder` tests to be kernel-based, so with luck I'll have a patch up later today or tomorrow\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-08T15:28:45.689+0000",
                    "updated": "2017-11-08T15:28:45.689+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16250843",
                    "id": "16250843",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: WIP: ARROW-1559: Add unique kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344145502\n \n \n   I started pulling on threads and the whole thing unraveled. I needed to add a variant output type (`arrow::compute::Datum`) and I wanted to change unique and dictionary-encode to both be kernels. I'm close to having things all working again, I will spend the next day or so re-writing the unit tests to work with the new code structure, and then making sure we don't have any obvious regressions.\r\n   \r\n   As one annoying matter, when you dictionary encode a chunked array, you may not have seen all the unique values yet, so the integer type output may change as you observe more chunks. As a result, for the time being I think it is best if we dictionary encode everything to int32 instead of using the adaptive integer builder. If we want to optimize space to make things smaller we can revisit after this patch\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-14T04:47:48.449+0000",
                    "updated": "2017-11-14T04:47:48.449+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16252895",
                    "id": "16252895",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344476480\n \n \n   OK, I'm finally ready for some code review on this.\r\n   \r\n   So we have one matter to resolve that is a bit annoying. I created a `arrow::compute::Datum` type that is based on `boost::variant`. Fundamentally, we need such a variant type in our compute kernels to be able to operate on different types of data: scalars, arrays, chunked arrays, etc. \r\n   \r\n   I suspect that we do not want to have boost in our public headers. So we have a decision to make here:\r\n   \r\n   * We could vendor an ASL 2.0-compatible header-only `boost::variant` replacement like https://github.com/mapbox/variant. We're lucky that one exists\r\n   * We can refactor `Datum` to use a PIMPL. That seems pretty heavy-weight for such a tiny struct -- in such case even a stack allocation of `Datum` would require a heap allocation, which is not free when you have a lot of these things.\r\n   \r\n   I'm inclined to take the former approach, mapbox/variant is ~1200 lines of headers and seems to want to be a drop-in replacement for `boost::variant`, is BSD-3 licensed, so probably not the worst dependency to take on. \r\n   \r\n   cc @xhochy @cpcloud \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T03:38:39.234+0000",
                    "updated": "2017-11-15T03:38:39.234+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16252896",
                    "id": "16252896",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344476480\n \n \n   OK, I'm finally ready for some code review on this.\r\n   \r\n   So we have one matter to resolve that is a bit annoying. I created a `arrow::compute::Datum` type that is based on `boost::variant`. Fundamentally, we need such a variant type in our compute kernels to be able to operate on different types of data: scalars, arrays, chunked arrays, etc. As an example, we would like to be able to add two `Datum` values together, each of which might be a scalar value (object model TBD) or an array.\r\n   \r\n   I suspect that we do not want to have boost in our public headers. So we have a decision to make here:\r\n   \r\n   * We could vendor an ASL 2.0-compatible header-only `boost::variant` replacement like https://github.com/mapbox/variant. We're lucky that one exists\r\n   * We can refactor `Datum` to use a PIMPL. That seems pretty heavy-weight for such a tiny struct -- in such case even a stack allocation of `Datum` would require a heap allocation, which is not free when you have a lot of these things.\r\n   \r\n   I'm inclined to take the former approach, mapbox/variant is ~1200 lines of headers and seems to want to be a drop-in replacement for `boost::variant`, is BSD-3 licensed, so probably not the worst dependency to take on. \r\n   \r\n   cc @xhochy @cpcloud \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T03:39:19.255+0000",
                    "updated": "2017-11-15T03:39:19.255+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16252904",
                    "id": "16252904",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "pcmoritz commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344477903\n \n \n   +1 on the mapbox variant\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T03:50:10.887+0000",
                    "updated": "2017-11-15T03:50:10.887+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16252909",
                    "id": "16252909",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151027415\n \n \n\n ##########\n File path: cpp/src/arrow/array.cc\n ##########\n @@ -92,7 +92,7 @@ static inline std::shared_ptr<ArrayData> SliceData(const ArrayData& data, int64_\n   auto new_data = data.ShallowCopy();\n   new_data->length = length;\n   new_data->offset = offset;\n-  new_data->null_count = kUnknownNullCount;\n+  new_data->null_count = data.null_count != 0 ? kUnknownNullCount : 0;\n \n Review comment:\n   This is a helpful optimization -- if you know your null count is already 0, no need to keep thinking it's unknown\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T04:05:05.686+0000",
                    "updated": "2017-11-15T04:05:05.686+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16252910",
                    "id": "16252910",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151027570\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/cast.h\n ##########\n @@ -60,7 +63,12 @@ Status Cast(FunctionContext* context, const Array& array,\n             const std::shared_ptr<DataType>& to_type, const CastOptions& options,\n             std::shared_ptr<Array>* out);\n \n+ARROW_EXPORT\n+Status Cast(FunctionContext* context, const Datum& value,\n+            const std::shared_ptr<DataType>& to_type, const CastOptions& options,\n+            Datum* out);\n+\n }  // namespace compute\n }  // namespace arrow\n \n-#endif  // ARROW_COMPUTE_CAST_H\n+#endif  // ARROW_COMPUTE_KERNELS_CAST_H\n \n Review comment:\n   I probably need to do a once over of Cast and make sure the ChunkedArray form also works\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T04:05:05.687+0000",
                    "updated": "2017-11-15T04:05:05.687+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16252911",
                    "id": "16252911",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151027685\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,890 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+// TODO(wesm): we do not need this macro yet\n+\n+// #define HASH_THROW_NOT_OK(EXPR)                     \\\n+//   do {                                              \\\n+//     Status _s = (EXPR);                             \\\n+//     if (ARROW_PREDICT_FALSE(!_s.ok())) {            \\\n+//       throw HashException(_s.message(), _s.code()); \\\n+//     }                                               \\\n+//   } while (false)\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(std::vector<Datum>* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n+  if (arr.null_count != 0) {                                                             \\\n+    internal::BitmapReader valid_reader(arr.buffers[0]->data(), arr.offset, arr.length); \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      const bool is_null = valid_reader.IsNotSet();                                      \\\n+      valid_reader.Next();                                                               \\\n+                                                                                         \\\n+      if (is_null) {                                                                     \\\n+        action->ObserveNull();                                                           \\\n+        continue;                                                                        \\\n+      }                                                                                  \\\n+                                                                                         \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  } else {                                                                               \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  }\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_has_c_type<Type>> : public HashTable {\n+ public:\n+  using T = typename Type::c_type;\n+\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_(pool) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_.Init());\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const T* values = GetValues<T>(arr, 1);\n+    auto action = static_cast<Action*>(this);\n+\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const T value = values[i];                                                      \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  while (kHashSlotEmpty != slot && dict_.values[slot] != value) {                 \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = static_cast<hash_slot_t>(dict_.size);                                  \\\n+    hash_slots_[j] = slot;                                                        \\\n+    dict_.values[dict_.size++] = value;                                           \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_.size > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    auto dict_data = dict_.buffer;\n+    RETURN_NOT_OK(dict_data->Resize(dict_.size * sizeof(T), false));\n+\n+    BufferVector buffers = {nullptr, dict_data};\n+    *out = std::make_shared<ArrayData>(type_, dict_.size, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const T& value) const {\n+    // TODO(wesm): Use faster hash function for C types\n+    return HashUtil::Hash(&value, sizeof(T), 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const T value = dict_.values[index];\n+\n+      // Find empty slot in the new hash table\n+      int64_t j = HashValue(value) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return dict_.Resize(new_size);\n+  }\n+\n+  HashDictionary<Type> dict_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for variable-length binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_binary<Type>> : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_offsets_(pool), dict_data_(pool), dict_size_(0) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_offsets_.Resize(kInitialHashTableSize));\n+\n+    // We append the end offset after each append to the dictionary, so this\n+    // sets the initial condition for the length-0 case\n+    //\n+    // initial offsets (dict size == 0): 0\n+    // after 1st dict entry of length 3: 0 3\n+    // after 2nd dict entry of length 4: 0 3 7\n+    RETURN_NOT_OK(dict_offsets_.Append(0));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const int32_t* offsets = GetValues<int32_t>(arr, 1);\n+    const uint8_t* data = GetValues<uint8_t>(arr, 2);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                           \\\n+  const int32_t position = offsets[i];                                              \\\n+  const int32_t length = offsets[i + 1] - position;                                 \\\n+  const uint8_t* value = data + position;                                           \\\n+                                                                                    \\\n+  int64_t j = HashValue(value, length) & mod_bitmask_;                              \\\n+  hash_slot_t slot = hash_slots_[j];                                                \\\n+                                                                                    \\\n+  const int32_t* dict_offsets = dict_offsets_.data();                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                     \\\n+  while (kHashSlotEmpty != slot &&                                                  \\\n+         !((dict_offsets[slot + 1] - dict_offsets[slot]) == length &&               \\\n+           0 == memcmp(value, dict_data + dict_offsets[slot], length))) {           \\\n+    ++j;                                                                            \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                               \\\n+      j = 0;                                                                        \\\n+    }                                                                               \\\n+    slot = hash_slots_[j];                                                          \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  if (slot == kHashSlotEmpty) {                                                     \\\n+    if (!Action::allow_expand) {                                                    \\\n+      throw HashException(\"Encountered new dictionary value\");                      \\\n+    }                                                                               \\\n+                                                                                    \\\n+    slot = dict_size_++;                                                            \\\n+    hash_slots_[j] = slot;                                                          \\\n+                                                                                    \\\n+    RETURN_NOT_OK(dict_data_.Append(value, length));                                \\\n+    RETURN_NOT_OK(dict_offsets_.Append(static_cast<int32_t>(dict_data_.length()))); \\\n+                                                                                    \\\n+    action->ObserveNotFound(slot);                                                  \\\n+                                                                                    \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) {   \\\n+      RETURN_NOT_OK(action->DoubleSize());                                          \\\n+    }                                                                               \\\n+  } else {                                                                          \\\n+    action->ObserveFound(slot);                                                     \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr, nullptr};\n+\n+    RETURN_NOT_OK(dict_offsets_.Finish(&buffers[1]));\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[2]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data, int32_t length) const {\n+    return HashUtil::Hash(data, length, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    const int32_t* dict_offsets = dict_offsets_.data();\n+    const uint8_t* dict_data = dict_data_.data();\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const int32_t length = dict_offsets[index + 1] - dict_offsets[index];\n+      const uint8_t* value = dict_data + dict_offsets[index];\n+\n+      // Find an empty slot in the new hash table\n+      int64_t j = HashValue(value, length) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return Status::OK();\n+  }\n+\n+  TypedBufferBuilder<int32_t> dict_offsets_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for fixed size binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_fixed_size_binary<Type>>\n+    : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_data_(pool), dict_size_(0) {\n+    const auto& fw_type = static_cast<const FixedSizeBinaryType&>(*type);\n+    byte_width_ = fw_type.bit_width() / 8;\n+  }\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_data_.Resize(kInitialHashTableSize * byte_width_));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const uint8_t* data = GetValues<uint8_t>(arr, 1);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const uint8_t* value = data + i * byte_width_;                                  \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  const uint8_t* dict_data = dict_data_.data();                                   \\\n+  while (kHashSlotEmpty != slot &&                                                \\\n+         !(0 == memcmp(value, dict_data + slot * byte_width_, byte_width_))) {    \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = dict_size_++;                                                          \\\n+    hash_slots_[j] = slot;                                                        \\\n+                                                                                  \\\n+    RETURN_NOT_OK(dict_data_.Append(value, byte_width_));                         \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr};\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[1]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data) const {\n+    return HashUtil::Hash(data, byte_width_, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    const uint8_t* dict_data = dict_data_.data();\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Find an empty slot in the new hash table\n+      int64_t j = HashValue(dict_data + index * byte_width_) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return Status::OK();\n+  }\n+\n+  int32_t byte_width_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Unique implementation\n+\n+template <typename Type>\n+class UniqueImpl : public HashTableKernel<Type, UniqueImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, UniqueImpl<Type>>;\n+  using Base::Base;\n+\n+  Status Reserve(const int64_t length) { return Status::OK(); }\n+\n+  void ObserveFound(const hash_slot_t slot) {}\n+  void ObserveNull() {}\n+  void ObserveNotFound(const hash_slot_t slot) {}\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Append(const ArrayData& input) override { return Base::Append(input); }\n+\n+  Status Flush(std::vector<Datum>* out) override {\n+    // No-op\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Dictionary encode implementation\n+\n+template <typename Type>\n+class DictEncodeImpl : public HashTableKernel<Type, DictEncodeImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, DictEncodeImpl>;\n+\n+  DictEncodeImpl(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : Base(type, pool), indices_builder_(pool) {}\n+\n+  Status Reserve(const int64_t length) { return indices_builder_.Reserve(length); }\n+\n+  void ObserveNull() { indices_builder_.UnsafeAppendToBitmap(false); }\n+\n+  void ObserveFound(const hash_slot_t slot) { indices_builder_.UnsafeAppend(slot); }\n+\n+  void ObserveNotFound(const hash_slot_t slot) { return ObserveFound(slot); }\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Flush(std::vector<Datum>* out) override {\n+    std::shared_ptr<ArrayData> result;\n+    RETURN_NOT_OK(indices_builder_.FinishInternal(&result));\n+    out->push_back(Datum(result));\n+    return Status::OK();\n+  }\n+\n+  using Base::Append;\n+\n+ private:\n+  Int32Builder indices_builder_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Kernel wrapper for generic hash table kernels\n+\n+class HashKernelImpl : public HashKernel {\n+ public:\n+  explicit HashKernelImpl(std::unique_ptr<HashTable> hasher)\n+      : hasher_(std::move(hasher)) {}\n+\n+  Status Call(FunctionContext* ctx, const ArrayData& input,\n+              std::vector<Datum>* out) override {\n+    RETURN_NOT_OK(Append(ctx, input));\n+    return Flush(out);\n+  }\n+\n+  Status Append(FunctionContext* ctx, const ArrayData& input) override {\n+    std::lock_guard<std::mutex> guard(lock_);\n+    try {\n+      RETURN_NOT_OK(hasher_->Append(input));\n+    } catch (const HashException& e) {\n+      return Status(e.code(), e.what());\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status Flush(std::vector<Datum>* out) override { return hasher_->Flush(out); }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    return hasher_->GetDictionary(out);\n+  }\n+\n+ private:\n+  std::mutex lock_;\n+  std::unique_ptr<HashTable> hasher_;\n+};\n+\n+}  // namespace\n+\n+Status GetUniqueKernel(FunctionContext* ctx, const std::shared_ptr<DataType>& type,\n+                       std::unique_ptr<HashKernel>* out) {\n+  std::unique_ptr<HashTable> hasher;\n+\n+#define UNIQUE_CASE(InType)                                         \\\n+  case InType::type_id:                                             \\\n+    hasher.reset(new UniqueImpl<InType>(type, ctx->memory_pool())); \\\n+    break\n+\n+  switch (type->id()) {\n+    UNIQUE_CASE(NullType);\n+    // UNIQUE_CASE(BooleanType);\n \n Review comment:\n   Still need to implement for `BooleanType`. This can be simplified since the hash table can have at most 2 (or 3, if you count nulls) values\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T04:05:05.753+0000",
                    "updated": "2017-11-15T04:05:05.753+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16252913",
                    "id": "16252913",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344480057\n \n \n   Consider it done. The Python tests are segfaulting due to something it seems I broke in the cast code path, so I'll look into that tomorrow and also address the `BooleanType` hashing\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T04:07:33.644+0000",
                    "updated": "2017-11-15T04:07:33.644+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16254058",
                    "id": "16254058",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151233906\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,880 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(std::vector<Datum>* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n+  if (arr.null_count != 0) {                                                             \\\n+    internal::BitmapReader valid_reader(arr.buffers[0]->data(), arr.offset, arr.length); \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      const bool is_null = valid_reader.IsNotSet();                                      \\\n+      valid_reader.Next();                                                               \\\n+                                                                                         \\\n+      if (is_null) {                                                                     \\\n+        action->ObserveNull();                                                           \\\n+        continue;                                                                        \\\n+      }                                                                                  \\\n+                                                                                         \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  } else {                                                                               \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  }\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_has_c_type<Type>> : public HashTable {\n+ public:\n+  using T = typename Type::c_type;\n+\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_(pool) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_.Init());\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const T* values = GetValues<T>(arr, 1);\n+    auto action = static_cast<Action*>(this);\n+\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const T value = values[i];                                                      \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  while (kHashSlotEmpty != slot && dict_.values[slot] != value) {                 \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = static_cast<hash_slot_t>(dict_.size);                                  \\\n+    hash_slots_[j] = slot;                                                        \\\n+    dict_.values[dict_.size++] = value;                                           \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_.size > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    auto dict_data = dict_.buffer;\n+    RETURN_NOT_OK(dict_data->Resize(dict_.size * sizeof(T), false));\n+\n+    BufferVector buffers = {nullptr, dict_data};\n+    *out = std::make_shared<ArrayData>(type_, dict_.size, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const T& value) const {\n+    // TODO(wesm): Use faster hash function for C types\n+    return HashUtil::Hash(&value, sizeof(T), 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const T value = dict_.values[index];\n+\n+      // Find empty slot in the new hash table\n+      int64_t j = HashValue(value) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return dict_.Resize(new_size);\n+  }\n+\n+  HashDictionary<Type> dict_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for variable-length binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_binary<Type>> : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_offsets_(pool), dict_data_(pool), dict_size_(0) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_offsets_.Resize(kInitialHashTableSize));\n+\n+    // We append the end offset after each append to the dictionary, so this\n+    // sets the initial condition for the length-0 case\n+    //\n+    // initial offsets (dict size == 0): 0\n+    // after 1st dict entry of length 3: 0 3\n+    // after 2nd dict entry of length 4: 0 3 7\n+    RETURN_NOT_OK(dict_offsets_.Append(0));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const int32_t* offsets = GetValues<int32_t>(arr, 1);\n+    const uint8_t* data = GetValues<uint8_t>(arr, 2);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                           \\\n+  const int32_t position = offsets[i];                                              \\\n+  const int32_t length = offsets[i + 1] - position;                                 \\\n+  const uint8_t* value = data + position;                                           \\\n+                                                                                    \\\n+  int64_t j = HashValue(value, length) & mod_bitmask_;                              \\\n+  hash_slot_t slot = hash_slots_[j];                                                \\\n+                                                                                    \\\n+  const int32_t* dict_offsets = dict_offsets_.data();                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                     \\\n+  while (kHashSlotEmpty != slot &&                                                  \\\n+         !((dict_offsets[slot + 1] - dict_offsets[slot]) == length &&               \\\n+           0 == memcmp(value, dict_data + dict_offsets[slot], length))) {           \\\n+    ++j;                                                                            \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                               \\\n+      j = 0;                                                                        \\\n+    }                                                                               \\\n+    slot = hash_slots_[j];                                                          \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  if (slot == kHashSlotEmpty) {                                                     \\\n+    if (!Action::allow_expand) {                                                    \\\n+      throw HashException(\"Encountered new dictionary value\");                      \\\n+    }                                                                               \\\n+                                                                                    \\\n+    slot = dict_size_++;                                                            \\\n+    hash_slots_[j] = slot;                                                          \\\n+                                                                                    \\\n+    RETURN_NOT_OK(dict_data_.Append(value, length));                                \\\n+    RETURN_NOT_OK(dict_offsets_.Append(static_cast<int32_t>(dict_data_.length()))); \\\n+                                                                                    \\\n+    action->ObserveNotFound(slot);                                                  \\\n+                                                                                    \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) {   \\\n+      RETURN_NOT_OK(action->DoubleSize());                                          \\\n+    }                                                                               \\\n+  } else {                                                                          \\\n+    action->ObserveFound(slot);                                                     \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr, nullptr};\n+\n+    RETURN_NOT_OK(dict_offsets_.Finish(&buffers[1]));\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[2]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data, int32_t length) const {\n+    return HashUtil::Hash(data, length, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    const int32_t* dict_offsets = dict_offsets_.data();\n+    const uint8_t* dict_data = dict_data_.data();\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const int32_t length = dict_offsets[index + 1] - dict_offsets[index];\n+      const uint8_t* value = dict_data + dict_offsets[index];\n+\n+      // Find an empty slot in the new hash table\n+      int64_t j = HashValue(value, length) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return Status::OK();\n+  }\n+\n+  TypedBufferBuilder<int32_t> dict_offsets_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for fixed size binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_fixed_size_binary<Type>>\n+    : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_data_(pool), dict_size_(0) {\n+    const auto& fw_type = static_cast<const FixedSizeBinaryType&>(*type);\n+    byte_width_ = fw_type.bit_width() / 8;\n+  }\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_data_.Resize(kInitialHashTableSize * byte_width_));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const uint8_t* data = GetValues<uint8_t>(arr, 1);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const uint8_t* value = data + i * byte_width_;                                  \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  const uint8_t* dict_data = dict_data_.data();                                   \\\n+  while (kHashSlotEmpty != slot &&                                                \\\n+         !(0 == memcmp(value, dict_data + slot * byte_width_, byte_width_))) {    \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = dict_size_++;                                                          \\\n+    hash_slots_[j] = slot;                                                        \\\n+                                                                                  \\\n+    RETURN_NOT_OK(dict_data_.Append(value, byte_width_));                         \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr};\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[1]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data) const {\n+    return HashUtil::Hash(data, byte_width_, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    const uint8_t* dict_data = dict_data_.data();\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Find an empty slot in the new hash table\n+      int64_t j = HashValue(dict_data + index * byte_width_) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return Status::OK();\n+  }\n+\n+  int32_t byte_width_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Unique implementation\n+\n+template <typename Type>\n+class UniqueImpl : public HashTableKernel<Type, UniqueImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, UniqueImpl<Type>>;\n+  using Base::Base;\n+\n+  Status Reserve(const int64_t length) { return Status::OK(); }\n+\n+  void ObserveFound(const hash_slot_t slot) {}\n+  void ObserveNull() {}\n+  void ObserveNotFound(const hash_slot_t slot) {}\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Append(const ArrayData& input) override { return Base::Append(input); }\n+\n+  Status Flush(std::vector<Datum>* out) override {\n+    // No-op\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Dictionary encode implementation\n+\n+template <typename Type>\n+class DictEncodeImpl : public HashTableKernel<Type, DictEncodeImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, DictEncodeImpl>;\n+\n+  DictEncodeImpl(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : Base(type, pool), indices_builder_(pool) {}\n+\n+  Status Reserve(const int64_t length) { return indices_builder_.Reserve(length); }\n+\n+  void ObserveNull() { indices_builder_.UnsafeAppendToBitmap(false); }\n+\n+  void ObserveFound(const hash_slot_t slot) { indices_builder_.UnsafeAppend(slot); }\n+\n+  void ObserveNotFound(const hash_slot_t slot) { return ObserveFound(slot); }\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Flush(std::vector<Datum>* out) override {\n+    std::shared_ptr<ArrayData> result;\n+    RETURN_NOT_OK(indices_builder_.FinishInternal(&result));\n+    out->push_back(Datum(result));\n+    return Status::OK();\n+  }\n+\n+  using Base::Append;\n+\n+ private:\n+  Int32Builder indices_builder_;\n \n Review comment:\n   Why do we resort to an Int32Builder? Because at the end all chunks shall have the same integer type for their indices?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T19:53:49.089+0000",
                    "updated": "2017-11-15T19:53:49.089+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16254059",
                    "id": "16254059",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151233579\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,880 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(std::vector<Datum>* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n+  if (arr.null_count != 0) {                                                             \\\n+    internal::BitmapReader valid_reader(arr.buffers[0]->data(), arr.offset, arr.length); \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      const bool is_null = valid_reader.IsNotSet();                                      \\\n+      valid_reader.Next();                                                               \\\n+                                                                                         \\\n+      if (is_null) {                                                                     \\\n+        action->ObserveNull();                                                           \\\n+        continue;                                                                        \\\n+      }                                                                                  \\\n+                                                                                         \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  } else {                                                                               \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  }\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_has_c_type<Type>> : public HashTable {\n+ public:\n+  using T = typename Type::c_type;\n+\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_(pool) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_.Init());\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const T* values = GetValues<T>(arr, 1);\n+    auto action = static_cast<Action*>(this);\n+\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const T value = values[i];                                                      \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  while (kHashSlotEmpty != slot && dict_.values[slot] != value) {                 \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = static_cast<hash_slot_t>(dict_.size);                                  \\\n+    hash_slots_[j] = slot;                                                        \\\n+    dict_.values[dict_.size++] = value;                                           \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_.size > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    auto dict_data = dict_.buffer;\n+    RETURN_NOT_OK(dict_data->Resize(dict_.size * sizeof(T), false));\n+\n+    BufferVector buffers = {nullptr, dict_data};\n+    *out = std::make_shared<ArrayData>(type_, dict_.size, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const T& value) const {\n+    // TODO(wesm): Use faster hash function for C types\n+    return HashUtil::Hash(&value, sizeof(T), 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const T value = dict_.values[index];\n+\n+      // Find empty slot in the new hash table\n+      int64_t j = HashValue(value) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return dict_.Resize(new_size);\n+  }\n+\n+  HashDictionary<Type> dict_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for variable-length binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_binary<Type>> : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_offsets_(pool), dict_data_(pool), dict_size_(0) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_offsets_.Resize(kInitialHashTableSize));\n+\n+    // We append the end offset after each append to the dictionary, so this\n+    // sets the initial condition for the length-0 case\n+    //\n+    // initial offsets (dict size == 0): 0\n+    // after 1st dict entry of length 3: 0 3\n+    // after 2nd dict entry of length 4: 0 3 7\n+    RETURN_NOT_OK(dict_offsets_.Append(0));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const int32_t* offsets = GetValues<int32_t>(arr, 1);\n+    const uint8_t* data = GetValues<uint8_t>(arr, 2);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                           \\\n+  const int32_t position = offsets[i];                                              \\\n+  const int32_t length = offsets[i + 1] - position;                                 \\\n+  const uint8_t* value = data + position;                                           \\\n+                                                                                    \\\n+  int64_t j = HashValue(value, length) & mod_bitmask_;                              \\\n+  hash_slot_t slot = hash_slots_[j];                                                \\\n+                                                                                    \\\n+  const int32_t* dict_offsets = dict_offsets_.data();                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                     \\\n+  while (kHashSlotEmpty != slot &&                                                  \\\n+         !((dict_offsets[slot + 1] - dict_offsets[slot]) == length &&               \\\n+           0 == memcmp(value, dict_data + dict_offsets[slot], length))) {           \\\n+    ++j;                                                                            \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                               \\\n+      j = 0;                                                                        \\\n+    }                                                                               \\\n+    slot = hash_slots_[j];                                                          \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  if (slot == kHashSlotEmpty) {                                                     \\\n+    if (!Action::allow_expand) {                                                    \\\n+      throw HashException(\"Encountered new dictionary value\");                      \\\n+    }                                                                               \\\n+                                                                                    \\\n+    slot = dict_size_++;                                                            \\\n+    hash_slots_[j] = slot;                                                          \\\n+                                                                                    \\\n+    RETURN_NOT_OK(dict_data_.Append(value, length));                                \\\n+    RETURN_NOT_OK(dict_offsets_.Append(static_cast<int32_t>(dict_data_.length()))); \\\n+                                                                                    \\\n+    action->ObserveNotFound(slot);                                                  \\\n+                                                                                    \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) {   \\\n+      RETURN_NOT_OK(action->DoubleSize());                                          \\\n+    }                                                                               \\\n+  } else {                                                                          \\\n+    action->ObserveFound(slot);                                                     \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr, nullptr};\n+\n+    RETURN_NOT_OK(dict_offsets_.Finish(&buffers[1]));\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[2]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data, int32_t length) const {\n+    return HashUtil::Hash(data, length, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n \n Review comment:\n   This is duplicated between the different HashKernel implementations?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T19:53:49.111+0000",
                    "updated": "2017-11-15T19:53:49.111+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16254353",
                    "id": "16254353",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151272186\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,880 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(std::vector<Datum>* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n+  if (arr.null_count != 0) {                                                             \\\n+    internal::BitmapReader valid_reader(arr.buffers[0]->data(), arr.offset, arr.length); \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      const bool is_null = valid_reader.IsNotSet();                                      \\\n+      valid_reader.Next();                                                               \\\n+                                                                                         \\\n+      if (is_null) {                                                                     \\\n+        action->ObserveNull();                                                           \\\n+        continue;                                                                        \\\n+      }                                                                                  \\\n+                                                                                         \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  } else {                                                                               \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  }\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_has_c_type<Type>> : public HashTable {\n+ public:\n+  using T = typename Type::c_type;\n+\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_(pool) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_.Init());\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const T* values = GetValues<T>(arr, 1);\n+    auto action = static_cast<Action*>(this);\n+\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const T value = values[i];                                                      \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  while (kHashSlotEmpty != slot && dict_.values[slot] != value) {                 \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = static_cast<hash_slot_t>(dict_.size);                                  \\\n+    hash_slots_[j] = slot;                                                        \\\n+    dict_.values[dict_.size++] = value;                                           \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_.size > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    auto dict_data = dict_.buffer;\n+    RETURN_NOT_OK(dict_data->Resize(dict_.size * sizeof(T), false));\n+\n+    BufferVector buffers = {nullptr, dict_data};\n+    *out = std::make_shared<ArrayData>(type_, dict_.size, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const T& value) const {\n+    // TODO(wesm): Use faster hash function for C types\n+    return HashUtil::Hash(&value, sizeof(T), 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const T value = dict_.values[index];\n+\n+      // Find empty slot in the new hash table\n+      int64_t j = HashValue(value) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return dict_.Resize(new_size);\n+  }\n+\n+  HashDictionary<Type> dict_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for variable-length binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_binary<Type>> : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_offsets_(pool), dict_data_(pool), dict_size_(0) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_offsets_.Resize(kInitialHashTableSize));\n+\n+    // We append the end offset after each append to the dictionary, so this\n+    // sets the initial condition for the length-0 case\n+    //\n+    // initial offsets (dict size == 0): 0\n+    // after 1st dict entry of length 3: 0 3\n+    // after 2nd dict entry of length 4: 0 3 7\n+    RETURN_NOT_OK(dict_offsets_.Append(0));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const int32_t* offsets = GetValues<int32_t>(arr, 1);\n+    const uint8_t* data = GetValues<uint8_t>(arr, 2);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                           \\\n+  const int32_t position = offsets[i];                                              \\\n+  const int32_t length = offsets[i + 1] - position;                                 \\\n+  const uint8_t* value = data + position;                                           \\\n+                                                                                    \\\n+  int64_t j = HashValue(value, length) & mod_bitmask_;                              \\\n+  hash_slot_t slot = hash_slots_[j];                                                \\\n+                                                                                    \\\n+  const int32_t* dict_offsets = dict_offsets_.data();                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                     \\\n+  while (kHashSlotEmpty != slot &&                                                  \\\n+         !((dict_offsets[slot + 1] - dict_offsets[slot]) == length &&               \\\n+           0 == memcmp(value, dict_data + dict_offsets[slot], length))) {           \\\n+    ++j;                                                                            \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                               \\\n+      j = 0;                                                                        \\\n+    }                                                                               \\\n+    slot = hash_slots_[j];                                                          \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  if (slot == kHashSlotEmpty) {                                                     \\\n+    if (!Action::allow_expand) {                                                    \\\n+      throw HashException(\"Encountered new dictionary value\");                      \\\n+    }                                                                               \\\n+                                                                                    \\\n+    slot = dict_size_++;                                                            \\\n+    hash_slots_[j] = slot;                                                          \\\n+                                                                                    \\\n+    RETURN_NOT_OK(dict_data_.Append(value, length));                                \\\n+    RETURN_NOT_OK(dict_offsets_.Append(static_cast<int32_t>(dict_data_.length()))); \\\n+                                                                                    \\\n+    action->ObserveNotFound(slot);                                                  \\\n+                                                                                    \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) {   \\\n+      RETURN_NOT_OK(action->DoubleSize());                                          \\\n+    }                                                                               \\\n+  } else {                                                                          \\\n+    action->ObserveFound(slot);                                                     \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr, nullptr};\n+\n+    RETURN_NOT_OK(dict_offsets_.Finish(&buffers[1]));\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[2]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data, int32_t length) const {\n+    return HashUtil::Hash(data, length, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    const int32_t* dict_offsets = dict_offsets_.data();\n+    const uint8_t* dict_data = dict_data_.data();\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const int32_t length = dict_offsets[index + 1] - dict_offsets[index];\n+      const uint8_t* value = dict_data + dict_offsets[index];\n+\n+      // Find an empty slot in the new hash table\n+      int64_t j = HashValue(value, length) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return Status::OK();\n+  }\n+\n+  TypedBufferBuilder<int32_t> dict_offsets_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for fixed size binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_fixed_size_binary<Type>>\n+    : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_data_(pool), dict_size_(0) {\n+    const auto& fw_type = static_cast<const FixedSizeBinaryType&>(*type);\n+    byte_width_ = fw_type.bit_width() / 8;\n+  }\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_data_.Resize(kInitialHashTableSize * byte_width_));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const uint8_t* data = GetValues<uint8_t>(arr, 1);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const uint8_t* value = data + i * byte_width_;                                  \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  const uint8_t* dict_data = dict_data_.data();                                   \\\n+  while (kHashSlotEmpty != slot &&                                                \\\n+         !(0 == memcmp(value, dict_data + slot * byte_width_, byte_width_))) {    \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = dict_size_++;                                                          \\\n+    hash_slots_[j] = slot;                                                        \\\n+                                                                                  \\\n+    RETURN_NOT_OK(dict_data_.Append(value, byte_width_));                         \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr};\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[1]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data) const {\n+    return HashUtil::Hash(data, byte_width_, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    const uint8_t* dict_data = dict_data_.data();\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Find an empty slot in the new hash table\n+      int64_t j = HashValue(dict_data + index * byte_width_) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return Status::OK();\n+  }\n+\n+  int32_t byte_width_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Unique implementation\n+\n+template <typename Type>\n+class UniqueImpl : public HashTableKernel<Type, UniqueImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, UniqueImpl<Type>>;\n+  using Base::Base;\n+\n+  Status Reserve(const int64_t length) { return Status::OK(); }\n+\n+  void ObserveFound(const hash_slot_t slot) {}\n+  void ObserveNull() {}\n+  void ObserveNotFound(const hash_slot_t slot) {}\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Append(const ArrayData& input) override { return Base::Append(input); }\n+\n+  Status Flush(std::vector<Datum>* out) override {\n+    // No-op\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Dictionary encode implementation\n+\n+template <typename Type>\n+class DictEncodeImpl : public HashTableKernel<Type, DictEncodeImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, DictEncodeImpl>;\n+\n+  DictEncodeImpl(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : Base(type, pool), indices_builder_(pool) {}\n+\n+  Status Reserve(const int64_t length) { return indices_builder_.Reserve(length); }\n+\n+  void ObserveNull() { indices_builder_.UnsafeAppendToBitmap(false); }\n+\n+  void ObserveFound(const hash_slot_t slot) { indices_builder_.UnsafeAppend(slot); }\n+\n+  void ObserveNotFound(const hash_slot_t slot) { return ObserveFound(slot); }\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Flush(std::vector<Datum>* out) override {\n+    std::shared_ptr<ArrayData> result;\n+    RETURN_NOT_OK(indices_builder_.FinishInternal(&result));\n+    out->push_back(Datum(result));\n+    return Status::OK();\n+  }\n+\n+  using Base::Append;\n+\n+ private:\n+  Int32Builder indices_builder_;\n \n Review comment:\n   Yes. In addition to being slower (more branching, possibility of appends failing), we would have to deal with promoting all chunks to the largest type at the end. I am open to using the adaptive builder but only after writing more benchmarks and doing some more analysis\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T22:32:54.579+0000",
                    "updated": "2017-11-15T22:32:54.579+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16254362",
                    "id": "16254362",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151273409\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,880 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(std::vector<Datum>* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n+  if (arr.null_count != 0) {                                                             \\\n+    internal::BitmapReader valid_reader(arr.buffers[0]->data(), arr.offset, arr.length); \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      const bool is_null = valid_reader.IsNotSet();                                      \\\n+      valid_reader.Next();                                                               \\\n+                                                                                         \\\n+      if (is_null) {                                                                     \\\n+        action->ObserveNull();                                                           \\\n+        continue;                                                                        \\\n+      }                                                                                  \\\n+                                                                                         \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  } else {                                                                               \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  }\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_has_c_type<Type>> : public HashTable {\n+ public:\n+  using T = typename Type::c_type;\n+\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_(pool) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_.Init());\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const T* values = GetValues<T>(arr, 1);\n+    auto action = static_cast<Action*>(this);\n+\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const T value = values[i];                                                      \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  while (kHashSlotEmpty != slot && dict_.values[slot] != value) {                 \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = static_cast<hash_slot_t>(dict_.size);                                  \\\n+    hash_slots_[j] = slot;                                                        \\\n+    dict_.values[dict_.size++] = value;                                           \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_.size > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    auto dict_data = dict_.buffer;\n+    RETURN_NOT_OK(dict_data->Resize(dict_.size * sizeof(T), false));\n+\n+    BufferVector buffers = {nullptr, dict_data};\n+    *out = std::make_shared<ArrayData>(type_, dict_.size, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const T& value) const {\n+    // TODO(wesm): Use faster hash function for C types\n+    return HashUtil::Hash(&value, sizeof(T), 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const T value = dict_.values[index];\n+\n+      // Find empty slot in the new hash table\n+      int64_t j = HashValue(value) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return dict_.Resize(new_size);\n+  }\n+\n+  HashDictionary<Type> dict_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for variable-length binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_binary<Type>> : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_offsets_(pool), dict_data_(pool), dict_size_(0) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_offsets_.Resize(kInitialHashTableSize));\n+\n+    // We append the end offset after each append to the dictionary, so this\n+    // sets the initial condition for the length-0 case\n+    //\n+    // initial offsets (dict size == 0): 0\n+    // after 1st dict entry of length 3: 0 3\n+    // after 2nd dict entry of length 4: 0 3 7\n+    RETURN_NOT_OK(dict_offsets_.Append(0));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const int32_t* offsets = GetValues<int32_t>(arr, 1);\n+    const uint8_t* data = GetValues<uint8_t>(arr, 2);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                           \\\n+  const int32_t position = offsets[i];                                              \\\n+  const int32_t length = offsets[i + 1] - position;                                 \\\n+  const uint8_t* value = data + position;                                           \\\n+                                                                                    \\\n+  int64_t j = HashValue(value, length) & mod_bitmask_;                              \\\n+  hash_slot_t slot = hash_slots_[j];                                                \\\n+                                                                                    \\\n+  const int32_t* dict_offsets = dict_offsets_.data();                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                     \\\n+  while (kHashSlotEmpty != slot &&                                                  \\\n+         !((dict_offsets[slot + 1] - dict_offsets[slot]) == length &&               \\\n+           0 == memcmp(value, dict_data + dict_offsets[slot], length))) {           \\\n+    ++j;                                                                            \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                               \\\n+      j = 0;                                                                        \\\n+    }                                                                               \\\n+    slot = hash_slots_[j];                                                          \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  if (slot == kHashSlotEmpty) {                                                     \\\n+    if (!Action::allow_expand) {                                                    \\\n+      throw HashException(\"Encountered new dictionary value\");                      \\\n+    }                                                                               \\\n+                                                                                    \\\n+    slot = dict_size_++;                                                            \\\n+    hash_slots_[j] = slot;                                                          \\\n+                                                                                    \\\n+    RETURN_NOT_OK(dict_data_.Append(value, length));                                \\\n+    RETURN_NOT_OK(dict_offsets_.Append(static_cast<int32_t>(dict_data_.length()))); \\\n+                                                                                    \\\n+    action->ObserveNotFound(slot);                                                  \\\n+                                                                                    \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) {   \\\n+      RETURN_NOT_OK(action->DoubleSize());                                          \\\n+    }                                                                               \\\n+  } else {                                                                          \\\n+    action->ObserveFound(slot);                                                     \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr, nullptr};\n+\n+    RETURN_NOT_OK(dict_offsets_.Finish(&buffers[1]));\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[2]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data, int32_t length) const {\n+    return HashUtil::Hash(data, length, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n \n Review comment:\n   The hot path on the inner loop of the hash table is different because of the different memory layout (primitive vs. varbytes vs fixed length bytes). \r\n   \r\n   Getting rid of the code duplication leaves some unsavory options:\r\n   \r\n   * Writing more macros (debugging harder)\r\n   * Introducing some auxiliary structs / functions to make the non-primitive arrays \"value-like\". E.g. in the old code there was the `WrappedBinary` type. We obviously need to write some benchmarks, but I suspect this would make things slower\r\n   \r\n   I think we should write a bunch of microbenchmarks for various cases (small cardinality categories, large cardinality categories) and use this implementation as the performance baseline. If we refactor to add different layers of indirection to enable code reuse and the performance gets worse, we should not make such changes.\r\n   \r\n   FWIW, I am OK with writing large amounts of nested macros in this particular set of kernels, particularly since the hot paths in this code, with the exception of appending to the dictionary, cannot fail. It's also useful to be able to easily change the hash table probing stretch from linear to quadratic by switching the macro name used\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T22:38:46.334+0000",
                    "updated": "2017-11-15T22:38:46.334+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16254375",
                    "id": "16254375",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151275822\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,880 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(std::vector<Datum>* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n+  if (arr.null_count != 0) {                                                             \\\n+    internal::BitmapReader valid_reader(arr.buffers[0]->data(), arr.offset, arr.length); \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      const bool is_null = valid_reader.IsNotSet();                                      \\\n+      valid_reader.Next();                                                               \\\n+                                                                                         \\\n+      if (is_null) {                                                                     \\\n+        action->ObserveNull();                                                           \\\n+        continue;                                                                        \\\n+      }                                                                                  \\\n+                                                                                         \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  } else {                                                                               \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  }\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_has_c_type<Type>> : public HashTable {\n+ public:\n+  using T = typename Type::c_type;\n+\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_(pool) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_.Init());\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const T* values = GetValues<T>(arr, 1);\n+    auto action = static_cast<Action*>(this);\n+\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const T value = values[i];                                                      \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  while (kHashSlotEmpty != slot && dict_.values[slot] != value) {                 \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = static_cast<hash_slot_t>(dict_.size);                                  \\\n+    hash_slots_[j] = slot;                                                        \\\n+    dict_.values[dict_.size++] = value;                                           \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_.size > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    auto dict_data = dict_.buffer;\n+    RETURN_NOT_OK(dict_data->Resize(dict_.size * sizeof(T), false));\n+\n+    BufferVector buffers = {nullptr, dict_data};\n+    *out = std::make_shared<ArrayData>(type_, dict_.size, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const T& value) const {\n+    // TODO(wesm): Use faster hash function for C types\n+    return HashUtil::Hash(&value, sizeof(T), 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const T value = dict_.values[index];\n+\n+      // Find empty slot in the new hash table\n+      int64_t j = HashValue(value) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return dict_.Resize(new_size);\n+  }\n+\n+  HashDictionary<Type> dict_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for variable-length binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_binary<Type>> : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_offsets_(pool), dict_data_(pool), dict_size_(0) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_offsets_.Resize(kInitialHashTableSize));\n+\n+    // We append the end offset after each append to the dictionary, so this\n+    // sets the initial condition for the length-0 case\n+    //\n+    // initial offsets (dict size == 0): 0\n+    // after 1st dict entry of length 3: 0 3\n+    // after 2nd dict entry of length 4: 0 3 7\n+    RETURN_NOT_OK(dict_offsets_.Append(0));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const int32_t* offsets = GetValues<int32_t>(arr, 1);\n+    const uint8_t* data = GetValues<uint8_t>(arr, 2);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                           \\\n+  const int32_t position = offsets[i];                                              \\\n+  const int32_t length = offsets[i + 1] - position;                                 \\\n+  const uint8_t* value = data + position;                                           \\\n+                                                                                    \\\n+  int64_t j = HashValue(value, length) & mod_bitmask_;                              \\\n+  hash_slot_t slot = hash_slots_[j];                                                \\\n+                                                                                    \\\n+  const int32_t* dict_offsets = dict_offsets_.data();                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                     \\\n+  while (kHashSlotEmpty != slot &&                                                  \\\n+         !((dict_offsets[slot + 1] - dict_offsets[slot]) == length &&               \\\n+           0 == memcmp(value, dict_data + dict_offsets[slot], length))) {           \\\n+    ++j;                                                                            \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                               \\\n+      j = 0;                                                                        \\\n+    }                                                                               \\\n+    slot = hash_slots_[j];                                                          \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  if (slot == kHashSlotEmpty) {                                                     \\\n+    if (!Action::allow_expand) {                                                    \\\n+      throw HashException(\"Encountered new dictionary value\");                      \\\n+    }                                                                               \\\n+                                                                                    \\\n+    slot = dict_size_++;                                                            \\\n+    hash_slots_[j] = slot;                                                          \\\n+                                                                                    \\\n+    RETURN_NOT_OK(dict_data_.Append(value, length));                                \\\n+    RETURN_NOT_OK(dict_offsets_.Append(static_cast<int32_t>(dict_data_.length()))); \\\n+                                                                                    \\\n+    action->ObserveNotFound(slot);                                                  \\\n+                                                                                    \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) {   \\\n+      RETURN_NOT_OK(action->DoubleSize());                                          \\\n+    }                                                                               \\\n+  } else {                                                                          \\\n+    action->ObserveFound(slot);                                                     \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr, nullptr};\n+\n+    RETURN_NOT_OK(dict_offsets_.Finish(&buffers[1]));\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[2]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data, int32_t length) const {\n+    return HashUtil::Hash(data, length, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n \n Review comment:\n   Added macros. In this particular case, I don't think it's so bad\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-15T22:50:39.637+0000",
                    "updated": "2017-11-15T22:50:39.637+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16254708",
                    "id": "16254708",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344807083\n \n \n   So we're about a factor of 2 or so away from `pandas.unique`:\r\n   \r\n   ```\r\n   In [1]: import pandas as pd\r\n   import numpy as np\r\n   import pyarrow as pa\r\n      ...: \r\n   \r\n   In [2]: arr = pa.array(list(range(10000)) * 100)\r\n   \r\n   In [3]: v = arr.to_pandas().copy()\r\n   \r\n   In [4]: timeit arr.unique()\r\n   pd.unique(100 loops, best of 3: 10.4 ms per loop\r\n   \r\n   In [5]: timeit pd.unique(v)\r\n   100 loops, best of 3: 4.82 ms per loop\r\n   ```\r\n   \r\n   I suspect this is because of our hash function being slow, but we'll need to do some more analysis\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T03:46:50.150+0000",
                    "updated": "2017-11-16T03:46:50.150+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16255535",
                    "id": "16255535",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344973038\n \n \n   This is ready to go modulo getting the MSVC build passing (looks like just compiler warnings remaining). Any more feedback?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T16:13:36.039+0000",
                    "updated": "2017-11-16T16:13:36.039+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16255537",
                    "id": "16255537",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344973188\n \n \n   Also let me know if you like the `x.dictionary_encode()` API. I can always rename that to something else\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T16:13:58.779+0000",
                    "updated": "2017-11-16T16:13:58.779+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16255546",
                    "id": "16255546",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151027570\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/cast.h\n ##########\n @@ -60,7 +63,12 @@ Status Cast(FunctionContext* context, const Array& array,\n             const std::shared_ptr<DataType>& to_type, const CastOptions& options,\n             std::shared_ptr<Array>* out);\n \n+ARROW_EXPORT\n+Status Cast(FunctionContext* context, const Datum& value,\n+            const std::shared_ptr<DataType>& to_type, const CastOptions& options,\n+            Datum* out);\n+\n }  // namespace compute\n }  // namespace arrow\n \n-#endif  // ARROW_COMPUTE_CAST_H\n+#endif  // ARROW_COMPUTE_KERNELS_CAST_H\n \n Review comment:\n   I probably need to do a once over of Cast and make sure the ChunkedArray form also works\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T16:21:09.239+0000",
                    "updated": "2017-11-16T16:21:09.239+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16255553",
                    "id": "16255553",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-344976728\n \n \n   I created https://issues.apache.org/jira/browse/ARROW-1828 to take care of the boolean hash table implementation, since this patch has already gotten quite large\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T16:24:41.557+0000",
                    "updated": "2017-11-16T16:24:41.557+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16255571",
                    "id": "16255571",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "cpcloud commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151464797\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,822 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(Datum* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  /// Size at which we decide to resize\n+  int64_t hash_table_load_threshold_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  hash_table_load_threshold_ =\n+      static_cast<int64_t>(static_cast<double>(elements) * kMaxHashTableLoad);\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n \n Review comment:\n   I suspect we can elide macros using CRTP, but that can be done in a follow-up patch. Something like this:\r\n   \r\n   ```cpp\r\n   \r\n   template <typename Kernel, typename Enable = void>\r\n   class HashTableKernelBase {\r\n   // interface methods, static_cast<Kernel*>(this) then call implementation\r\n   };\r\n   \r\n   template <typename Type, typename Action>\r\n   class HashTableKernel : public HashTableKernelBase<HashTableKernel<Type, Action>, enable_if_null<Type>> {\r\n   // specific methods: inner loop, hash pass, etc\r\n   };\r\n   ```\r\n   \r\n   No macros, no runtime polymorphism overhead :)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T16:38:08.560+0000",
                    "updated": "2017-11-16T16:38:08.560+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16255572",
                    "id": "16255572",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "cpcloud commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151463749\n \n \n\n ##########\n File path: cpp/src/arrow/type_traits.h\n ##########\n @@ -430,6 +430,17 @@ static inline bool is_binary_like(Type::type type_id) {\n   return false;\n }\n \n+static inline bool is_dictionary(Type::type type_id) {\n+  switch (type_id) {\n \n Review comment:\n   Any reason not to simplify this to `return type_id == Type::DICTIONARY`?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T16:38:08.565+0000",
                    "updated": "2017-11-16T16:38:08.565+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16255573",
                    "id": "16255573",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "cpcloud commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151463494\n \n \n\n ##########\n File path: cpp/src/arrow/type.h\n ##########\n @@ -498,25 +498,34 @@ class ARROW_EXPORT StructType : public NestedType {\n   std::vector<BufferDescr> GetBufferLayout() const override;\n };\n \n-class ARROW_EXPORT Decimal128Type : public FixedSizeBinaryType {\n+class ARROW_EXPORT _DecimalBaseType : public FixedSizeBinaryType {\n \n Review comment:\n   I don't think underscores are allowed by the C++ standard. How about `DecimalBaseType`?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T16:38:08.571+0000",
                    "updated": "2017-11-16T16:38:08.571+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16255589",
                    "id": "16255589",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "cpcloud commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151464797\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,822 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(Datum* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  /// Size at which we decide to resize\n+  int64_t hash_table_load_threshold_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  hash_table_load_threshold_ =\n+      static_cast<int64_t>(static_cast<double>(elements) * kMaxHashTableLoad);\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n \n Review comment:\n   I suspect we can elide macros using CRTP, but that can be done in a follow-up patch. Something like this:\r\n   \r\n   ```cpp\r\n   \r\n   template <typename Kernel>\r\n   class HashTableKernelBase {\r\n   // interface methods, static_cast<Kernel*>(this) then call implementation\r\n   };\r\n   \r\n   template <typename Type, typename Action, typename Enable = void>\r\n   class HashTableKernel : public HashTableKernelBase<HashTableKernel<Type, Action, Enable>> {\r\n   // specific methods: inner loop, hash pass, etc\r\n   };\r\n   \r\n   template <typename Type, typename Action>\r\n   class HashTableKernel<Type, Action, enable_if_null<Type>> {\r\n   };\r\n   ```\r\n   \r\n   No macros, no runtime polymorphism overhead :)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T16:53:32.438+0000",
                    "updated": "2017-11-16T16:53:32.438+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16256007",
                    "id": "16256007",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151543979\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,822 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(Datum* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  /// Size at which we decide to resize\n+  int64_t hash_table_load_threshold_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  hash_table_load_threshold_ =\n+      static_cast<int64_t>(static_cast<double>(elements) * kMaxHashTableLoad);\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n \n Review comment:\n   This module is already using CRTP in some other places. I hear what you're saying but it's pretty difficult because the other array types do not have value-like semantics. I suggest we _first_ set up a comprehensive set of benchmarks and then (and _only_ then) work on refactoring to get rid of macros in a performance-neutral way\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T21:37:04.822+0000",
                    "updated": "2017-11-16T21:37:04.822+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16256008",
                    "id": "16256008",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151544074\n \n \n\n ##########\n File path: cpp/src/arrow/type.h\n ##########\n @@ -498,25 +498,34 @@ class ARROW_EXPORT StructType : public NestedType {\n   std::vector<BufferDescr> GetBufferLayout() const override;\n };\n \n-class ARROW_EXPORT Decimal128Type : public FixedSizeBinaryType {\n+class ARROW_EXPORT _DecimalBaseType : public FixedSizeBinaryType {\n \n Review comment:\n   Sure, this is only temporary until I can remove usages of `DecimalType` from parquet-cpp\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T21:37:30.937+0000",
                    "updated": "2017-11-16T21:37:30.937+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16256010",
                    "id": "16256010",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151545209\n \n \n\n ##########\n File path: cpp/src/arrow/type_traits.h\n ##########\n @@ -430,6 +430,17 @@ static inline bool is_binary_like(Type::type type_id) {\n   return false;\n }\n \n+static inline bool is_dictionary(Type::type type_id) {\n+  switch (type_id) {\n \n Review comment:\n   nope, fixing\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-16T21:42:15.419+0000",
                    "updated": "2017-11-16T21:42:15.419+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16257138",
                    "id": "16257138",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-345279306\n \n \n   Lost access to the Windows machine I had been using for local debugging, will have to set something up today to figure out why this isn't linking\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-17T15:45:18.487+0000",
                    "updated": "2017-11-17T15:45:18.487+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16257728",
                    "id": "16257728",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-345390578\n \n \n   Phew, looks like we are finally in the clear. +1, I will merge when the build passes and then get busy with follow ups\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-17T23:11:06.579+0000",
                    "updated": "2017-11-17T23:11:06.579+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16257747",
                    "id": "16257747",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 1266\n[https://github.com/apache/arrow/pull/1266]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-11-17T23:29:57.813+0000",
                    "updated": "2017-11-17T23:29:57.813+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16257748",
                    "id": "16257748",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm closed pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/LICENSE.txt b/LICENSE.txt\nindex 00cb9ece2..038518a5d 100644\n--- a/LICENSE.txt\n+++ b/LICENSE.txt\n@@ -398,3 +398,62 @@ DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+\n+--------------------------------------------------------------------------------\n+\n+This project includes code from the Boost project\n+\n+Boost Software License - Version 1.0 - August 17th, 2003\n+\n+Permission is hereby granted, free of charge, to any person or organization\n+obtaining a copy of the software and accompanying documentation covered by\n+this license (the \"Software\") to use, reproduce, display, distribute,\n+execute, and transmit the Software, and to prepare derivative works of the\n+Software, and to permit third-parties to whom the Software is furnished to\n+do so, all subject to the following:\n+\n+The copyright notices in the Software and this entire statement, including\n+the above license grant, this restriction and the following disclaimer,\n+must be included in all copies of the Software, in whole or in part, and\n+all derivative works of the Software, unless such copies or derivative\n+works are solely in the form of machine-executable object code generated by\n+a source language processor.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT\n+SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE\n+FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,\n+ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n+DEALINGS IN THE SOFTWARE.\n+\n+--------------------------------------------------------------------------------\n+\n+This project includes code from the mapbox/variant project, BSD 3-clause\n+license\n+\n+Copyright (c) MapBox\n+All rights reserved.\n+\n+Redistribution and use in source and binary forms, with or without modification,\n+are permitted provided that the following conditions are met:\n+\n+- Redistributions of source code must retain the above copyright notice, this\n+  list of conditions and the following disclaimer.\n+- Redistributions in binary form must reproduce the above copyright notice, this\n+  list of conditions and the following disclaimer in the documentation and/or\n+  other materials provided with the distribution.\n+- Neither the name \"MapBox\" nor the names of its contributors may be\n+  used to endorse or promote products derived from this software without\n+  specific prior written permission.\n+\n+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n+ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n+ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\ndiff --git a/cpp/CMakeLists.txt b/cpp/CMakeLists.txt\nindex 5f0c431d5..f4b7b29b9 100644\n--- a/cpp/CMakeLists.txt\n+++ b/cpp/CMakeLists.txt\n@@ -414,6 +414,7 @@ if (UNIX)\n             (item MATCHES \"xxhash.h\") OR\n             (item MATCHES \"xxhash.cc\") OR\n             (item MATCHES \"config.h\") OR\n+            (item MATCHES \"util/variant\") OR\n             (item MATCHES \"zmalloc.h\") OR\n             (item MATCHES \"ae.h\")))\n       LIST(APPEND FILTERED_LINT_FILES ${item})\ndiff --git a/cpp/build-support/clang_format_exclusions.txt b/cpp/build-support/clang_format_exclusions.txt\nindex 2d5d86d4e..d31d8a00d 100644\n--- a/cpp/build-support/clang_format_exclusions.txt\n+++ b/cpp/build-support/clang_format_exclusions.txt\n@@ -3,6 +3,8 @@\n *pyarrow_api.h\n *python/config.h\n *python/platform.h\n+*util/variant.h\n+*util/variant/*\n *thirdparty/ae/*\n *xxhash.cc\n *xxhash.h\ndiff --git a/cpp/cmake_modules/SetupCxxFlags.cmake b/cpp/cmake_modules/SetupCxxFlags.cmake\nindex 4b1950f7a..6b0974b3d 100644\n--- a/cpp/cmake_modules/SetupCxxFlags.cmake\n+++ b/cpp/cmake_modules/SetupCxxFlags.cmake\n@@ -68,6 +68,9 @@ if (\"${UPPERCASE_BUILD_WARNING_LEVEL}\" STREQUAL \"CHECKIN\")\n     set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /W3\")\n     # Treat all compiler warnings as errors\n     set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /WX\")\n+\n+    # MSVC version of -Wno-deprecated\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /wd4996\")\n   elseif (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n     set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Weverything -Wno-c++98-compat \\\n -Wno-c++98-compat-pedantic -Wno-deprecated -Wno-weak-vtables -Wno-padded \\\n@@ -115,6 +118,9 @@ elseif (\"${UPPERCASE_BUILD_WARNING_LEVEL}\" STREQUAL \"EVERYTHING\")\n     # /wdnnnn disables a warning where \"nnnn\" is a warning number\n     # Treat all compiler warnings as errors\n     set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS}  /WX\")\n+\n+    # MSVC version of -Wno-deprecated\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /wd4996\")\n   elseif (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n     set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Weverything -Wno-c++98-compat -Wno-c++98-compat-pedantic\")\n     # Treat all compiler warnings as errors\n@@ -134,6 +140,9 @@ else()\n     # /wdnnnn disables a warning where \"nnnn\" is a warning number\n     string(REPLACE \"/W3\" \"\" CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS}\")\n     set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /W3\")\n+\n+    # MSVC version of -Wno-deprecated\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /wd4996\")\n   elseif (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n     set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wall\")\n   elseif (\"${COMPILER_FAMILY}\" STREQUAL \"gcc\")\ndiff --git a/cpp/src/arrow/CMakeLists.txt b/cpp/src/arrow/CMakeLists.txt\nindex 69d505233..496e0da9d 100644\n--- a/cpp/src/arrow/CMakeLists.txt\n+++ b/cpp/src/arrow/CMakeLists.txt\n@@ -50,8 +50,10 @@ endif()\n if (ARROW_COMPUTE)\n   add_subdirectory(compute)\n   set(ARROW_SRCS ${ARROW_SRCS}\n-    compute/cast.cc\n     compute/context.cc\n+    compute/kernels/cast.cc\n+    compute/kernels/hash.cc\n+    compute/kernels/util-internal.cc\n   )\n endif()\n \ndiff --git a/cpp/src/arrow/array-test.cc b/cpp/src/arrow/array-test.cc\nindex 15c75534e..d894df131 100644\n--- a/cpp/src/arrow/array-test.cc\n+++ b/cpp/src/arrow/array-test.cc\n@@ -402,10 +402,6 @@ typedef ::testing::Types<PBoolean, PUInt8, PUInt16, PUInt32, PUInt64, PInt8, PIn\n \n TYPED_TEST_CASE(TestPrimitiveBuilder, Primitives);\n \n-#define DECL_T() typedef typename TestFixture::T T;\n-\n-#define DECL_TYPE() typedef typename TestFixture::Type Type;\n-\n TYPED_TEST(TestPrimitiveBuilder, TestInit) {\n   DECL_TYPE();\n \n@@ -1623,353 +1619,6 @@ TEST_F(TestAdaptiveUIntBuilder, TestAppendVector) {\n   ASSERT_TRUE(expected_->Equals(result_));\n }\n \n-// ----------------------------------------------------------------------\n-// Dictionary tests\n-\n-template <typename Type>\n-class TestDictionaryBuilder : public TestBuilder {};\n-\n-typedef ::testing::Types<Int8Type, UInt8Type, Int16Type, UInt16Type, Int32Type,\n-                         UInt32Type, Int64Type, UInt64Type, FloatType, DoubleType>\n-    PrimitiveDictionaries;\n-\n-TYPED_TEST_CASE(TestDictionaryBuilder, PrimitiveDictionaries);\n-\n-TYPED_TEST(TestDictionaryBuilder, Basic) {\n-  DictionaryBuilder<TypeParam> builder(default_memory_pool());\n-  ASSERT_OK(builder.Append(static_cast<typename TypeParam::c_type>(1)));\n-  ASSERT_OK(builder.Append(static_cast<typename TypeParam::c_type>(2)));\n-  ASSERT_OK(builder.Append(static_cast<typename TypeParam::c_type>(1)));\n-\n-  std::shared_ptr<Array> result;\n-  ASSERT_OK(builder.Finish(&result));\n-\n-  // Build expected data\n-  NumericBuilder<TypeParam> dict_builder;\n-  ASSERT_OK(dict_builder.Append(static_cast<typename TypeParam::c_type>(1)));\n-  ASSERT_OK(dict_builder.Append(static_cast<typename TypeParam::c_type>(2)));\n-  std::shared_ptr<Array> dict_array;\n-  ASSERT_OK(dict_builder.Finish(&dict_array));\n-  auto dtype = std::make_shared<DictionaryType>(int8(), dict_array);\n-\n-  Int8Builder int_builder;\n-  ASSERT_OK(int_builder.Append(0));\n-  ASSERT_OK(int_builder.Append(1));\n-  ASSERT_OK(int_builder.Append(0));\n-  std::shared_ptr<Array> int_array;\n-  ASSERT_OK(int_builder.Finish(&int_array));\n-\n-  DictionaryArray expected(dtype, int_array);\n-  ASSERT_TRUE(expected.Equals(result));\n-}\n-\n-TYPED_TEST(TestDictionaryBuilder, ArrayConversion) {\n-  NumericBuilder<TypeParam> builder;\n-  // DictionaryBuilder<TypeParam> builder;\n-  ASSERT_OK(builder.Append(static_cast<typename TypeParam::c_type>(1)));\n-  ASSERT_OK(builder.Append(static_cast<typename TypeParam::c_type>(2)));\n-  ASSERT_OK(builder.Append(static_cast<typename TypeParam::c_type>(1)));\n-\n-  std::shared_ptr<Array> intermediate_result;\n-  ASSERT_OK(builder.Finish(&intermediate_result));\n-  DictionaryBuilder<TypeParam> dictionary_builder(default_memory_pool());\n-  ASSERT_OK(dictionary_builder.AppendArray(*intermediate_result));\n-  std::shared_ptr<Array> result;\n-  ASSERT_OK(dictionary_builder.Finish(&result));\n-\n-  // Build expected data\n-  NumericBuilder<TypeParam> dict_builder;\n-  ASSERT_OK(dict_builder.Append(static_cast<typename TypeParam::c_type>(1)));\n-  ASSERT_OK(dict_builder.Append(static_cast<typename TypeParam::c_type>(2)));\n-  std::shared_ptr<Array> dict_array;\n-  ASSERT_OK(dict_builder.Finish(&dict_array));\n-  auto dtype = std::make_shared<DictionaryType>(int8(), dict_array);\n-\n-  Int8Builder int_builder;\n-  ASSERT_OK(int_builder.Append(0));\n-  ASSERT_OK(int_builder.Append(1));\n-  ASSERT_OK(int_builder.Append(0));\n-  std::shared_ptr<Array> int_array;\n-  ASSERT_OK(int_builder.Finish(&int_array));\n-\n-  DictionaryArray expected(dtype, int_array);\n-  ASSERT_TRUE(expected.Equals(result));\n-}\n-\n-TYPED_TEST(TestDictionaryBuilder, DoubleTableSize) {\n-  using Scalar = typename TypeParam::c_type;\n-  // Skip this test for (u)int8\n-  if (sizeof(Scalar) > 1) {\n-    // Build the dictionary Array\n-    DictionaryBuilder<TypeParam> builder(default_memory_pool());\n-    // Build expected data\n-    NumericBuilder<TypeParam> dict_builder;\n-    Int16Builder int_builder;\n-\n-    // Fill with 1024 different values\n-    for (int64_t i = 0; i < 1024; i++) {\n-      ASSERT_OK(builder.Append(static_cast<Scalar>(i)));\n-      ASSERT_OK(dict_builder.Append(static_cast<Scalar>(i)));\n-      ASSERT_OK(int_builder.Append(static_cast<uint16_t>(i)));\n-    }\n-    // Fill with an already existing value\n-    for (int64_t i = 0; i < 1024; i++) {\n-      ASSERT_OK(builder.Append(static_cast<Scalar>(1)));\n-      ASSERT_OK(int_builder.Append(1));\n-    }\n-\n-    // Finalize result\n-    std::shared_ptr<Array> result;\n-    ASSERT_OK(builder.Finish(&result));\n-\n-    // Finalize expected data\n-    std::shared_ptr<Array> dict_array;\n-    ASSERT_OK(dict_builder.Finish(&dict_array));\n-    auto dtype = std::make_shared<DictionaryType>(int16(), dict_array);\n-    std::shared_ptr<Array> int_array;\n-    ASSERT_OK(int_builder.Finish(&int_array));\n-\n-    DictionaryArray expected(dtype, int_array);\n-    ASSERT_TRUE(expected.Equals(result));\n-  }\n-}\n-\n-TEST(TestStringDictionaryBuilder, Basic) {\n-  // Build the dictionary Array\n-  StringDictionaryBuilder builder(default_memory_pool());\n-  ASSERT_OK(builder.Append(\"test\"));\n-  ASSERT_OK(builder.Append(\"test2\"));\n-  ASSERT_OK(builder.Append(\"test\"));\n-\n-  std::shared_ptr<Array> result;\n-  ASSERT_OK(builder.Finish(&result));\n-\n-  // Build expected data\n-  StringBuilder str_builder;\n-  ASSERT_OK(str_builder.Append(\"test\"));\n-  ASSERT_OK(str_builder.Append(\"test2\"));\n-  std::shared_ptr<Array> str_array;\n-  ASSERT_OK(str_builder.Finish(&str_array));\n-  auto dtype = std::make_shared<DictionaryType>(int8(), str_array);\n-\n-  Int8Builder int_builder;\n-  ASSERT_OK(int_builder.Append(0));\n-  ASSERT_OK(int_builder.Append(1));\n-  ASSERT_OK(int_builder.Append(0));\n-  std::shared_ptr<Array> int_array;\n-  ASSERT_OK(int_builder.Finish(&int_array));\n-\n-  DictionaryArray expected(dtype, int_array);\n-  ASSERT_TRUE(expected.Equals(result));\n-}\n-\n-TEST(TestStringDictionaryBuilder, DoubleTableSize) {\n-  // Build the dictionary Array\n-  StringDictionaryBuilder builder(default_memory_pool());\n-  // Build expected data\n-  StringBuilder str_builder;\n-  Int16Builder int_builder;\n-\n-  // Fill with 1024 different values\n-  for (int64_t i = 0; i < 1024; i++) {\n-    std::stringstream ss;\n-    ss << \"test\" << i;\n-    ASSERT_OK(builder.Append(ss.str()));\n-    ASSERT_OK(str_builder.Append(ss.str()));\n-    ASSERT_OK(int_builder.Append(static_cast<uint16_t>(i)));\n-  }\n-  // Fill with an already existing value\n-  for (int64_t i = 0; i < 1024; i++) {\n-    ASSERT_OK(builder.Append(\"test1\"));\n-    ASSERT_OK(int_builder.Append(1));\n-  }\n-\n-  // Finalize result\n-  std::shared_ptr<Array> result;\n-  ASSERT_OK(builder.Finish(&result));\n-\n-  // Finalize expected data\n-  std::shared_ptr<Array> str_array;\n-  ASSERT_OK(str_builder.Finish(&str_array));\n-  auto dtype = std::make_shared<DictionaryType>(int16(), str_array);\n-  std::shared_ptr<Array> int_array;\n-  ASSERT_OK(int_builder.Finish(&int_array));\n-\n-  DictionaryArray expected(dtype, int_array);\n-  ASSERT_TRUE(expected.Equals(result));\n-}\n-\n-TEST(TestFixedSizeBinaryDictionaryBuilder, Basic) {\n-  // Build the dictionary Array\n-  DictionaryBuilder<FixedSizeBinaryType> builder(arrow::fixed_size_binary(4),\n-                                                 default_memory_pool());\n-  std::vector<uint8_t> test{12, 12, 11, 12};\n-  std::vector<uint8_t> test2{12, 12, 11, 11};\n-  ASSERT_OK(builder.Append(test.data()));\n-  ASSERT_OK(builder.Append(test2.data()));\n-  ASSERT_OK(builder.Append(test.data()));\n-\n-  std::shared_ptr<Array> result;\n-  ASSERT_OK(builder.Finish(&result));\n-\n-  // Build expected data\n-  FixedSizeBinaryBuilder fsb_builder(arrow::fixed_size_binary(4));\n-  ASSERT_OK(fsb_builder.Append(test.data()));\n-  ASSERT_OK(fsb_builder.Append(test2.data()));\n-  std::shared_ptr<Array> fsb_array;\n-  ASSERT_OK(fsb_builder.Finish(&fsb_array));\n-  auto dtype = std::make_shared<DictionaryType>(int8(), fsb_array);\n-\n-  Int8Builder int_builder;\n-  ASSERT_OK(int_builder.Append(0));\n-  ASSERT_OK(int_builder.Append(1));\n-  ASSERT_OK(int_builder.Append(0));\n-  std::shared_ptr<Array> int_array;\n-  ASSERT_OK(int_builder.Finish(&int_array));\n-\n-  DictionaryArray expected(dtype, int_array);\n-  ASSERT_TRUE(expected.Equals(result));\n-}\n-\n-TEST(TestFixedSizeBinaryDictionaryBuilder, DoubleTableSize) {\n-  // Build the dictionary Array\n-  DictionaryBuilder<FixedSizeBinaryType> builder(arrow::fixed_size_binary(4),\n-                                                 default_memory_pool());\n-  // Build expected data\n-  FixedSizeBinaryBuilder fsb_builder(arrow::fixed_size_binary(4));\n-  Int16Builder int_builder;\n-\n-  // Fill with 1024 different values\n-  for (int64_t i = 0; i < 1024; i++) {\n-    std::vector<uint8_t> value{12, 12, static_cast<uint8_t>(i / 128),\n-                               static_cast<uint8_t>(i % 128)};\n-    ASSERT_OK(builder.Append(value.data()));\n-    ASSERT_OK(fsb_builder.Append(value.data()));\n-    ASSERT_OK(int_builder.Append(static_cast<uint16_t>(i)));\n-  }\n-  // Fill with an already existing value\n-  std::vector<uint8_t> known_value{12, 12, 0, 1};\n-  for (int64_t i = 0; i < 1024; i++) {\n-    ASSERT_OK(builder.Append(known_value.data()));\n-    ASSERT_OK(int_builder.Append(1));\n-  }\n-\n-  // Finalize result\n-  std::shared_ptr<Array> result;\n-  ASSERT_OK(builder.Finish(&result));\n-\n-  // Finalize expected data\n-  std::shared_ptr<Array> fsb_array;\n-  ASSERT_OK(fsb_builder.Finish(&fsb_array));\n-  auto dtype = std::make_shared<DictionaryType>(int16(), fsb_array);\n-  std::shared_ptr<Array> int_array;\n-  ASSERT_OK(int_builder.Finish(&int_array));\n-\n-  DictionaryArray expected(dtype, int_array);\n-  ASSERT_TRUE(expected.Equals(result));\n-}\n-\n-TEST(TestFixedSizeBinaryDictionaryBuilder, InvalidTypeAppend) {\n-  // Build the dictionary Array\n-  DictionaryBuilder<FixedSizeBinaryType> builder(arrow::fixed_size_binary(4),\n-                                                 default_memory_pool());\n-  // Build an array with different byte width\n-  FixedSizeBinaryBuilder fsb_builder(arrow::fixed_size_binary(5));\n-  std::vector<uint8_t> value{100, 1, 1, 1, 1};\n-  ASSERT_OK(fsb_builder.Append(value.data()));\n-  std::shared_ptr<Array> fsb_array;\n-  ASSERT_OK(fsb_builder.Finish(&fsb_array));\n-\n-  ASSERT_RAISES(Invalid, builder.AppendArray(*fsb_array));\n-}\n-\n-TEST(TestDecimalDictionaryBuilder, Basic) {\n-  // Build the dictionary Array\n-  const auto& decimal_type = arrow::decimal(2, 0);\n-  DictionaryBuilder<FixedSizeBinaryType> builder(decimal_type, default_memory_pool());\n-\n-  // Test data\n-  std::vector<Decimal128> test{12, 12, 11, 12};\n-  for (const auto& value : test) {\n-    ASSERT_OK(builder.Append(value.ToBytes().data()));\n-  }\n-\n-  std::shared_ptr<Array> result;\n-  ASSERT_OK(builder.Finish(&result));\n-\n-  // Build expected data\n-  FixedSizeBinaryBuilder decimal_builder(decimal_type);\n-  ASSERT_OK(decimal_builder.Append(Decimal128(12).ToBytes()));\n-  ASSERT_OK(decimal_builder.Append(Decimal128(11).ToBytes()));\n-\n-  std::shared_ptr<Array> decimal_array;\n-  ASSERT_OK(decimal_builder.Finish(&decimal_array));\n-  auto dtype = arrow::dictionary(int8(), decimal_array);\n-\n-  Int8Builder int_builder;\n-  ASSERT_OK(int_builder.Append({0, 0, 1, 0}));\n-  std::shared_ptr<Array> int_array;\n-  ASSERT_OK(int_builder.Finish(&int_array));\n-\n-  DictionaryArray expected(dtype, int_array);\n-  ASSERT_TRUE(expected.Equals(result));\n-}\n-\n-TEST(TestDecimalDictionaryBuilder, DoubleTableSize) {\n-  const auto& decimal_type = arrow::decimal(21, 0);\n-\n-  // Build the dictionary Array\n-  DictionaryBuilder<FixedSizeBinaryType> builder(decimal_type, default_memory_pool());\n-\n-  // Build expected data\n-  FixedSizeBinaryBuilder fsb_builder(decimal_type);\n-  Int16Builder int_builder;\n-\n-  // Fill with 1024 different values\n-  for (int64_t i = 0; i < 1024; i++) {\n-    const uint8_t bytes[] = {0,\n-                             0,\n-                             0,\n-                             0,\n-                             0,\n-                             0,\n-                             0,\n-                             0,\n-                             0,\n-                             0,\n-                             0,\n-                             0,\n-                             12,\n-                             12,\n-                             static_cast<uint8_t>(i / 128),\n-                             static_cast<uint8_t>(i % 128)};\n-    ASSERT_OK(builder.Append(bytes));\n-    ASSERT_OK(fsb_builder.Append(bytes));\n-    ASSERT_OK(int_builder.Append(static_cast<uint16_t>(i)));\n-  }\n-  // Fill with an already existing value\n-  const uint8_t known_value[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 0, 1};\n-  for (int64_t i = 0; i < 1024; i++) {\n-    ASSERT_OK(builder.Append(known_value));\n-    ASSERT_OK(int_builder.Append(1));\n-  }\n-\n-  // Finalize result\n-  std::shared_ptr<Array> result;\n-  ASSERT_OK(builder.Finish(&result));\n-\n-  // Finalize expected data\n-  std::shared_ptr<Array> fsb_array;\n-  ASSERT_OK(fsb_builder.Finish(&fsb_array));\n-\n-  auto dtype = std::make_shared<DictionaryType>(int16(), fsb_array);\n-  std::shared_ptr<Array> int_array;\n-  ASSERT_OK(int_builder.Finish(&int_array));\n-\n-  DictionaryArray expected(dtype, int_array);\n-  ASSERT_TRUE(expected.Equals(result));\n-}\n-\n // ----------------------------------------------------------------------\n // List tests\n \n@@ -2766,9 +2415,8 @@ class DecimalTest : public ::testing::TestWithParam<int> {\n   template <size_t BYTE_WIDTH = 16>\n   void TestCreate(int32_t precision, const DecimalVector& draw,\n                   const std::vector<uint8_t>& valid_bytes, int64_t offset) const {\n-    auto type = std::make_shared<DecimalType>(precision, 4);\n-\n-    auto builder = std::make_shared<DecimalBuilder>(type);\n+    auto type = std::make_shared<Decimal128Type>(precision, 4);\n+    auto builder = std::make_shared<Decimal128Builder>(type);\n \n     size_t null_count = 0;\n \ndiff --git a/cpp/src/arrow/array.cc b/cpp/src/arrow/array.cc\nindex f2dd75335..4ceb071ac 100644\n--- a/cpp/src/arrow/array.cc\n+++ b/cpp/src/arrow/array.cc\n@@ -92,7 +92,7 @@ static inline std::shared_ptr<ArrayData> SliceData(const ArrayData& data, int64_\n   auto new_data = data.ShallowCopy();\n   new_data->length = length;\n   new_data->offset = offset;\n-  new_data->null_count = kUnknownNullCount;\n+  new_data->null_count = data.null_count != 0 ? kUnknownNullCount : 0;\n   return new_data;\n }\n \ndiff --git a/cpp/src/arrow/buffer.h b/cpp/src/arrow/buffer.h\nindex 7c5f6174f..450a4c78b 100644\n--- a/cpp/src/arrow/buffer.h\n+++ b/cpp/src/arrow/buffer.h\n@@ -340,12 +340,13 @@ Status AllocateResizableBuffer(MemoryPool* pool, const int64_t size,\n #ifndef ARROW_NO_DEPRECATED_API\n \n /// \\brief Create Buffer referencing std::string memory\n-/// \\note Deprecated since 0.8.0\n ///\n /// Warning: string instance must stay alive\n ///\n /// \\param str std::string instance\n /// \\return std::shared_ptr<Buffer>\n+///\n+/// \\note Deprecated Since 0.8.0\n static inline std::shared_ptr<Buffer> GetBufferFromString(const std::string& str) {\n   return std::make_shared<Buffer>(str);\n }\ndiff --git a/cpp/src/arrow/builder-benchmark.cc b/cpp/src/arrow/builder-benchmark.cc\nindex 7ac7fe3be..12dfbe817 100644\n--- a/cpp/src/arrow/builder-benchmark.cc\n+++ b/cpp/src/arrow/builder-benchmark.cc\n@@ -115,47 +115,6 @@ static void BM_BuildAdaptiveUIntNoNulls(\n   state.SetBytesProcessed(state.iterations() * data.size() * sizeof(int64_t));\n }\n \n-static void BM_BuildDictionary(benchmark::State& state) {  // NOLINT non-const reference\n-  const int64_t iterations = 1024;\n-  while (state.KeepRunning()) {\n-    DictionaryBuilder<Int64Type> builder(default_memory_pool());\n-    for (int64_t i = 0; i < iterations; i++) {\n-      for (int64_t j = 0; j < i; j++) {\n-        ABORT_NOT_OK(builder.Append(j));\n-      }\n-    }\n-    std::shared_ptr<Array> out;\n-    ABORT_NOT_OK(builder.Finish(&out));\n-  }\n-  state.SetBytesProcessed(state.iterations() * iterations * (iterations + 1) / 2 *\n-                          sizeof(int64_t));\n-}\n-\n-static void BM_BuildStringDictionary(\n-    benchmark::State& state) {  // NOLINT non-const reference\n-  const int64_t iterations = 1024;\n-  // Pre-render strings\n-  std::vector<std::string> data;\n-  for (int64_t i = 0; i < iterations; i++) {\n-    std::stringstream ss;\n-    ss << i;\n-    data.push_back(ss.str());\n-  }\n-  while (state.KeepRunning()) {\n-    StringDictionaryBuilder builder(default_memory_pool());\n-    for (int64_t i = 0; i < iterations; i++) {\n-      for (int64_t j = 0; j < i; j++) {\n-        ABORT_NOT_OK(builder.Append(data[j]));\n-      }\n-    }\n-    std::shared_ptr<Array> out;\n-    ABORT_NOT_OK(builder.Finish(&out));\n-  }\n-  // Assuming a string here needs on average 2 bytes\n-  state.SetBytesProcessed(state.iterations() * iterations * (iterations + 1) / 2 *\n-                          sizeof(int32_t));\n-}\n-\n static void BM_BuildBinaryArray(benchmark::State& state) {  // NOLINT non-const reference\n   const int64_t iterations = 1 << 20;\n \n@@ -179,8 +138,6 @@ BENCHMARK(BM_BuildAdaptiveIntNoNullsScalarAppend)\n     ->Repetitions(3)\n     ->Unit(benchmark::kMicrosecond);\n BENCHMARK(BM_BuildAdaptiveUIntNoNulls)->Repetitions(3)->Unit(benchmark::kMicrosecond);\n-BENCHMARK(BM_BuildDictionary)->Repetitions(3)->Unit(benchmark::kMicrosecond);\n-BENCHMARK(BM_BuildStringDictionary)->Repetitions(3)->Unit(benchmark::kMicrosecond);\n \n BENCHMARK(BM_BuildBinaryArray)->Repetitions(3)->Unit(benchmark::kMicrosecond);\n \ndiff --git a/cpp/src/arrow/builder.cc b/cpp/src/arrow/builder.cc\nindex d2d3dbdf7..3e213fcd5 100644\n--- a/cpp/src/arrow/builder.cc\n+++ b/cpp/src/arrow/builder.cc\n@@ -40,7 +40,6 @@\n namespace arrow {\n \n using internal::AdaptiveIntBuilderBase;\n-using internal::WrappedBinary;\n \n Status ArrayBuilder::AppendToBitmap(bool is_valid) {\n   if (length_ == capacity_) {\n@@ -814,338 +813,6 @@ Status BooleanBuilder::Append(const std::vector<bool>& values) {\n   return Status::OK();\n }\n \n-// ----------------------------------------------------------------------\n-// DictionaryBuilder\n-\n-template <typename T>\n-DictionaryBuilder<T>::DictionaryBuilder(const std::shared_ptr<DataType>& type,\n-                                        MemoryPool* pool)\n-    : ArrayBuilder(type, pool),\n-      hash_table_(new PoolBuffer(pool)),\n-      hash_slots_(nullptr),\n-      dict_builder_(type, pool),\n-      values_builder_(pool),\n-      byte_width_(-1) {\n-  if (!::arrow::CpuInfo::initialized()) {\n-    ::arrow::CpuInfo::Init();\n-  }\n-}\n-\n-DictionaryBuilder<NullType>::DictionaryBuilder(const std::shared_ptr<DataType>& type,\n-                                               MemoryPool* pool)\n-    : ArrayBuilder(type, pool), values_builder_(pool) {\n-  if (!::arrow::CpuInfo::initialized()) {\n-    ::arrow::CpuInfo::Init();\n-  }\n-}\n-\n-DictionaryBuilder<NullType>::~DictionaryBuilder() {}\n-\n-template <>\n-DictionaryBuilder<FixedSizeBinaryType>::DictionaryBuilder(\n-    const std::shared_ptr<DataType>& type, MemoryPool* pool)\n-    : ArrayBuilder(type, pool),\n-      hash_table_(new PoolBuffer(pool)),\n-      hash_slots_(nullptr),\n-      dict_builder_(type, pool),\n-      values_builder_(pool),\n-      byte_width_(static_cast<const FixedSizeBinaryType&>(*type).byte_width()) {\n-  if (!::arrow::CpuInfo::initialized()) {\n-    ::arrow::CpuInfo::Init();\n-  }\n-}\n-\n-template <typename T>\n-Status DictionaryBuilder<T>::Init(int64_t elements) {\n-  RETURN_NOT_OK(ArrayBuilder::Init(elements));\n-\n-  // Fill the initial hash table\n-  RETURN_NOT_OK(hash_table_->Resize(sizeof(hash_slot_t) * kInitialHashTableSize));\n-  hash_slots_ = reinterpret_cast<int32_t*>(hash_table_->mutable_data());\n-  std::fill(hash_slots_, hash_slots_ + kInitialHashTableSize, kHashSlotEmpty);\n-  hash_table_size_ = kInitialHashTableSize;\n-  mod_bitmask_ = kInitialHashTableSize - 1;\n-\n-  return values_builder_.Init(elements);\n-}\n-\n-Status DictionaryBuilder<NullType>::Init(int64_t elements) {\n-  RETURN_NOT_OK(ArrayBuilder::Init(elements));\n-  return values_builder_.Init(elements);\n-}\n-\n-template <typename T>\n-Status DictionaryBuilder<T>::Resize(int64_t capacity) {\n-  if (capacity < kMinBuilderCapacity) {\n-    capacity = kMinBuilderCapacity;\n-  }\n-\n-  if (capacity_ == 0) {\n-    return Init(capacity);\n-  } else {\n-    return ArrayBuilder::Resize(capacity);\n-  }\n-}\n-\n-Status DictionaryBuilder<NullType>::Resize(int64_t capacity) {\n-  if (capacity < kMinBuilderCapacity) {\n-    capacity = kMinBuilderCapacity;\n-  }\n-\n-  if (capacity_ == 0) {\n-    return Init(capacity);\n-  } else {\n-    return ArrayBuilder::Resize(capacity);\n-  }\n-}\n-\n-template <typename T>\n-Status DictionaryBuilder<T>::FinishInternal(std::shared_ptr<ArrayData>* out) {\n-  std::shared_ptr<Array> dictionary;\n-  RETURN_NOT_OK(dict_builder_.Finish(&dictionary));\n-\n-  RETURN_NOT_OK(values_builder_.FinishInternal(out));\n-  (*out)->type = std::make_shared<DictionaryType>((*out)->type, dictionary);\n-  return Status::OK();\n-}\n-\n-Status DictionaryBuilder<NullType>::FinishInternal(std::shared_ptr<ArrayData>* out) {\n-  std::shared_ptr<Array> dictionary = std::make_shared<NullArray>(0);\n-\n-  RETURN_NOT_OK(values_builder_.FinishInternal(out));\n-  (*out)->type = std::make_shared<DictionaryType>((*out)->type, dictionary);\n-  return Status::OK();\n-}\n-\n-template <typename T>\n-Status DictionaryBuilder<T>::Append(const Scalar& value) {\n-  RETURN_NOT_OK(Reserve(1));\n-  // Based on DictEncoder<DType>::Put\n-  int j = HashValue(value) & mod_bitmask_;\n-  hash_slot_t index = hash_slots_[j];\n-\n-  // Find an empty slot\n-  while (kHashSlotEmpty != index && SlotDifferent(index, value)) {\n-    // Linear probing\n-    ++j;\n-    if (j == hash_table_size_) {\n-      j = 0;\n-    }\n-    index = hash_slots_[j];\n-  }\n-\n-  if (index == kHashSlotEmpty) {\n-    // Not in the hash table, so we insert it now\n-    index = static_cast<hash_slot_t>(dict_builder_.length());\n-    hash_slots_[j] = index;\n-    RETURN_NOT_OK(AppendDictionary(value));\n-\n-    if (ARROW_PREDICT_FALSE(static_cast<int32_t>(dict_builder_.length()) >\n-                            hash_table_size_ * kMaxHashTableLoad)) {\n-      RETURN_NOT_OK(DoubleTableSize());\n-    }\n-  }\n-\n-  RETURN_NOT_OK(values_builder_.Append(index));\n-\n-  return Status::OK();\n-}\n-\n-template <typename T>\n-Status DictionaryBuilder<T>::AppendArray(const Array& array) {\n-  const auto& numeric_array = static_cast<const NumericArray<T>&>(array);\n-  for (int64_t i = 0; i < array.length(); i++) {\n-    if (array.IsNull(i)) {\n-      RETURN_NOT_OK(AppendNull());\n-    } else {\n-      RETURN_NOT_OK(Append(numeric_array.Value(i)));\n-    }\n-  }\n-  return Status::OK();\n-}\n-\n-Status DictionaryBuilder<NullType>::AppendArray(const Array& array) {\n-  for (int64_t i = 0; i < array.length(); i++) {\n-    RETURN_NOT_OK(AppendNull());\n-  }\n-  return Status::OK();\n-}\n-\n-template <>\n-Status DictionaryBuilder<FixedSizeBinaryType>::AppendArray(const Array& array) {\n-  if (!type_->Equals(*array.type())) {\n-    return Status::Invalid(\"Cannot append FixedSizeBinary array with non-matching type\");\n-  }\n-\n-  const auto& numeric_array = static_cast<const FixedSizeBinaryArray&>(array);\n-  for (int64_t i = 0; i < array.length(); i++) {\n-    if (array.IsNull(i)) {\n-      RETURN_NOT_OK(AppendNull());\n-    } else {\n-      RETURN_NOT_OK(Append(numeric_array.Value(i)));\n-    }\n-  }\n-  return Status::OK();\n-}\n-\n-template <typename T>\n-Status DictionaryBuilder<T>::AppendNull() {\n-  return values_builder_.AppendNull();\n-}\n-\n-Status DictionaryBuilder<NullType>::AppendNull() { return values_builder_.AppendNull(); }\n-\n-template <typename T>\n-Status DictionaryBuilder<T>::DoubleTableSize() {\n-  int new_size = hash_table_size_ * 2;\n-  auto new_hash_table = std::make_shared<PoolBuffer>(pool_);\n-\n-  RETURN_NOT_OK(new_hash_table->Resize(sizeof(hash_slot_t) * new_size));\n-  int32_t* new_hash_slots = reinterpret_cast<int32_t*>(new_hash_table->mutable_data());\n-  std::fill(new_hash_slots, new_hash_slots + new_size, kHashSlotEmpty);\n-  int new_mod_bitmask = new_size - 1;\n-\n-  for (int i = 0; i < hash_table_size_; ++i) {\n-    hash_slot_t index = hash_slots_[i];\n-\n-    if (index == kHashSlotEmpty) {\n-      continue;\n-    }\n-\n-    // Compute the hash value mod the new table size to start looking for an\n-    // empty slot\n-    Scalar value = GetDictionaryValue(static_cast<int64_t>(index));\n-\n-    // Find an empty slot in the new hash table\n-    int j = HashValue(value) & new_mod_bitmask;\n-    hash_slot_t slot = new_hash_slots[j];\n-\n-    while (kHashSlotEmpty != slot && SlotDifferent(slot, value)) {\n-      ++j;\n-      if (j == new_size) {\n-        j = 0;\n-      }\n-      slot = new_hash_slots[j];\n-    }\n-\n-    // Copy the old slot index to the new hash table\n-    new_hash_slots[j] = index;\n-  }\n-\n-  hash_table_ = new_hash_table;\n-  hash_slots_ = reinterpret_cast<int32_t*>(hash_table_->mutable_data());\n-  hash_table_size_ = new_size;\n-  mod_bitmask_ = new_size - 1;\n-\n-  return Status::OK();\n-}\n-\n-template <typename T>\n-typename DictionaryBuilder<T>::Scalar DictionaryBuilder<T>::GetDictionaryValue(\n-    int64_t index) {\n-  const Scalar* data = reinterpret_cast<const Scalar*>(dict_builder_.data()->data());\n-  return data[index];\n-}\n-\n-template <>\n-const uint8_t* DictionaryBuilder<FixedSizeBinaryType>::GetDictionaryValue(int64_t index) {\n-  return dict_builder_.GetValue(index);\n-}\n-\n-template <typename T>\n-int DictionaryBuilder<T>::HashValue(const Scalar& value) {\n-  return HashUtil::Hash(&value, sizeof(Scalar), 0);\n-}\n-\n-template <>\n-int DictionaryBuilder<FixedSizeBinaryType>::HashValue(const Scalar& value) {\n-  return HashUtil::Hash(value, byte_width_, 0);\n-}\n-\n-template <typename T>\n-bool DictionaryBuilder<T>::SlotDifferent(hash_slot_t index, const Scalar& value) {\n-  const Scalar other = GetDictionaryValue(static_cast<int64_t>(index));\n-  return other != value;\n-}\n-\n-template <>\n-bool DictionaryBuilder<FixedSizeBinaryType>::SlotDifferent(hash_slot_t index,\n-                                                           const Scalar& value) {\n-  int32_t width = static_cast<const FixedSizeBinaryType&>(*type_).byte_width();\n-  const Scalar other = GetDictionaryValue(static_cast<int64_t>(index));\n-  return memcmp(other, value, width) != 0;\n-}\n-\n-template <typename T>\n-Status DictionaryBuilder<T>::AppendDictionary(const Scalar& value) {\n-  return dict_builder_.Append(value);\n-}\n-\n-#define BINARY_DICTIONARY_SPECIALIZATIONS(Type)                                     \\\n-  template <>                                                                       \\\n-  WrappedBinary DictionaryBuilder<Type>::GetDictionaryValue(int64_t index) {        \\\n-    int32_t v_len;                                                                  \\\n-    const uint8_t* v = dict_builder_.GetValue(static_cast<int64_t>(index), &v_len); \\\n-    return WrappedBinary(v, v_len);                                                 \\\n-  }                                                                                 \\\n-                                                                                    \\\n-  template <>                                                                       \\\n-  Status DictionaryBuilder<Type>::AppendDictionary(const WrappedBinary& value) {    \\\n-    return dict_builder_.Append(value.ptr_, value.length_);                         \\\n-  }                                                                                 \\\n-                                                                                    \\\n-  template <>                                                                       \\\n-  Status DictionaryBuilder<Type>::AppendArray(const Array& array) {                 \\\n-    const BinaryArray& binary_array = static_cast<const BinaryArray&>(array);       \\\n-    WrappedBinary value(nullptr, 0);                                                \\\n-    for (int64_t i = 0; i < array.length(); i++) {                                  \\\n-      if (array.IsNull(i)) {                                                        \\\n-        RETURN_NOT_OK(AppendNull());                                                \\\n-      } else {                                                                      \\\n-        value.ptr_ = binary_array.GetValue(i, &value.length_);                      \\\n-        RETURN_NOT_OK(Append(value));                                               \\\n-      }                                                                             \\\n-    }                                                                               \\\n-    return Status::OK();                                                            \\\n-  }                                                                                 \\\n-                                                                                    \\\n-  template <>                                                                       \\\n-  int DictionaryBuilder<Type>::HashValue(const WrappedBinary& value) {              \\\n-    return HashUtil::Hash(value.ptr_, value.length_, 0);                            \\\n-  }                                                                                 \\\n-                                                                                    \\\n-  template <>                                                                       \\\n-  bool DictionaryBuilder<Type>::SlotDifferent(hash_slot_t index,                    \\\n-                                              const WrappedBinary& value) {         \\\n-    int32_t other_length;                                                           \\\n-    const uint8_t* other_value =                                                    \\\n-        dict_builder_.GetValue(static_cast<int64_t>(index), &other_length);         \\\n-    return !(other_length == value.length_ &&                                       \\\n-             0 == memcmp(other_value, value.ptr_, value.length_));                  \\\n-  }\n-\n-BINARY_DICTIONARY_SPECIALIZATIONS(StringType);\n-BINARY_DICTIONARY_SPECIALIZATIONS(BinaryType);\n-\n-template class DictionaryBuilder<UInt8Type>;\n-template class DictionaryBuilder<UInt16Type>;\n-template class DictionaryBuilder<UInt32Type>;\n-template class DictionaryBuilder<UInt64Type>;\n-template class DictionaryBuilder<Int8Type>;\n-template class DictionaryBuilder<Int16Type>;\n-template class DictionaryBuilder<Int32Type>;\n-template class DictionaryBuilder<Int64Type>;\n-template class DictionaryBuilder<Date32Type>;\n-template class DictionaryBuilder<Date64Type>;\n-template class DictionaryBuilder<Time32Type>;\n-template class DictionaryBuilder<Time64Type>;\n-template class DictionaryBuilder<TimestampType>;\n-template class DictionaryBuilder<FloatType>;\n-template class DictionaryBuilder<DoubleType>;\n-template class DictionaryBuilder<FixedSizeBinaryType>;\n-template class DictionaryBuilder<BinaryType>;\n-template class DictionaryBuilder<StringType>;\n-\n // ----------------------------------------------------------------------\n // Decimal128Builder\n \n@@ -1446,7 +1113,7 @@ Status MakeBuilder(MemoryPool* pool, const std::shared_ptr<DataType>& type,\n       BUILDER_CASE(STRING, StringBuilder);\n       BUILDER_CASE(BINARY, BinaryBuilder);\n       BUILDER_CASE(FIXED_SIZE_BINARY, FixedSizeBinaryBuilder);\n-      BUILDER_CASE(DECIMAL, DecimalBuilder);\n+      BUILDER_CASE(DECIMAL, Decimal128Builder);\n     case Type::LIST: {\n       std::unique_ptr<ArrayBuilder> value_builder;\n       std::shared_ptr<DataType> value_type =\n@@ -1477,125 +1144,4 @@ Status MakeBuilder(MemoryPool* pool, const std::shared_ptr<DataType>& type,\n   }\n }\n \n-#define DICTIONARY_BUILDER_CASE(ENUM, BuilderType) \\\n-  case Type::ENUM:                                 \\\n-    out->reset(new BuilderType(type, pool));       \\\n-    return Status::OK();\n-\n-Status MakeDictionaryBuilder(MemoryPool* pool, const std::shared_ptr<DataType>& type,\n-                             std::shared_ptr<ArrayBuilder>* out) {\n-  switch (type->id()) {\n-    DICTIONARY_BUILDER_CASE(NA, DictionaryBuilder<NullType>);\n-    DICTIONARY_BUILDER_CASE(UINT8, DictionaryBuilder<UInt8Type>);\n-    DICTIONARY_BUILDER_CASE(INT8, DictionaryBuilder<Int8Type>);\n-    DICTIONARY_BUILDER_CASE(UINT16, DictionaryBuilder<UInt16Type>);\n-    DICTIONARY_BUILDER_CASE(INT16, DictionaryBuilder<Int16Type>);\n-    DICTIONARY_BUILDER_CASE(UINT32, DictionaryBuilder<UInt32Type>);\n-    DICTIONARY_BUILDER_CASE(INT32, DictionaryBuilder<Int32Type>);\n-    DICTIONARY_BUILDER_CASE(UINT64, DictionaryBuilder<UInt64Type>);\n-    DICTIONARY_BUILDER_CASE(INT64, DictionaryBuilder<Int64Type>);\n-    DICTIONARY_BUILDER_CASE(DATE32, DictionaryBuilder<Date32Type>);\n-    DICTIONARY_BUILDER_CASE(DATE64, DictionaryBuilder<Date64Type>);\n-    DICTIONARY_BUILDER_CASE(TIME32, DictionaryBuilder<Time32Type>);\n-    DICTIONARY_BUILDER_CASE(TIME64, DictionaryBuilder<Time64Type>);\n-    DICTIONARY_BUILDER_CASE(TIMESTAMP, DictionaryBuilder<TimestampType>);\n-    DICTIONARY_BUILDER_CASE(FLOAT, DictionaryBuilder<FloatType>);\n-    DICTIONARY_BUILDER_CASE(DOUBLE, DictionaryBuilder<DoubleType>);\n-    DICTIONARY_BUILDER_CASE(STRING, StringDictionaryBuilder);\n-    DICTIONARY_BUILDER_CASE(BINARY, BinaryDictionaryBuilder);\n-    DICTIONARY_BUILDER_CASE(FIXED_SIZE_BINARY, DictionaryBuilder<FixedSizeBinaryType>);\n-    DICTIONARY_BUILDER_CASE(DECIMAL, DictionaryBuilder<FixedSizeBinaryType>);\n-    default:\n-      return Status::NotImplemented(type->ToString());\n-  }\n-}\n-\n-#define DICTIONARY_ARRAY_CASE(ENUM, BuilderType)                           \\\n-  case Type::ENUM:                                                         \\\n-    builder = std::make_shared<BuilderType>(type, pool);                   \\\n-    RETURN_NOT_OK(static_cast<BuilderType&>(*builder).AppendArray(input)); \\\n-    RETURN_NOT_OK(builder->Finish(out));                                   \\\n-    return Status::OK();\n-\n-Status EncodeArrayToDictionary(const Array& input, MemoryPool* pool,\n-                               std::shared_ptr<Array>* out) {\n-  const std::shared_ptr<DataType>& type = input.data()->type;\n-  std::shared_ptr<ArrayBuilder> builder;\n-  switch (type->id()) {\n-    DICTIONARY_ARRAY_CASE(NA, DictionaryBuilder<NullType>);\n-    DICTIONARY_ARRAY_CASE(UINT8, DictionaryBuilder<UInt8Type>);\n-    DICTIONARY_ARRAY_CASE(INT8, DictionaryBuilder<Int8Type>);\n-    DICTIONARY_ARRAY_CASE(UINT16, DictionaryBuilder<UInt16Type>);\n-    DICTIONARY_ARRAY_CASE(INT16, DictionaryBuilder<Int16Type>);\n-    DICTIONARY_ARRAY_CASE(UINT32, DictionaryBuilder<UInt32Type>);\n-    DICTIONARY_ARRAY_CASE(INT32, DictionaryBuilder<Int32Type>);\n-    DICTIONARY_ARRAY_CASE(UINT64, DictionaryBuilder<UInt64Type>);\n-    DICTIONARY_ARRAY_CASE(INT64, DictionaryBuilder<Int64Type>);\n-    DICTIONARY_ARRAY_CASE(DATE32, DictionaryBuilder<Date32Type>);\n-    DICTIONARY_ARRAY_CASE(DATE64, DictionaryBuilder<Date64Type>);\n-    DICTIONARY_ARRAY_CASE(TIME32, DictionaryBuilder<Time32Type>);\n-    DICTIONARY_ARRAY_CASE(TIME64, DictionaryBuilder<Time64Type>);\n-    DICTIONARY_ARRAY_CASE(TIMESTAMP, DictionaryBuilder<TimestampType>);\n-    DICTIONARY_ARRAY_CASE(FLOAT, DictionaryBuilder<FloatType>);\n-    DICTIONARY_ARRAY_CASE(DOUBLE, DictionaryBuilder<DoubleType>);\n-    DICTIONARY_ARRAY_CASE(STRING, StringDictionaryBuilder);\n-    DICTIONARY_ARRAY_CASE(BINARY, BinaryDictionaryBuilder);\n-    DICTIONARY_ARRAY_CASE(FIXED_SIZE_BINARY, DictionaryBuilder<FixedSizeBinaryType>);\n-    DICTIONARY_ARRAY_CASE(DECIMAL, DictionaryBuilder<FixedSizeBinaryType>);\n-    default:\n-      std::stringstream ss;\n-      ss << \"Cannot encode array of type \" << type->ToString();\n-      ss << \" to dictionary\";\n-      return Status::NotImplemented(ss.str());\n-  }\n-}\n-#define DICTIONARY_COLUMN_CASE(ENUM, BuilderType)                             \\\n-  case Type::ENUM:                                                            \\\n-    builder = std::make_shared<BuilderType>(type, pool);                      \\\n-    chunks = input.data();                                                    \\\n-    for (auto chunk : chunks->chunks()) {                                     \\\n-      RETURN_NOT_OK(static_cast<BuilderType&>(*builder).AppendArray(*chunk)); \\\n-    }                                                                         \\\n-    RETURN_NOT_OK(builder->Finish(&arr));                                     \\\n-    *out = std::make_shared<Column>(input.name(), arr);                       \\\n-    return Status::OK();\n-\n-/// \\brief Encodes a column to a suitable dictionary type\n-/// \\param input Column to be encoded\n-/// \\param pool MemoryPool to allocate the dictionary\n-/// \\param out The new column\n-/// \\return Status\n-Status EncodeColumnToDictionary(const Column& input, MemoryPool* pool,\n-                                std::shared_ptr<Column>* out) {\n-  const std::shared_ptr<DataType>& type = input.type();\n-  std::shared_ptr<ArrayBuilder> builder;\n-  std::shared_ptr<Array> arr;\n-  std::shared_ptr<ChunkedArray> chunks;\n-  switch (type->id()) {\n-    DICTIONARY_COLUMN_CASE(UINT8, DictionaryBuilder<UInt8Type>);\n-    DICTIONARY_COLUMN_CASE(INT8, DictionaryBuilder<Int8Type>);\n-    DICTIONARY_COLUMN_CASE(UINT16, DictionaryBuilder<UInt16Type>);\n-    DICTIONARY_COLUMN_CASE(INT16, DictionaryBuilder<Int16Type>);\n-    DICTIONARY_COLUMN_CASE(UINT32, DictionaryBuilder<UInt32Type>);\n-    DICTIONARY_COLUMN_CASE(INT32, DictionaryBuilder<Int32Type>);\n-    DICTIONARY_COLUMN_CASE(UINT64, DictionaryBuilder<UInt64Type>);\n-    DICTIONARY_COLUMN_CASE(INT64, DictionaryBuilder<Int64Type>);\n-    DICTIONARY_COLUMN_CASE(DATE32, DictionaryBuilder<Date32Type>);\n-    DICTIONARY_COLUMN_CASE(DATE64, DictionaryBuilder<Date64Type>);\n-    DICTIONARY_COLUMN_CASE(TIME32, DictionaryBuilder<Time32Type>);\n-    DICTIONARY_COLUMN_CASE(TIME64, DictionaryBuilder<Time64Type>);\n-    DICTIONARY_COLUMN_CASE(TIMESTAMP, DictionaryBuilder<TimestampType>);\n-    DICTIONARY_COLUMN_CASE(FLOAT, DictionaryBuilder<FloatType>);\n-    DICTIONARY_COLUMN_CASE(DOUBLE, DictionaryBuilder<DoubleType>);\n-    DICTIONARY_COLUMN_CASE(STRING, StringDictionaryBuilder);\n-    DICTIONARY_COLUMN_CASE(BINARY, BinaryDictionaryBuilder);\n-    DICTIONARY_COLUMN_CASE(FIXED_SIZE_BINARY, DictionaryBuilder<FixedSizeBinaryType>);\n-    default:\n-      std::stringstream ss;\n-      ss << \"Cannot encode column of type \" << type->ToString();\n-      ss << \" to dictionary\";\n-      return Status::NotImplemented(ss.str());\n-  }\n-}\n-\n }  // namespace arrow\ndiff --git a/cpp/src/arrow/builder.h b/cpp/src/arrow/builder.h\nindex bc25d0d21..32741b53a 100644\n--- a/cpp/src/arrow/builder.h\n+++ b/cpp/src/arrow/builder.h\n@@ -123,6 +123,18 @@ class ARROW_EXPORT ArrayBuilder {\n \n   std::shared_ptr<DataType> type() const { return type_; }\n \n+  // Unsafe operations (don't check capacity/don't resize)\n+\n+  // Append to null bitmap.\n+  void UnsafeAppendToBitmap(bool is_valid) {\n+    if (is_valid) {\n+      BitUtil::SetBit(null_bitmap_data_, length_);\n+    } else {\n+      ++null_count_;\n+    }\n+    ++length_;\n+  }\n+\n  protected:\n   ArrayBuilder() {}\n \n@@ -143,18 +155,6 @@ class ARROW_EXPORT ArrayBuilder {\n \n   void Reset();\n \n-  // Unsafe operations (don't check capacity/don't resize)\n-\n-  // Append to null bitmap.\n-  void UnsafeAppendToBitmap(bool is_valid) {\n-    if (is_valid) {\n-      BitUtil::SetBit(null_bitmap_data_, length_);\n-    } else {\n-      ++null_count_;\n-    }\n-    ++length_;\n-  }\n-\n   // Vector append. Treat each zero byte as a nullzero. If valid_bytes is null\n   // assume all of length bits are valid.\n   void UnsafeAppendToBitmap(const uint8_t* valid_bytes, int64_t length);\n@@ -810,191 +810,12 @@ class ARROW_EXPORT StructBuilder : public ArrayBuilder {\n   std::vector<std::unique_ptr<ArrayBuilder>> field_builders_;\n };\n \n-// ----------------------------------------------------------------------\n-// Dictionary builder\n-\n-// Based on Apache Parquet-cpp's DictEncoder\n-\n-// Initially 1024 elements\n-static constexpr int kInitialHashTableSize = 1 << 10;\n-\n-typedef int32_t hash_slot_t;\n-static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n-\n-// The maximum load factor for the hash table before resizing.\n-static constexpr double kMaxHashTableLoad = 0.7;\n-\n-namespace internal {\n-\n-// TODO(ARROW-1176): Use Tensorflow's StringPiece instead of this here.\n-struct WrappedBinary {\n-  WrappedBinary(const uint8_t* ptr, int32_t length) : ptr_(ptr), length_(length) {}\n-\n-  const uint8_t* ptr_;\n-  int32_t length_;\n-};\n-\n-template <typename T>\n-struct DictionaryScalar {\n-  using type = typename T::c_type;\n-};\n-\n-template <>\n-struct DictionaryScalar<BinaryType> {\n-  using type = WrappedBinary;\n-};\n-\n-template <>\n-struct DictionaryScalar<StringType> {\n-  using type = WrappedBinary;\n-};\n-\n-template <>\n-struct DictionaryScalar<FixedSizeBinaryType> {\n-  using type = uint8_t const*;\n-};\n-\n-}  // namespace internal\n-\n-/// \\brief Array builder for created encoded DictionaryArray from dense array\n-/// data\n-template <typename T>\n-class ARROW_EXPORT DictionaryBuilder : public ArrayBuilder {\n- public:\n-  using Scalar = typename internal::DictionaryScalar<T>::type;\n-\n-  ~DictionaryBuilder() {}\n-\n-  DictionaryBuilder(const std::shared_ptr<DataType>& type, MemoryPool* pool);\n-\n-  template <typename T1 = T>\n-  explicit DictionaryBuilder(\n-      typename std::enable_if<TypeTraits<T1>::is_parameter_free, MemoryPool*>::type pool)\n-      : DictionaryBuilder<T1>(TypeTraits<T1>::type_singleton(), pool) {}\n-\n-  /// \\brief Append a scalar value\n-  Status Append(const Scalar& value);\n-\n-  /// \\brief Append a scalar null value\n-  Status AppendNull();\n-\n-  /// \\brief Append a whole dense array to the builder\n-  Status AppendArray(const Array& array);\n-\n-  Status Init(int64_t elements) override;\n-  Status Resize(int64_t capacity) override;\n-  Status FinishInternal(std::shared_ptr<ArrayData>* out) override;\n-\n- protected:\n-  Status DoubleTableSize();\n-  Scalar GetDictionaryValue(int64_t index);\n-  int HashValue(const Scalar& value);\n-  bool SlotDifferent(hash_slot_t slot, const Scalar& value);\n-  Status AppendDictionary(const Scalar& value);\n-\n-  std::shared_ptr<PoolBuffer> hash_table_;\n-  int32_t* hash_slots_;\n-\n-  /// Size of the table. Must be a power of 2.\n-  int hash_table_size_;\n-\n-  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n-  // hash_table_size_, but uses far fewer CPU cycles\n-  int mod_bitmask_;\n-\n-  typename TypeTraits<T>::BuilderType dict_builder_;\n-  AdaptiveIntBuilder values_builder_;\n-  int32_t byte_width_;\n-};\n-\n-template <>\n-class ARROW_EXPORT DictionaryBuilder<NullType> : public ArrayBuilder {\n- public:\n-  ~DictionaryBuilder();\n-\n-  DictionaryBuilder(const std::shared_ptr<DataType>& type, MemoryPool* pool);\n-  explicit DictionaryBuilder(MemoryPool* pool);\n-\n-  /// \\brief Append a scalar null value\n-  Status AppendNull();\n-\n-  /// \\brief Append a whole dense array to the builder\n-  Status AppendArray(const Array& array);\n-\n-  Status Init(int64_t elements) override;\n-  Status Resize(int64_t capacity) override;\n-  Status FinishInternal(std::shared_ptr<ArrayData>* out) override;\n-\n- protected:\n-  AdaptiveIntBuilder values_builder_;\n-};\n-\n-class ARROW_EXPORT BinaryDictionaryBuilder : public DictionaryBuilder<BinaryType> {\n- public:\n-  using DictionaryBuilder::Append;\n-  using DictionaryBuilder::DictionaryBuilder;\n-\n-  Status Append(const uint8_t* value, int32_t length) {\n-    return Append(internal::WrappedBinary(value, length));\n-  }\n-\n-  Status Append(const char* value, int32_t length) {\n-    return Append(\n-        internal::WrappedBinary(reinterpret_cast<const uint8_t*>(value), length));\n-  }\n-\n-  Status Append(const std::string& value) {\n-    return Append(internal::WrappedBinary(reinterpret_cast<const uint8_t*>(value.c_str()),\n-                                          static_cast<int32_t>(value.size())));\n-  }\n-};\n-\n-/// \\brief Dictionary array builder with convenience methods for strings\n-class ARROW_EXPORT StringDictionaryBuilder : public DictionaryBuilder<StringType> {\n- public:\n-  using DictionaryBuilder::Append;\n-  using DictionaryBuilder::DictionaryBuilder;\n-\n-  Status Append(const uint8_t* value, int32_t length) {\n-    return Append(internal::WrappedBinary(value, length));\n-  }\n-\n-  Status Append(const char* value, int32_t length) {\n-    return Append(\n-        internal::WrappedBinary(reinterpret_cast<const uint8_t*>(value), length));\n-  }\n-\n-  Status Append(const std::string& value) {\n-    return Append(internal::WrappedBinary(reinterpret_cast<const uint8_t*>(value.c_str()),\n-                                          static_cast<int32_t>(value.size())));\n-  }\n-};\n-\n // ----------------------------------------------------------------------\n // Helper functions\n \n Status ARROW_EXPORT MakeBuilder(MemoryPool* pool, const std::shared_ptr<DataType>& type,\n                                 std::unique_ptr<ArrayBuilder>* out);\n \n-Status ARROW_EXPORT MakeDictionaryBuilder(MemoryPool* pool,\n-                                          const std::shared_ptr<DataType>& type,\n-                                          std::shared_ptr<ArrayBuilder>* out);\n-\n-/// \\brief Convert Array to encoded DictionaryArray form\n-///\n-/// \\param[in] input The Array to be encoded\n-/// \\param[in] pool MemoryPool to allocate memory for the hash table\n-/// \\param[out] out Array encoded to DictionaryArray\n-Status ARROW_EXPORT EncodeArrayToDictionary(const Array& input, MemoryPool* pool,\n-                                            std::shared_ptr<Array>* out);\n-\n-/// \\brief Convert a Column's data internally to DictionaryArray\n-///\n-/// \\param[in] input The ChunkedArray to be encoded\n-/// \\param[in] pool MemoryPool to allocate memory for the hash table\n-/// \\param[out] out Column with data converted to DictionaryArray\n-Status ARROW_EXPORT EncodeColumnToDictionary(const Column& input, MemoryPool* pool,\n-                                             std::shared_ptr<Column>* out);\n }  // namespace arrow\n \n #endif  // ARROW_BUILDER_H_\ndiff --git a/cpp/src/arrow/compute/CMakeLists.txt b/cpp/src/arrow/compute/CMakeLists.txt\nindex 4589afb95..d4369ed27 100644\n--- a/cpp/src/arrow/compute/CMakeLists.txt\n+++ b/cpp/src/arrow/compute/CMakeLists.txt\n@@ -18,7 +18,6 @@\n # Headers: top level\n install(FILES\n   api.h\n-  cast.h\n   context.h\n   kernel.h\n   DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}/arrow/compute\")\n@@ -36,3 +35,6 @@ install(\n #######################################\n \n ADD_ARROW_TEST(compute-test)\n+ADD_ARROW_BENCHMARK(compute-benchmark)\n+\n+add_subdirectory(kernels)\ndiff --git a/cpp/src/arrow/compute/api.h b/cpp/src/arrow/compute/api.h\nindex da7df1cbb..b3700b4c5 100644\n--- a/cpp/src/arrow/compute/api.h\n+++ b/cpp/src/arrow/compute/api.h\n@@ -18,8 +18,10 @@\n #ifndef ARROW_COMPUTE_API_H\n #define ARROW_COMPUTE_API_H\n \n-#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/context.h\"\n #include \"arrow/compute/kernel.h\"\n \n+#include \"arrow/compute/kernels/cast.h\"\n+#include \"arrow/compute/kernels/hash.h\"\n+\n #endif  // ARROW_COMPUTE_API_H\ndiff --git a/cpp/src/arrow/compute/compute-benchmark.cc b/cpp/src/arrow/compute/compute-benchmark.cc\nnew file mode 100644\nindex 000000000..974fffcd6\n--- /dev/null\n+++ b/cpp/src/arrow/compute/compute-benchmark.cc\n@@ -0,0 +1,88 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/memory_pool.h\"\n+#include \"arrow/test-util.h\"\n+\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+static void BM_BuildDictionary(benchmark::State& state) {  // NOLINT non-const reference\n+  const int64_t iterations = 1024;\n+\n+  std::vector<int64_t> values;\n+  std::vector<bool> is_valid;\n+  for (int64_t i = 0; i < iterations; i++) {\n+    for (int64_t j = 0; j < i; j++) {\n+      is_valid.push_back((i + j) % 9 == 0);\n+      values.push_back(j);\n+    }\n+  }\n+\n+  std::shared_ptr<Array> arr;\n+  ArrayFromVector<Int64Type, int64_t>(is_valid, values, &arr);\n+\n+  FunctionContext ctx;\n+\n+  while (state.KeepRunning()) {\n+    Datum out;\n+    ABORT_NOT_OK(DictionaryEncode(&ctx, Datum(arr), &out));\n+  }\n+  state.SetBytesProcessed(state.iterations() * values.size() * sizeof(int64_t));\n+}\n+\n+static void BM_BuildStringDictionary(\n+    benchmark::State& state) {  // NOLINT non-const reference\n+  const int64_t iterations = 1024 * 64;\n+  // Pre-render strings\n+  std::vector<std::string> data;\n+\n+  int64_t total_bytes = 0;\n+  for (int64_t i = 0; i < iterations; i++) {\n+    std::stringstream ss;\n+    ss << i;\n+    auto val = ss.str();\n+    data.push_back(val);\n+    total_bytes += static_cast<int64_t>(val.size());\n+  }\n+\n+  std::shared_ptr<Array> arr;\n+  ArrayFromVector<StringType, std::string>(data, &arr);\n+\n+  FunctionContext ctx;\n+\n+  while (state.KeepRunning()) {\n+    Datum out;\n+    ABORT_NOT_OK(DictionaryEncode(&ctx, Datum(arr), &out));\n+  }\n+  // Assuming a string here needs on average 2 bytes\n+  state.SetBytesProcessed(state.iterations() * total_bytes);\n+}\n+\n+BENCHMARK(BM_BuildDictionary)->Repetitions(3)->Unit(benchmark::kMicrosecond);\n+BENCHMARK(BM_BuildStringDictionary)->Repetitions(3)->Unit(benchmark::kMicrosecond);\n+\n+}  // namespace compute\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/compute/compute-test.cc b/cpp/src/arrow/compute/compute-test.cc\nindex 61d53c4d5..5eada398d 100644\n--- a/cpp/src/arrow/compute/compute-test.cc\n+++ b/cpp/src/arrow/compute/compute-test.cc\n@@ -37,10 +37,12 @@\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n \n-#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/context.h\"\n #include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/cast.h\"\n+#include \"arrow/compute/kernels/hash.h\"\n \n+using std::shared_ptr;\n using std::vector;\n \n namespace arrow {\n@@ -54,6 +56,18 @@ class ComputeFixture {\n   FunctionContext ctx_;\n };\n \n+template <typename Type, typename T>\n+shared_ptr<Array> _MakeArray(const shared_ptr<DataType>& type, const vector<T>& values,\n+                             const vector<bool>& is_valid) {\n+  shared_ptr<Array> result;\n+  if (is_valid.size() > 0) {\n+    ArrayFromVector<Type, T>(type, is_valid, values, &result);\n+  } else {\n+    ArrayFromVector<Type, T>(type, values, &result);\n+  }\n+  return result;\n+}\n+\n // ----------------------------------------------------------------------\n // Cast\n \n@@ -65,17 +79,17 @@ static void AssertBufferSame(const Array& left, const Array& right, int buffer_i\n class TestCast : public ComputeFixture, public TestBase {\n  public:\n   void CheckPass(const Array& input, const Array& expected,\n-                 const std::shared_ptr<DataType>& out_type, const CastOptions& options) {\n-    std::shared_ptr<Array> result;\n+                 const shared_ptr<DataType>& out_type, const CastOptions& options) {\n+    shared_ptr<Array> result;\n     ASSERT_OK(Cast(&ctx_, input, out_type, options, &result));\n     ASSERT_ARRAYS_EQUAL(expected, *result);\n   }\n \n   template <typename InType, typename I_TYPE>\n-  void CheckFails(const std::shared_ptr<DataType>& in_type,\n-                  const std::vector<I_TYPE>& in_values, const std::vector<bool>& is_valid,\n-                  const std::shared_ptr<DataType>& out_type, const CastOptions& options) {\n-    std::shared_ptr<Array> input, result;\n+  void CheckFails(const shared_ptr<DataType>& in_type, const vector<I_TYPE>& in_values,\n+                  const vector<bool>& is_valid, const shared_ptr<DataType>& out_type,\n+                  const CastOptions& options) {\n+    shared_ptr<Array> input, result;\n     if (is_valid.size() > 0) {\n       ArrayFromVector<InType, I_TYPE>(in_type, is_valid, in_values, &input);\n     } else {\n@@ -84,19 +98,18 @@ class TestCast : public ComputeFixture, public TestBase {\n     ASSERT_RAISES(Invalid, Cast(&ctx_, *input, out_type, options, &result));\n   }\n \n-  void CheckZeroCopy(const Array& input, const std::shared_ptr<DataType>& out_type) {\n-    std::shared_ptr<Array> result;\n+  void CheckZeroCopy(const Array& input, const shared_ptr<DataType>& out_type) {\n+    shared_ptr<Array> result;\n     ASSERT_OK(Cast(&ctx_, input, out_type, {}, &result));\n     AssertBufferSame(input, *result, 0);\n     AssertBufferSame(input, *result, 1);\n   }\n \n   template <typename InType, typename I_TYPE, typename OutType, typename O_TYPE>\n-  void CheckCase(const std::shared_ptr<DataType>& in_type,\n-                 const std::vector<I_TYPE>& in_values, const std::vector<bool>& is_valid,\n-                 const std::shared_ptr<DataType>& out_type,\n-                 const std::vector<O_TYPE>& out_values, const CastOptions& options) {\n-    std::shared_ptr<Array> input, expected;\n+  void CheckCase(const shared_ptr<DataType>& in_type, const vector<I_TYPE>& in_values,\n+                 const vector<bool>& is_valid, const shared_ptr<DataType>& out_type,\n+                 const vector<O_TYPE>& out_values, const CastOptions& options) {\n+    shared_ptr<Array> input, expected;\n     if (is_valid.size() > 0) {\n       ArrayFromVector<InType, I_TYPE>(in_type, is_valid, in_values, &input);\n       ArrayFromVector<OutType, O_TYPE>(out_type, is_valid, out_values, &expected);\n@@ -117,10 +130,10 @@ TEST_F(TestCast, SameTypeZeroCopy) {\n   vector<bool> is_valid = {true, false, true, true, true};\n   vector<int32_t> v1 = {0, 1, 2, 3, 4};\n \n-  std::shared_ptr<Array> arr;\n+  shared_ptr<Array> arr;\n   ArrayFromVector<Int32Type, int32_t>(int32(), is_valid, v1, &arr);\n \n-  std::shared_ptr<Array> result;\n+  shared_ptr<Array> result;\n   ASSERT_OK(Cast(&this->ctx_, *arr, int32(), {}, &result));\n \n   AssertBufferSame(*arr, *result, 0);\n@@ -185,7 +198,7 @@ TEST_F(TestCast, OverflowInNullSlot) {\n   vector<int32_t> v11 = {0, 70000, 2000, 1000, 0};\n   vector<int16_t> e11 = {0, 0, 2000, 1000, 0};\n \n-  std::shared_ptr<Array> expected;\n+  shared_ptr<Array> expected;\n   ArrayFromVector<Int16Type, int16_t>(int16(), is_valid, e11, &expected);\n \n   auto buf = std::make_shared<Buffer>(reinterpret_cast<const uint8_t*>(v11.data()),\n@@ -280,8 +293,8 @@ TEST_F(TestCast, TimestampToTimestamp) {\n \n   auto CheckTimestampCast = [this](\n       const CastOptions& options, TimeUnit::type from_unit, TimeUnit::type to_unit,\n-      const std::vector<int64_t>& from_values, const std::vector<int64_t>& to_values,\n-      const std::vector<bool>& is_valid) {\n+      const vector<int64_t>& from_values, const vector<int64_t>& to_values,\n+      const vector<bool>& is_valid) {\n     CheckCase<TimestampType, int64_t, TimestampType, int64_t>(\n         timestamp(from_unit), from_values, is_valid, timestamp(to_unit), to_values,\n         options);\n@@ -315,8 +328,8 @@ TEST_F(TestCast, TimestampToTimestamp) {\n   CheckTimestampCast(options, TimeUnit::MICRO, TimeUnit::NANO, v6, e6, is_valid);\n \n   // Zero copy\n-  std::shared_ptr<Array> arr;\n   vector<int64_t> v7 = {0, 70000, 2000, 1000, 0};\n+  shared_ptr<Array> arr;\n   ArrayFromVector<TimestampType, int64_t>(timestamp(TimeUnit::SECOND), is_valid, v7,\n                                           &arr);\n   CheckZeroCopy(*arr, timestamp(TimeUnit::SECOND));\n@@ -456,8 +469,8 @@ TEST_F(TestCast, TimeToTime) {\n       time64(TimeUnit::MICRO), v6, is_valid, time64(TimeUnit::NANO), e6, options);\n \n   // Zero copy\n-  std::shared_ptr<Array> arr;\n   vector<int64_t> v7 = {0, 70000, 2000, 1000, 0};\n+  shared_ptr<Array> arr;\n   ArrayFromVector<Time64Type, int64_t>(time64(TimeUnit::MICRO), is_valid, v7, &arr);\n   CheckZeroCopy(*arr, time64(TimeUnit::MICRO));\n \n@@ -516,9 +529,9 @@ TEST_F(TestCast, DateToDate) {\n                                                       e1, options);\n \n   // Zero copy\n-  std::shared_ptr<Array> arr;\n   vector<int32_t> v2 = {0, 70000, 2000, 1000, 0};\n   vector<int64_t> v3 = {0, 70000, 2000, 1000, 0};\n+  shared_ptr<Array> arr;\n   ArrayFromVector<Date32Type, int32_t>(date32(), is_valid, v2, &arr);\n   CheckZeroCopy(*arr, date32());\n \n@@ -561,22 +574,54 @@ TEST_F(TestCast, ToDouble) {\n                                                    options);\n }\n \n+TEST_F(TestCast, ChunkedArray) {\n+  vector<int16_t> values1 = {0, 1, 2};\n+  vector<int16_t> values2 = {3, 4, 5};\n+\n+  auto type = int16();\n+  auto out_type = int64();\n+\n+  auto a1 = _MakeArray<Int16Type, int16_t>(type, values1, {});\n+  auto a2 = _MakeArray<Int16Type, int16_t>(type, values2, {});\n+\n+  ArrayVector arrays = {a1, a2};\n+  auto carr = std::make_shared<ChunkedArray>(arrays);\n+\n+  CastOptions options;\n+\n+  Datum out;\n+  ASSERT_OK(Cast(&this->ctx_, Datum(carr), out_type, options, &out));\n+  ASSERT_EQ(Datum::CHUNKED_ARRAY, out.kind());\n+\n+  auto out_carr = out.chunked_array();\n+\n+  vector<int64_t> ex_values1 = {0, 1, 2};\n+  vector<int64_t> ex_values2 = {3, 4, 5};\n+  auto a3 = _MakeArray<Int64Type, int64_t>(out_type, ex_values1, {});\n+  auto a4 = _MakeArray<Int64Type, int64_t>(out_type, ex_values2, {});\n+\n+  ArrayVector ex_arrays = {a3, a4};\n+  auto ex_carr = std::make_shared<ChunkedArray>(ex_arrays);\n+\n+  ASSERT_TRUE(out.chunked_array()->Equals(*ex_carr));\n+}\n+\n TEST_F(TestCast, UnsupportedTarget) {\n   vector<bool> is_valid = {true, false, true, true, true};\n   vector<int32_t> v1 = {0, 1, 2, 3, 4};\n \n-  std::shared_ptr<Array> arr;\n+  shared_ptr<Array> arr;\n   ArrayFromVector<Int32Type, int32_t>(int32(), is_valid, v1, &arr);\n \n-  std::shared_ptr<Array> result;\n+  shared_ptr<Array> result;\n   ASSERT_RAISES(NotImplemented, Cast(&this->ctx_, *arr, utf8(), {}, &result));\n }\n \n TEST_F(TestCast, DateTimeZeroCopy) {\n   vector<bool> is_valid = {true, false, true, true, true};\n \n-  std::shared_ptr<Array> arr;\n   vector<int32_t> v1 = {0, 70000, 2000, 1000, 0};\n+  shared_ptr<Array> arr;\n   ArrayFromVector<Int32Type, int32_t>(int32(), is_valid, v1, &arr);\n \n   CheckZeroCopy(*arr, time32(TimeUnit::SECOND));\n@@ -596,7 +641,7 @@ TEST_F(TestCast, FromNull) {\n \n   NullArray arr(length);\n \n-  std::shared_ptr<Array> result;\n+  shared_ptr<Array> result;\n   ASSERT_OK(Cast(&ctx_, arr, int32(), {}, &result));\n \n   ASSERT_EQ(length, result->length());\n@@ -614,7 +659,7 @@ TEST_F(TestCast, PreallocatedMemory) {\n \n   const int64_t length = 5;\n \n-  std::shared_ptr<Array> arr;\n+  shared_ptr<Array> arr;\n   vector<int32_t> v1 = {0, 70000, 2000, 1000, 0};\n   vector<int64_t> e1 = {0, 70000, 2000, 1000, 0};\n   ArrayFromVector<Int32Type, int32_t>(int32(), is_valid, v1, &arr);\n@@ -626,19 +671,20 @@ TEST_F(TestCast, PreallocatedMemory) {\n \n   auto out_data = std::make_shared<ArrayData>(out_type, length);\n \n-  std::shared_ptr<Buffer> out_values;\n+  shared_ptr<Buffer> out_values;\n   ASSERT_OK(this->ctx_.Allocate(length * sizeof(int64_t), &out_values));\n \n   out_data->buffers.push_back(nullptr);\n   out_data->buffers.push_back(out_values);\n \n-  ASSERT_OK(kernel->Call(&this->ctx_, *arr, out_data.get()));\n+  Datum out(out_data);\n+  ASSERT_OK(kernel->Call(&this->ctx_, *arr->data(), &out));\n \n   // Buffer address unchanged\n   ASSERT_EQ(out_values.get(), out_data->buffers[1].get());\n \n-  std::shared_ptr<Array> result = MakeArray(out_data);\n-  std::shared_ptr<Array> expected;\n+  shared_ptr<Array> result = MakeArray(out_data);\n+  shared_ptr<Array> expected;\n   ArrayFromVector<Int64Type, int64_t>(int64(), is_valid, e1, &expected);\n \n   ASSERT_ARRAYS_EQUAL(*expected, *result);\n@@ -656,13 +702,268 @@ TYPED_TEST_CASE(TestDictionaryCast, TestTypes);\n \n TYPED_TEST(TestDictionaryCast, Basic) {\n   CastOptions options;\n-  std::shared_ptr<Array> plain_array =\n+  shared_ptr<Array> plain_array =\n+      TestBase::MakeRandomArray<typename TypeTraits<TypeParam>::ArrayType>(10, 2);\n+\n+  Datum out;\n+  ASSERT_OK(DictionaryEncode(&this->ctx_, Datum(plain_array->data()), &out));\n+\n+  this->CheckPass(*MakeArray(out.array()), *plain_array, plain_array->type(), options);\n+}\n+\n+/*TYPED_TEST(TestDictionaryCast, Reverse) {\n+  CastOptions options;\n+  shared_ptr<Array> plain_array =\n       TestBase::MakeRandomArray<typename TypeTraits<TypeParam>::ArrayType>(10, 2);\n \n-  std::shared_ptr<Array> dict_array;\n+  shared_ptr<Array> dict_array;\n   ASSERT_OK(EncodeArrayToDictionary(*plain_array, this->pool_, &dict_array));\n \n-  this->CheckPass(*dict_array, *plain_array, plain_array->type(), options);\n+  this->CheckPass(*plain_array, *dict_array, dict_array->type(), options);\n+}*/\n+\n+// ----------------------------------------------------------------------\n+// Dictionary tests\n+\n+template <typename Type, typename T>\n+void CheckUnique(FunctionContext* ctx, const shared_ptr<DataType>& type,\n+                 const vector<T>& in_values, const vector<bool>& in_is_valid,\n+                 const vector<T>& out_values, const vector<bool>& out_is_valid) {\n+  shared_ptr<Array> input = _MakeArray<Type, T>(type, in_values, in_is_valid);\n+  shared_ptr<Array> expected = _MakeArray<Type, T>(type, out_values, out_is_valid);\n+\n+  shared_ptr<Array> result;\n+  ASSERT_OK(Unique(ctx, Datum(input), &result));\n+  ASSERT_ARRAYS_EQUAL(*expected, *result);\n+}\n+\n+template <typename Type, typename T>\n+void CheckDictEncode(FunctionContext* ctx, const shared_ptr<DataType>& type,\n+                     const vector<T>& in_values, const vector<bool>& in_is_valid,\n+                     const vector<T>& out_values, const vector<bool>& out_is_valid,\n+                     const vector<int32_t>& out_indices) {\n+  shared_ptr<Array> input = _MakeArray<Type, T>(type, in_values, in_is_valid);\n+  shared_ptr<Array> ex_dict = _MakeArray<Type, T>(type, out_values, out_is_valid);\n+  shared_ptr<Array> ex_indices =\n+      _MakeArray<Int32Type, int32_t>(int32(), out_indices, in_is_valid);\n+\n+  DictionaryArray expected(dictionary(int32(), ex_dict), ex_indices);\n+\n+  Datum datum_out;\n+  ASSERT_OK(DictionaryEncode(ctx, Datum(input), &datum_out));\n+  shared_ptr<Array> result = MakeArray(datum_out.array());\n+\n+  ASSERT_ARRAYS_EQUAL(expected, *result);\n+}\n+\n+class TestHashKernel : public ComputeFixture, public TestBase {};\n+\n+template <typename Type>\n+class TestHashKernelPrimitive : public ComputeFixture, public TestBase {};\n+\n+typedef ::testing::Types<Int8Type, UInt8Type, Int16Type, UInt16Type, Int32Type,\n+                         UInt32Type, Int64Type, UInt64Type, FloatType, DoubleType,\n+                         Date32Type, Date64Type>\n+    PrimitiveDictionaries;\n+\n+TYPED_TEST_CASE(TestHashKernelPrimitive, PrimitiveDictionaries);\n+\n+TYPED_TEST(TestHashKernelPrimitive, Unique) {\n+  using T = typename TypeParam::c_type;\n+  auto type = TypeTraits<TypeParam>::type_singleton();\n+  CheckUnique<TypeParam, T>(&this->ctx_, type, {2, 1, 2, 1}, {true, false, true, true},\n+                            {2, 1}, {});\n+}\n+\n+TYPED_TEST(TestHashKernelPrimitive, DictEncode) {\n+  using T = typename TypeParam::c_type;\n+  auto type = TypeTraits<TypeParam>::type_singleton();\n+  CheckDictEncode<TypeParam, T>(&this->ctx_, type, {2, 1, 2, 1, 2, 3},\n+                                {true, false, true, true, true, true}, {2, 1, 3}, {},\n+                                {0, 0, 0, 1, 0, 2});\n+}\n+\n+TYPED_TEST(TestHashKernelPrimitive, PrimitiveResizeTable) {\n+  using T = typename TypeParam::c_type;\n+  // Skip this test for (u)int8\n+  if (sizeof(Scalar) == 1) {\n+    return;\n+  }\n+\n+  const int64_t kTotalValues = 10000;\n+  const int64_t kRepeats = 10;\n+\n+  vector<T> values;\n+  vector<T> uniques;\n+  vector<int32_t> indices;\n+  for (int64_t i = 0; i < kTotalValues * kRepeats; i++) {\n+    const auto val = static_cast<T>(i % kTotalValues);\n+    values.push_back(val);\n+\n+    if (i < kTotalValues) {\n+      uniques.push_back(val);\n+    }\n+    indices.push_back(static_cast<int32_t>(i % kTotalValues));\n+  }\n+\n+  auto type = TypeTraits<TypeParam>::type_singleton();\n+  CheckUnique<TypeParam, T>(&this->ctx_, type, values, {}, uniques, {});\n+\n+  CheckDictEncode<TypeParam, T>(&this->ctx_, type, values, {}, uniques, {}, indices);\n+}\n+\n+TEST_F(TestHashKernel, UniqueTimeTimestamp) {\n+  CheckUnique<Time32Type, int32_t>(&this->ctx_, time32(TimeUnit::SECOND), {2, 1, 2, 1},\n+                                   {true, false, true, true}, {2, 1}, {});\n+\n+  CheckUnique<Time64Type, int64_t>(&this->ctx_, time64(TimeUnit::NANO), {2, 1, 2, 1},\n+                                   {true, false, true, true}, {2, 1}, {});\n+\n+  CheckUnique<TimestampType, int64_t>(&this->ctx_, timestamp(TimeUnit::NANO),\n+                                      {2, 1, 2, 1}, {true, false, true, true}, {2, 1},\n+                                      {});\n+}\n+\n+TEST_F(TestHashKernel, UniqueBinary) {\n+  CheckUnique<BinaryType, std::string>(&this->ctx_, binary(),\n+                                       {\"test\", \"\", \"test2\", \"test\"},\n+                                       {true, false, true, true}, {\"test\", \"test2\"}, {});\n+\n+  CheckUnique<StringType, std::string>(&this->ctx_, utf8(), {\"test\", \"\", \"test2\", \"test\"},\n+                                       {true, false, true, true}, {\"test\", \"test2\"}, {});\n+}\n+\n+TEST_F(TestHashKernel, DictEncodeBinary) {\n+  CheckDictEncode<BinaryType, std::string>(\n+      &this->ctx_, binary(), {\"test\", \"\", \"test2\", \"test\", \"baz\"},\n+      {true, false, true, true, true}, {\"test\", \"test2\", \"baz\"}, {}, {0, 0, 1, 0, 2});\n+\n+  CheckDictEncode<StringType, std::string>(\n+      &this->ctx_, utf8(), {\"test\", \"\", \"test2\", \"test\", \"baz\"},\n+      {true, false, true, true, true}, {\"test\", \"test2\", \"baz\"}, {}, {0, 0, 1, 0, 2});\n+}\n+\n+TEST_F(TestHashKernel, BinaryResizeTable) {\n+  const int64_t kTotalValues = 10000;\n+  const int64_t kRepeats = 10;\n+\n+  vector<std::string> values;\n+  vector<std::string> uniques;\n+  vector<int32_t> indices;\n+  for (int64_t i = 0; i < kTotalValues * kRepeats; i++) {\n+    int64_t index = i % kTotalValues;\n+    std::stringstream ss;\n+    ss << \"test\" << index;\n+    std::string val = ss.str();\n+\n+    values.push_back(val);\n+\n+    if (i < kTotalValues) {\n+      uniques.push_back(val);\n+    }\n+    indices.push_back(static_cast<int32_t>(i % kTotalValues));\n+  }\n+\n+  CheckUnique<BinaryType, std::string>(&this->ctx_, binary(), values, {}, uniques, {});\n+  CheckDictEncode<BinaryType, std::string>(&this->ctx_, binary(), values, {}, uniques, {},\n+                                           indices);\n+\n+  CheckUnique<StringType, std::string>(&this->ctx_, utf8(), values, {}, uniques, {});\n+  CheckDictEncode<StringType, std::string>(&this->ctx_, utf8(), values, {}, uniques, {},\n+                                           indices);\n+}\n+\n+TEST_F(TestHashKernel, UniqueFixedSizeBinary) {\n+  CheckUnique<FixedSizeBinaryType, std::string>(\n+      &this->ctx_, fixed_size_binary(5), {\"aaaaa\", \"\", \"bbbbb\", \"aaaaa\"},\n+      {true, false, true, true}, {\"aaaaa\", \"bbbbb\"}, {});\n+}\n+\n+TEST_F(TestHashKernel, DictEncodeFixedSizeBinary) {\n+  CheckDictEncode<FixedSizeBinaryType, std::string>(\n+      &this->ctx_, fixed_size_binary(5), {\"bbbbb\", \"\", \"bbbbb\", \"aaaaa\", \"ccccc\"},\n+      {true, false, true, true, true}, {\"bbbbb\", \"aaaaa\", \"ccccc\"}, {}, {0, 0, 0, 1, 2});\n+}\n+\n+TEST_F(TestHashKernel, FixedSizeBinaryResizeTable) {\n+  const int64_t kTotalValues = 10000;\n+  const int64_t kRepeats = 10;\n+\n+  vector<std::string> values;\n+  vector<std::string> uniques;\n+  vector<int32_t> indices;\n+  for (int64_t i = 0; i < kTotalValues * kRepeats; i++) {\n+    int64_t index = i % kTotalValues;\n+    std::stringstream ss;\n+    ss << \"test\" << static_cast<char>(index / 128) << static_cast<char>(index % 128);\n+    std::string val = ss.str();\n+\n+    values.push_back(val);\n+\n+    if (i < kTotalValues) {\n+      uniques.push_back(val);\n+    }\n+    indices.push_back(static_cast<int32_t>(i % kTotalValues));\n+  }\n+\n+  auto type = fixed_size_binary(6);\n+  CheckUnique<FixedSizeBinaryType, std::string>(&this->ctx_, type, values, {}, uniques,\n+                                                {});\n+  CheckDictEncode<FixedSizeBinaryType, std::string>(&this->ctx_, type, values, {},\n+                                                    uniques, {}, indices);\n+}\n+\n+TEST_F(TestHashKernel, UniqueDecimal) {\n+  vector<Decimal128> values{12, 12, 11, 12};\n+  vector<Decimal128> expected{12, 11};\n+\n+  CheckUnique<Decimal128Type, Decimal128>(&this->ctx_, decimal(2, 0), values,\n+                                          {true, false, true, true}, expected, {});\n+}\n+\n+TEST_F(TestHashKernel, DictEncodeDecimal) {\n+  vector<Decimal128> values{12, 12, 11, 12, 13};\n+  vector<Decimal128> expected{12, 11, 13};\n+\n+  CheckDictEncode<Decimal128Type, Decimal128>(&this->ctx_, decimal(2, 0), values,\n+                                              {true, false, true, true, true}, expected,\n+                                              {}, {0, 0, 1, 0, 2});\n+}\n+\n+TEST_F(TestHashKernel, ChunkedArrayInvoke) {\n+  vector<std::string> values1 = {\"foo\", \"bar\", \"foo\"};\n+  vector<std::string> values2 = {\"bar\", \"baz\", \"quuux\", \"foo\"};\n+\n+  auto type = utf8();\n+  auto a1 = _MakeArray<StringType, std::string>(type, values1, {});\n+  auto a2 = _MakeArray<StringType, std::string>(type, values2, {});\n+\n+  vector<std::string> dict_values = {\"foo\", \"bar\", \"baz\", \"quuux\"};\n+  auto ex_dict = _MakeArray<StringType, std::string>(type, dict_values, {});\n+\n+  ArrayVector arrays = {a1, a2};\n+  auto carr = std::make_shared<ChunkedArray>(arrays);\n+\n+  // Unique\n+  shared_ptr<Array> result;\n+  ASSERT_OK(Unique(&this->ctx_, Datum(carr), &result));\n+  ASSERT_ARRAYS_EQUAL(*ex_dict, *result);\n+\n+  // Dictionary encode\n+  auto dict_type = dictionary(int32(), ex_dict);\n+\n+  auto i1 = _MakeArray<Int32Type, int32_t>(int32(), {0, 1, 0}, {});\n+  auto i2 = _MakeArray<Int32Type, int32_t>(int32(), {1, 2, 3, 0}, {});\n+\n+  ArrayVector dict_arrays = {std::make_shared<DictionaryArray>(dict_type, i1),\n+                             std::make_shared<DictionaryArray>(dict_type, i2)};\n+  auto dict_carr = std::make_shared<ChunkedArray>(dict_arrays);\n+\n+  Datum encoded_out;\n+  ASSERT_OK(DictionaryEncode(&this->ctx_, Datum(carr), &encoded_out));\n+  ASSERT_EQ(Datum::CHUNKED_ARRAY, encoded_out.kind());\n+\n+  ASSERT_TRUE(encoded_out.chunked_array()->Equals(*dict_carr));\n }\n \n }  // namespace compute\ndiff --git a/cpp/src/arrow/compute/context.cc b/cpp/src/arrow/compute/context.cc\nindex 792dc4f38..63aa341a3 100644\n--- a/cpp/src/arrow/compute/context.cc\n+++ b/cpp/src/arrow/compute/context.cc\n@@ -20,11 +20,16 @@\n #include <memory>\n \n #include \"arrow/buffer.h\"\n+#include \"arrow/util/cpu-info.h\"\n \n namespace arrow {\n namespace compute {\n \n-FunctionContext::FunctionContext(MemoryPool* pool) : pool_(pool) {}\n+FunctionContext::FunctionContext(MemoryPool* pool) : pool_(pool) {\n+  if (!::arrow::CpuInfo::initialized()) {\n+    ::arrow::CpuInfo::Init();\n+  }\n+}\n \n MemoryPool* FunctionContext::memory_pool() const { return pool_; }\n \ndiff --git a/cpp/src/arrow/compute/kernel.h b/cpp/src/arrow/compute/kernel.h\nindex 4e072a7c1..0037245d6 100644\n--- a/cpp/src/arrow/compute/kernel.h\n+++ b/cpp/src/arrow/compute/kernel.h\n@@ -18,7 +18,14 @@\n #ifndef ARROW_COMPUTE_KERNEL_H\n #define ARROW_COMPUTE_KERNEL_H\n \n+#include <memory>\n+#include <vector>\n+\n #include \"arrow/array.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/util/macros.h\"\n+#include \"arrow/util/variant.h\"\n+#include \"arrow/util/visibility.h\"\n \n namespace arrow {\n namespace compute {\n@@ -32,11 +39,99 @@ class ARROW_EXPORT OpKernel {\n   virtual ~OpKernel() = default;\n };\n \n+/// \\brief Placeholder for Scalar values until we implement these\n+struct ARROW_EXPORT Scalar {\n+  ~Scalar() {}\n+\n+  ARROW_DISALLOW_COPY_AND_ASSIGN(Scalar);\n+};\n+\n+/// \\class Datum\n+/// \\brief Variant type for various Arrow C++ data structures\n+struct ARROW_EXPORT Datum {\n+  enum type { NONE, SCALAR, ARRAY, CHUNKED_ARRAY, RECORD_BATCH, TABLE, COLLECTION };\n+\n+  util::variant<decltype(NULLPTR), std::shared_ptr<Scalar>, std::shared_ptr<ArrayData>,\n+                std::shared_ptr<ChunkedArray>, std::shared_ptr<RecordBatch>,\n+                std::shared_ptr<Table>, std::vector<Datum>>\n+      value;\n+\n+  /// \\brief Empty datum, to be populated elsewhere\n+  Datum() : value(nullptr) {}\n+\n+  explicit Datum(const std::shared_ptr<Scalar>& value) : value(value) {}\n+\n+  explicit Datum(const std::shared_ptr<ArrayData>& value) : value(value) {}\n+\n+  explicit Datum(const std::shared_ptr<Array>& value) : Datum(value->data()) {}\n+\n+  explicit Datum(const std::shared_ptr<ChunkedArray>& value) : value(value) {}\n+\n+  explicit Datum(const std::shared_ptr<RecordBatch>& value) : value(value) {}\n+\n+  explicit Datum(const std::shared_ptr<Table>& value) : value(value) {}\n+\n+  explicit Datum(const std::vector<Datum>& value) : value(value) {}\n+\n+  ~Datum() {}\n+\n+  Datum(const Datum& other) noexcept { this->value = other.value; }\n+\n+  Datum::type kind() const {\n+    switch (this->value.which()) {\n+      case 0:\n+        return Datum::NONE;\n+      case 1:\n+        return Datum::SCALAR;\n+      case 2:\n+        return Datum::ARRAY;\n+      case 3:\n+        return Datum::CHUNKED_ARRAY;\n+      case 4:\n+        return Datum::RECORD_BATCH;\n+      case 5:\n+        return Datum::TABLE;\n+      case 6:\n+        return Datum::COLLECTION;\n+      default:\n+        return Datum::NONE;\n+    }\n+  }\n+\n+  std::shared_ptr<ArrayData> array() const {\n+    return util::get<std::shared_ptr<ArrayData>>(this->value);\n+  }\n+\n+  std::shared_ptr<ChunkedArray> chunked_array() const {\n+    return util::get<std::shared_ptr<ChunkedArray>>(this->value);\n+  }\n+\n+  const std::vector<Datum> collection() const {\n+    return util::get<std::vector<Datum>>(this->value);\n+  }\n+\n+  bool is_arraylike() const {\n+    return this->kind() == Datum::ARRAY || this->kind() == Datum::CHUNKED_ARRAY;\n+  }\n+\n+  /// \\brief The value type of the variant, if any\n+  ///\n+  /// \\return nullptr if no type\n+  std::shared_ptr<DataType> type() const {\n+    if (this->kind() == Datum::ARRAY) {\n+      return util::get<std::shared_ptr<ArrayData>>(this->value)->type;\n+    } else if (this->kind() == Datum::CHUNKED_ARRAY) {\n+      return util::get<std::shared_ptr<ChunkedArray>>(this->value)->type();\n+    }\n+    return nullptr;\n+  }\n+};\n+\n /// \\class UnaryKernel\n /// \\brief An array-valued function of a single input argument\n class ARROW_EXPORT UnaryKernel : public OpKernel {\n  public:\n-  virtual Status Call(FunctionContext* ctx, const Array& input, ArrayData* out) = 0;\n+  virtual Status Call(FunctionContext* ctx, const ArrayData& input, Datum* out) = 0;\n };\n \n }  // namespace compute\ndiff --git a/cpp/src/arrow/compute/kernels/CMakeLists.txt b/cpp/src/arrow/compute/kernels/CMakeLists.txt\nnew file mode 100644\nindex 000000000..715e6c661\n--- /dev/null\n+++ b/cpp/src/arrow/compute/kernels/CMakeLists.txt\n@@ -0,0 +1,21 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+install(FILES\n+  cast.h\n+  hash.h\n+  DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}/arrow/compute/kernels\")\ndiff --git a/cpp/src/arrow/compute/cast.cc b/cpp/src/arrow/compute/kernels/cast.cc\nsimilarity index 82%\nrename from cpp/src/arrow/compute/cast.cc\nrename to cpp/src/arrow/compute/kernels/cast.cc\nindex 114ab9af0..6a42ec8b2 100644\n--- a/cpp/src/arrow/compute/cast.cc\n+++ b/cpp/src/arrow/compute/kernels/cast.cc\n@@ -15,7 +15,7 @@\n // specific language governing permissions and limitations\n // under the License.\n \n-#include \"arrow/compute/cast.h\"\n+#include \"arrow/compute/kernels/cast.h\"\n \n #include <cstdint>\n #include <cstring>\n@@ -26,6 +26,7 @@\n #include <string>\n #include <type_traits>\n #include <utility>\n+#include <vector>\n \n #include \"arrow/array.h\"\n #include \"arrow/buffer.h\"\n@@ -39,6 +40,7 @@\n \n #include \"arrow/compute/context.h\"\n #include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n \n #ifdef ARROW_EXTRA_ERROR_CONTEXT\n \n@@ -71,29 +73,6 @@ namespace compute {\n \n constexpr int64_t kMillisecondsInDay = 86400000;\n \n-template <typename T>\n-inline const T* GetValues(const ArrayData& data, int i) {\n-  return reinterpret_cast<const T*>(data.buffers[i]->data()) + data.offset;\n-}\n-\n-template <typename T>\n-inline T* GetMutableValues(const ArrayData* data, int i) {\n-  return reinterpret_cast<T*>(data->buffers[i]->mutable_data()) + data->offset;\n-}\n-\n-namespace {\n-\n-void CopyData(const Array& input, ArrayData* output) {\n-  auto in_data = input.data();\n-  output->length = in_data->length;\n-  output->null_count = input.null_count();\n-  output->buffers = in_data->buffers;\n-  output->offset = in_data->offset;\n-  output->child_data = in_data->child_data;\n-}\n-\n-}  // namespace\n-\n // ----------------------------------------------------------------------\n // Zero copy casts\n \n@@ -128,8 +107,8 @@ struct CastFunctor {};\n // Indicated no computation required\n template <typename O, typename I>\n struct CastFunctor<O, I, typename std::enable_if<is_zero_copy_cast<O, I>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     CopyData(input, output);\n   }\n };\n@@ -140,8 +119,8 @@ struct CastFunctor<O, I, typename std::enable_if<is_zero_copy_cast<O, I>::value>\n template <typename T>\n struct CastFunctor<T, NullType, typename std::enable_if<\n                                     std::is_base_of<FixedWidthType, T>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     // Simply initialize data to 0\n     auto buf = output->buffers[1];\n     DCHECK_EQ(output->offset, 0);\n@@ -151,8 +130,8 @@ struct CastFunctor<T, NullType, typename std::enable_if<\n \n template <>\n struct CastFunctor<NullType, DictionaryType> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {}\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {}\n };\n \n // ----------------------------------------------------------------------\n@@ -160,19 +139,17 @@ struct CastFunctor<NullType, DictionaryType> {\n \n // Cast from Boolean to other numbers\n template <typename T>\n-struct CastFunctor<T, BooleanType,\n-                   typename std::enable_if<std::is_base_of<Number, T>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+struct CastFunctor<T, BooleanType, enable_if_number<T>> {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     using c_type = typename T::c_type;\n     constexpr auto kOne = static_cast<c_type>(1);\n     constexpr auto kZero = static_cast<c_type>(0);\n \n-    auto in_data = input.data();\n-    internal::BitmapReader bit_reader(in_data->buffers[1]->data(), in_data->offset,\n-                                      in_data->length);\n+    internal::BitmapReader bit_reader(input.buffers[1]->data(), input.offset,\n+                                      input.length);\n     auto out = GetMutableValues<c_type>(output, 1);\n-    for (int64_t i = 0; i < input.length(); ++i) {\n+    for (int64_t i = 0; i < input.length; ++i) {\n       *out++ = bit_reader.IsSet() ? kOne : kZero;\n       bit_reader.Next();\n     }\n@@ -216,14 +193,14 @@ template <typename O, typename I>\n struct CastFunctor<O, I, typename std::enable_if<std::is_same<BooleanType, O>::value &&\n                                                  std::is_base_of<Number, I>::value &&\n                                                  !std::is_same<O, I>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     using in_type = typename I::c_type;\n     DCHECK_EQ(output->offset, 0);\n \n-    const in_type* in_data = GetValues<in_type>(*input.data(), 1);\n+    const in_type* in_data = GetValues<in_type>(input, 1);\n     uint8_t* out_data = GetMutableValues<uint8_t>(output, 1);\n-    for (int64_t i = 0; i < input.length(); ++i) {\n+    for (int64_t i = 0; i < input.length; ++i) {\n       BitUtil::SetBitTo(out_data, i, (*in_data++) != 0);\n     }\n   }\n@@ -232,25 +209,26 @@ struct CastFunctor<O, I, typename std::enable_if<std::is_same<BooleanType, O>::v\n template <typename O, typename I>\n struct CastFunctor<O, I,\n                    typename std::enable_if<is_integer_downcast<O, I>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     using in_type = typename I::c_type;\n     using out_type = typename O::c_type;\n     DCHECK_EQ(output->offset, 0);\n \n-    auto in_offset = input.offset();\n+    auto in_offset = input.offset;\n \n-    const in_type* in_data = GetValues<in_type>(*input.data(), 1);\n+    const in_type* in_data = GetValues<in_type>(input, 1);\n     auto out_data = GetMutableValues<out_type>(output, 1);\n \n     if (!options.allow_int_overflow) {\n       constexpr in_type kMax = static_cast<in_type>(std::numeric_limits<out_type>::max());\n       constexpr in_type kMin = static_cast<in_type>(std::numeric_limits<out_type>::min());\n \n-      if (input.null_count() > 0) {\n-        internal::BitmapReader is_valid_reader(input.data()->buffers[0]->data(),\n-                                               in_offset, input.length());\n-        for (int64_t i = 0; i < input.length(); ++i) {\n+      // Null count may be -1 if the input array had been sliced\n+      if (input.null_count != 0) {\n+        internal::BitmapReader is_valid_reader(input.buffers[0]->data(), in_offset,\n+                                               input.length);\n+        for (int64_t i = 0; i < input.length; ++i) {\n           if (ARROW_PREDICT_FALSE(is_valid_reader.IsSet() &&\n                                   (*in_data > kMax || *in_data < kMin))) {\n             ctx->SetStatus(Status::Invalid(\"Integer value out of bounds\"));\n@@ -259,7 +237,7 @@ struct CastFunctor<O, I,\n           is_valid_reader.Next();\n         }\n       } else {\n-        for (int64_t i = 0; i < input.length(); ++i) {\n+        for (int64_t i = 0; i < input.length; ++i) {\n           if (ARROW_PREDICT_FALSE(*in_data > kMax || *in_data < kMin)) {\n             ctx->SetStatus(Status::Invalid(\"Integer value out of bounds\"));\n           }\n@@ -267,7 +245,7 @@ struct CastFunctor<O, I,\n         }\n       }\n     } else {\n-      for (int64_t i = 0; i < input.length(); ++i) {\n+      for (int64_t i = 0; i < input.length; ++i) {\n         *out_data++ = static_cast<out_type>(*in_data++);\n       }\n     }\n@@ -278,14 +256,14 @@ template <typename O, typename I>\n struct CastFunctor<O, I,\n                    typename std::enable_if<is_numeric_cast<O, I>::value &&\n                                            !is_integer_downcast<O, I>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     using in_type = typename I::c_type;\n     using out_type = typename O::c_type;\n \n-    const in_type* in_data = GetValues<in_type>(*input.data(), 1);\n+    const in_type* in_data = GetValues<in_type>(input, 1);\n     auto out_data = GetMutableValues<out_type>(output, 1);\n-    for (int64_t i = 0; i < input.length(); ++i) {\n+    for (int64_t i = 0; i < input.length; ++i) {\n       *out_data++ = static_cast<out_type>(*in_data++);\n     }\n   }\n@@ -296,34 +274,52 @@ struct CastFunctor<O, I,\n \n template <typename in_type, typename out_type>\n void ShiftTime(FunctionContext* ctx, const CastOptions& options, const bool is_multiply,\n-               const int64_t factor, const Array& input, ArrayData* output) {\n-  const in_type* in_data = GetValues<in_type>(*input.data(), 1);\n+               const int64_t factor, const ArrayData& input, ArrayData* output) {\n+  const in_type* in_data = GetValues<in_type>(input, 1);\n   auto out_data = GetMutableValues<out_type>(output, 1);\n \n   if (factor == 1) {\n-    for (int64_t i = 0; i < input.length(); i++) {\n+    for (int64_t i = 0; i < input.length; i++) {\n       out_data[i] = static_cast<out_type>(in_data[i]);\n     }\n   } else if (is_multiply) {\n-    for (int64_t i = 0; i < input.length(); i++) {\n+    for (int64_t i = 0; i < input.length; i++) {\n       out_data[i] = static_cast<out_type>(in_data[i] * factor);\n     }\n   } else {\n     if (options.allow_time_truncate) {\n-      for (int64_t i = 0; i < input.length(); i++) {\n+      for (int64_t i = 0; i < input.length; i++) {\n         out_data[i] = static_cast<out_type>(in_data[i] / factor);\n       }\n     } else {\n-      for (int64_t i = 0; i < input.length(); i++) {\n-        out_data[i] = static_cast<out_type>(in_data[i] / factor);\n-        if (input.IsValid(i) && (out_data[i] * factor != in_data[i])) {\n-          std::stringstream ss;\n-          ss << \"Casting from \" << input.type()->ToString() << \" to \"\n-             << output->type->ToString() << \" would lose data: \" << in_data[i];\n-          ctx->SetStatus(Status::Invalid(ss.str()));\n-          break;\n+#define RAISE_INVALID_CAST(VAL)                                                         \\\n+  std::stringstream ss;                                                                 \\\n+  ss << \"Casting from \" << input.type->ToString() << \" to \" << output->type->ToString() \\\n+     << \" would lose data: \" << VAL;                                                    \\\n+  ctx->SetStatus(Status::Invalid(ss.str()));\n+\n+      if (input.null_count != 0) {\n+        internal::BitmapReader bit_reader(input.buffers[0]->data(), input.offset,\n+                                          input.length);\n+        for (int64_t i = 0; i < input.length; i++) {\n+          out_data[i] = static_cast<out_type>(in_data[i] / factor);\n+          if (bit_reader.IsSet() && (out_data[i] * factor != in_data[i])) {\n+            RAISE_INVALID_CAST(in_data[i]);\n+            break;\n+          }\n+          bit_reader.Next();\n+        }\n+      } else {\n+        for (int64_t i = 0; i < input.length; i++) {\n+          out_data[i] = static_cast<out_type>(in_data[i] / factor);\n+          if (out_data[i] * factor != in_data[i]) {\n+            RAISE_INVALID_CAST(in_data[i]);\n+            break;\n+          }\n         }\n       }\n+\n+#undef RAISE_INVALID_CAST\n     }\n   }\n }\n@@ -342,10 +338,10 @@ const std::pair<bool, int64_t> kTimeConversionTable[4][4] = {\n \n template <>\n struct CastFunctor<TimestampType, TimestampType> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     // If units are the same, zero copy, otherwise convert\n-    const auto& in_type = static_cast<const TimestampType&>(*input.type());\n+    const auto& in_type = static_cast<const TimestampType&>(*input.type);\n     const auto& out_type = static_cast<const TimestampType&>(*output->type);\n \n     if (in_type.unit() == out_type.unit()) {\n@@ -364,9 +360,9 @@ struct CastFunctor<TimestampType, TimestampType> {\n \n template <>\n struct CastFunctor<Date32Type, TimestampType> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n-    const auto& in_type = static_cast<const TimestampType&>(*input.type());\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n+    const auto& in_type = static_cast<const TimestampType&>(*input.type);\n \n     static const int64_t kTimestampToDateFactors[4] = {\n         86400LL,                             // SECOND\n@@ -382,9 +378,9 @@ struct CastFunctor<Date32Type, TimestampType> {\n \n template <>\n struct CastFunctor<Date64Type, TimestampType> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n-    const auto& in_type = static_cast<const TimestampType&>(*input.type());\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n+    const auto& in_type = static_cast<const TimestampType&>(*input.type);\n \n     std::pair<bool, int64_t> conversion =\n         kTimeConversionTable[static_cast<int>(in_type.unit())]\n@@ -393,17 +389,21 @@ struct CastFunctor<Date64Type, TimestampType> {\n     ShiftTime<int64_t, int64_t>(ctx, options, conversion.first, conversion.second, input,\n                                 output);\n \n+    internal::BitmapReader bit_reader(input.buffers[0]->data(), input.offset,\n+                                      input.length);\n+\n     // Ensure that intraday milliseconds have been zeroed out\n     auto out_data = GetMutableValues<int64_t>(output, 1);\n-    for (int64_t i = 0; i < input.length(); ++i) {\n+    for (int64_t i = 0; i < input.length; ++i) {\n       const int64_t remainder = out_data[i] % kMillisecondsInDay;\n-      if (ARROW_PREDICT_FALSE(!options.allow_time_truncate && input.IsValid(i) &&\n+      if (ARROW_PREDICT_FALSE(!options.allow_time_truncate && bit_reader.IsSet() &&\n                               remainder > 0)) {\n         ctx->SetStatus(\n             Status::Invalid(\"Timestamp value had non-zero intraday milliseconds\"));\n         break;\n       }\n       out_data[i] -= remainder;\n+      bit_reader.Next();\n     }\n   }\n };\n@@ -415,13 +415,13 @@ template <typename O, typename I>\n struct CastFunctor<O, I,\n                    typename std::enable_if<std::is_base_of<TimeType, I>::value &&\n                                            std::is_base_of<TimeType, O>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     using in_t = typename I::c_type;\n     using out_t = typename O::c_type;\n \n     // If units are the same, zero copy, otherwise convert\n-    const auto& in_type = static_cast<const I&>(*input.type());\n+    const auto& in_type = static_cast<const I&>(*input.type);\n     const auto& out_type = static_cast<const O&>(*output->type);\n \n     if (in_type.unit() == out_type.unit()) {\n@@ -443,16 +443,16 @@ struct CastFunctor<O, I,\n \n template <>\n struct CastFunctor<Date64Type, Date32Type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     ShiftTime<int32_t, int64_t>(ctx, options, true, kMillisecondsInDay, input, output);\n   }\n };\n \n template <>\n struct CastFunctor<Date32Type, Date64Type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     ShiftTime<int64_t, int32_t>(ctx, options, false, kMillisecondsInDay, input, output);\n   }\n };\n@@ -487,10 +487,11 @@ template <typename T>\n struct CastFunctor<\n     T, DictionaryType,\n     typename std::enable_if<std::is_base_of<FixedSizeBinaryType, T>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n-    const DictionaryArray& dict_array = static_cast<const DictionaryArray&>(input);\n-    const DictionaryType& type = static_cast<const DictionaryType&>(*input.type());\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n+    DictionaryArray dict_array(input.ShallowCopy());\n+\n+    const DictionaryType& type = static_cast<const DictionaryType&>(*input.type);\n     const DataType& values_type = *type.dictionary()->type();\n     const FixedSizeBinaryArray& dictionary =\n         static_cast<const FixedSizeBinaryArray&>(*type.dictionary());\n@@ -558,10 +559,11 @@ Status UnpackBinaryDictionary(FunctionContext* ctx, const Array& indices,\n template <typename T>\n struct CastFunctor<T, DictionaryType,\n                    typename std::enable_if<std::is_base_of<BinaryType, T>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n-    const DictionaryArray& dict_array = static_cast<const DictionaryArray&>(input);\n-    const DictionaryType& type = static_cast<const DictionaryType&>(*input.type());\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n+    DictionaryArray dict_array(input.ShallowCopy());\n+\n+    const DictionaryType& type = static_cast<const DictionaryType&>(*input.type);\n     const DataType& values_type = *type.dictionary()->type();\n     const BinaryArray& dictionary = static_cast<const BinaryArray&>(*type.dictionary());\n \n@@ -617,12 +619,13 @@ void UnpackPrimitiveDictionary(const Array& indices, const c_type* dictionary,\n template <typename T>\n struct CastFunctor<T, DictionaryType,\n                    typename std::enable_if<IsNumeric<T>::value>::type> {\n-  void operator()(FunctionContext* ctx, const CastOptions& options, const Array& input,\n-                  ArrayData* output) {\n+  void operator()(FunctionContext* ctx, const CastOptions& options,\n+                  const ArrayData& input, ArrayData* output) {\n     using c_type = typename T::c_type;\n \n-    const DictionaryArray& dict_array = static_cast<const DictionaryArray&>(input);\n-    const DictionaryType& type = static_cast<const DictionaryType&>(*input.type());\n+    DictionaryArray dict_array(input.ShallowCopy());\n+\n+    const DictionaryType& type = static_cast<const DictionaryType&>(*input.type);\n     const DataType& values_type = *type.dictionary()->type();\n \n     // Check if values and output type match\n@@ -657,24 +660,23 @@ struct CastFunctor<T, DictionaryType,\n \n // ----------------------------------------------------------------------\n \n-typedef std::function<void(FunctionContext*, const CastOptions& options, const Array&,\n+typedef std::function<void(FunctionContext*, const CastOptions& options, const ArrayData&,\n                            ArrayData*)>\n     CastFunction;\n \n-static Status AllocateIfNotPreallocated(FunctionContext* ctx, const Array& input,\n+static Status AllocateIfNotPreallocated(FunctionContext* ctx, const ArrayData& input,\n                                         bool can_pre_allocate_values, ArrayData* out) {\n-  const int64_t length = input.length();\n-\n-  out->null_count = input.null_count();\n+  const int64_t length = input.length;\n+  out->null_count = input.null_count;\n \n   // Propagate bitmap unless we are null type\n-  std::shared_ptr<Buffer> validity_bitmap = input.data()->buffers[0];\n-  if (input.type_id() == Type::NA) {\n+  std::shared_ptr<Buffer> validity_bitmap = input.buffers[0];\n+  if (input.type->id() == Type::NA) {\n     int64_t bitmap_size = BitUtil::BytesForBits(length);\n     RETURN_NOT_OK(ctx->Allocate(bitmap_size, &validity_bitmap));\n     memset(validity_bitmap->mutable_data(), 0, bitmap_size);\n-  } else if (input.offset() != 0) {\n-    RETURN_NOT_OK(CopyBitmap(ctx->memory_pool(), validity_bitmap->data(), input.offset(),\n+  } else if (input.offset != 0) {\n+    RETURN_NOT_OK(CopyBitmap(ctx->memory_pool(), validity_bitmap->data(), input.offset,\n                              length, &validity_bitmap));\n   }\n \n@@ -727,17 +729,28 @@ static Status AllocateIfNotPreallocated(FunctionContext* ctx, const Array& input\n class CastKernel : public UnaryKernel {\n  public:\n   CastKernel(const CastOptions& options, const CastFunction& func, bool is_zero_copy,\n-             bool can_pre_allocate_values)\n+             bool can_pre_allocate_values, const std::shared_ptr<DataType>& out_type)\n       : options_(options),\n         func_(func),\n         is_zero_copy_(is_zero_copy),\n-        can_pre_allocate_values_(can_pre_allocate_values) {}\n+        can_pre_allocate_values_(can_pre_allocate_values),\n+        out_type_(out_type) {}\n+\n+  Status Call(FunctionContext* ctx, const ArrayData& input, Datum* out) override {\n+    ArrayData* result;\n+\n+    if (out->kind() == Datum::NONE) {\n+      out->value = std::make_shared<ArrayData>(out_type_, input.length);\n+    }\n+\n+    result = out->array().get();\n \n-  Status Call(FunctionContext* ctx, const Array& input, ArrayData* out) override {\n     if (!is_zero_copy_) {\n-      RETURN_NOT_OK(AllocateIfNotPreallocated(ctx, input, can_pre_allocate_values_, out));\n+      RETURN_NOT_OK(\n+          AllocateIfNotPreallocated(ctx, input, can_pre_allocate_values_, result));\n     }\n-    func_(ctx, options_, input, out);\n+    func_(ctx, options_, input, result);\n+\n     RETURN_IF_ERROR(ctx);\n     return Status::OK();\n   }\n@@ -747,18 +760,19 @@ class CastKernel : public UnaryKernel {\n   CastFunction func_;\n   bool is_zero_copy_;\n   bool can_pre_allocate_values_;\n+  std::shared_ptr<DataType> out_type_;\n };\n \n-#define CAST_CASE(InType, OutType)                                                  \\\n-  case OutType::type_id:                                                            \\\n-    is_zero_copy = is_zero_copy_cast<OutType, InType>::value;                       \\\n-    can_pre_allocate_values =                                                       \\\n-        !(!is_binary_like(InType::type_id) && is_binary_like(OutType::type_id));    \\\n-    func = [](FunctionContext* ctx, const CastOptions& options, const Array& input, \\\n-              ArrayData* out) {                                                     \\\n-      CastFunctor<OutType, InType> func;                                            \\\n-      func(ctx, options, input, out);                                               \\\n-    };                                                                              \\\n+#define CAST_CASE(InType, OutType)                                                      \\\n+  case OutType::type_id:                                                                \\\n+    is_zero_copy = is_zero_copy_cast<OutType, InType>::value;                           \\\n+    can_pre_allocate_values =                                                           \\\n+        !(!is_binary_like(InType::type_id) && is_binary_like(OutType::type_id));        \\\n+    func = [](FunctionContext* ctx, const CastOptions& options, const ArrayData& input, \\\n+              ArrayData* out) {                                                         \\\n+      CastFunctor<OutType, InType> func;                                                \\\n+      func(ctx, options, input, out);                                                   \\\n+    };                                                                                  \\\n     break;\n \n #define NUMERIC_CASES(FN, IN_TYPE) \\\n@@ -832,26 +846,26 @@ class CastKernel : public UnaryKernel {\n   FN(IN_TYPE, FloatType);             \\\n   FN(IN_TYPE, DoubleType);            \\\n   FN(IN_TYPE, FixedSizeBinaryType);   \\\n-  FN(IN_TYPE, DecimalType);           \\\n+  FN(IN_TYPE, Decimal128Type);        \\\n   FN(IN_TYPE, BinaryType);            \\\n   FN(IN_TYPE, StringType);\n \n-#define GET_CAST_FUNCTION(CASE_GENERATOR, InType)                                \\\n-  static std::unique_ptr<UnaryKernel> Get##InType##CastFunc(                     \\\n-      const std::shared_ptr<DataType>& out_type, const CastOptions& options) {   \\\n-    CastFunction func;                                                           \\\n-    bool is_zero_copy = false;                                                   \\\n-    bool can_pre_allocate_values = true;                                         \\\n-    switch (out_type->id()) {                                                    \\\n-      CASE_GENERATOR(CAST_CASE, InType);                                         \\\n-      default:                                                                   \\\n-        break;                                                                   \\\n-    }                                                                            \\\n-    if (func != nullptr) {                                                       \\\n-      return std::unique_ptr<UnaryKernel>(                                       \\\n-          new CastKernel(options, func, is_zero_copy, can_pre_allocate_values)); \\\n-    }                                                                            \\\n-    return nullptr;                                                              \\\n+#define GET_CAST_FUNCTION(CASE_GENERATOR, InType)                              \\\n+  static std::unique_ptr<UnaryKernel> Get##InType##CastFunc(                   \\\n+      const std::shared_ptr<DataType>& out_type, const CastOptions& options) { \\\n+    CastFunction func;                                                         \\\n+    bool is_zero_copy = false;                                                 \\\n+    bool can_pre_allocate_values = true;                                       \\\n+    switch (out_type->id()) {                                                  \\\n+      CASE_GENERATOR(CAST_CASE, InType);                                       \\\n+      default:                                                                 \\\n+        break;                                                                 \\\n+    }                                                                          \\\n+    if (func != nullptr) {                                                     \\\n+      return std::unique_ptr<UnaryKernel>(new CastKernel(                      \\\n+          options, func, is_zero_copy, can_pre_allocate_values, out_type));    \\\n+    }                                                                          \\\n+    return nullptr;                                                            \\\n   }\n \n GET_CAST_FUNCTION(NULL_CASES, NullType);\n@@ -912,18 +926,27 @@ Status GetCastFunction(const DataType& in_type, const std::shared_ptr<DataType>&\n   return Status::OK();\n }\n \n-Status Cast(FunctionContext* ctx, const Array& array,\n+Status Cast(FunctionContext* ctx, const Datum& value,\n             const std::shared_ptr<DataType>& out_type, const CastOptions& options,\n-            std::shared_ptr<Array>* out) {\n+            Datum* out) {\n   // Dynamic dispatch to obtain right cast function\n   std::unique_ptr<UnaryKernel> func;\n-  RETURN_NOT_OK(GetCastFunction(*array.type(), out_type, options, &func));\n+  RETURN_NOT_OK(GetCastFunction(*value.type(), out_type, options, &func));\n \n-  // Data structure for output\n-  auto out_data = std::make_shared<ArrayData>(out_type, array.length());\n+  std::vector<Datum> result;\n+  RETURN_NOT_OK(detail::InvokeUnaryArrayKernel(ctx, func.get(), value, &result));\n \n-  RETURN_NOT_OK(func->Call(ctx, array, out_data.get()));\n-  *out = MakeArray(out_data);\n+  *out = detail::WrapDatumsLike(value, result);\n+  return Status::OK();\n+}\n+\n+Status Cast(FunctionContext* ctx, const Array& array,\n+            const std::shared_ptr<DataType>& out_type, const CastOptions& options,\n+            std::shared_ptr<Array>* out) {\n+  Datum datum_out;\n+  RETURN_NOT_OK(Cast(ctx, Datum(array.data()), out_type, options, &datum_out));\n+  DCHECK_EQ(Datum::ARRAY, datum_out.kind());\n+  *out = MakeArray(datum_out.array());\n   return Status::OK();\n }\n \ndiff --git a/cpp/src/arrow/compute/cast.h b/cpp/src/arrow/compute/kernels/cast.h\nsimilarity index 69%\nrename from cpp/src/arrow/compute/cast.h\nrename to cpp/src/arrow/compute/kernels/cast.h\nindex d7bde20d6..b75bb7b6c 100644\n--- a/cpp/src/arrow/compute/cast.h\n+++ b/cpp/src/arrow/compute/kernels/cast.h\n@@ -15,25 +15,26 @@\n // specific language governing permissions and limitations\n // under the License.\n \n-#ifndef ARROW_COMPUTE_CAST_H\n-#define ARROW_COMPUTE_CAST_H\n+#ifndef ARROW_COMPUTE_KERNELS_CAST_H\n+#define ARROW_COMPUTE_KERNELS_CAST_H\n \n #include <memory>\n \n #include \"arrow/status.h\"\n #include \"arrow/util/visibility.h\"\n \n+#include \"arrow/compute/kernel.h\"\n+\n namespace arrow {\n \n class Array;\n+class ChunkedArray;\n+class Column;\n class DataType;\n \n namespace compute {\n \n-class FunctionContext;\n-class UnaryKernel;\n-\n-struct CastOptions {\n+struct ARROW_EXPORT CastOptions {\n   CastOptions() : allow_int_overflow(false), allow_time_truncate(false) {}\n \n   bool allow_int_overflow;\n@@ -48,7 +49,7 @@ Status GetCastFunction(const DataType& in_type, const std::shared_ptr<DataType>&\n \n /// \\brief Cast from one array type to another\n /// \\param[in] context the FunctionContext\n-/// \\param[in] array array to cast\n+/// \\param[in] value array to cast\n /// \\param[in] to_type type to cast to\n /// \\param[in] options casting options\n /// \\param[out] out resulting array\n@@ -56,11 +57,25 @@ Status GetCastFunction(const DataType& in_type, const std::shared_ptr<DataType>&\n /// \\since 0.7.0\n /// \\note API not yet finalized\n ARROW_EXPORT\n-Status Cast(FunctionContext* context, const Array& array,\n+Status Cast(FunctionContext* context, const Array& value,\n             const std::shared_ptr<DataType>& to_type, const CastOptions& options,\n             std::shared_ptr<Array>* out);\n \n+/// \\brief Cast from one value to another\n+/// \\param[in] context the FunctionContext\n+/// \\param[in] value datum to cast\n+/// \\param[in] to_type type to cast to\n+/// \\param[in] options casting options\n+/// \\param[out] out resulting datum\n+///\n+/// \\since 0.8.0\n+/// \\note API not yet finalized\n+ARROW_EXPORT\n+Status Cast(FunctionContext* context, const Datum& value,\n+            const std::shared_ptr<DataType>& to_type, const CastOptions& options,\n+            Datum* out);\n+\n }  // namespace compute\n }  // namespace arrow\n \n-#endif  // ARROW_COMPUTE_CAST_H\n+#endif  // ARROW_COMPUTE_KERNELS_CAST_H\ndiff --git a/cpp/src/arrow/compute/kernels/hash.cc b/cpp/src/arrow/compute/kernels/hash.cc\nnew file mode 100644\nindex 000000000..3af41609f\n--- /dev/null\n+++ b/cpp/src/arrow/compute/kernels/hash.cc\n@@ -0,0 +1,822 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(Datum* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  /// Size at which we decide to resize\n+  int64_t hash_table_load_threshold_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  hash_table_load_threshold_ =\n+      static_cast<int64_t>(static_cast<double>(elements) * kMaxHashTableLoad);\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n+  if (arr.null_count != 0) {                                                             \\\n+    internal::BitmapReader valid_reader(arr.buffers[0]->data(), arr.offset, arr.length); \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      const bool is_null = valid_reader.IsNotSet();                                      \\\n+      valid_reader.Next();                                                               \\\n+                                                                                         \\\n+      if (is_null) {                                                                     \\\n+        action->ObserveNull();                                                           \\\n+        continue;                                                                        \\\n+      }                                                                                  \\\n+                                                                                         \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  } else {                                                                               \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  }\n+\n+#define DOUBLE_TABLE_SIZE(SETUP_CODE, COMPUTE_HASH)                              \\\n+  do {                                                                           \\\n+    int64_t new_size = hash_table_size_ * 2;                                     \\\n+                                                                                 \\\n+    std::shared_ptr<Buffer> new_hash_table;                                      \\\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));               \\\n+    int32_t* new_hash_slots =                                                    \\\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());          \\\n+    int64_t new_mod_bitmask = new_size - 1;                                      \\\n+                                                                                 \\\n+    SETUP_CODE;                                                                  \\\n+                                                                                 \\\n+    for (int i = 0; i < hash_table_size_; ++i) {                                 \\\n+      hash_slot_t index = hash_slots_[i];                                        \\\n+                                                                                 \\\n+      if (index == kHashSlotEmpty) {                                             \\\n+        continue;                                                                \\\n+      }                                                                          \\\n+                                                                                 \\\n+      COMPUTE_HASH;                                                              \\\n+      while (kHashSlotEmpty != new_hash_slots[j]) {                              \\\n+        ++j;                                                                     \\\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                        \\\n+          j = 0;                                                                 \\\n+        }                                                                        \\\n+      }                                                                          \\\n+                                                                                 \\\n+      new_hash_slots[j] = index;                                                 \\\n+    }                                                                            \\\n+                                                                                 \\\n+    hash_table_ = new_hash_table;                                                \\\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());   \\\n+    hash_table_size_ = new_size;                                                 \\\n+    hash_table_load_threshold_ =                                                 \\\n+        static_cast<int64_t>(static_cast<double>(new_size) * kMaxHashTableLoad); \\\n+    mod_bitmask_ = new_size - 1;                                                 \\\n+  } while (false)\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_has_c_type<Type>> : public HashTable {\n+ public:\n+  using T = typename Type::c_type;\n+\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_(pool) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_.Init());\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const T* values = GetValues<T>(arr, 1);\n+    auto action = static_cast<Action*>(this);\n+\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                               \\\n+  const T value = values[i];                                            \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                          \\\n+  hash_slot_t slot = hash_slots_[j];                                    \\\n+                                                                        \\\n+  while (kHashSlotEmpty != slot && dict_.values[slot] != value) {       \\\n+    ++j;                                                                \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                   \\\n+      j = 0;                                                            \\\n+    }                                                                   \\\n+    slot = hash_slots_[j];                                              \\\n+  }                                                                     \\\n+                                                                        \\\n+  if (slot == kHashSlotEmpty) {                                         \\\n+    if (!Action::allow_expand) {                                        \\\n+      throw HashException(\"Encountered new dictionary value\");          \\\n+    }                                                                   \\\n+                                                                        \\\n+    slot = static_cast<hash_slot_t>(dict_.size);                        \\\n+    hash_slots_[j] = slot;                                              \\\n+    dict_.values[dict_.size++] = value;                                 \\\n+                                                                        \\\n+    action->ObserveNotFound(slot);                                      \\\n+                                                                        \\\n+    if (ARROW_PREDICT_FALSE(dict_.size > hash_table_load_threshold_)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                              \\\n+    }                                                                   \\\n+  } else {                                                              \\\n+    action->ObserveFound(slot);                                         \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    auto dict_data = dict_.buffer;\n+    RETURN_NOT_OK(dict_data->Resize(dict_.size * sizeof(T), false));\n+\n+    BufferVector buffers = {nullptr, dict_data};\n+    *out = std::make_shared<ArrayData>(type_, dict_.size, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const T& value) const {\n+    // TODO(wesm): Use faster hash function for C types\n+    return HashUtil::Hash(&value, sizeof(T), 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+#define PRIMITIVE_INNER_LOOP           \\\n+  const T value = dict_.values[index]; \\\n+  int64_t j = HashValue(value) & new_mod_bitmask;\n+\n+    DOUBLE_TABLE_SIZE(, PRIMITIVE_INNER_LOOP);\n+\n+#undef PRIMITIVE_INNER_LOOP\n+\n+    return dict_.Resize(hash_table_size_);\n+  }\n+\n+  HashDictionary<Type> dict_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for variable-length binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_binary<Type>> : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_offsets_(pool), dict_data_(pool), dict_size_(0) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_offsets_.Resize(kInitialHashTableSize));\n+\n+    // We append the end offset after each append to the dictionary, so this\n+    // sets the initial condition for the length-0 case\n+    //\n+    // initial offsets (dict size == 0): 0\n+    // after 1st dict entry of length 3: 0 3\n+    // after 2nd dict entry of length 4: 0 3 7\n+    RETURN_NOT_OK(dict_offsets_.Append(0));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const int32_t* offsets = GetValues<int32_t>(arr, 1);\n+    const uint8_t* data = GetValues<uint8_t>(arr, 2);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                           \\\n+  const int32_t position = offsets[i];                                              \\\n+  const int32_t length = offsets[i + 1] - position;                                 \\\n+  const uint8_t* value = data + position;                                           \\\n+                                                                                    \\\n+  int64_t j = HashValue(value, length) & mod_bitmask_;                              \\\n+  hash_slot_t slot = hash_slots_[j];                                                \\\n+                                                                                    \\\n+  const int32_t* dict_offsets = dict_offsets_.data();                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                     \\\n+  while (kHashSlotEmpty != slot &&                                                  \\\n+         !((dict_offsets[slot + 1] - dict_offsets[slot]) == length &&               \\\n+           0 == memcmp(value, dict_data + dict_offsets[slot], length))) {           \\\n+    ++j;                                                                            \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                               \\\n+      j = 0;                                                                        \\\n+    }                                                                               \\\n+    slot = hash_slots_[j];                                                          \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  if (slot == kHashSlotEmpty) {                                                     \\\n+    if (!Action::allow_expand) {                                                    \\\n+      throw HashException(\"Encountered new dictionary value\");                      \\\n+    }                                                                               \\\n+                                                                                    \\\n+    slot = dict_size_++;                                                            \\\n+    hash_slots_[j] = slot;                                                          \\\n+                                                                                    \\\n+    RETURN_NOT_OK(dict_data_.Append(value, length));                                \\\n+    RETURN_NOT_OK(dict_offsets_.Append(static_cast<int32_t>(dict_data_.length()))); \\\n+                                                                                    \\\n+    action->ObserveNotFound(slot);                                                  \\\n+                                                                                    \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_load_threshold_)) {             \\\n+      RETURN_NOT_OK(action->DoubleSize());                                          \\\n+    }                                                                               \\\n+  } else {                                                                          \\\n+    action->ObserveFound(slot);                                                     \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr, nullptr};\n+\n+    RETURN_NOT_OK(dict_offsets_.Finish(&buffers[1]));\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[2]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data, int32_t length) const {\n+    return HashUtil::Hash(data, length, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+#define VARBYTES_SETUP                                \\\n+  const int32_t* dict_offsets = dict_offsets_.data(); \\\n+  const uint8_t* dict_data = dict_data_.data()\n+\n+#define VARBYTES_COMPUTE_HASH                                           \\\n+  const int32_t length = dict_offsets[index + 1] - dict_offsets[index]; \\\n+  const uint8_t* value = dict_data + dict_offsets[index];               \\\n+  int64_t j = HashValue(value, length) & new_mod_bitmask\n+\n+    DOUBLE_TABLE_SIZE(VARBYTES_SETUP, VARBYTES_COMPUTE_HASH);\n+\n+#undef VARBYTES_SETUP\n+#undef VARBYTES_COMPUTE_HASH\n+\n+    return Status::OK();\n+  }\n+\n+  TypedBufferBuilder<int32_t> dict_offsets_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for fixed size binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_fixed_size_binary<Type>>\n+    : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_data_(pool), dict_size_(0) {\n+    const auto& fw_type = static_cast<const FixedSizeBinaryType&>(*type);\n+    byte_width_ = fw_type.bit_width() / 8;\n+  }\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_data_.Resize(kInitialHashTableSize * byte_width_));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const uint8_t* data = GetValues<uint8_t>(arr, 1);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                      \\\n+  const uint8_t* value = data + i * byte_width_;                               \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                 \\\n+  hash_slot_t slot = hash_slots_[j];                                           \\\n+                                                                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                \\\n+  while (kHashSlotEmpty != slot &&                                             \\\n+         !(0 == memcmp(value, dict_data + slot * byte_width_, byte_width_))) { \\\n+    ++j;                                                                       \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                          \\\n+      j = 0;                                                                   \\\n+    }                                                                          \\\n+    slot = hash_slots_[j];                                                     \\\n+  }                                                                            \\\n+                                                                               \\\n+  if (slot == kHashSlotEmpty) {                                                \\\n+    if (!Action::allow_expand) {                                               \\\n+      throw HashException(\"Encountered new dictionary value\");                 \\\n+    }                                                                          \\\n+                                                                               \\\n+    slot = dict_size_++;                                                       \\\n+    hash_slots_[j] = slot;                                                     \\\n+                                                                               \\\n+    RETURN_NOT_OK(dict_data_.Append(value, byte_width_));                      \\\n+                                                                               \\\n+    action->ObserveNotFound(slot);                                             \\\n+                                                                               \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_load_threshold_)) {        \\\n+      RETURN_NOT_OK(action->DoubleSize());                                     \\\n+    }                                                                          \\\n+  } else {                                                                     \\\n+    action->ObserveFound(slot);                                                \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr};\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[1]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data) const {\n+    return HashUtil::Hash(data, byte_width_, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+#define FIXED_BYTES_SETUP const uint8_t* dict_data = dict_data_.data()\n+\n+#define FIXED_BYTES_COMPUTE_HASH \\\n+  int64_t j = HashValue(dict_data + index * byte_width_) & new_mod_bitmask\n+\n+    DOUBLE_TABLE_SIZE(FIXED_BYTES_SETUP, FIXED_BYTES_COMPUTE_HASH);\n+\n+#undef FIXED_BYTES_SETUP\n+#undef FIXED_BYTES_COMPUTE_HASH\n+\n+    return Status::OK();\n+  }\n+\n+  int32_t byte_width_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Unique implementation\n+\n+template <typename Type>\n+class UniqueImpl : public HashTableKernel<Type, UniqueImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, UniqueImpl<Type>>;\n+  using Base::Base;\n+\n+  Status Reserve(const int64_t length) { return Status::OK(); }\n+\n+  void ObserveFound(const hash_slot_t slot) {}\n+  void ObserveNull() {}\n+  void ObserveNotFound(const hash_slot_t slot) {}\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Append(const ArrayData& input) override { return Base::Append(input); }\n+\n+  Status Flush(Datum* out) override {\n+    // No-op\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Dictionary encode implementation\n+\n+template <typename Type>\n+class DictEncodeImpl : public HashTableKernel<Type, DictEncodeImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, DictEncodeImpl>;\n+\n+  DictEncodeImpl(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : Base(type, pool), indices_builder_(pool) {}\n+\n+  Status Reserve(const int64_t length) { return indices_builder_.Reserve(length); }\n+\n+  void ObserveNull() { indices_builder_.UnsafeAppendToBitmap(false); }\n+\n+  void ObserveFound(const hash_slot_t slot) { indices_builder_.UnsafeAppend(slot); }\n+\n+  void ObserveNotFound(const hash_slot_t slot) { return ObserveFound(slot); }\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Flush(Datum* out) override {\n+    std::shared_ptr<ArrayData> result;\n+    RETURN_NOT_OK(indices_builder_.FinishInternal(&result));\n+    out->value = std::move(result);\n+    return Status::OK();\n+  }\n+\n+  using Base::Append;\n+\n+ private:\n+  Int32Builder indices_builder_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Kernel wrapper for generic hash table kernels\n+\n+class HashKernelImpl : public HashKernel {\n+ public:\n+  explicit HashKernelImpl(std::unique_ptr<HashTable> hasher)\n+      : hasher_(std::move(hasher)) {}\n+\n+  Status Call(FunctionContext* ctx, const ArrayData& input, Datum* out) override {\n+    RETURN_NOT_OK(Append(ctx, input));\n+    return Flush(out);\n+  }\n+\n+  Status Append(FunctionContext* ctx, const ArrayData& input) override {\n+    std::lock_guard<std::mutex> guard(lock_);\n+    try {\n+      RETURN_NOT_OK(hasher_->Append(input));\n+    } catch (const HashException& e) {\n+      return Status(e.code(), e.what());\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status Flush(Datum* out) override { return hasher_->Flush(out); }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    return hasher_->GetDictionary(out);\n+  }\n+\n+ private:\n+  std::mutex lock_;\n+  std::unique_ptr<HashTable> hasher_;\n+};\n+\n+}  // namespace\n+\n+Status GetUniqueKernel(FunctionContext* ctx, const std::shared_ptr<DataType>& type,\n+                       std::unique_ptr<HashKernel>* out) {\n+  std::unique_ptr<HashTable> hasher;\n+\n+#define UNIQUE_CASE(InType)                                         \\\n+  case InType::type_id:                                             \\\n+    hasher.reset(new UniqueImpl<InType>(type, ctx->memory_pool())); \\\n+    break\n+\n+  switch (type->id()) {\n+    UNIQUE_CASE(NullType);\n+    // UNIQUE_CASE(BooleanType);\n+    UNIQUE_CASE(UInt8Type);\n+    UNIQUE_CASE(Int8Type);\n+    UNIQUE_CASE(UInt16Type);\n+    UNIQUE_CASE(Int16Type);\n+    UNIQUE_CASE(UInt32Type);\n+    UNIQUE_CASE(Int32Type);\n+    UNIQUE_CASE(UInt64Type);\n+    UNIQUE_CASE(Int64Type);\n+    UNIQUE_CASE(FloatType);\n+    UNIQUE_CASE(DoubleType);\n+    UNIQUE_CASE(Date32Type);\n+    UNIQUE_CASE(Date64Type);\n+    UNIQUE_CASE(Time32Type);\n+    UNIQUE_CASE(Time64Type);\n+    UNIQUE_CASE(TimestampType);\n+    UNIQUE_CASE(BinaryType);\n+    UNIQUE_CASE(StringType);\n+    UNIQUE_CASE(FixedSizeBinaryType);\n+    UNIQUE_CASE(Decimal128Type);\n+    default:\n+      break;\n+  }\n+\n+#undef UNIQUE_CASE\n+\n+  CHECK_IMPLEMENTED(hasher, \"unique\", type);\n+  out->reset(new HashKernelImpl(std::move(hasher)));\n+  return Status::OK();\n+}\n+\n+Status GetDictionaryEncodeKernel(FunctionContext* ctx,\n+                                 const std::shared_ptr<DataType>& type,\n+                                 std::unique_ptr<HashKernel>* out) {\n+  std::unique_ptr<HashTable> hasher;\n+\n+#define DICTIONARY_ENCODE_CASE(InType)                                  \\\n+  case InType::type_id:                                                 \\\n+    hasher.reset(new DictEncodeImpl<InType>(type, ctx->memory_pool())); \\\n+    break\n+\n+  switch (type->id()) {\n+    DICTIONARY_ENCODE_CASE(NullType);\n+    // DICTIONARY_ENCODE_CASE(BooleanType);\n+    DICTIONARY_ENCODE_CASE(UInt8Type);\n+    DICTIONARY_ENCODE_CASE(Int8Type);\n+    DICTIONARY_ENCODE_CASE(UInt16Type);\n+    DICTIONARY_ENCODE_CASE(Int16Type);\n+    DICTIONARY_ENCODE_CASE(UInt32Type);\n+    DICTIONARY_ENCODE_CASE(Int32Type);\n+    DICTIONARY_ENCODE_CASE(UInt64Type);\n+    DICTIONARY_ENCODE_CASE(Int64Type);\n+    DICTIONARY_ENCODE_CASE(FloatType);\n+    DICTIONARY_ENCODE_CASE(DoubleType);\n+    DICTIONARY_ENCODE_CASE(Date32Type);\n+    DICTIONARY_ENCODE_CASE(Date64Type);\n+    DICTIONARY_ENCODE_CASE(Time32Type);\n+    DICTIONARY_ENCODE_CASE(Time64Type);\n+    DICTIONARY_ENCODE_CASE(TimestampType);\n+    DICTIONARY_ENCODE_CASE(BinaryType);\n+    DICTIONARY_ENCODE_CASE(StringType);\n+    DICTIONARY_ENCODE_CASE(FixedSizeBinaryType);\n+    DICTIONARY_ENCODE_CASE(Decimal128Type);\n+    default:\n+      break;\n+  }\n+\n+#undef DICTIONARY_ENCODE_CASE\n+\n+  CHECK_IMPLEMENTED(hasher, \"dictionary-encode\", type);\n+  out->reset(new HashKernelImpl(std::move(hasher)));\n+  return Status::OK();\n+}\n+\n+namespace {\n+\n+Status InvokeHash(FunctionContext* ctx, HashKernel* func, const Datum& value,\n+                  std::vector<Datum>* kernel_outputs,\n+                  std::shared_ptr<Array>* dictionary) {\n+  RETURN_NOT_OK(detail::InvokeUnaryArrayKernel(ctx, func, value, kernel_outputs));\n+\n+  std::shared_ptr<ArrayData> dict_data;\n+  RETURN_NOT_OK(func->GetDictionary(&dict_data));\n+  *dictionary = MakeArray(dict_data);\n+  return Status::OK();\n+}\n+\n+}  // namespace\n+\n+Status Unique(FunctionContext* ctx, const Datum& value, std::shared_ptr<Array>* out) {\n+  std::unique_ptr<HashKernel> func;\n+  RETURN_NOT_OK(GetUniqueKernel(ctx, value.type(), &func));\n+\n+  std::vector<Datum> dummy_outputs;\n+  return InvokeHash(ctx, func.get(), value, &dummy_outputs, out);\n+}\n+\n+Status DictionaryEncode(FunctionContext* ctx, const Datum& value, Datum* out) {\n+  std::unique_ptr<HashKernel> func;\n+  RETURN_NOT_OK(GetDictionaryEncodeKernel(ctx, value.type(), &func));\n+\n+  std::shared_ptr<Array> dictionary;\n+  std::vector<Datum> indices_outputs;\n+  RETURN_NOT_OK(InvokeHash(ctx, func.get(), value, &indices_outputs, &dictionary));\n+\n+  // Create the dictionary type\n+  DCHECK_EQ(indices_outputs[0].kind(), Datum::ARRAY);\n+  std::shared_ptr<DataType> dict_type =\n+      ::arrow::dictionary(indices_outputs[0].array()->type, dictionary);\n+\n+  // Create DictionaryArray for each piece yielded by the kernel invocations\n+  std::vector<std::shared_ptr<Array>> dict_chunks;\n+  for (const Datum& datum : indices_outputs) {\n+    dict_chunks.emplace_back(\n+        std::make_shared<DictionaryArray>(dict_type, MakeArray(datum.array())));\n+  }\n+\n+  *out = detail::WrapArraysLike(value, dict_chunks);\n+  return Status::OK();\n+}\n+\n+}  // namespace compute\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/compute/kernels/hash.h b/cpp/src/arrow/compute/kernels/hash.h\nnew file mode 100644\nindex 000000000..05f242949\n--- /dev/null\n+++ b/cpp/src/arrow/compute/kernels/hash.h\n@@ -0,0 +1,106 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#ifndef ARROW_COMPUTE_KERNELS_HASH_H\n+#define ARROW_COMPUTE_KERNELS_HASH_H\n+\n+#include <memory>\n+#include <vector>\n+\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/visibility.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class FunctionContext;\n+\n+/// \\brief Invoke hash table kernel on input array, returning any output\n+/// values. Implementations should be thread-safe\n+class ARROW_EXPORT HashKernel : public UnaryKernel {\n+ public:\n+  virtual Status Append(FunctionContext* ctx, const ArrayData& input) = 0;\n+  virtual Status Flush(Datum* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+};\n+\n+/// \\since 0.8.0\n+/// \\note API not yet finalized\n+ARROW_EXPORT\n+Status GetUniqueKernel(FunctionContext* ctx, const std::shared_ptr<DataType>& type,\n+                       std::unique_ptr<HashKernel>* kernel);\n+\n+ARROW_EXPORT\n+Status GetDictionaryEncodeKernel(FunctionContext* ctx,\n+                                 const std::shared_ptr<DataType>& type,\n+                                 std::unique_ptr<HashKernel>* kernel);\n+\n+/// \\brief Compute unique elements from an array-like object\n+/// \\param[in] context the FunctionContext\n+/// \\param[in] datum array-like input\n+/// \\param[out] out result as Array\n+///\n+/// \\since 0.8.0\n+/// \\note API not yet finalized\n+ARROW_EXPORT\n+Status Unique(FunctionContext* context, const Datum& datum, std::shared_ptr<Array>* out);\n+\n+/// \\brief Dictionary-encode values in an array-like object\n+/// \\param[in] context the FunctionContext\n+/// \\param[in] data array-like input\n+/// \\param[out] out result with same shape and type as input\n+///\n+/// \\since 0.8.0\n+/// \\note API not yet finalized\n+ARROW_EXPORT\n+Status DictionaryEncode(FunctionContext* context, const Datum& data, Datum* out);\n+\n+// TODO(wesm): Define API for incremental dictionary encoding\n+\n+// TODO(wesm): Define API for regularizing DictionaryArray objects with\n+// different dictionaries\n+\n+// class DictionaryEncoder {\n+//  public:\n+//   virtual Encode(const Datum& data, Datum* out) = 0;\n+// };\n+\n+//\n+// ARROW_EXPORT\n+// Status DictionaryEncode(FunctionContext* context, const Datum& data,\n+//                         const Array& prior_dictionary, Datum* out);\n+\n+// TODO(wesm): Implement these next\n+// ARROW_EXPORT\n+// Status Match(FunctionContext* context, const Datum& values, const Datum& member_set,\n+//              Datum* out);\n+\n+// ARROW_EXPORT\n+// Status IsIn(FunctionContext* context, const Datum& values, const Datum& member_set,\n+//             Datum* out);\n+\n+// ARROW_EXPORT\n+// Status CountValues(FunctionContext* context, const Datum& values,\n+//                    std::shared_ptr<Array>* out_uniques,\n+//                    std::shared_ptr<Array>* out_counts);\n+\n+}  // namespace compute\n+}  // namespace arrow\n+\n+#endif  // ARROW_COMPUTE_KERNELS_HASH_H\ndiff --git a/cpp/src/arrow/compute/kernels/util-internal.cc b/cpp/src/arrow/compute/kernels/util-internal.cc\nnew file mode 100644\nindex 000000000..df68637e0\n--- /dev/null\n+++ b/cpp/src/arrow/compute/kernels/util-internal.cc\n@@ -0,0 +1,85 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/util-internal.h\"\n+\n+#include <vector>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+\n+namespace arrow {\n+namespace compute {\n+namespace detail {\n+\n+Status InvokeUnaryArrayKernel(FunctionContext* ctx, UnaryKernel* kernel,\n+                              const Datum& value, std::vector<Datum>* outputs) {\n+  if (value.kind() == Datum::ARRAY) {\n+    Datum output;\n+    RETURN_NOT_OK(kernel->Call(ctx, *value.array(), &output));\n+    outputs->push_back(output);\n+  } else if (value.kind() == Datum::CHUNKED_ARRAY) {\n+    const ChunkedArray& array = *value.chunked_array();\n+    for (int i = 0; i < array.num_chunks(); i++) {\n+      Datum output;\n+      RETURN_NOT_OK(kernel->Call(ctx, *(array.chunk(i)->data()), &output));\n+      outputs->push_back(output);\n+    }\n+  } else {\n+    return Status::Invalid(\"Input Datum was not array-like\");\n+  }\n+  return Status::OK();\n+}\n+\n+Datum WrapArraysLike(const Datum& value,\n+                     const std::vector<std::shared_ptr<Array>>& arrays) {\n+  // Create right kind of datum\n+  if (value.kind() == Datum::ARRAY) {\n+    return Datum(arrays[0]->data());\n+  } else if (value.kind() == Datum::CHUNKED_ARRAY) {\n+    return Datum(std::make_shared<ChunkedArray>(arrays));\n+  } else {\n+    DCHECK(false) << \"unhandled datum kind\";\n+    return Datum();\n+  }\n+}\n+\n+Datum WrapDatumsLike(const Datum& value, const std::vector<Datum>& datums) {\n+  // Create right kind of datum\n+  if (value.kind() == Datum::ARRAY) {\n+    DCHECK_EQ(1, datums.size());\n+    return Datum(datums[0].array());\n+  } else if (value.kind() == Datum::CHUNKED_ARRAY) {\n+    std::vector<std::shared_ptr<Array>> arrays;\n+    for (const Datum& datum : datums) {\n+      DCHECK_EQ(Datum::ARRAY, datum.kind());\n+      arrays.emplace_back(MakeArray(datum.array()));\n+    }\n+    return Datum(std::make_shared<ChunkedArray>(arrays));\n+  } else {\n+    DCHECK(false) << \"unhandled datum kind\";\n+    return Datum();\n+  }\n+}\n+\n+}  // namespace detail\n+}  // namespace compute\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/compute/kernels/util-internal.h b/cpp/src/arrow/compute/kernels/util-internal.h\nnew file mode 100644\nindex 000000000..70c506286\n--- /dev/null\n+++ b/cpp/src/arrow/compute/kernels/util-internal.h\n@@ -0,0 +1,105 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#ifndef ARROW_COMPUTE_KERNELS_UTIL_INTERNAL_H\n+#define ARROW_COMPUTE_KERNELS_UTIL_INTERNAL_H\n+\n+#include <vector>\n+\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/type_fwd.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class FunctionContext;\n+\n+template <typename T>\n+using is_number = std::is_base_of<Number, T>;\n+\n+template <typename T>\n+using enable_if_primitive_ctype =\n+    typename std::enable_if<std::is_base_of<PrimitiveCType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_date = typename std::enable_if<std::is_base_of<DateType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_time = typename std::enable_if<std::is_base_of<TimeType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_timestamp =\n+    typename std::enable_if<std::is_base_of<TimestampType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_has_c_type =\n+    typename std::enable_if<std::is_base_of<PrimitiveCType, T>::value ||\n+                            std::is_base_of<DateType, T>::value ||\n+                            std::is_base_of<TimeType, T>::value ||\n+                            std::is_base_of<TimestampType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_null = typename std::enable_if<std::is_same<NullType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_binary =\n+    typename std::enable_if<std::is_base_of<BinaryType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_fixed_size_binary =\n+    typename std::enable_if<std::is_base_of<FixedSizeBinaryType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_list = typename std::enable_if<std::is_base_of<ListType, T>::value>::type;\n+\n+template <typename T>\n+using enable_if_number = typename std::enable_if<is_number<T>::value>::type;\n+\n+template <typename T>\n+inline const T* GetValues(const ArrayData& data, int i) {\n+  return reinterpret_cast<const T*>(data.buffers[i]->data()) + data.offset;\n+}\n+\n+template <typename T>\n+inline T* GetMutableValues(const ArrayData* data, int i) {\n+  return reinterpret_cast<T*>(data->buffers[i]->mutable_data()) + data->offset;\n+}\n+\n+static inline void CopyData(const ArrayData& input, ArrayData* output) {\n+  output->length = input.length;\n+  output->null_count = input.null_count;\n+  output->buffers = input.buffers;\n+  output->offset = input.offset;\n+  output->child_data = input.child_data;\n+}\n+\n+namespace detail {\n+\n+Status InvokeUnaryArrayKernel(FunctionContext* ctx, UnaryKernel* kernel,\n+                              const Datum& value, std::vector<Datum>* outputs);\n+\n+Datum WrapArraysLike(const Datum& value,\n+                     const std::vector<std::shared_ptr<Array>>& arrays);\n+\n+Datum WrapDatumsLike(const Datum& value, const std::vector<Datum>& datums);\n+\n+}  // namespace detail\n+\n+}  // namespace compute\n+}  // namespace arrow\n+\n+#endif  // ARROW_COMPUTE_KERNELS_UTIL_INTERNAL_H\ndiff --git a/cpp/src/arrow/pretty_print-test.cc b/cpp/src/arrow/pretty_print-test.cc\nindex 8b9a24fec..bf29d6a03 100644\n--- a/cpp/src/arrow/pretty_print-test.cc\n+++ b/cpp/src/arrow/pretty_print-test.cc\n@@ -107,14 +107,13 @@ TEST_F(TestPrettyPrint, FixedSizeBinaryType) {\n   CheckArray(*array, 0, ex);\n }\n \n-TEST_F(TestPrettyPrint, DecimalType) {\n+TEST_F(TestPrettyPrint, Decimal128Type) {\n   int32_t p = 19;\n   int32_t s = 4;\n \n   auto type = decimal(p, s);\n \n-  DecimalBuilder builder(type);\n-\n+  Decimal128Builder builder(type);\n   Decimal128 val;\n \n   ASSERT_OK(Decimal128::FromString(\"123.4567\", &val));\ndiff --git a/cpp/src/arrow/python/arrow_to_pandas.cc b/cpp/src/arrow/python/arrow_to_pandas.cc\nindex 2b0f96439..8814fc190 100644\n--- a/cpp/src/arrow/python/arrow_to_pandas.cc\n+++ b/cpp/src/arrow/python/arrow_to_pandas.cc\n@@ -42,6 +42,8 @@\n #include \"arrow/util/parallel.h\"\n #include \"arrow/visitor_inline.h\"\n \n+#include \"arrow/compute/api.h\"\n+\n #include \"arrow/python/builtin_convert.h\"\n #include \"arrow/python/common.h\"\n #include \"arrow/python/config.h\"\n@@ -57,6 +59,8 @@ namespace py {\n using internal::kPandasTimestampNull;\n using internal::kNanosecondsInDay;\n \n+using compute::Datum;\n+\n // ----------------------------------------------------------------------\n // Utility code\n \n@@ -1028,8 +1032,13 @@ class CategoricalBlock : public PandasBlock {\n     std::shared_ptr<Column> converted_col;\n     if (options_.strings_to_categorical &&\n         (col->type()->id() == Type::STRING || col->type()->id() == Type::BINARY)) {\n-      RETURN_NOT_OK(EncodeColumnToDictionary(static_cast<const Column&>(*col), pool_,\n-                                             &converted_col));\n+      compute::FunctionContext ctx(pool_);\n+\n+      Datum out;\n+      RETURN_NOT_OK(compute::DictionaryEncode(&ctx, Datum(col->data()), &out));\n+      DCHECK_EQ(out.kind(), Datum::CHUNKED_ARRAY);\n+      converted_col =\n+          std::make_shared<Column>(field(col->name(), out.type()), out.chunked_array());\n     } else {\n       converted_col = col;\n     }\n@@ -1646,7 +1655,7 @@ class ArrowDeserializer {\n       CONVERTVALUES_LISTSLIKE_CASE(FloatType, FLOAT)\n       CONVERTVALUES_LISTSLIKE_CASE(DoubleType, DOUBLE)\n       CONVERTVALUES_LISTSLIKE_CASE(StringType, STRING)\n-      CONVERTVALUES_LISTSLIKE_CASE(DecimalType, DECIMAL)\n+      CONVERTVALUES_LISTSLIKE_CASE(Decimal128Type, DECIMAL)\n       CONVERTVALUES_LISTSLIKE_CASE(ListType, LIST)\n       default: {\n         std::stringstream ss;\ndiff --git a/cpp/src/arrow/python/numpy_to_arrow.cc b/cpp/src/arrow/python/numpy_to_arrow.cc\nindex a6c28af9b..0d2df9315 100644\n--- a/cpp/src/arrow/python/numpy_to_arrow.cc\n+++ b/cpp/src/arrow/python/numpy_to_arrow.cc\n@@ -42,8 +42,8 @@\n #include \"arrow/util/macros.h\"\n #include \"arrow/visitor_inline.h\"\n \n-#include \"arrow/compute/cast.h\"\n #include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernels/cast.h\"\n \n #include \"arrow/python/builtin_convert.h\"\n #include \"arrow/python/common.h\"\n@@ -466,13 +466,14 @@ Status NumPyConverter::Convert() {\n \n namespace {\n \n-Status CastBuffer(const std::shared_ptr<Buffer>& input, const int64_t length,\n-                  const std::shared_ptr<DataType>& in_type,\n+Status CastBuffer(const std::shared_ptr<DataType>& in_type,\n+                  const std::shared_ptr<Buffer>& input, const int64_t length,\n+                  const std::shared_ptr<Buffer>& valid_bitmap, const int64_t null_count,\n                   const std::shared_ptr<DataType>& out_type, MemoryPool* pool,\n                   std::shared_ptr<Buffer>* out) {\n   // Must cast\n-  std::vector<std::shared_ptr<Buffer>> buffers = {nullptr, input};\n-  auto tmp_data = std::make_shared<ArrayData>(in_type, length, buffers, 0);\n+  std::vector<std::shared_ptr<Buffer>> buffers = {valid_bitmap, input};\n+  auto tmp_data = std::make_shared<ArrayData>(in_type, length, buffers, null_count);\n \n   std::shared_ptr<Array> tmp_array = MakeArray(tmp_data);\n   std::shared_ptr<Array> casted_array;\n@@ -488,6 +489,21 @@ Status CastBuffer(const std::shared_ptr<Buffer>& input, const int64_t length,\n   return Status::OK();\n }\n \n+template <typename FromType, typename ToType>\n+Status StaticCastBuffer(const Buffer& input, const int64_t length, MemoryPool* pool,\n+                        std::shared_ptr<Buffer>* out) {\n+  auto result = std::make_shared<PoolBuffer>(pool);\n+  RETURN_NOT_OK(result->Resize(sizeof(ToType) * length));\n+\n+  auto in_values = reinterpret_cast<const FromType*>(input.data());\n+  auto out_values = reinterpret_cast<ToType*>(result->mutable_data());\n+  for (int64_t i = 0; i < length; ++i) {\n+    *out_values++ = static_cast<ToType>(*in_values++);\n+  }\n+  *out = result;\n+  return Status::OK();\n+}\n+\n template <typename T, typename T2>\n void CopyStrided(T* input_data, int64_t length, int64_t stride, T2* output_data) {\n   // Passing input_data as non-const is a concession to PyObject*\n@@ -531,7 +547,7 @@ inline Status NumPyConverter::ConvertData(std::shared_ptr<Buffer>* data) {\n   RETURN_NOT_OK(NumPyDtypeToArrow(reinterpret_cast<PyObject*>(dtype_), &input_type));\n \n   if (!input_type->Equals(*type_)) {\n-    RETURN_NOT_OK(CastBuffer(*data, length_, input_type, type_, pool_, data));\n+    RETURN_NOT_OK(CastBuffer(input_type, *data, length_, nullptr, 0, type_, pool_, data));\n   }\n \n   return Status::OK();\n@@ -567,27 +583,32 @@ inline Status NumPyConverter::ConvertData<Date32Type>(std::shared_ptr<Buffer>* d\n     *data = std::make_shared<NumPyBuffer>(reinterpret_cast<PyObject*>(arr_));\n   }\n \n-  // If we have inbound datetime64[D] data, this needs to be downcasted\n-  // separately here from int64_t to int32_t, because this data is not\n-  // supported in compute::Cast\n+  std::shared_ptr<DataType> input_type;\n+\n   auto date_dtype = reinterpret_cast<PyArray_DatetimeDTypeMetaData*>(dtype_->c_metadata);\n-  if (dtype_->type_num == NPY_DATETIME && date_dtype->meta.base == NPY_FR_D) {\n-    auto date32_buffer = std::make_shared<PoolBuffer>(pool_);\n-    RETURN_NOT_OK(date32_buffer->Resize(sizeof(int32_t) * length_));\n+  if (dtype_->type_num == NPY_DATETIME) {\n+    const int64_t null_count = ValuesToBitmap<NPY_DATETIME>(arr_, null_bitmap_data_);\n \n-    auto datetime64_values = reinterpret_cast<const int64_t*>((*data)->data());\n-    auto date32_values = reinterpret_cast<int32_t*>(date32_buffer->mutable_data());\n-    for (int64_t i = 0; i < length_; ++i) {\n+    // If we have inbound datetime64[D] data, this needs to be downcasted\n+    // separately here from int64_t to int32_t, because this data is not\n+    // supported in compute::Cast\n+    if (date_dtype->meta.base == NPY_FR_D) {\n       // TODO(wesm): How pedantic do we really want to be about checking for int32\n       // overflow here?\n-      *date32_values++ = static_cast<int32_t>(*datetime64_values++);\n+      Status s = StaticCastBuffer<int64_t, int32_t>(**data, length_, pool_, data);\n+      RETURN_NOT_OK(s);\n+    } else {\n+      RETURN_NOT_OK(NumPyDtypeToArrow(reinterpret_cast<PyObject*>(dtype_), &input_type));\n+      if (!input_type->Equals(*type_)) {\n+        RETURN_NOT_OK(CastBuffer(input_type, *data, length_, null_bitmap_, null_count,\n+                                 type_, pool_, data));\n+      }\n     }\n-    *data = date32_buffer;\n   } else {\n-    std::shared_ptr<DataType> input_type;\n     RETURN_NOT_OK(NumPyDtypeToArrow(reinterpret_cast<PyObject*>(dtype_), &input_type));\n     if (!input_type->Equals(*type_)) {\n-      RETURN_NOT_OK(CastBuffer(*data, length_, input_type, type_, pool_, data));\n+      RETURN_NOT_OK(\n+          CastBuffer(input_type, *data, length_, nullptr, 0, type_, pool_, data));\n     }\n   }\n \ndiff --git a/cpp/src/arrow/table.h b/cpp/src/arrow/table.h\nindex d3145ff10..2cff32f74 100644\n--- a/cpp/src/arrow/table.h\n+++ b/cpp/src/arrow/table.h\n@@ -63,6 +63,9 @@ class ARROW_EXPORT ChunkedArray {\n   ArrayVector chunks_;\n   int64_t length_;\n   int64_t null_count_;\n+\n+ private:\n+  ARROW_DISALLOW_COPY_AND_ASSIGN(ChunkedArray);\n };\n \n /// \\brief An immutable column data structure consisting of a field (type\ndiff --git a/cpp/src/arrow/test-util.h b/cpp/src/arrow/test-util.h\nindex 9b875ce11..77f489ab1 100644\n--- a/cpp/src/arrow/test-util.h\n+++ b/cpp/src/arrow/test-util.h\n@@ -365,6 +365,10 @@ Status MakeArray(const std::vector<uint8_t>& valid_bytes, const std::vector<T>&\n     }                                                                                  \\\n   } while (false)\n \n+#define DECL_T() typedef typename TestFixture::T T;\n+\n+#define DECL_TYPE() typedef typename TestFixture::Type Type;\n+\n void AssertArraysEqual(const Array& expected, const Array& actual) {\n   ASSERT_ARRAYS_EQUAL(expected, actual);\n }\ndiff --git a/cpp/src/arrow/type-test.cc b/cpp/src/arrow/type-test.cc\nindex 3242fadd5..48982cad4 100644\n--- a/cpp/src/arrow/type-test.cc\n+++ b/cpp/src/arrow/type-test.cc\n@@ -400,7 +400,7 @@ TEST(TestStructType, Basics) {\n }\n \n TEST(TypesTest, TestDecimal128Small) {\n-  DecimalType t1(8, 4);\n+  Decimal128Type t1(8, 4);\n \n   ASSERT_EQ(t1.id(), Type::DECIMAL);\n   ASSERT_EQ(t1.precision(), 8);\n@@ -414,7 +414,7 @@ TEST(TypesTest, TestDecimal128Small) {\n }\n \n TEST(TypesTest, TestDecimal128Medium) {\n-  DecimalType t1(12, 5);\n+  Decimal128Type t1(12, 5);\n \n   ASSERT_EQ(t1.id(), Type::DECIMAL);\n   ASSERT_EQ(t1.precision(), 12);\n@@ -428,7 +428,7 @@ TEST(TypesTest, TestDecimal128Medium) {\n }\n \n TEST(TypesTest, TestDecimal128Large) {\n-  DecimalType t1(27, 7);\n+  Decimal128Type t1(27, 7);\n \n   ASSERT_EQ(t1.id(), Type::DECIMAL);\n   ASSERT_EQ(t1.precision(), 27);\ndiff --git a/cpp/src/arrow/type.h b/cpp/src/arrow/type.h\nindex d86e7ef57..70f275c0f 100644\n--- a/cpp/src/arrow/type.h\n+++ b/cpp/src/arrow/type.h\n@@ -498,25 +498,34 @@ class ARROW_EXPORT StructType : public NestedType {\n   std::vector<BufferDescr> GetBufferLayout() const override;\n };\n \n-class ARROW_EXPORT Decimal128Type : public FixedSizeBinaryType {\n+class ARROW_EXPORT DecimalBaseType : public FixedSizeBinaryType {\n+ public:\n+  explicit DecimalBaseType(int32_t byte_width, int32_t precision, int32_t scale)\n+      : FixedSizeBinaryType(byte_width, Type::DECIMAL),\n+        precision_(precision),\n+        scale_(scale) {}\n+\n+  int32_t precision() const { return precision_; }\n+  int32_t scale() const { return scale_; }\n+\n+ protected:\n+  int32_t precision_;\n+  int32_t scale_;\n+};\n+\n+class ARROW_EXPORT Decimal128Type : public DecimalBaseType {\n  public:\n   static constexpr Type::type type_id = Type::DECIMAL;\n \n   explicit Decimal128Type(int32_t precision, int32_t scale)\n-      : FixedSizeBinaryType(16, Type::DECIMAL), precision_(precision), scale_(scale) {}\n+      : DecimalBaseType(16, precision, scale) {}\n \n   Status Accept(TypeVisitor* visitor) const override;\n   std::string ToString() const override;\n   std::string name() const override { return \"decimal\"; }\n-\n-  int32_t precision() const { return precision_; }\n-  int32_t scale() const { return scale_; }\n-\n- private:\n-  int32_t precision_;\n-  int32_t scale_;\n };\n \n+// TODO(wesm): Remove this\n using DecimalType = Decimal128Type;\n \n struct UnionMode {\ndiff --git a/cpp/src/arrow/type_fwd.h b/cpp/src/arrow/type_fwd.h\nindex b8b3c5aa5..9d8a23ce0 100644\n--- a/cpp/src/arrow/type_fwd.h\n+++ b/cpp/src/arrow/type_fwd.h\n@@ -28,10 +28,16 @@ class Status;\n \n class DataType;\n class Array;\n+struct ArrayData;\n class ArrayBuilder;\n class Field;\n class Tensor;\n \n+class ChunkedArray;\n+class Column;\n+class RecordBatch;\n+class Table;\n+\n class Buffer;\n class MemoryPool;\n class RecordBatch;\ndiff --git a/cpp/src/arrow/type_traits.h b/cpp/src/arrow/type_traits.h\nindex 6707f3759..4bfce9b5f 100644\n--- a/cpp/src/arrow/type_traits.h\n+++ b/cpp/src/arrow/type_traits.h\n@@ -430,6 +430,10 @@ static inline bool is_binary_like(Type::type type_id) {\n   return false;\n }\n \n+static inline bool is_dictionary(Type::type type_id) {\n+  return type_id == Type::DICTIONARY;\n+}\n+\n }  // namespace arrow\n \n #endif  // ARROW_TYPE_TRAITS_H\ndiff --git a/cpp/src/arrow/util/CMakeLists.txt b/cpp/src/arrow/util/CMakeLists.txt\nindex 7810a3be4..29b18a935 100644\n--- a/cpp/src/arrow/util/CMakeLists.txt\n+++ b/cpp/src/arrow/util/CMakeLists.txt\n@@ -42,6 +42,7 @@ install(FILES\n   sse-util.h\n   stl.h\n   type_traits.h\n+  variant.h\n   visibility.h\n   DESTINATION include/arrow/util)\n \n@@ -72,3 +73,5 @@ ADD_ARROW_TEST(decimal-test)\n ADD_ARROW_TEST(key-value-metadata-test)\n ADD_ARROW_TEST(rle-encoding-test)\n ADD_ARROW_TEST(stl-util-test)\n+\n+add_subdirectory(variant)\ndiff --git a/cpp/src/arrow/util/variant.h b/cpp/src/arrow/util/variant.h\nnew file mode 100644\nindex 000000000..923a8685f\n--- /dev/null\n+++ b/cpp/src/arrow/util/variant.h\n@@ -0,0 +1,1127 @@\n+// Copyright (c) MapBox\n+// All rights reserved.\n+//\n+// Redistribution and use in source and binary forms, with or without modification,\n+// are permitted provided that the following conditions are met:\n+//\n+// - Redistributions of source code must retain the above copyright notice, this\n+//   list of conditions and the following disclaimer.\n+// - Redistributions in binary form must reproduce the above copyright notice, this\n+//   list of conditions and the following disclaimer in the documentation and/or\n+//   other materials provided with the distribution.\n+// - Neither the name \"MapBox\" nor the names of its contributors may be\n+//   used to endorse or promote products derived from this software without\n+//   specific prior written permission.\n+//\n+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n+// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n+// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n+// ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+\n+#ifndef ARROW_UTIL_VARIANT_H\n+#define ARROW_UTIL_VARIANT_H\n+\n+#include <cassert>\n+#include <cstddef>   // size_t\n+#include <new>       // operator new\n+#include <stdexcept> // runtime_error\n+#include <string>\n+#include <tuple>\n+#include <type_traits>\n+#include <typeinfo>\n+#include <utility>\n+#include <functional>\n+#include <limits>\n+\n+#include <arrow/util/variant/recursive_wrapper.h>\n+#include <arrow/util/variant/variant_visitor.h>\n+\n+// clang-format off\n+// [[deprecated]] is only available in C++14, use this for the time being\n+#if __cplusplus <= 201103L\n+# ifdef __GNUC__\n+#  define ARROW_VARIANT_DEPRECATED __attribute__((deprecated))\n+# elif defined(_MSC_VER)\n+#  define ARROW_VARIANT_DEPRECATED __declspec(deprecated)\n+# else\n+#  define ARROW_VARIANT_DEPRECATED\n+# endif\n+#else\n+#  define ARROW_VARIANT_DEPRECATED [[deprecated]]\n+#endif\n+\n+\n+#ifdef _MSC_VER\n+// https://msdn.microsoft.com/en-us/library/bw1hbe6y.aspx\n+# ifdef NDEBUG\n+#  define VARIANT_INLINE __forceinline\n+# else\n+#  define VARIANT_INLINE //__declspec(noinline)\n+# endif\n+#else\n+# ifdef NDEBUG\n+#  define VARIANT_INLINE //inline __attribute__((always_inline))\n+# else\n+#  define VARIANT_INLINE __attribute__((noinline))\n+# endif\n+#endif\n+// clang-format on\n+\n+// Exceptions\n+#if defined( __EXCEPTIONS) || defined( _MSC_VER)\n+#define HAS_EXCEPTIONS\n+#endif\n+\n+#define VARIANT_MAJOR_VERSION 1\n+#define VARIANT_MINOR_VERSION 1\n+#define VARIANT_PATCH_VERSION 0\n+\n+#define VARIANT_VERSION (VARIANT_MAJOR_VERSION * 100000) + (VARIANT_MINOR_VERSION * 100) + (VARIANT_PATCH_VERSION)\n+\n+namespace arrow {\n+namespace util {\n+\n+// XXX This should derive from std::logic_error instead of std::runtime_error.\n+//     See https://github.com/mapbox/variant/issues/48 for details.\n+class bad_variant_access : public std::runtime_error\n+{\n+\n+public:\n+    explicit bad_variant_access(const std::string& what_arg)\n+        : runtime_error(what_arg) {}\n+\n+    explicit bad_variant_access(const char* what_arg)\n+        : runtime_error(what_arg) {}\n+\n+}; // class bad_variant_access\n+\n+template <typename R = void>\n+struct ARROW_VARIANT_DEPRECATED static_visitor\n+{\n+    using result_type = R;\n+\n+protected:\n+    static_visitor() {}\n+    ~static_visitor() {}\n+};\n+\n+#if !defined(ARROW_VARIANT_MINIMIZE_SIZE)\n+using type_index_t = unsigned int;\n+#else\n+#if defined(ARROW_VARIANT_OPTIMIZE_FOR_SPEED)\n+using type_index_t = std::uint_fast8_t;\n+#else\n+using type_index_t = std::uint_least8_t;\n+#endif\n+#endif\n+\n+namespace detail {\n+\n+static constexpr type_index_t invalid_value = type_index_t(-1);\n+\n+template <typename T, typename... Types>\n+struct direct_type;\n+\n+template <typename T, typename First, typename... Types>\n+struct direct_type<T, First, Types...>\n+{\n+    static constexpr type_index_t index = std::is_same<T, First>::value\n+        ? sizeof...(Types)\n+        : direct_type<T, Types...>::index;\n+};\n+\n+template <typename T>\n+struct direct_type<T>\n+{\n+    static constexpr type_index_t index = invalid_value;\n+};\n+\n+#if __cpp_lib_logical_traits >= 201510L\n+\n+using std::conjunction;\n+using std::disjunction;\n+\n+#else\n+\n+template <typename...>\n+struct conjunction : std::true_type {};\n+\n+template <typename B1>\n+struct conjunction<B1> : B1 {};\n+\n+template <typename B1, typename B2>\n+struct conjunction<B1, B2> : std::conditional<B1::value, B2, B1>::type {};\n+\n+template <typename B1, typename... Bs>\n+struct conjunction<B1, Bs...> : std::conditional<B1::value, conjunction<Bs...>, B1>::type {};\n+\n+template <typename...>\n+struct disjunction : std::false_type {};\n+\n+template <typename B1>\n+struct disjunction<B1> : B1 {};\n+\n+template <typename B1, typename B2>\n+struct disjunction<B1, B2> : std::conditional<B1::value, B1, B2>::type {};\n+\n+template <typename B1, typename... Bs>\n+struct disjunction<B1, Bs...> : std::conditional<B1::value, B1, disjunction<Bs...>>::type {};\n+\n+#endif\n+\n+template <typename T, typename... Types>\n+struct convertible_type;\n+\n+template <typename T, typename First, typename... Types>\n+struct convertible_type<T, First, Types...>\n+{\n+    static constexpr type_index_t index = std::is_convertible<T, First>::value\n+        ? disjunction<std::is_convertible<T, Types>...>::value ? invalid_value : sizeof...(Types)\n+        : convertible_type<T, Types...>::index;\n+};\n+\n+template <typename T>\n+struct convertible_type<T>\n+{\n+    static constexpr type_index_t index = invalid_value;\n+};\n+\n+template <typename T, typename... Types>\n+struct value_traits\n+{\n+    using value_type = typename std::remove_const<typename std::remove_reference<T>::type>::type;\n+    using value_type_wrapper = recursive_wrapper<value_type>;\n+    static constexpr type_index_t direct_index = direct_type<value_type, Types...>::index;\n+    static constexpr bool is_direct = direct_index != invalid_value;\n+    static constexpr type_index_t index_direct_or_wrapper = is_direct ? direct_index : direct_type<value_type_wrapper, Types...>::index;\n+    static constexpr bool is_direct_or_wrapper = index_direct_or_wrapper != invalid_value;\n+    static constexpr type_index_t index = is_direct_or_wrapper ? index_direct_or_wrapper : convertible_type<value_type, Types...>::index;\n+    static constexpr bool is_valid = index != invalid_value;\n+    static constexpr type_index_t tindex = is_valid ? sizeof...(Types)-index : 0;\n+    using target_type = typename std::tuple_element<tindex, std::tuple<void, Types...>>::type;\n+};\n+\n+template <typename T, typename R = void>\n+struct enable_if_type\n+{\n+    using type = R;\n+};\n+\n+template <typename F, typename V, typename Enable = void>\n+struct result_of_unary_visit\n+{\n+    using type = typename std::result_of<F(V&)>::type;\n+};\n+\n+template <typename F, typename V>\n+struct result_of_unary_visit<F, V, typename enable_if_type<typename F::result_type>::type>\n+{\n+    using type = typename F::result_type;\n+};\n+\n+template <typename F, typename V, typename Enable = void>\n+struct result_of_binary_visit\n+{\n+    using type = typename std::result_of<F(V&, V&)>::type;\n+};\n+\n+template <typename F, typename V>\n+struct result_of_binary_visit<F, V, typename enable_if_type<typename F::result_type>::type>\n+{\n+    using type = typename F::result_type;\n+};\n+\n+template <type_index_t arg1, type_index_t... others>\n+struct static_max;\n+\n+template <type_index_t arg>\n+struct static_max<arg>\n+{\n+    static const type_index_t value = arg;\n+};\n+\n+template <type_index_t arg1, type_index_t arg2, type_index_t... others>\n+struct static_max<arg1, arg2, others...>\n+{\n+    static const type_index_t value = arg1 >= arg2 ? static_max<arg1, others...>::value : static_max<arg2, others...>::value;\n+};\n+\n+template <typename... Types>\n+struct variant_helper;\n+\n+template <typename T, typename... Types>\n+struct variant_helper<T, Types...>\n+{\n+    VARIANT_INLINE static void destroy(const type_index_t type_index, void* data)\n+    {\n+        if (type_index == sizeof...(Types))\n+        {\n+            reinterpret_cast<T*>(data)->~T();\n+        }\n+        else\n+        {\n+            variant_helper<Types...>::destroy(type_index, data);\n+        }\n+    }\n+\n+    VARIANT_INLINE static void move(const type_index_t old_type_index, void* old_value, void* new_value)\n+    {\n+        if (old_type_index == sizeof...(Types))\n+        {\n+            new (new_value) T(std::move(*reinterpret_cast<T*>(old_value)));\n+        }\n+        else\n+        {\n+            variant_helper<Types...>::move(old_type_index, old_value, new_value);\n+        }\n+    }\n+\n+    VARIANT_INLINE static void copy(const type_index_t old_type_index, const void* old_value, void* new_value)\n+    {\n+        if (old_type_index == sizeof...(Types))\n+        {\n+            new (new_value) T(*reinterpret_cast<const T*>(old_value));\n+        }\n+        else\n+        {\n+            variant_helper<Types...>::copy(old_type_index, old_value, new_value);\n+        }\n+    }\n+};\n+\n+template <>\n+struct variant_helper<>\n+{\n+    VARIANT_INLINE static void destroy(const type_index_t, void*) {}\n+    VARIANT_INLINE static void move(const type_index_t, void*, void*) {}\n+    VARIANT_INLINE static void copy(const type_index_t, const void*, void*) {}\n+};\n+\n+template <typename T>\n+struct unwrapper\n+{\n+    static T const& apply_const(T const& obj) { return obj; }\n+    static T& apply(T& obj) { return obj; }\n+};\n+\n+template <typename T>\n+struct unwrapper<recursive_wrapper<T>>\n+{\n+    static auto apply_const(recursive_wrapper<T> const& obj)\n+        -> typename recursive_wrapper<T>::type const&\n+    {\n+        return obj.get();\n+    }\n+    static auto apply(recursive_wrapper<T>& obj)\n+        -> typename recursive_wrapper<T>::type&\n+    {\n+        return obj.get();\n+    }\n+};\n+\n+template <typename T>\n+struct unwrapper<std::reference_wrapper<T>>\n+{\n+    static auto apply_const(std::reference_wrapper<T> const& obj)\n+        -> typename std::reference_wrapper<T>::type const&\n+    {\n+        return obj.get();\n+    }\n+    static auto apply(std::reference_wrapper<T>& obj)\n+        -> typename std::reference_wrapper<T>::type&\n+    {\n+        return obj.get();\n+    }\n+};\n+\n+template <typename F, typename V, typename R, typename... Types>\n+struct dispatcher;\n+\n+template <typename F, typename V, typename R, typename T, typename... Types>\n+struct dispatcher<F, V, R, T, Types...>\n+{\n+    VARIANT_INLINE static R apply_const(V const& v, F&& f)\n+    {\n+        if (v.template is<T>())\n+        {\n+            return f(unwrapper<T>::apply_const(v.template get_unchecked<T>()));\n+        }\n+        else\n+        {\n+            return dispatcher<F, V, R, Types...>::apply_const(v, std::forward<F>(f));\n+        }\n+    }\n+\n+    VARIANT_INLINE static R apply(V& v, F&& f)\n+    {\n+        if (v.template is<T>())\n+        {\n+            return f(unwrapper<T>::apply(v.template get_unchecked<T>()));\n+        }\n+        else\n+        {\n+            return dispatcher<F, V, R, Types...>::apply(v, std::forward<F>(f));\n+        }\n+    }\n+};\n+\n+template <typename F, typename V, typename R, typename T>\n+struct dispatcher<F, V, R, T>\n+{\n+    VARIANT_INLINE static R apply_const(V const& v, F&& f)\n+    {\n+        return f(unwrapper<T>::apply_const(v.template get_unchecked<T>()));\n+    }\n+\n+    VARIANT_INLINE static R apply(V& v, F&& f)\n+    {\n+        return f(unwrapper<T>::apply(v.template get_unchecked<T>()));\n+    }\n+};\n+\n+template <typename F, typename V, typename R, typename T, typename... Types>\n+struct binary_dispatcher_rhs;\n+\n+template <typename F, typename V, typename R, typename T0, typename T1, typename... Types>\n+struct binary_dispatcher_rhs<F, V, R, T0, T1, Types...>\n+{\n+    VARIANT_INLINE static R apply_const(V const& lhs, V const& rhs, F&& f)\n+    {\n+        if (rhs.template is<T1>()) // call binary functor\n+        {\n+            return f(unwrapper<T0>::apply_const(lhs.template get_unchecked<T0>()),\n+                     unwrapper<T1>::apply_const(rhs.template get_unchecked<T1>()));\n+        }\n+        else\n+        {\n+            return binary_dispatcher_rhs<F, V, R, T0, Types...>::apply_const(lhs, rhs, std::forward<F>(f));\n+        }\n+    }\n+\n+    VARIANT_INLINE static R apply(V& lhs, V& rhs, F&& f)\n+    {\n+        if (rhs.template is<T1>()) // call binary functor\n+        {\n+            return f(unwrapper<T0>::apply(lhs.template get_unchecked<T0>()),\n+                     unwrapper<T1>::apply(rhs.template get_unchecked<T1>()));\n+        }\n+        else\n+        {\n+            return binary_dispatcher_rhs<F, V, R, T0, Types...>::apply(lhs, rhs, std::forward<F>(f));\n+        }\n+    }\n+};\n+\n+template <typename F, typename V, typename R, typename T0, typename T1>\n+struct binary_dispatcher_rhs<F, V, R, T0, T1>\n+{\n+    VARIANT_INLINE static R apply_const(V const& lhs, V const& rhs, F&& f)\n+    {\n+        return f(unwrapper<T0>::apply_const(lhs.template get_unchecked<T0>()),\n+                 unwrapper<T1>::apply_const(rhs.template get_unchecked<T1>()));\n+    }\n+\n+    VARIANT_INLINE static R apply(V& lhs, V& rhs, F&& f)\n+    {\n+        return f(unwrapper<T0>::apply(lhs.template get_unchecked<T0>()),\n+                 unwrapper<T1>::apply(rhs.template get_unchecked<T1>()));\n+    }\n+};\n+\n+template <typename F, typename V, typename R, typename T, typename... Types>\n+struct binary_dispatcher_lhs;\n+\n+template <typename F, typename V, typename R, typename T0, typename T1, typename... Types>\n+struct binary_dispatcher_lhs<F, V, R, T0, T1, Types...>\n+{\n+    VARIANT_INLINE static R apply_const(V const& lhs, V const& rhs, F&& f)\n+    {\n+        if (lhs.template is<T1>()) // call binary functor\n+        {\n+            return f(unwrapper<T1>::apply_const(lhs.template get_unchecked<T1>()),\n+                     unwrapper<T0>::apply_const(rhs.template get_unchecked<T0>()));\n+        }\n+        else\n+        {\n+            return binary_dispatcher_lhs<F, V, R, T0, Types...>::apply_const(lhs, rhs, std::forward<F>(f));\n+        }\n+    }\n+\n+    VARIANT_INLINE static R apply(V& lhs, V& rhs, F&& f)\n+    {\n+        if (lhs.template is<T1>()) // call binary functor\n+        {\n+            return f(unwrapper<T1>::apply(lhs.template get_unchecked<T1>()),\n+                     unwrapper<T0>::apply(rhs.template get_unchecked<T0>()));\n+        }\n+        else\n+        {\n+            return binary_dispatcher_lhs<F, V, R, T0, Types...>::apply(lhs, rhs, std::forward<F>(f));\n+        }\n+    }\n+};\n+\n+template <typename F, typename V, typename R, typename T0, typename T1>\n+struct binary_dispatcher_lhs<F, V, R, T0, T1>\n+{\n+    VARIANT_INLINE static R apply_const(V const& lhs, V const& rhs, F&& f)\n+    {\n+        return f(unwrapper<T1>::apply_const(lhs.template get_unchecked<T1>()),\n+                 unwrapper<T0>::apply_const(rhs.template get_unchecked<T0>()));\n+    }\n+\n+    VARIANT_INLINE static R apply(V& lhs, V& rhs, F&& f)\n+    {\n+        return f(unwrapper<T1>::apply(lhs.template get_unchecked<T1>()),\n+                 unwrapper<T0>::apply(rhs.template get_unchecked<T0>()));\n+    }\n+};\n+\n+template <typename F, typename V, typename R, typename... Types>\n+struct binary_dispatcher;\n+\n+template <typename F, typename V, typename R, typename T, typename... Types>\n+struct binary_dispatcher<F, V, R, T, Types...>\n+{\n+    VARIANT_INLINE static R apply_const(V const& v0, V const& v1, F&& f)\n+    {\n+        if (v0.template is<T>())\n+        {\n+            if (v1.template is<T>())\n+            {\n+                return f(unwrapper<T>::apply_const(v0.template get_unchecked<T>()),\n+                         unwrapper<T>::apply_const(v1.template get_unchecked<T>())); // call binary functor\n+            }\n+            else\n+            {\n+                return binary_dispatcher_rhs<F, V, R, T, Types...>::apply_const(v0, v1, std::forward<F>(f));\n+            }\n+        }\n+        else if (v1.template is<T>())\n+        {\n+            return binary_dispatcher_lhs<F, V, R, T, Types...>::apply_const(v0, v1, std::forward<F>(f));\n+        }\n+        return binary_dispatcher<F, V, R, Types...>::apply_const(v0, v1, std::forward<F>(f));\n+    }\n+\n+    VARIANT_INLINE static R apply(V& v0, V& v1, F&& f)\n+    {\n+        if (v0.template is<T>())\n+        {\n+            if (v1.template is<T>())\n+            {\n+                return f(unwrapper<T>::apply(v0.template get_unchecked<T>()),\n+                         unwrapper<T>::apply(v1.template get_unchecked<T>())); // call binary functor\n+            }\n+            else\n+            {\n+                return binary_dispatcher_rhs<F, V, R, T, Types...>::apply(v0, v1, std::forward<F>(f));\n+            }\n+        }\n+        else if (v1.template is<T>())\n+        {\n+            return binary_dispatcher_lhs<F, V, R, T, Types...>::apply(v0, v1, std::forward<F>(f));\n+        }\n+        return binary_dispatcher<F, V, R, Types...>::apply(v0, v1, std::forward<F>(f));\n+    }\n+};\n+\n+template <typename F, typename V, typename R, typename T>\n+struct binary_dispatcher<F, V, R, T>\n+{\n+    VARIANT_INLINE static R apply_const(V const& v0, V const& v1, F&& f)\n+    {\n+        return f(unwrapper<T>::apply_const(v0.template get_unchecked<T>()),\n+                 unwrapper<T>::apply_const(v1.template get_unchecked<T>())); // call binary functor\n+    }\n+\n+    VARIANT_INLINE static R apply(V& v0, V& v1, F&& f)\n+    {\n+        return f(unwrapper<T>::apply(v0.template get_unchecked<T>()),\n+                 unwrapper<T>::apply(v1.template get_unchecked<T>())); // call binary functor\n+    }\n+};\n+\n+// comparator functors\n+struct equal_comp\n+{\n+    template <typename T>\n+    bool operator()(T const& lhs, T const& rhs) const\n+    {\n+        return lhs == rhs;\n+    }\n+};\n+\n+struct less_comp\n+{\n+    template <typename T>\n+    bool operator()(T const& lhs, T const& rhs) const\n+    {\n+        return lhs < rhs;\n+    }\n+};\n+\n+template <typename Variant, typename Comp>\n+class comparer\n+{\n+public:\n+    explicit comparer(Variant const& lhs) noexcept\n+        : lhs_(lhs) {}\n+    comparer& operator=(comparer const&) = delete;\n+    // visitor\n+    template <typename T>\n+    bool operator()(T const& rhs_content) const\n+    {\n+        T const& lhs_content = lhs_.template get_unchecked<T>();\n+        return Comp()(lhs_content, rhs_content);\n+    }\n+\n+private:\n+    Variant const& lhs_;\n+};\n+\n+// hashing visitor\n+struct hasher\n+{\n+    template <typename T>\n+    std::size_t operator()(const T& hashable) const\n+    {\n+        return std::hash<T>{}(hashable);\n+    }\n+};\n+\n+} // namespace detail\n+\n+struct no_init {};\n+\n+template <typename... Types>\n+class variant\n+{\n+    static_assert(sizeof...(Types) > 0, \"Template parameter type list of variant can not be empty.\");\n+    static_assert(!detail::disjunction<std::is_reference<Types>...>::value, \"Variant can not hold reference types. Maybe use std::reference_wrapper?\");\n+    static_assert(!detail::disjunction<std::is_array<Types>...>::value, \"Variant can not hold array types.\");\n+    static_assert(sizeof...(Types) < std::numeric_limits<type_index_t>::max(), \"Internal index type must be able to accommodate all alternatives.\");\n+private:\n+    static const std::size_t data_size = detail::static_max<sizeof(Types)...>::value;\n+    static const std::size_t data_align = detail::static_max<alignof(Types)...>::value;\n+public:\n+    struct adapted_variant_tag;\n+    using types = std::tuple<Types...>;\n+private:\n+    using first_type = typename std::tuple_element<0, types>::type;\n+    using data_type = typename std::aligned_storage<data_size, data_align>::type;\n+    using helper_type = detail::variant_helper<Types...>;\n+\n+    type_index_t type_index;\n+    data_type data;\n+\n+public:\n+    VARIANT_INLINE variant() noexcept(std::is_nothrow_default_constructible<first_type>::value)\n+        : type_index(sizeof...(Types)-1)\n+    {\n+        static_assert(std::is_default_constructible<first_type>::value, \"First type in variant must be default constructible to allow default construction of variant.\");\n+        new (&data) first_type();\n+    }\n+\n+    VARIANT_INLINE variant(no_init) noexcept\n+        : type_index(detail::invalid_value) {}\n+\n+    // http://isocpp.org/blog/2012/11/universal-references-in-c11-scott-meyers\n+    template <typename T, typename Traits = detail::value_traits<T, Types...>,\n+              typename Enable = typename std::enable_if<Traits::is_valid && !std::is_same<variant<Types...>, typename Traits::value_type>::value>::type >\n+    VARIANT_INLINE variant(T&& val) noexcept(std::is_nothrow_constructible<typename Traits::target_type, T&&>::value)\n+        : type_index(Traits::index)\n+    {\n+        new (&data) typename Traits::target_type(std::forward<T>(val));\n+    }\n+\n+    VARIANT_INLINE variant(variant<Types...> const& old)\n+        : type_index(old.type_index)\n+    {\n+        helper_type::copy(old.type_index, &old.data, &data);\n+    }\n+\n+    VARIANT_INLINE variant(variant<Types...>&& old)\n+        noexcept(detail::conjunction<std::is_nothrow_move_constructible<Types>...>::value)\n+        : type_index(old.type_index)\n+    {\n+        helper_type::move(old.type_index, &old.data, &data);\n+    }\n+\n+private:\n+    VARIANT_INLINE void copy_assign(variant<Types...> const& rhs)\n+    {\n+        helper_type::destroy(type_index, &data);\n+        type_index = detail::invalid_value;\n+        helper_type::copy(rhs.type_index, &rhs.data, &data);\n+        type_index = rhs.type_index;\n+    }\n+\n+    VARIANT_INLINE void move_assign(variant<Types...>&& rhs)\n+    {\n+        helper_type::destroy(type_index, &data);\n+        type_index = detail::invalid_value;\n+        helper_type::move(rhs.type_index, &rhs.data, &data);\n+        type_index = rhs.type_index;\n+    }\n+\n+public:\n+    VARIANT_INLINE variant<Types...>& operator=(variant<Types...>&& other)\n+    {\n+        move_assign(std::move(other));\n+        return *this;\n+    }\n+\n+    VARIANT_INLINE variant<Types...>& operator=(variant<Types...> const& other)\n+    {\n+        copy_assign(other);\n+        return *this;\n+    }\n+\n+    // conversions\n+    // move-assign\n+    template <typename T>\n+    VARIANT_INLINE variant<Types...>& operator=(T&& rhs) noexcept\n+    {\n+        variant<Types...> temp(std::forward<T>(rhs));\n+        move_assign(std::move(temp));\n+        return *this;\n+    }\n+\n+    // copy-assign\n+    template <typename T>\n+    VARIANT_INLINE variant<Types...>& operator=(T const& rhs)\n+    {\n+        variant<Types...> temp(rhs);\n+        copy_assign(temp);\n+        return *this;\n+    }\n+\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<T, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE bool is() const\n+    {\n+        return type_index == detail::direct_type<T, Types...>::index;\n+    }\n+\n+    template <typename T,typename std::enable_if<\n+                         (detail::direct_type<recursive_wrapper<T>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE bool is() const\n+    {\n+        return type_index == detail::direct_type<recursive_wrapper<T>, Types...>::index;\n+    }\n+\n+    VARIANT_INLINE bool valid() const\n+    {\n+        return type_index != detail::invalid_value;\n+    }\n+\n+    template <typename T, typename... Args>\n+    VARIANT_INLINE void set(Args&&... args)\n+    {\n+        helper_type::destroy(type_index, &data);\n+        type_index = detail::invalid_value;\n+        new (&data) T(std::forward<Args>(args)...);\n+        type_index = detail::direct_type<T, Types...>::index;\n+    }\n+\n+    // get_unchecked<T>()\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<T, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T& get_unchecked()\n+    {\n+        return *reinterpret_cast<T*>(&data);\n+    }\n+\n+#ifdef HAS_EXCEPTIONS\n+    // get<T>()\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<T, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T& get()\n+    {\n+        if (type_index == detail::direct_type<T, Types...>::index)\n+        {\n+            return *reinterpret_cast<T*>(&data);\n+        }\n+        else\n+        {\n+            throw bad_variant_access(\"in get<T>()\");\n+        }\n+    }\n+#endif\n+\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<T, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T const& get_unchecked() const\n+    {\n+        return *reinterpret_cast<T const*>(&data);\n+    }\n+\n+#ifdef HAS_EXCEPTIONS\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<T, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T const& get() const\n+    {\n+        if (type_index == detail::direct_type<T, Types...>::index)\n+        {\n+            return *reinterpret_cast<T const*>(&data);\n+        }\n+        else\n+        {\n+            throw bad_variant_access(\"in get<T>()\");\n+        }\n+    }\n+#endif\n+\n+    // get_unchecked<T>() - T stored as recursive_wrapper<T>\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<recursive_wrapper<T>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T& get_unchecked()\n+    {\n+        return (*reinterpret_cast<recursive_wrapper<T>*>(&data)).get();\n+    }\n+\n+#ifdef HAS_EXCEPTIONS\n+    // get<T>() - T stored as recursive_wrapper<T>\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<recursive_wrapper<T>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T& get()\n+    {\n+        if (type_index == detail::direct_type<recursive_wrapper<T>, Types...>::index)\n+        {\n+            return (*reinterpret_cast<recursive_wrapper<T>*>(&data)).get();\n+        }\n+        else\n+        {\n+            throw bad_variant_access(\"in get<T>()\");\n+        }\n+    }\n+#endif\n+\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<recursive_wrapper<T>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T const& get_unchecked() const\n+    {\n+        return (*reinterpret_cast<recursive_wrapper<T> const*>(&data)).get();\n+    }\n+\n+#ifdef HAS_EXCEPTIONS\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<recursive_wrapper<T>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T const& get() const\n+    {\n+        if (type_index == detail::direct_type<recursive_wrapper<T>, Types...>::index)\n+        {\n+            return (*reinterpret_cast<recursive_wrapper<T> const*>(&data)).get();\n+        }\n+        else\n+        {\n+            throw bad_variant_access(\"in get<T>()\");\n+        }\n+    }\n+#endif\n+\n+    // get_unchecked<T>() - T stored as std::reference_wrapper<T>\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<std::reference_wrapper<T>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T& get_unchecked()\n+    {\n+        return (*reinterpret_cast<std::reference_wrapper<T>*>(&data)).get();\n+    }\n+\n+#ifdef HAS_EXCEPTIONS\n+    // get<T>() - T stored as std::reference_wrapper<T>\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<std::reference_wrapper<T>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T& get()\n+    {\n+        if (type_index == detail::direct_type<std::reference_wrapper<T>, Types...>::index)\n+        {\n+            return (*reinterpret_cast<std::reference_wrapper<T>*>(&data)).get();\n+        }\n+        else\n+        {\n+            throw bad_variant_access(\"in get<T>()\");\n+        }\n+    }\n+#endif\n+\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<std::reference_wrapper<T const>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T const& get_unchecked() const\n+    {\n+        return (*reinterpret_cast<std::reference_wrapper<T const> const*>(&data)).get();\n+    }\n+\n+#ifdef HAS_EXCEPTIONS\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<std::reference_wrapper<T const>, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE T const& get() const\n+    {\n+        if (type_index == detail::direct_type<std::reference_wrapper<T const>, Types...>::index)\n+        {\n+            return (*reinterpret_cast<std::reference_wrapper<T const> const*>(&data)).get();\n+        }\n+        else\n+        {\n+            throw bad_variant_access(\"in get<T>()\");\n+        }\n+    }\n+#endif\n+\n+    // This function is deprecated because it returns an internal index field.\n+    // Use which() instead.\n+    ARROW_VARIANT_DEPRECATED VARIANT_INLINE type_index_t get_type_index() const\n+    {\n+        return type_index;\n+    }\n+\n+    VARIANT_INLINE int which() const noexcept\n+    {\n+        return static_cast<int>(sizeof...(Types) - type_index - 1);\n+    }\n+\n+    template <typename T, typename std::enable_if<\n+                          (detail::direct_type<T, Types...>::index != detail::invalid_value)>::type* = nullptr>\n+    VARIANT_INLINE static constexpr int which() noexcept\n+    {\n+        return static_cast<int>(sizeof...(Types)-detail::direct_type<T, Types...>::index - 1);\n+    }\n+\n+    // visitor\n+    // unary\n+    template <typename F, typename V, typename R = typename detail::result_of_unary_visit<F, first_type>::type>\n+    auto VARIANT_INLINE static visit(V const& v, F&& f)\n+        -> decltype(detail::dispatcher<F, V, R, Types...>::apply_const(v, std::forward<F>(f)))\n+    {\n+        return detail::dispatcher<F, V, R, Types...>::apply_const(v, std::forward<F>(f));\n+    }\n+    // non-const\n+    template <typename F, typename V, typename R = typename detail::result_of_unary_visit<F, first_type>::type>\n+    auto VARIANT_INLINE static visit(V& v, F&& f)\n+        -> decltype(detail::dispatcher<F, V, R, Types...>::apply(v, std::forward<F>(f)))\n+    {\n+        return detail::dispatcher<F, V, R, Types...>::apply(v, std::forward<F>(f));\n+    }\n+\n+    // binary\n+    // const\n+    template <typename F, typename V, typename R = typename detail::result_of_binary_visit<F, first_type>::type>\n+    auto VARIANT_INLINE static binary_visit(V const& v0, V const& v1, F&& f)\n+        -> decltype(detail::binary_dispatcher<F, V, R, Types...>::apply_const(v0, v1, std::forward<F>(f)))\n+    {\n+        return detail::binary_dispatcher<F, V, R, Types...>::apply_const(v0, v1, std::forward<F>(f));\n+    }\n+    // non-const\n+    template <typename F, typename V, typename R = typename detail::result_of_binary_visit<F, first_type>::type>\n+    auto VARIANT_INLINE static binary_visit(V& v0, V& v1, F&& f)\n+        -> decltype(detail::binary_dispatcher<F, V, R, Types...>::apply(v0, v1, std::forward<F>(f)))\n+    {\n+        return detail::binary_dispatcher<F, V, R, Types...>::apply(v0, v1, std::forward<F>(f));\n+    }\n+\n+    // match\n+    // unary\n+    template <typename... Fs>\n+    auto VARIANT_INLINE match(Fs&&... fs) const\n+        -> decltype(variant::visit(*this, ::arrow::util::make_visitor(std::forward<Fs>(fs)...)))\n+    {\n+        return variant::visit(*this, ::arrow::util::make_visitor(std::forward<Fs>(fs)...));\n+    }\n+    // non-const\n+    template <typename... Fs>\n+    auto VARIANT_INLINE match(Fs&&... fs)\n+        -> decltype(variant::visit(*this, ::arrow::util::make_visitor(std::forward<Fs>(fs)...)))\n+    {\n+        return variant::visit(*this, ::arrow::util::make_visitor(std::forward<Fs>(fs)...));\n+    }\n+\n+    ~variant() noexcept // no-throw destructor\n+    {\n+        helper_type::destroy(type_index, &data);\n+    }\n+\n+    // comparison operators\n+    // equality\n+    VARIANT_INLINE bool operator==(variant const& rhs) const\n+    {\n+        assert(valid() && rhs.valid());\n+        if (this->which() != rhs.which())\n+        {\n+            return false;\n+        }\n+        detail::comparer<variant, detail::equal_comp> visitor(*this);\n+        return visit(rhs, visitor);\n+    }\n+\n+    VARIANT_INLINE bool operator!=(variant const& rhs) const\n+    {\n+        return !(*this == rhs);\n+    }\n+\n+    // less than\n+    VARIANT_INLINE bool operator<(variant const& rhs) const\n+    {\n+        assert(valid() && rhs.valid());\n+        if (this->which() != rhs.which())\n+        {\n+            return this->which() < rhs.which();\n+        }\n+        detail::comparer<variant, detail::less_comp> visitor(*this);\n+        return visit(rhs, visitor);\n+    }\n+    VARIANT_INLINE bool operator>(variant const& rhs) const\n+    {\n+        return rhs < *this;\n+    }\n+    VARIANT_INLINE bool operator<=(variant const& rhs) const\n+    {\n+        return !(*this > rhs);\n+    }\n+    VARIANT_INLINE bool operator>=(variant const& rhs) const\n+    {\n+        return !(*this < rhs);\n+    }\n+};\n+\n+// unary visitor interface\n+// const\n+template <typename F, typename V>\n+auto VARIANT_INLINE apply_visitor(F&& f, V const& v) -> decltype(V::visit(v, std::forward<F>(f)))\n+{\n+    return V::visit(v, std::forward<F>(f));\n+}\n+\n+// non-const\n+template <typename F, typename V>\n+auto VARIANT_INLINE apply_visitor(F&& f, V& v) -> decltype(V::visit(v, std::forward<F>(f)))\n+{\n+    return V::visit(v, std::forward<F>(f));\n+}\n+\n+// binary visitor interface\n+// const\n+template <typename F, typename V>\n+auto VARIANT_INLINE apply_visitor(F&& f, V const& v0, V const& v1) -> decltype(V::binary_visit(v0, v1, std::forward<F>(f)))\n+{\n+    return V::binary_visit(v0, v1, std::forward<F>(f));\n+}\n+\n+// non-const\n+template <typename F, typename V>\n+auto VARIANT_INLINE apply_visitor(F&& f, V& v0, V& v1) -> decltype(V::binary_visit(v0, v1, std::forward<F>(f)))\n+{\n+    return V::binary_visit(v0, v1, std::forward<F>(f));\n+}\n+\n+// getter interface\n+\n+#ifdef HAS_EXCEPTIONS\n+template <typename ResultType, typename T>\n+auto get(T& var)->decltype(var.template get<ResultType>())\n+{\n+    return var.template get<ResultType>();\n+}\n+#endif\n+\n+template <typename ResultType, typename T>\n+ResultType& get_unchecked(T& var)\n+{\n+    return var.template get_unchecked<ResultType>();\n+}\n+\n+#ifdef HAS_EXCEPTIONS\n+template <typename ResultType, typename T>\n+auto get(T const& var)->decltype(var.template get<ResultType>())\n+{\n+    return var.template get<ResultType>();\n+}\n+#endif\n+\n+template <typename ResultType, typename T>\n+ResultType const& get_unchecked(T const& var)\n+{\n+    return var.template get_unchecked<ResultType>();\n+}\n+// variant_size\n+template <typename T>\n+struct variant_size;\n+\n+//variable templates is c++14\n+//template <typename T>\n+//constexpr std::size_t variant_size_v = variant_size<T>::value;\n+\n+template <typename T>\n+struct variant_size<const T>\n+    : variant_size<T> {};\n+\n+template <typename T>\n+struct variant_size<volatile T>\n+    : variant_size<T> {};\n+\n+template <typename T>\n+struct variant_size<const volatile T>\n+    : variant_size<T> {};\n+\n+template <typename... Types>\n+struct variant_size<variant<Types...>>\n+    : std::integral_constant<std::size_t, sizeof...(Types)> {};\n+\n+// variant_alternative\n+template <std::size_t Index, typename T>\n+struct variant_alternative;\n+\n+#if defined(__clang__)\n+#if __has_builtin(__type_pack_element)\n+#define has_type_pack_element\n+#endif\n+#endif\n+\n+#if defined(has_type_pack_element)\n+template <std::size_t Index, typename ...Types>\n+struct variant_alternative<Index, variant<Types...>>\n+{\n+    static_assert(sizeof...(Types) > Index , \"Index out of range\");\n+    using type = __type_pack_element<Index, Types...>;\n+};\n+#else\n+template <std::size_t Index, typename First, typename...Types>\n+struct variant_alternative<Index, variant<First, Types...>>\n+    : variant_alternative<Index - 1, variant<Types...>>\n+{\n+    static_assert(sizeof...(Types) > Index -1 , \"Index out of range\");\n+};\n+\n+template <typename First, typename...Types>\n+struct variant_alternative<0, variant<First, Types...>>\n+{\n+    using type = First;\n+};\n+\n+#endif\n+\n+template <size_t Index, typename T>\n+using variant_alternative_t = typename variant_alternative<Index, T>::type;\n+\n+template <size_t Index, typename T>\n+struct variant_alternative<Index, const T>\n+    : std::add_const<variant_alternative<Index, T>> {};\n+\n+template <size_t Index, typename T>\n+struct variant_alternative<Index, volatile T>\n+    : std::add_volatile<variant_alternative<Index, T>> {};\n+\n+template <size_t Index, typename T>\n+struct variant_alternative<Index, const volatile T>\n+    : std::add_cv<variant_alternative<Index, T>> {};\n+\n+} // namespace util\n+} // namespace arrow\n+\n+#endif // ARROW_UTIL_VARIANT_H\ndiff --git a/cpp/src/arrow/util/variant/CMakeLists.txt b/cpp/src/arrow/util/variant/CMakeLists.txt\nnew file mode 100644\nindex 000000000..0ebb25162\n--- /dev/null\n+++ b/cpp/src/arrow/util/variant/CMakeLists.txt\n@@ -0,0 +1,28 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+#######################################\n+# arrow_util_variant\n+#######################################\n+\n+install(FILES\n+  optional.h\n+  recursive_wrapper.h\n+  variant_cast.h\n+  variant_io.h\n+  variant_visitor.h\n+  DESTINATION include/arrow/util/variant)\ndiff --git a/cpp/src/arrow/util/variant/optional.h b/cpp/src/arrow/util/variant/optional.h\nnew file mode 100644\nindex 000000000..4c6671061\n--- /dev/null\n+++ b/cpp/src/arrow/util/variant/optional.h\n@@ -0,0 +1,100 @@\n+// Copyright (c) MapBox\n+// All rights reserved.\n+//\n+// Redistribution and use in source and binary forms, with or without modification,\n+// are permitted provided that the following conditions are met:\n+//\n+// - Redistributions of source code must retain the above copyright notice, this\n+//   list of conditions and the following disclaimer.\n+// - Redistributions in binary form must reproduce the above copyright notice, this\n+//   list of conditions and the following disclaimer in the documentation and/or\n+//   other materials provided with the distribution.\n+// - Neither the name \"MapBox\" nor the names of its contributors may be\n+//   used to endorse or promote products derived from this software without\n+//   specific prior written permission.\n+//\n+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n+// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n+// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n+// ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+\n+#ifndef ARROW_UTIL_VARIANT_OPTIONAL_H\n+#define ARROW_UTIL_VARIANT_OPTIONAL_H\n+\n+#pragma message(\"This implementation of optional is deprecated. See https://github.com/mapbox/variant/issues/64.\")\n+\n+#include <type_traits>\n+#include <utility>\n+\n+#include <arrow/util/variant.h>\n+\n+namespace arrow {\n+namespace util {\n+\n+template <typename T>\n+class optional\n+{\n+    static_assert(!std::is_reference<T>::value, \"optional doesn't support references\");\n+\n+    struct none_type\n+    {\n+    };\n+\n+    variant<none_type, T> variant_;\n+\n+public:\n+    optional() = default;\n+\n+    optional(optional const& rhs)\n+    {\n+        if (this != &rhs)\n+        { // protect against invalid self-assignment\n+            variant_ = rhs.variant_;\n+        }\n+    }\n+\n+    optional(T const& v) { variant_ = v; }\n+\n+    explicit operator bool() const noexcept { return variant_.template is<T>(); }\n+\n+    T const& get() const { return variant_.template get<T>(); }\n+    T& get() { return variant_.template get<T>(); }\n+\n+    T const& operator*() const { return this->get(); }\n+    T operator*() { return this->get(); }\n+\n+    optional& operator=(T const& v)\n+    {\n+        variant_ = v;\n+        return *this;\n+    }\n+\n+    optional& operator=(optional const& rhs)\n+    {\n+        if (this != &rhs)\n+        {\n+            variant_ = rhs.variant_;\n+        }\n+        return *this;\n+    }\n+\n+    template <typename... Args>\n+    void emplace(Args&&... args)\n+    {\n+        variant_ = T{std::forward<Args>(args)...};\n+    }\n+\n+    void reset() { variant_ = none_type{}; }\n+\n+}; // class optional\n+\n+} // namespace util\n+} // namespace arrow\n+\n+#endif // ARROW_UTIL_VARIANT_OPTIONAL_H\ndiff --git a/cpp/src/arrow/util/variant/recursive_wrapper.h b/cpp/src/arrow/util/variant/recursive_wrapper.h\nnew file mode 100644\nindex 000000000..c9d938539\n--- /dev/null\n+++ b/cpp/src/arrow/util/variant/recursive_wrapper.h\n@@ -0,0 +1,122 @@\n+#ifndef ARROW_UTIL_VARIANT_RECURSIVE_WRAPPER_H\n+#define ARROW_UTIL_VARIANT_RECURSIVE_WRAPPER_H\n+\n+// Based on variant/recursive_wrapper.h from boost.\n+//\n+// Original license:\n+//\n+// Copyright (c) 2002-2003\n+// Eric Friedman, Itay Maman\n+//\n+// Distributed under the Boost Software License, Version 1.0. (See\n+// accompanying file LICENSE_1_0.txt or copy at\n+// http://www.boost.org/LICENSE_1_0.txt)\n+\n+#include <cassert>\n+#include <utility>\n+\n+namespace arrow {\n+namespace util {\n+\n+template <typename T>\n+class recursive_wrapper\n+{\n+\n+    T* p_;\n+\n+    void assign(T const& rhs)\n+    {\n+        this->get() = rhs;\n+    }\n+\n+public:\n+    using type = T;\n+\n+    /**\n+     * Default constructor default initializes the internally stored value.\n+     * For POD types this means nothing is done and the storage is\n+     * uninitialized.\n+     *\n+     * @throws std::bad_alloc if there is insufficient memory for an object\n+     *         of type T.\n+     * @throws any exception thrown by the default constructur of T.\n+     */\n+    recursive_wrapper()\n+        : p_(new T){}\n+\n+    ~recursive_wrapper() noexcept { delete p_; }\n+\n+    recursive_wrapper(recursive_wrapper const& operand)\n+        : p_(new T(operand.get())) {}\n+\n+    recursive_wrapper(T const& operand)\n+        : p_(new T(operand)) {}\n+\n+    recursive_wrapper(recursive_wrapper&& operand)\n+        : p_(new T(std::move(operand.get()))) {}\n+\n+    recursive_wrapper(T&& operand)\n+        : p_(new T(std::move(operand))) {}\n+\n+    inline recursive_wrapper& operator=(recursive_wrapper const& rhs)\n+    {\n+        assign(rhs.get());\n+        return *this;\n+    }\n+\n+    inline recursive_wrapper& operator=(T const& rhs)\n+    {\n+        assign(rhs);\n+        return *this;\n+    }\n+\n+    inline void swap(recursive_wrapper& operand) noexcept\n+    {\n+        T* temp = operand.p_;\n+        operand.p_ = p_;\n+        p_ = temp;\n+    }\n+\n+    recursive_wrapper& operator=(recursive_wrapper&& rhs) noexcept\n+    {\n+        swap(rhs);\n+        return *this;\n+    }\n+\n+    recursive_wrapper& operator=(T&& rhs)\n+    {\n+        get() = std::move(rhs);\n+        return *this;\n+    }\n+\n+    T& get()\n+    {\n+        assert(p_);\n+        return *get_pointer();\n+    }\n+\n+    T const& get() const\n+    {\n+        assert(p_);\n+        return *get_pointer();\n+    }\n+\n+    T* get_pointer() { return p_; }\n+\n+    const T* get_pointer() const { return p_; }\n+\n+    operator T const&() const { return this->get(); }\n+\n+    operator T&() { return this->get(); }\n+\n+}; // class recursive_wrapper\n+\n+template <typename T>\n+inline void swap(recursive_wrapper<T>& lhs, recursive_wrapper<T>& rhs) noexcept\n+{\n+    lhs.swap(rhs);\n+}\n+} // namespace util\n+} // namespace arrow\n+\n+#endif // ARROW_UTIL_VARIANT_RECURSIVE_WRAPPER_H\ndiff --git a/cpp/src/arrow/util/variant/variant_cast.h b/cpp/src/arrow/util/variant/variant_cast.h\nnew file mode 100644\nindex 000000000..558f1d9a6\n--- /dev/null\n+++ b/cpp/src/arrow/util/variant/variant_cast.h\n@@ -0,0 +1,112 @@\n+// Copyright (c) MapBox\n+// All rights reserved.\n+//\n+// Redistribution and use in source and binary forms, with or without modification,\n+// are permitted provided that the following conditions are met:\n+//\n+// - Redistributions of source code must retain the above copyright notice, this\n+//   list of conditions and the following disclaimer.\n+// - Redistributions in binary form must reproduce the above copyright notice, this\n+//   list of conditions and the following disclaimer in the documentation and/or\n+//   other materials provided with the distribution.\n+// - Neither the name \"MapBox\" nor the names of its contributors may be\n+//   used to endorse or promote products derived from this software without\n+//   specific prior written permission.\n+//\n+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n+// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n+// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n+// ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+\n+#ifndef ARROW_UTIL_VARIANT_CAST_H\n+#define ARROW_UTIL_VARIANT_CAST_H\n+\n+#include <type_traits>\n+\n+namespace arrow {\n+namespace util {\n+\n+namespace detail {\n+\n+template <class T>\n+class static_caster\n+{\n+public:\n+    template <class V>\n+    T& operator()(V& v) const\n+    {\n+        return static_cast<T&>(v);\n+    }\n+};\n+\n+template <class T>\n+class dynamic_caster\n+{\n+public:\n+    using result_type = T&;\n+    template <class V>\n+    T& operator()(V& v, typename std::enable_if<!std::is_polymorphic<V>::value>::type* = nullptr) const\n+    {\n+        throw std::bad_cast();\n+    }\n+    template <class V>\n+    T& operator()(V& v, typename std::enable_if<std::is_polymorphic<V>::value>::type* = nullptr) const\n+    {\n+        return dynamic_cast<T&>(v);\n+    }\n+};\n+\n+template <class T>\n+class dynamic_caster<T*>\n+{\n+public:\n+    using result_type = T*;\n+    template <class V>\n+    T* operator()(V& v, typename std::enable_if<!std::is_polymorphic<V>::value>::type* = nullptr) const\n+    {\n+        return nullptr;\n+    }\n+    template <class V>\n+    T* operator()(V& v, typename std::enable_if<std::is_polymorphic<V>::value>::type* = nullptr) const\n+    {\n+        return dynamic_cast<T*>(&v);\n+    }\n+};\n+}\n+\n+template <class T, class V>\n+typename detail::dynamic_caster<T>::result_type\n+dynamic_variant_cast(V& v)\n+{\n+    return arrow::util::apply_visitor(detail::dynamic_caster<T>(), v);\n+}\n+\n+template <class T, class V>\n+typename detail::dynamic_caster<const T>::result_type\n+dynamic_variant_cast(const V& v)\n+{\n+    return arrow::util::apply_visitor(detail::dynamic_caster<const T>(), v);\n+}\n+\n+template <class T, class V>\n+T& static_variant_cast(V& v)\n+{\n+    return arrow::util::apply_visitor(detail::static_caster<T>(), v);\n+}\n+\n+template <class T, class V>\n+const T& static_variant_cast(const V& v)\n+{\n+    return arrow::util::apply_visitor(detail::static_caster<const T>(), v);\n+}\n+\n+}  // namespace util\n+}  // namespace arrow\n+\n+#endif // ARROW_UTIL_VARIANT_CAST_H\ndiff --git a/cpp/src/arrow/util/variant/variant_io.h b/cpp/src/arrow/util/variant/variant_io.h\nnew file mode 100644\nindex 000000000..5541a81f7\n--- /dev/null\n+++ b/cpp/src/arrow/util/variant/variant_io.h\n@@ -0,0 +1,72 @@\n+// Copyright (c) MapBox\n+// All rights reserved.\n+//\n+// Redistribution and use in source and binary forms, with or without modification,\n+// are permitted provided that the following conditions are met:\n+//\n+// - Redistributions of source code must retain the above copyright notice, this\n+//   list of conditions and the following disclaimer.\n+// - Redistributions in binary form must reproduce the above copyright notice, this\n+//   list of conditions and the following disclaimer in the documentation and/or\n+//   other materials provided with the distribution.\n+// - Neither the name \"MapBox\" nor the names of its contributors may be\n+//   used to endorse or promote products derived from this software without\n+//   specific prior written permission.\n+//\n+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n+// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n+// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n+// ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+\n+#ifndef ARROW_UTIL_VARIANT_IO_H\n+#define ARROW_UTIL_VARIANT_IO_H\n+\n+#include <iosfwd>\n+\n+#include <arrow/util/variant.h>\n+\n+namespace arrow {\n+namespace util {\n+\n+namespace detail {\n+// operator<< helper\n+template <typename Out>\n+class printer\n+{\n+public:\n+    explicit printer(Out& out)\n+        : out_(out) {}\n+    printer& operator=(printer const&) = delete;\n+\n+    // visitor\n+    template <typename T>\n+    void operator()(T const& operand) const\n+    {\n+        out_ << operand;\n+    }\n+\n+private:\n+    Out& out_;\n+};\n+}\n+\n+// operator<<\n+template <typename CharT, typename Traits, typename... Types>\n+VARIANT_INLINE std::basic_ostream<CharT, Traits>&\n+operator<<(std::basic_ostream<CharT, Traits>& out, variant<Types...> const& rhs)\n+{\n+    detail::printer<std::basic_ostream<CharT, Traits>> visitor(out);\n+    apply_visitor(visitor, rhs);\n+    return out;\n+}\n+\n+} // namespace util\n+} // namespace arrow\n+\n+#endif // ARROW_UTIL_VARIANT_IO_H\ndiff --git a/cpp/src/arrow/util/variant/variant_visitor.h b/cpp/src/arrow/util/variant/variant_visitor.h\nnew file mode 100644\nindex 000000000..66b1dfea3\n--- /dev/null\n+++ b/cpp/src/arrow/util/variant/variant_visitor.h\n@@ -0,0 +1,69 @@\n+// Copyright (c) MapBox\n+// All rights reserved.\n+//\n+// Redistribution and use in source and binary forms, with or without modification,\n+// are permitted provided that the following conditions are met:\n+//\n+// - Redistributions of source code must retain the above copyright notice, this\n+//   list of conditions and the following disclaimer.\n+// - Redistributions in binary form must reproduce the above copyright notice, this\n+//   list of conditions and the following disclaimer in the documentation and/or\n+//   other materials provided with the distribution.\n+// - Neither the name \"MapBox\" nor the names of its contributors may be\n+//   used to endorse or promote products derived from this software without\n+//   specific prior written permission.\n+//\n+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n+// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n+// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n+// ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n+// (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n+// LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n+// ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+\n+#ifndef ARROW_UTIL_VARIANT_VISITOR_HPP\n+#define ARROW_UTIL_VARIANT_VISITOR_HPP\n+\n+#include <utility>\n+\n+namespace arrow {\n+namespace util {\n+\n+template <typename... Fns>\n+struct visitor;\n+\n+template <typename Fn>\n+struct visitor<Fn> : Fn\n+{\n+    using Fn::operator();\n+\n+    template<typename T>\n+    visitor(T&& fn) : Fn(std::forward<T>(fn)) {}\n+};\n+\n+template <typename Fn, typename... Fns>\n+struct visitor<Fn, Fns...> : Fn, visitor<Fns...>\n+{\n+    using Fn::operator();\n+    using visitor<Fns...>::operator();\n+\n+    template<typename T, typename... Ts>\n+    visitor(T&& fn, Ts&&... fns)\n+        : Fn(std::forward<T>(fn))\n+        , visitor<Fns...>(std::forward<Ts>(fns)...) {}\n+};\n+\n+template <typename... Fns>\n+visitor<typename std::decay<Fns>::type...> make_visitor(Fns&&... fns)\n+{\n+    return visitor<typename std::decay<Fns>::type...>\n+        (std::forward<Fns>(fns)...);\n+}\n+\n+} // namespace util\n+} // namespace arrow\n+\n+#endif // ARROW_UTIL_VARIANT_VISITOR_HPP\ndiff --git a/cpp/src/arrow/visitor.cc b/cpp/src/arrow/visitor.cc\nindex 3739e89f3..47dba6cd8 100644\n--- a/cpp/src/arrow/visitor.cc\n+++ b/cpp/src/arrow/visitor.cc\n@@ -90,7 +90,7 @@ TYPE_VISITOR_DEFAULT(Time32Type);\n TYPE_VISITOR_DEFAULT(Time64Type);\n TYPE_VISITOR_DEFAULT(TimestampType);\n TYPE_VISITOR_DEFAULT(IntervalType);\n-TYPE_VISITOR_DEFAULT(DecimalType);\n+TYPE_VISITOR_DEFAULT(Decimal128Type);\n TYPE_VISITOR_DEFAULT(ListType);\n TYPE_VISITOR_DEFAULT(StructType);\n TYPE_VISITOR_DEFAULT(UnionType);\ndiff --git a/cpp/src/arrow/visitor_inline.h b/cpp/src/arrow/visitor_inline.h\nindex 72c82a3bb..41b0108ae 100644\n--- a/cpp/src/arrow/visitor_inline.h\n+++ b/cpp/src/arrow/visitor_inline.h\n@@ -97,7 +97,7 @@ inline Status VisitArrayInline(const Array& array, VISITOR* visitor) {\n     ARRAY_VISIT_INLINE(TimestampType);\n     ARRAY_VISIT_INLINE(Time32Type);\n     ARRAY_VISIT_INLINE(Time64Type);\n-    ARRAY_VISIT_INLINE(DecimalType);\n+    ARRAY_VISIT_INLINE(Decimal128Type);\n     ARRAY_VISIT_INLINE(ListType);\n     ARRAY_VISIT_INLINE(StructType);\n     ARRAY_VISIT_INLINE(UnionType);\ndiff --git a/dev/release/rat_exclude_files.txt b/dev/release/rat_exclude_files.txt\nindex db3b3aa5f..bf962bcd4 100644\n--- a/dev/release/rat_exclude_files.txt\n+++ b/dev/release/rat_exclude_files.txt\n@@ -10,6 +10,12 @@ cpp/src/arrow/io/mman.h\n cpp/src/arrow/util/random.h\n cpp/src/arrow/status.cc\n cpp/src/arrow/status.h\n+cpp/src/arrow/util/variant.h\n+cpp/src/arrow/util/variant/optional.h\n+cpp/src/arrow/util/variant/recursive_wrapper.h\n+cpp/src/arrow/util/variant/variant_cast.h\n+cpp/src/arrow/util/variant/variant_io.h\n+cpp/src/arrow/util/variant/variant_visitor.h\n cpp/build-support/asan_symbolize.py\n cpp/build-support/cpplint.py\n cpp/build-support/clang_format_exclusions.txt\ndiff --git a/python/doc/source/api.rst b/python/doc/source/api.rst\nindex c52024044..8f2f23d9f 100644\n--- a/python/doc/source/api.rst\n+++ b/python/doc/source/api.rst\n@@ -132,6 +132,7 @@ Array Types\n .. autosummary::\n    :toctree: generated/\n \n+   array\n    Array\n    BooleanArray\n    DictionaryArray\n@@ -168,6 +169,8 @@ Tables and Record Batches\n .. autosummary::\n    :toctree: generated/\n \n+   column\n+   chunked_array\n    ChunkedArray\n    Column\n    RecordBatch\ndiff --git a/python/doc/source/development.rst b/python/doc/source/development.rst\nindex 7ef6a722b..885613725 100644\n--- a/python/doc/source/development.rst\n+++ b/python/doc/source/development.rst\n@@ -84,7 +84,7 @@ from conda-forge:\n    conda create -y -q -n pyarrow-dev \\\n          python=3.6 numpy six setuptools cython pandas pytest \\\n          cmake flatbuffers rapidjson boost-cpp thrift-cpp snappy zlib \\\n-         brotli jemalloc lz4-c zstd -c conda-forge\n+         gflags brotli jemalloc lz4-c zstd -c conda-forge\n    source activate pyarrow-dev\n \n \n@@ -256,17 +256,11 @@ First, starting from fresh clones of Apache Arrow and parquet-cpp:\n \n .. code-block:: shell\n \n-   conda create -n arrow-dev cmake git boost-cpp ^\n-         flatbuffers snappy zlib brotli thrift-cpp rapidjson ^\n-         -c conda-forge\n-   activate arrow-dev\n-\n-As one git housekeeping item, we must run this command in our Arrow clone:\n-\n-.. code-block:: shell\n-\n-   cd arrow\n-   git config core.symlinks true\n+   conda create -y -q -n pyarrow-dev ^\n+         python=3.6 numpy six setuptools cython pandas pytest ^\n+         cmake flatbuffers rapidjson boost-cpp thrift-cpp snappy zlib ^\n+         gflags brotli lz4-c zstd -c conda-forge\n+   activate pyarrow-dev\n \n Now, we build and install Arrow C++ libraries\n \n@@ -280,7 +274,7 @@ Now, we build and install Arrow C++ libraries\n          -DCMAKE_INSTALL_PREFIX=%ARROW_HOME% ^\n          -DCMAKE_BUILD_TYPE=Release ^\n          -DARROW_BUILD_TESTS=on ^\n-         -DARROW_CXXFLAGS=\"/WX\" ^\n+         -DARROW_CXXFLAGS=\"/WX /MP\" ^\n          -DARROW_PYTHON=on ..\n    cmake --build . --target INSTALL --config Release\n    cd ..\\..\ndiff --git a/python/pyarrow/__init__.py b/python/pyarrow/__init__.py\nindex 09bf6b35f..c8ded2d3c 100644\n--- a/python/pyarrow/__init__.py\n+++ b/python/pyarrow/__init__.py\n@@ -43,7 +43,7 @@\n                          Schema,\n                          schema,\n                          Array, Tensor,\n-                         array,\n+                         array, chunked_array, column,\n                          from_numpy_dtype,\n                          NullArray,\n                          NumericArray, IntegerArray, FloatingPointArray,\ndiff --git a/python/pyarrow/array.pxi b/python/pyarrow/array.pxi\nindex 2ef592ff7..cca942588 100644\n--- a/python/pyarrow/array.pxi\n+++ b/python/pyarrow/array.pxi\n@@ -77,7 +77,7 @@ cdef _ndarray_to_array(object values, object mask, DataType type,\n         return pyarrow_wrap_array(chunked_out.get().chunk(0))\n \n \n-cdef DataType _ensure_type(object type):\n+cdef inline DataType _ensure_type(object type):\n     if type is None:\n         return None\n     elif not isinstance(type, DataType):\n@@ -228,6 +228,15 @@ cdef CFunctionContext* _context() nogil:\n     return _global_ctx.ctx.get()\n \n \n+cdef wrap_datum(const CDatum& datum):\n+    if datum.kind() == DatumType_ARRAY:\n+        return pyarrow_wrap_array(MakeArray(datum.array()))\n+    elif datum.kind() == DatumType_CHUNKED_ARRAY:\n+        return pyarrow_wrap_chunked_array(datum.chunked_array())\n+    else:\n+        raise ValueError(\"Unable to wrap Datum in a Python object\")\n+\n+\n cdef class Array:\n \n     cdef void init(self, const shared_ptr[CArray]& sp_array):\n@@ -270,6 +279,29 @@ cdef class Array:\n \n         return pyarrow_wrap_array(result)\n \n+    def unique(self):\n+        \"\"\"\n+        Compute distinct elements in array\n+        \"\"\"\n+        cdef shared_ptr[CArray] result\n+\n+        with nogil:\n+            check_status(Unique(_context(), CDatum(self.sp_array), &result))\n+\n+        return pyarrow_wrap_array(result)\n+\n+    def dictionary_encode(self):\n+        \"\"\"\n+        Compute dictionary-encoded representation of array\n+        \"\"\"\n+        cdef CDatum out\n+\n+        with nogil:\n+            check_status(DictionaryEncode(_context(), CDatum(self.sp_array),\n+                                          &out))\n+\n+        return wrap_datum(out)\n+\n     @staticmethod\n     def from_pandas(obj, mask=None, type=None, MemoryPool memory_pool=None):\n         \"\"\"\n@@ -702,6 +734,9 @@ cdef class DictionaryArray(Array):\n             return box_scalar(dictionary.type, dictionary.sp_array,\n                               index.as_py())\n \n+    def dictionary_encode(self):\n+        return self\n+\n     property dictionary:\n \n         def __get__(self):\ndiff --git a/python/pyarrow/includes/libarrow.pxd b/python/pyarrow/includes/libarrow.pxd\nindex 11cc6b3ff..dbfd89cc3 100644\n--- a/python/pyarrow/includes/libarrow.pxd\n+++ b/python/pyarrow/includes/libarrow.pxd\n@@ -90,6 +90,14 @@ cdef extern from \"arrow/api.h\" namespace \"arrow\" nogil:\n \n         c_string ToString()\n \n+    cdef cppclass CArrayData\" arrow::ArrayData\":\n+        shared_ptr[CDataType] type\n+        int64_t length\n+        int64_t null_count\n+        int64_t offset\n+        vector[shared_ptr[CBuffer]] buffers\n+        vector[shared_ptr[CArrayData]] child_data\n+\n     cdef cppclass CArray\" arrow::Array\":\n         shared_ptr[CDataType] type()\n \n@@ -102,9 +110,13 @@ cdef extern from \"arrow/api.h\" namespace \"arrow\" nogil:\n         c_bool Equals(const CArray& arr)\n         c_bool IsNull(int i)\n \n+        shared_ptr[CArrayData] data()\n+\n         shared_ptr[CArray] Slice(int64_t offset)\n         shared_ptr[CArray] Slice(int64_t offset, int64_t length)\n \n+    shared_ptr[CArray] MakeArray(const shared_ptr[CArrayData]& data)\n+\n     CStatus DebugPrint(const CArray& arr, int indent)\n \n     cdef cppclass CFixedWidthType\" arrow::FixedWidthType\"(CDataType):\n@@ -363,6 +375,7 @@ cdef extern from \"arrow/api.h\" namespace \"arrow\" nogil:\n     CStatus ValidateArray(const CArray& array)\n \n     cdef cppclass CChunkedArray\" arrow::ChunkedArray\":\n+        CChunkedArray(const vector[shared_ptr[CArray]]& arrays)\n         int64_t length()\n         int64_t null_count()\n         int num_chunks()\n@@ -376,8 +389,13 @@ cdef extern from \"arrow/api.h\" namespace \"arrow\" nogil:\n         CColumn(const shared_ptr[CField]& field,\n                 const vector[shared_ptr[CArray]]& chunks)\n \n+        CColumn(const shared_ptr[CField]& field,\n+                const shared_ptr[CChunkedArray]& data)\n+\n         c_bool Equals(const CColumn& other)\n \n+        shared_ptr[CField] field()\n+\n         int64_t length()\n         int64_t null_count()\n         const c_string& name()\n@@ -776,11 +794,42 @@ cdef extern from \"arrow/compute/api.h\" namespace \"arrow::compute\" nogil:\n         c_bool allow_int_overflow\n         c_bool allow_time_truncate\n \n+    enum DatumType\" arrow::compute::Datum::type\":\n+        DatumType_NONE\" arrow::compute::Datum::NONE\"\n+        DatumType_SCALAR\" arrow::compute::Datum::SCALAR\"\n+        DatumType_ARRAY\" arrow::compute::Datum::ARRAY\"\n+        DatumType_CHUNKED_ARRAY\" arrow::compute::Datum::CHUNKED_ARRAY\"\n+        DatumType_RECORD_BATCH\" arrow::compute::Datum::RECORD_BATCH\"\n+        DatumType_TABLE\" arrow::compute::Datum::TABLE\"\n+        DatumType_COLLECTION\" arrow::compute::Datum::COLLECTION\"\n+\n+    cdef cppclass CDatum\" arrow::compute::Datum\":\n+        CDatum()\n+        CDatum(const shared_ptr[CArray]& value)\n+        CDatum(const shared_ptr[CChunkedArray]& value)\n+        CDatum(const shared_ptr[CRecordBatch]& value)\n+        CDatum(const shared_ptr[CTable]& value)\n+\n+        DatumType kind()\n+\n+        shared_ptr[CArrayData] array()\n+        shared_ptr[CChunkedArray] chunked_array()\n+\n     CStatus Cast(CFunctionContext* context, const CArray& array,\n                  const shared_ptr[CDataType]& to_type,\n                  const CCastOptions& options,\n                  shared_ptr[CArray]* out)\n \n+    CStatus Cast(CFunctionContext* context, const CDatum& value,\n+                 const shared_ptr[CDataType]& to_type,\n+                 const CCastOptions& options, CDatum* out)\n+\n+    CStatus Unique(CFunctionContext* context, const CDatum& value,\n+                   shared_ptr[CArray]* out)\n+\n+    CStatus DictionaryEncode(CFunctionContext* context, const CDatum& value,\n+                             CDatum* out)\n+\n \n cdef extern from \"arrow/python/api.h\" namespace \"arrow::py\" nogil:\n     shared_ptr[CDataType] GetPrimitiveType(Type type)\ndiff --git a/python/pyarrow/pandas_compat.py b/python/pyarrow/pandas_compat.py\nindex 41eaf0bac..0aab9a41b 100644\n--- a/python/pyarrow/pandas_compat.py\n+++ b/python/pyarrow/pandas_compat.py\n@@ -362,7 +362,8 @@ def backwards_compatible_index_name(raw_name, logical_name):\n         return logical_name\n \n \n-def table_to_blockmanager(options, table, memory_pool, nthreads=1):\n+def table_to_blockmanager(options, table, memory_pool, nthreads=1,\n+                          categoricals=None):\n     import pandas.core.internals as _int\n     import pyarrow.lib as lib\n \ndiff --git a/python/pyarrow/table.pxi b/python/pyarrow/table.pxi\nindex 5ba5f83d2..1a9d23db4 100644\n--- a/python/pyarrow/table.pxi\n+++ b/python/pyarrow/table.pxi\n@@ -108,6 +108,15 @@ cdef class ChunkedArray:\n \n         return pyarrow_wrap_array(self.chunked_array.chunk(i))\n \n+    property chunks:\n+\n+        def __get__(self):\n+            cdef int i\n+            chunks = []\n+            for i in range(self.num_chunks):\n+                chunks.append(self.chunk(i))\n+            return chunks\n+\n     def iterchunks(self):\n         for i in range(self.num_chunks):\n             yield self.chunk(i)\n@@ -122,6 +131,74 @@ cdef class ChunkedArray:\n         return result\n \n \n+def chunked_array(arrays, type=None):\n+    \"\"\"\n+    Construct chunked array from list of array-like objects\n+\n+    Parameters\n+    ----------\n+    arrays : list of Array or values coercible to arrays\n+    type : DataType or string coercible to DataType\n+\n+    Returns\n+    -------\n+    ChunkedArray\n+    \"\"\"\n+    cdef:\n+        Array arr\n+        vector[shared_ptr[CArray]] c_arrays\n+        shared_ptr[CChunkedArray] sp_chunked_array\n+\n+    for x in arrays:\n+        if isinstance(x, Array):\n+            arr = x\n+            if type is not None:\n+                assert x.type == type\n+        else:\n+            arr = array(x, type=type)\n+\n+        c_arrays.push_back(arr.sp_array)\n+\n+    sp_chunked_array.reset(new CChunkedArray(c_arrays))\n+    return pyarrow_wrap_chunked_array(sp_chunked_array)\n+\n+\n+def column(object field_or_name, arr):\n+    \"\"\"\n+    Create Column object from field/string and array-like data\n+    \"\"\"\n+    cdef:\n+        Field boxed_field\n+        Array _arr\n+        ChunkedArray _carr\n+        shared_ptr[CColumn] sp_column\n+\n+    if isinstance(arr, list):\n+        arr = chunked_array(arr)\n+    elif not isinstance(arr, (Array, ChunkedArray)):\n+        arr = array(arr)\n+\n+    if isinstance(field_or_name, Field):\n+        boxed_field = field_or_name\n+        if arr.type != boxed_field.type:\n+            raise ValueError('Passed field type does not match array')\n+    else:\n+        boxed_field = field(field_or_name, arr.type)\n+\n+    if isinstance(arr, Array):\n+        _arr = arr\n+        sp_column.reset(new CColumn(boxed_field.sp_field, _arr.sp_array))\n+    elif isinstance(arr, ChunkedArray):\n+        _carr = arr\n+        sp_column.reset(new CColumn(boxed_field.sp_field,\n+                                    _carr.sp_chunked_array))\n+    else:\n+        raise ValueError(\"Unsupported type for column(...): {}\"\n+                         .format(type(arr)))\n+\n+    return pyarrow_wrap_column(sp_column)\n+\n+\n cdef class Column:\n     \"\"\"\n     Named vector of elements of equal type.\n@@ -143,25 +220,47 @@ cdef class Column:\n         result = StringIO()\n         result.write(object.__repr__(self))\n         data = self.data\n-        for i in range(len(data)):\n-            result.write('\\nchunk {0}: {1}'.format(i, repr(data.chunk(0))))\n+        for i, chunk in enumerate(data.chunks):\n+            result.write('\\nchunk {0}: {1}'.format(i, repr(chunk)))\n \n         return result.getvalue()\n \n     @staticmethod\n-    def from_array(object field_or_name, Array arr):\n-        cdef Field boxed_field\n+    def from_array(*args):\n+        return column(*args)\n \n-        if isinstance(field_or_name, Field):\n-            boxed_field = field_or_name\n-            if arr.type != boxed_field.type:\n-                raise ValueError('Passed field type does not match array')\n-        else:\n-            boxed_field = field(field_or_name, arr.type)\n+    def cast(self, object target_type, safe=True):\n+        \"\"\"\n+        Cast column values to another data type\n+\n+        Parameters\n+        ----------\n+        target_type : DataType\n+            Type to cast to\n+        safe : boolean, default True\n+            Check for overflows or other unsafe conversions\n+\n+        Returns\n+        -------\n+        casted : Column\n+        \"\"\"\n+        cdef:\n+            CCastOptions options\n+            shared_ptr[CArray] result\n+            DataType type\n+            CDatum out\n+\n+        type = _ensure_type(target_type)\n \n-        cdef shared_ptr[CColumn] sp_column\n-        sp_column.reset(new CColumn(boxed_field.sp_field, arr.sp_array))\n-        return pyarrow_wrap_column(sp_column)\n+        options.allow_int_overflow = not safe\n+        options.allow_time_truncate = not safe\n+\n+        with nogil:\n+            check_status(Cast(_context(), CDatum(self.column.data()),\n+                              type.sp_type, options, &out))\n+\n+        casted_data = pyarrow_wrap_chunked_array(out.chunked_array())\n+        return column(self.name, casted_data)\n \n     def to_pandas(self, strings_to_categorical=False, zero_copy_only=False):\n         \"\"\"\n@@ -241,6 +340,10 @@ cdef class Column:\n         self._check_nullptr()\n         return self.column.length()\n \n+    @property\n+    def field(self):\n+        return pyarrow_wrap_field(self.column.field())\n+\n     @property\n     def shape(self):\n         \"\"\"\ndiff --git a/python/pyarrow/tests/test_array.py b/python/pyarrow/tests/test_array.py\nindex 7dc93c28e..c061e6820 100644\n--- a/python/pyarrow/tests/test_array.py\n+++ b/python/pyarrow/tests/test_array.py\n@@ -295,6 +295,18 @@ def test_cast_integers_safe():\n             in_arr.cast(out_type)\n \n \n+def test_cast_column():\n+    arrays = [pa.array([1, 2, 3]), pa.array([4, 5, 6])]\n+\n+    col = pa.column('foo', arrays)\n+\n+    target = pa.float64()\n+    casted = col.cast(target)\n+\n+    expected = pa.column('foo', [x.cast(target) for x in arrays])\n+    assert casted.equals(expected)\n+\n+\n def test_cast_integers_unsafe():\n     # We let NumPy do the unsafe casting\n     unsafe_cases = [\n@@ -350,6 +362,33 @@ def test_cast_signed_to_unsigned():\n         _check_cast_case(case)\n \n \n+def test_unique_simple():\n+    cases = [\n+        (pa.array([1, 2, 3, 1, 2, 3]), pa.array([1, 2, 3])),\n+        (pa.array(['foo', None, 'bar', 'foo']),\n+         pa.array(['foo', 'bar']))\n+    ]\n+    for arr, expected in cases:\n+        result = arr.unique()\n+        assert result.equals(expected)\n+\n+\n+def test_dictionary_encode_simple():\n+    cases = [\n+        (pa.array([1, 2, 3, None, 1, 2, 3]),\n+         pa.DictionaryArray.from_arrays(\n+             pa.array([0, 1, 2, None, 0, 1, 2], type='int32'),\n+             [1, 2, 3])),\n+        (pa.array(['foo', None, 'bar', 'foo']),\n+         pa.DictionaryArray.from_arrays(\n+             pa.array([0, None, 1, 0], type='int32'),\n+             ['foo', 'bar']))\n+    ]\n+    for arr, expected in cases:\n+        result = arr.dictionary_encode()\n+        assert result.equals(expected)\n+\n+\n def test_simple_type_construction():\n     result = pa.lib.TimestampType()\n     with pytest.raises(TypeError):\ndiff --git a/python/pyarrow/tests/test_parquet.py b/python/pyarrow/tests/test_parquet.py\nindex 6ba4fd2fa..1df80acc0 100644\n--- a/python/pyarrow/tests/test_parquet.py\n+++ b/python/pyarrow/tests/test_parquet.py\n@@ -1436,6 +1436,7 @@ def test_large_table_int32_overflow():\n     _write_table(table, f)\n \n \n+@parquet\n def test_index_column_name_duplicate(tmpdir):\n     data = {\n         'close': {\n@@ -1460,6 +1461,7 @@ def test_index_column_name_duplicate(tmpdir):\n     tm.assert_frame_equal(result_df, dfx)\n \n \n+@parquet\n def test_backwards_compatible_index_naming():\n     expected_string = b\"\"\"\\\n carat        cut  color  clarity  depth  table  price     x     y     z\n@@ -1482,6 +1484,7 @@ def test_backwards_compatible_index_naming():\n     tm.assert_frame_equal(result, expected)\n \n \n+@parquet\n def test_backwards_compatible_index_multi_level_named():\n     expected_string = b\"\"\"\\\n carat        cut  color  clarity  depth  table  price     x     y     z\n@@ -1507,6 +1510,7 @@ def test_backwards_compatible_index_multi_level_named():\n     tm.assert_frame_equal(result, expected)\n \n \n+@parquet\n def test_backwards_compatible_index_multi_level_some_named():\n     expected_string = b\"\"\"\\\n carat        cut  color  clarity  depth  table  price     x     y     z\ndiff --git a/python/pyarrow/types.pxi b/python/pyarrow/types.pxi\nindex edf0d8a30..abd678bac 100644\n--- a/python/pyarrow/types.pxi\n+++ b/python/pyarrow/types.pxi\n@@ -1108,6 +1108,7 @@ def union(children_fields, mode):\n \n     return pyarrow_wrap_data_type(union_type)\n \n+\n cdef dict _type_aliases = {\n     'null': null,\n     'i1': int8,\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-17T23:29:57.813+0000",
                    "updated": "2017-11-17T23:29:57.813+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16258064",
                    "id": "16258064",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on a change in pull request #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#discussion_r151837230\n \n \n\n ##########\n File path: cpp/src/arrow/compute/kernels/hash.cc\n ##########\n @@ -0,0 +1,880 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/kernels/hash.h\"\n+\n+#include <exception>\n+#include <limits>\n+#include <memory>\n+#include <mutex>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/builder.h\"\n+#include \"arrow/compute/context.h\"\n+#include \"arrow/compute/kernel.h\"\n+#include \"arrow/compute/kernels/util-internal.h\"\n+#include \"arrow/util/hash-util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+namespace {\n+\n+// Initially 1024 elements\n+static constexpr int64_t kInitialHashTableSize = 1 << 10;\n+\n+typedef int32_t hash_slot_t;\n+static constexpr hash_slot_t kHashSlotEmpty = std::numeric_limits<int32_t>::max();\n+\n+// The maximum load factor for the hash table before resizing.\n+static constexpr double kMaxHashTableLoad = 0.7;\n+\n+enum class SIMDMode : char { NOSIMD, SSE4, AVX2 };\n+\n+#define CHECK_IMPLEMENTED(KERNEL, FUNCNAME, TYPE)                  \\\n+  if (!KERNEL) {                                                   \\\n+    std::stringstream ss;                                          \\\n+    ss << FUNCNAME << \" not implemented for \" << type->ToString(); \\\n+    return Status::NotImplemented(ss.str());                       \\\n+  }\n+\n+Status NewHashTable(int64_t size, MemoryPool* pool, std::shared_ptr<Buffer>* out) {\n+  auto hash_table = std::make_shared<PoolBuffer>(pool);\n+\n+  RETURN_NOT_OK(hash_table->Resize(sizeof(hash_slot_t) * size));\n+  int32_t* slots = reinterpret_cast<hash_slot_t*>(hash_table->mutable_data());\n+  std::fill(slots, slots + size, kHashSlotEmpty);\n+\n+  *out = hash_table;\n+  return Status::OK();\n+}\n+\n+// This is a slight design concession -- some hash actions have the possibility\n+// of failure. Rather than introduce extra error checking into all actions, we\n+// will raise an internal exception so that only the actions where errors can\n+// occur will experience the extra overhead\n+class HashException : public std::exception {\n+ public:\n+  explicit HashException(const std::string& msg, StatusCode code = StatusCode::Invalid)\n+      : msg_(msg), code_(code) {}\n+\n+  ~HashException() throw() {}\n+\n+  const char* what() const throw() override;\n+\n+  StatusCode code() const { return code_; }\n+\n+ private:\n+  std::string msg_;\n+  StatusCode code_;\n+};\n+\n+const char* HashException::what() const throw() { return msg_.c_str(); }\n+\n+class HashTable {\n+ public:\n+  HashTable(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : type_(type),\n+        pool_(pool),\n+        initialized_(false),\n+        hash_table_(nullptr),\n+        hash_slots_(nullptr),\n+        hash_table_size_(0),\n+        mod_bitmask_(0) {}\n+\n+  virtual ~HashTable() {}\n+\n+  virtual Status Append(const ArrayData& input) = 0;\n+  virtual Status Flush(std::vector<Datum>* out) = 0;\n+  virtual Status GetDictionary(std::shared_ptr<ArrayData>* out) = 0;\n+\n+ protected:\n+  Status Init(int64_t elements);\n+\n+  std::shared_ptr<DataType> type_;\n+  MemoryPool* pool_;\n+  bool initialized_;\n+\n+  // The hash table contains integer indices that reference the set of observed\n+  // distinct values\n+  std::shared_ptr<Buffer> hash_table_;\n+  hash_slot_t* hash_slots_;\n+\n+  /// Size of the table. Must be a power of 2.\n+  int64_t hash_table_size_;\n+\n+  // Store hash_table_size_ - 1, so that j & mod_bitmask_ is equivalent to j %\n+  // hash_table_size_, but uses far fewer CPU cycles\n+  int64_t mod_bitmask_;\n+};\n+\n+Status HashTable::Init(int64_t elements) {\n+  DCHECK_EQ(elements, BitUtil::NextPower2(elements));\n+  RETURN_NOT_OK(NewHashTable(elements, pool_, &hash_table_));\n+  hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+  hash_table_size_ = elements;\n+  mod_bitmask_ = elements - 1;\n+  initialized_ = true;\n+  return Status::OK();\n+}\n+\n+template <typename Type, typename Action, typename Enable = void>\n+class HashTableKernel : public HashTable {};\n+\n+// Types of hash actions\n+//\n+// unique: append to dictionary when not found, no-op with slot\n+// dictionary-encode: append to dictionary when not found, append slot #\n+// match: raise or set null when not found, otherwise append slot #\n+// isin: set false when not found, otherwise true\n+// value counts: append to dictionary when not found, increment count for slot\n+\n+template <typename Type, typename Enable = void>\n+class HashDictionary {};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for nulls\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_null<Type>> : public HashTable {\n+ public:\n+  using HashTable::HashTable;\n+\n+  Status Init() {\n+    // No-op, do not even need to initialize hash table\n+    return Status::OK();\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      action->ObserveNull();\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being a valid dictionary value\n+    auto null_array = std::make_shared<NullArray>(0);\n+    *out = null_array->data();\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for primitive types\n+\n+template <typename Type>\n+struct HashDictionary<Type, enable_if_has_c_type<Type>> {\n+  using T = typename Type::c_type;\n+\n+  explicit HashDictionary(MemoryPool* pool)\n+      : pool(pool), buffer(std::make_shared<PoolBuffer>(pool)), size(0), capacity(0) {}\n+\n+  Status Init() {\n+    this->size = 0;\n+    return Resize(kInitialHashTableSize);\n+  }\n+\n+  Status DoubleSize() { return Resize(this->size * 2); }\n+\n+  Status Resize(const int64_t elements) {\n+    RETURN_NOT_OK(this->buffer->Resize(elements * sizeof(T)));\n+\n+    this->capacity = elements;\n+    this->values = reinterpret_cast<T*>(this->buffer->mutable_data());\n+    return Status::OK();\n+  }\n+\n+  MemoryPool* pool;\n+  std::shared_ptr<ResizableBuffer> buffer;\n+  T* values;\n+  int64_t size;\n+  int64_t capacity;\n+};\n+\n+#define GENERIC_HASH_PASS(HASH_INNER_LOOP)                                               \\\n+  if (arr.null_count != 0) {                                                             \\\n+    internal::BitmapReader valid_reader(arr.buffers[0]->data(), arr.offset, arr.length); \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      const bool is_null = valid_reader.IsNotSet();                                      \\\n+      valid_reader.Next();                                                               \\\n+                                                                                         \\\n+      if (is_null) {                                                                     \\\n+        action->ObserveNull();                                                           \\\n+        continue;                                                                        \\\n+      }                                                                                  \\\n+                                                                                         \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  } else {                                                                               \\\n+    for (int64_t i = 0; i < arr.length; ++i) {                                           \\\n+      HASH_INNER_LOOP();                                                                 \\\n+    }                                                                                    \\\n+  }\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_has_c_type<Type>> : public HashTable {\n+ public:\n+  using T = typename Type::c_type;\n+\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_(pool) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_.Init());\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const T* values = GetValues<T>(arr, 1);\n+    auto action = static_cast<Action*>(this);\n+\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const T value = values[i];                                                      \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  while (kHashSlotEmpty != slot && dict_.values[slot] != value) {                 \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = static_cast<hash_slot_t>(dict_.size);                                  \\\n+    hash_slots_[j] = slot;                                                        \\\n+    dict_.values[dict_.size++] = value;                                           \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_.size > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    auto dict_data = dict_.buffer;\n+    RETURN_NOT_OK(dict_data->Resize(dict_.size * sizeof(T), false));\n+\n+    BufferVector buffers = {nullptr, dict_data};\n+    *out = std::make_shared<ArrayData>(type_, dict_.size, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const T& value) const {\n+    // TODO(wesm): Use faster hash function for C types\n+    return HashUtil::Hash(&value, sizeof(T), 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const T value = dict_.values[index];\n+\n+      // Find empty slot in the new hash table\n+      int64_t j = HashValue(value) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return dict_.Resize(new_size);\n+  }\n+\n+  HashDictionary<Type> dict_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for variable-length binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_binary<Type>> : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_offsets_(pool), dict_data_(pool), dict_size_(0) {}\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_offsets_.Resize(kInitialHashTableSize));\n+\n+    // We append the end offset after each append to the dictionary, so this\n+    // sets the initial condition for the length-0 case\n+    //\n+    // initial offsets (dict size == 0): 0\n+    // after 1st dict entry of length 3: 0 3\n+    // after 2nd dict entry of length 4: 0 3 7\n+    RETURN_NOT_OK(dict_offsets_.Append(0));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const int32_t* offsets = GetValues<int32_t>(arr, 1);\n+    const uint8_t* data = GetValues<uint8_t>(arr, 2);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                           \\\n+  const int32_t position = offsets[i];                                              \\\n+  const int32_t length = offsets[i + 1] - position;                                 \\\n+  const uint8_t* value = data + position;                                           \\\n+                                                                                    \\\n+  int64_t j = HashValue(value, length) & mod_bitmask_;                              \\\n+  hash_slot_t slot = hash_slots_[j];                                                \\\n+                                                                                    \\\n+  const int32_t* dict_offsets = dict_offsets_.data();                               \\\n+  const uint8_t* dict_data = dict_data_.data();                                     \\\n+  while (kHashSlotEmpty != slot &&                                                  \\\n+         !((dict_offsets[slot + 1] - dict_offsets[slot]) == length &&               \\\n+           0 == memcmp(value, dict_data + dict_offsets[slot], length))) {           \\\n+    ++j;                                                                            \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                               \\\n+      j = 0;                                                                        \\\n+    }                                                                               \\\n+    slot = hash_slots_[j];                                                          \\\n+  }                                                                                 \\\n+                                                                                    \\\n+  if (slot == kHashSlotEmpty) {                                                     \\\n+    if (!Action::allow_expand) {                                                    \\\n+      throw HashException(\"Encountered new dictionary value\");                      \\\n+    }                                                                               \\\n+                                                                                    \\\n+    slot = dict_size_++;                                                            \\\n+    hash_slots_[j] = slot;                                                          \\\n+                                                                                    \\\n+    RETURN_NOT_OK(dict_data_.Append(value, length));                                \\\n+    RETURN_NOT_OK(dict_offsets_.Append(static_cast<int32_t>(dict_data_.length()))); \\\n+                                                                                    \\\n+    action->ObserveNotFound(slot);                                                  \\\n+                                                                                    \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) {   \\\n+      RETURN_NOT_OK(action->DoubleSize());                                          \\\n+    }                                                                               \\\n+  } else {                                                                          \\\n+    action->ObserveFound(slot);                                                     \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr, nullptr};\n+\n+    RETURN_NOT_OK(dict_offsets_.Finish(&buffers[1]));\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[2]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data, int32_t length) const {\n+    return HashUtil::Hash(data, length, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    const int32_t* dict_offsets = dict_offsets_.data();\n+    const uint8_t* dict_data = dict_data_.data();\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Compute the hash value mod the new table size to start looking for an\n+      // empty slot\n+      const int32_t length = dict_offsets[index + 1] - dict_offsets[index];\n+      const uint8_t* value = dict_data + dict_offsets[index];\n+\n+      // Find an empty slot in the new hash table\n+      int64_t j = HashValue(value, length) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return Status::OK();\n+  }\n+\n+  TypedBufferBuilder<int32_t> dict_offsets_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Hash table pass for fixed size binary types\n+\n+template <typename Type, typename Action>\n+class HashTableKernel<Type, Action, enable_if_fixed_size_binary<Type>>\n+    : public HashTable {\n+ public:\n+  HashTableKernel(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : HashTable(type, pool), dict_data_(pool), dict_size_(0) {\n+    const auto& fw_type = static_cast<const FixedSizeBinaryType&>(*type);\n+    byte_width_ = fw_type.bit_width() / 8;\n+  }\n+\n+  Status Init() {\n+    RETURN_NOT_OK(dict_data_.Resize(kInitialHashTableSize * byte_width_));\n+    return HashTable::Init(kInitialHashTableSize);\n+  }\n+\n+  Status Append(const ArrayData& arr) override {\n+    if (!initialized_) {\n+      RETURN_NOT_OK(Init());\n+    }\n+\n+    const uint8_t* data = GetValues<uint8_t>(arr, 1);\n+\n+    auto action = static_cast<Action*>(this);\n+    RETURN_NOT_OK(action->Reserve(arr.length));\n+\n+#define HASH_INNER_LOOP()                                                         \\\n+  const uint8_t* value = data + i * byte_width_;                                  \\\n+  int64_t j = HashValue(value) & mod_bitmask_;                                    \\\n+  hash_slot_t slot = hash_slots_[j];                                              \\\n+                                                                                  \\\n+  const uint8_t* dict_data = dict_data_.data();                                   \\\n+  while (kHashSlotEmpty != slot &&                                                \\\n+         !(0 == memcmp(value, dict_data + slot * byte_width_, byte_width_))) {    \\\n+    ++j;                                                                          \\\n+    if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {                             \\\n+      j = 0;                                                                      \\\n+    }                                                                             \\\n+    slot = hash_slots_[j];                                                        \\\n+  }                                                                               \\\n+                                                                                  \\\n+  if (slot == kHashSlotEmpty) {                                                   \\\n+    if (!Action::allow_expand) {                                                  \\\n+      throw HashException(\"Encountered new dictionary value\");                    \\\n+    }                                                                             \\\n+                                                                                  \\\n+    slot = dict_size_++;                                                          \\\n+    hash_slots_[j] = slot;                                                        \\\n+                                                                                  \\\n+    RETURN_NOT_OK(dict_data_.Append(value, byte_width_));                         \\\n+                                                                                  \\\n+    action->ObserveNotFound(slot);                                                \\\n+                                                                                  \\\n+    if (ARROW_PREDICT_FALSE(dict_size_ > hash_table_size_ * kMaxHashTableLoad)) { \\\n+      RETURN_NOT_OK(action->DoubleSize());                                        \\\n+    }                                                                             \\\n+  } else {                                                                        \\\n+    action->ObserveFound(slot);                                                   \\\n+  }\n+\n+    GENERIC_HASH_PASS(HASH_INNER_LOOP);\n+\n+#undef HASH_INNER_LOOP\n+\n+    return Status::OK();\n+  }\n+\n+  Status GetDictionary(std::shared_ptr<ArrayData>* out) override {\n+    // TODO(wesm): handle null being in the dictionary\n+    BufferVector buffers = {nullptr, nullptr};\n+    RETURN_NOT_OK(dict_data_.Finish(&buffers[1]));\n+\n+    *out = std::make_shared<ArrayData>(type_, dict_size_, std::move(buffers), 0);\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  int64_t HashValue(const uint8_t* data) const {\n+    return HashUtil::Hash(data, byte_width_, 0);\n+  }\n+\n+  Status DoubleTableSize() {\n+    int64_t new_size = hash_table_size_ * 2;\n+\n+    std::shared_ptr<Buffer> new_hash_table;\n+    RETURN_NOT_OK(NewHashTable(new_size, pool_, &new_hash_table));\n+    int32_t* new_hash_slots =\n+        reinterpret_cast<hash_slot_t*>(new_hash_table->mutable_data());\n+    int64_t new_mod_bitmask = new_size - 1;\n+\n+    const uint8_t* dict_data = dict_data_.data();\n+\n+    for (int i = 0; i < hash_table_size_; ++i) {\n+      hash_slot_t index = hash_slots_[i];\n+\n+      if (index == kHashSlotEmpty) {\n+        continue;\n+      }\n+\n+      // Find an empty slot in the new hash table\n+      int64_t j = HashValue(dict_data + index * byte_width_) & new_mod_bitmask;\n+      while (kHashSlotEmpty != new_hash_slots[j]) {\n+        ++j;\n+        if (ARROW_PREDICT_FALSE(j == hash_table_size_)) {\n+          j = 0;\n+        }\n+      }\n+\n+      // Copy the old slot index to the new hash table\n+      new_hash_slots[j] = index;\n+    }\n+\n+    hash_table_ = new_hash_table;\n+    hash_slots_ = reinterpret_cast<hash_slot_t*>(hash_table_->mutable_data());\n+    hash_table_size_ = new_size;\n+    mod_bitmask_ = new_size - 1;\n+\n+    return Status::OK();\n+  }\n+\n+  int32_t byte_width_;\n+  TypedBufferBuilder<uint8_t> dict_data_;\n+  int32_t dict_size_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Unique implementation\n+\n+template <typename Type>\n+class UniqueImpl : public HashTableKernel<Type, UniqueImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, UniqueImpl<Type>>;\n+  using Base::Base;\n+\n+  Status Reserve(const int64_t length) { return Status::OK(); }\n+\n+  void ObserveFound(const hash_slot_t slot) {}\n+  void ObserveNull() {}\n+  void ObserveNotFound(const hash_slot_t slot) {}\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Append(const ArrayData& input) override { return Base::Append(input); }\n+\n+  Status Flush(std::vector<Datum>* out) override {\n+    // No-op\n+    return Status::OK();\n+  }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Dictionary encode implementation\n+\n+template <typename Type>\n+class DictEncodeImpl : public HashTableKernel<Type, DictEncodeImpl<Type>> {\n+ public:\n+  static constexpr bool allow_expand = true;\n+  using Base = HashTableKernel<Type, DictEncodeImpl>;\n+\n+  DictEncodeImpl(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : Base(type, pool), indices_builder_(pool) {}\n+\n+  Status Reserve(const int64_t length) { return indices_builder_.Reserve(length); }\n+\n+  void ObserveNull() { indices_builder_.UnsafeAppendToBitmap(false); }\n+\n+  void ObserveFound(const hash_slot_t slot) { indices_builder_.UnsafeAppend(slot); }\n+\n+  void ObserveNotFound(const hash_slot_t slot) { return ObserveFound(slot); }\n+\n+  Status DoubleSize() { return Base::DoubleTableSize(); }\n+\n+  Status Flush(std::vector<Datum>* out) override {\n+    std::shared_ptr<ArrayData> result;\n+    RETURN_NOT_OK(indices_builder_.FinishInternal(&result));\n+    out->push_back(Datum(result));\n+    return Status::OK();\n+  }\n+\n+  using Base::Append;\n+\n+ private:\n+  Int32Builder indices_builder_;\n \n Review comment:\n   I will have a look at benchmarks. For my use cases it would sometimes be better to get as small as possible arrays instead of the fastest path quite often. \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-18T13:10:38.507+0000",
                    "updated": "2017-11-18T13:10:38.507+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13103395/comment/16258230",
                    "id": "16258230",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1266: ARROW-1559: [C++] Add Unique kernel and refactor DictionaryBuilder to be a stateful kernel\nURL: https://github.com/apache/arrow/pull/1266#issuecomment-345473160\n \n \n   I think the hash functions we are using are pretty expensive. We don't need super high quality hash functions for this code, they only need to be reasonable but use limited CPU cycles. We're also going to want to add SSE4.2 accelerated versions (since sse4.2 has instrinsics for crc32 hashes) that we select at runtime if the host processor supports it\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-18T21:37:59.602+0000",
                    "updated": "2017-11-18T21:37:59.602+0000"
                }
            ],
            "maxResults": 48,
            "total": 48,
            "startAt": 0
        },
        "customfield_12311820": "0|i3k9g7:",
        "customfield_12314139": null
    }
}