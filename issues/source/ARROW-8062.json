{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13290885",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885",
    "key": "ARROW-8062",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12347769",
                "id": "12347769",
                "description": "",
                "name": "1.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2020-07-24"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "dataset",
            "dataset-dask-integration",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12585788",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12585788",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13298270",
                    "key": "ARROW-8446",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13298270",
                    "fields": {
                        "summary": "[Python][Dataset] Detect and use _metadata file in a list of file paths",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                            "description": "The issue is open and ready for the assignee to start work on it.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                            "name": "Open",
                            "id": "1",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                                "id": 2,
                                "key": "new",
                                "colorName": "blue-gray",
                                "name": "To Do"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "id": "1",
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "name": "Bug",
                            "subtask": false,
                            "avatarId": 21133
                        }
                    }
                }
            },
            {
                "id": "12589019",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12589019",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13306232",
                    "key": "ARROW-8874",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13306232",
                    "fields": {
                        "summary": "[C++][Dataset] Scanner::ToTable race when ScanTask exit early with an error",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "id": "1",
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "name": "Bug",
                            "subtask": false,
                            "avatarId": 21133
                        }
                    }
                }
            },
            {
                "id": "12585761",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12585761",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13135615",
                    "key": "ARROW-2079",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13135615",
                    "fields": {
                        "summary": "[Python][C++] Possibly use `_common_metadata` for schema if `_metadata` isn't available",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                            "description": "The issue is open and ready for the assignee to start work on it.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                            "name": "Open",
                            "id": "1",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                                "id": 2,
                                "key": "new",
                                "colorName": "blue-gray",
                                "name": "To Do"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                            "name": "Minor",
                            "id": "4"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12583478",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12583478",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13185393",
                    "key": "ARROW-3244",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185393",
                    "fields": {
                        "summary": "[Python] Multi-file parquet loading without scan",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12588044",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12588044",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13303427",
                    "key": "ARROW-8733",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13303427",
                    "fields": {
                        "summary": "[C++][Dataset][Python] ParquetFileFragment should provide access to parquet FileMetadata",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=fsaintjacques",
            "name": "fsaintjacques",
            "key": "fsaintjacques",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=fsaintjacques&avatarId=37276",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=fsaintjacques&avatarId=37276",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=fsaintjacques&avatarId=37276",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=fsaintjacques&avatarId=37276"
            },
            "displayName": "Francois Saint-Jacques",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
            "name": "jorisvandenbossche",
            "key": "jorisvandenbossche",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Joris Van den Bossche",
            "active": true,
            "timeZone": "Europe/Brussels"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
            "name": "jorisvandenbossche",
            "key": "jorisvandenbossche",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Joris Van den Bossche",
            "active": true,
            "timeZone": "Europe/Brussels"
        },
        "aggregateprogress": {
            "progress": 14400,
            "total": 14400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 14400,
            "total": 14400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-8062/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 24,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/433262",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques opened a new pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180\n\n\n   - Implement ParquetDatasetFactory\r\n   - Replace ParquetFileFormat::GetRowGroupFragments with\r\n     ParquetFileFragment::SplitByRowGroup (and the corresponding bindings).\r\n   - Add various optimizations, notably in ColumnChunkStatisticsAsExpression.\r\n   - Consolidate RowGroupSkipper logic in ParquetFileFragment::GetRowGroupFragments.\r\n   - Ensure FileMetaData::AppendRowGroups checks for schema equality.\r\n   - Implement dataset._parquet_dataset\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-14T18:35:23.732+0000",
                    "updated": "2020-05-14T18:35:23.732+0000",
                    "started": "2020-05-14T18:35:23.732+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "433262",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/433272",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#issuecomment-628821011\n\n\n   https://issues.apache.org/jira/browse/ARROW-8062\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-14T18:46:50.705+0000",
                    "updated": "2020-05-14T18:46:50.705+0000",
                    "started": "2020-05-14T18:46:50.705+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "433272",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/434516",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r426804654\n\n\n\n##########\nFile path: cpp/src/parquet/arrow/reader_internal.cc\n##########\n@@ -747,8 +747,7 @@ Status TypedIntegralStatisticsAsScalars(const Statistics& statistics,\n       using CType = typename StatisticsType::T;\n       return MakeMinMaxScalar<CType, StatisticsType>(statistics, min, max);\n     default:\n-      return Status::NotImplemented(\"Cannot extract statistics for type \",\n-                                    logical_type->ToString());\n+      return Status::NotImplemented(\"Cannot extract statistics for type \");\n\nReview comment:\n       It turns out that this one gave a non-negligible performance boost because there's a lot of type we don't support, multiply by the number of row group and columns...\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-18T18:05:10.146+0000",
                    "updated": "2020-05-18T18:05:10.146+0000",
                    "started": "2020-05-18T18:05:10.145+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "434516",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/434930",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r427163007\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.h\n##########\n@@ -97,53 +103,167 @@ class ARROW_DS_EXPORT ParquetFileFormat : public FileFormat {\n   Result<ScanTaskIterator> ScanFile(const FileSource& source,\n                                     std::shared_ptr<ScanOptions> options,\n                                     std::shared_ptr<ScanContext> context,\n-                                    const std::vector<int>& row_groups) const;\n+                                    std::vector<RowGroupInfo> row_groups) const;\n \n   using FileFormat::MakeFragment;\n \n+  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n-      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+      FileSource source, std::shared_ptr<Expression> partition_expression,\n+      std::vector<RowGroupInfo> row_groups);\n \n-  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n       FileSource source, std::shared_ptr<Expression> partition_expression,\n       std::vector<int> row_groups);\n \n-  /// \\brief Split a ParquetFileFragment into a Fragment for each row group.\n+  /// \\brief Create a Fragment targeting all RowGroups.\n+  Result<std::shared_ptr<FileFragment>> MakeFragment(\n+      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+\n+  /// \\brief Return a FileReader on the given source.\n+  Result<std::unique_ptr<parquet::arrow::FileReader>> GetReader(\n+      const FileSource& source, ScanOptions* = NULLPTR, ScanContext* = NULLPTR) const;\n+};\n+\n+/// \\brief Represents a parquet's RowGroup with extra information.\n+class ARROW_DS_EXPORT RowGroupInfo : public util::EqualityComparable<RowGroupInfo> {\n+ public:\n+  RowGroupInfo() : RowGroupInfo(-1) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier.\n+  explicit RowGroupInfo(int id) : RowGroupInfo(id, -1, NULLPTR) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier with statistics.\n+  RowGroupInfo(int id, int64_t num_rows, std::shared_ptr<Expression> statistics)\n+      : id_(id), num_rows_(num_rows), statistics_(std::move(statistics)) {}\n+\n+  /// \\brief Transform a vector of identifiers into a vector of RowGroupInfos\n+  static std::vector<RowGroupInfo> FromIdentifiers(const std::vector<int> ids);\n+  static std::vector<RowGroupInfo> FromCount(int count);\n+\n+  /// \\brief Return the RowGroup's identifier (index in the file).\n+  int id() const { return id_; }\n+\n+  /// \\brief Return the RowGroup's number of rows.\n   ///\n-  /// \\param[in] fragment to split\n-  /// \\param[in] filter expression that will ignore RowGroup that can't satisfy\n-  ///            the filter.\n+  /// If statistics are not provided, return 0.\n+  int64_t num_rows() const { return num_rows_; }\n+  void set_num_rows(int64_t num_rows) { num_rows_ = num_rows; }\n+\n+  /// \\brief Return the RowGroup's statistics\n+  const std::shared_ptr<Expression>& statistics() const { return statistics_; }\n+  void set_statistics(std::shared_ptr<Expression> statistics) {\n+    statistics_ = std::move(statistics);\n+  }\n+\n+  /// \\brief Indicate if statistics are set.\n+  bool HasStatistics() const { return statistics_ != NULLPTR; }\n+\n+  /// \\brief Indicate if the RowGroup's statistics satisfy the predicate.\n   ///\n-  /// \\return An iterator of fragment.\n-  Result<FragmentIterator> GetRowGroupFragments(\n-      const ParquetFileFragment& fragment,\n-      std::shared_ptr<Expression> filter = scalar(true));\n+  /// If the RowGroup was not initialized with statistics, it is deemd\n\nReview comment:\n       missing word at the end?\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -443,6 +445,53 @@ def _union_dataset(children, schema=None, **kwargs):\n     return UnionDataset(schema, children)\n \n \n+def parquet_dataset(metadata_path, schema=None, filesystem=None, format=None):\n+    \"\"\"\n+    Create a FileSystemDataset from a `_metadata` file created via\n+    `pyarrrow.parquet.write_metadata`.\n+\n+    Parameters\n+    ----------\n+    metadata_path : path,\n+        Path pointing to a single file parquet metadata file\n+    schema : Schema, optional\n+        Optionally provide the Schema for the Dataset, in which case it will\n+        not be inferred from the source.\n+    filesystem : FileSystem or URI string, default None\n+        If a single path is given as source and filesystem is None, then the\n+        filesystem will be inferred from the path.\n+        If an URI string is passed, then a filesystem object is constructed\n+        using the URI's optional path component as a directory prefix. See the\n+        examples below.\n+        Note that the URIs on Windows must follow 'file:///C:...' or\n+        'file:/C:...' patterns.\n+    format : ParquetFileFormat\n+        An instance of a ParquetFileFormat if special options needs to be\n+        passed.\n+\n+    Returns\n+    -------\n+    FileSystemDataset\n+    \"\"\"\n+    from pyarrow.fs import LocalFileSystem\n+\n+    if not isinstance(metadata_path, str):\n+        raise ValueError(\"metadata_path argument must be a string\")\n\nReview comment:\n       ```suggestion\r\n       metadata_path = _stringify_path(metadata_path)\r\n   ```\r\n   \r\n   (then it also handles pathlib.Path objects, and will also raise an error if is not a string / cannot be converted to a string)\n\n##########\nFile path: cpp/src/parquet/metadata.h\n##########\n@@ -258,10 +258,26 @@ class PARQUET_EXPORT FileMetaData {\n \n   const std::shared_ptr<const KeyValueMetadata>& key_value_metadata() const;\n \n-  // Set file_path ColumnChunk fields to a particular value\n+  /// \\brief Set a path to all ColumnChunk for all RowGroups.\n+  ///\n+  /// Commonly used by systems (Dask, Spark) who generates an metadata-only\n+  /// parquet file. The path are usually relative to said index file.\n+  ///\n+  /// \\param[in] path to set.\n   void set_file_path(const std::string& path);\n \n-  // Merge row-group metadata from \"other\" FileMetaData object\n+  /// \\brief Merge row groups from another metadata file into this one.\n+  ///\n+  /// The schema of the input FileMetaData must be equal to the\n+  /// schema of this object.\n+  ///\n+  /// This is used by systems who creates an aggregate metadata only file by\n\nReview comment:\n       ```suggestion\r\n     /// This is used by systems who creates an aggregate metadata-only file by\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n\nReview comment:\n       Does this mean that using `SplitByRowGroup` currently always invokes IO? (in principle it could be filtered/split based on the already-read RowGroupInfo objects ?\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n+    auto n_columns = row_group.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"RowGroup must have a least one columns to extract path\");\n+    }\n+\n+    auto first_column = row_group.ColumnChunk(0);\n+    auto path = first_column->file_path();\n+    if (path == \"\") {\n+      return Status::Invalid(\"Got empty file path\");\n\nReview comment:\n       Or reusing the phrase you used below as well: \"Could not infer file paths from FileMetaData: ..\"\n\n##########\nFile path: python/pyarrow/_dataset.pyx\n##########\n@@ -770,31 +806,43 @@ cdef class ParquetFileFragment(FileFragment):\n \n     @property\n     def row_groups(self):\n-        row_groups = set(self.parquet_file_fragment.row_groups())\n-        if len(row_groups) != 0:\n-            return row_groups\n-        return None\n+        cdef:\n+            vector[CRowGroupInfo] c_row_groups\n+        c_row_groups = self.parquet_file_fragment.row_groups()\n+        if c_row_groups.empty():\n+            return None\n+        return [RowGroupInfo.wrap(row_group) for row_group in c_row_groups]\n \n-    def get_row_group_fragments(self, Expression extra_filter=None):\n+    def split_by_row_group(self, Expression predicate=None):\n         \"\"\"\n+        Split the fragment in multiple fragments.\n+\n         Yield a Fragment wrapping each row group in this ParquetFileFragment.\n-        Row groups will be excluded whose metadata contradicts the either the\n-        filter provided on construction of this Fragment or the extra_filter\n-        argument.\n+        Row groups will be excluded whose metadata contradicts the optional\n+        predicate.\n+\n+        Parameters\n+        ----------\n+        predicate : Expression, default None\n+            Exclude RowGroups whose statistics contradicts the predicate.\n+\n+        Returns\n+        -------\n+        A generator of Fragment.\n         \"\"\"\n         cdef:\n-            CParquetFileFormat* c_format\n-            CFragmentIterator c_fragments\n-            shared_ptr[CExpression] c_extra_filter\n+            vector[shared_ptr[CFragment]] c_fragments\n+            shared_ptr[CExpression] c_predicate\n+            shared_ptr[CFragment] c_fragment\n \n         schema = self.physical_schema\n-        c_extra_filter = _insert_implicit_casts(extra_filter, schema)\n-        c_format = <CParquetFileFormat*> self.file_fragment.format().get()\n-        c_fragments = move(GetResultValue(c_format.GetRowGroupFragments(deref(\n-            self.parquet_file_fragment), move(c_extra_filter))))\n+        c_predicate = _insert_implicit_casts(predicate, schema)\n+        with nogil:\n+            c_fragments = move(GetResultValue(\n+                self.parquet_file_fragment.SplitByRowGroup(move(c_predicate))))\n \n-        for maybe_fragment in c_fragments:\n-            yield Fragment.wrap(GetResultValue(move(maybe_fragment)))\n+        for c_fragment in c_fragments:\n+            yield Fragment.wrap(c_fragment)\n\nReview comment:\n       I am wondering, is it still needed to have this yield (generator) instead of returning a list of fragments? As by now, all the heavy work is already done, I think? (the potentially reading of parquet metadata and filtering, since the `SplitByRowGroup` is not a lazy iterator on the C++ side)\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -35,11 +35,13 @@\n     Fragment,\n     HivePartitioning,\n     IpcFileFormat,\n+    ParquetDatasetFactory,\n     ParquetFileFormat,\n     ParquetFileFragment,\n     ParquetReadOptions,\n     Partitioning,\n     PartitioningFactory,\n+    RowGroupInfo,\n\nReview comment:\n       Maybe we should not expose this publicly? (is there a reason you would want to use this directly?)\n\n##########\nFile path: cpp/src/parquet/metadata.h\n##########\n@@ -258,10 +258,26 @@ class PARQUET_EXPORT FileMetaData {\n \n   const std::shared_ptr<const KeyValueMetadata>& key_value_metadata() const;\n \n-  // Set file_path ColumnChunk fields to a particular value\n+  /// \\brief Set a path to all ColumnChunk for all RowGroups.\n+  ///\n+  /// Commonly used by systems (Dask, Spark) who generates an metadata-only\n+  /// parquet file. The path are usually relative to said index file.\n\nReview comment:\n       ```suggestion\r\n     /// parquet file. The path is usually relative to said index file.\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.h\n##########\n@@ -97,53 +103,167 @@ class ARROW_DS_EXPORT ParquetFileFormat : public FileFormat {\n   Result<ScanTaskIterator> ScanFile(const FileSource& source,\n                                     std::shared_ptr<ScanOptions> options,\n                                     std::shared_ptr<ScanContext> context,\n-                                    const std::vector<int>& row_groups) const;\n+                                    std::vector<RowGroupInfo> row_groups) const;\n \n   using FileFormat::MakeFragment;\n \n+  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n-      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+      FileSource source, std::shared_ptr<Expression> partition_expression,\n+      std::vector<RowGroupInfo> row_groups);\n \n-  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n       FileSource source, std::shared_ptr<Expression> partition_expression,\n       std::vector<int> row_groups);\n \n-  /// \\brief Split a ParquetFileFragment into a Fragment for each row group.\n+  /// \\brief Create a Fragment targeting all RowGroups.\n+  Result<std::shared_ptr<FileFragment>> MakeFragment(\n+      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+\n+  /// \\brief Return a FileReader on the given source.\n+  Result<std::unique_ptr<parquet::arrow::FileReader>> GetReader(\n+      const FileSource& source, ScanOptions* = NULLPTR, ScanContext* = NULLPTR) const;\n+};\n+\n+/// \\brief Represents a parquet's RowGroup with extra information.\n+class ARROW_DS_EXPORT RowGroupInfo : public util::EqualityComparable<RowGroupInfo> {\n+ public:\n+  RowGroupInfo() : RowGroupInfo(-1) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier.\n+  explicit RowGroupInfo(int id) : RowGroupInfo(id, -1, NULLPTR) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier with statistics.\n+  RowGroupInfo(int id, int64_t num_rows, std::shared_ptr<Expression> statistics)\n+      : id_(id), num_rows_(num_rows), statistics_(std::move(statistics)) {}\n+\n+  /// \\brief Transform a vector of identifiers into a vector of RowGroupInfos\n+  static std::vector<RowGroupInfo> FromIdentifiers(const std::vector<int> ids);\n+  static std::vector<RowGroupInfo> FromCount(int count);\n+\n+  /// \\brief Return the RowGroup's identifier (index in the file).\n+  int id() const { return id_; }\n+\n+  /// \\brief Return the RowGroup's number of rows.\n   ///\n-  /// \\param[in] fragment to split\n-  /// \\param[in] filter expression that will ignore RowGroup that can't satisfy\n-  ///            the filter.\n+  /// If statistics are not provided, return 0.\n\nReview comment:\n       On line 134 above, it seems that the default is -1 when the number of rows is not specified / not known (and not 0) ?\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n+    auto n_columns = row_group.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"RowGroup must have a least one columns to extract path\");\n+    }\n+\n+    auto first_column = row_group.ColumnChunk(0);\n+    auto path = first_column->file_path();\n+    if (path == \"\") {\n+      return Status::Invalid(\"Got empty file path\");\n\nReview comment:\n       It might be useful to have a bit longer / more explicit error message here (I actually ran into this, because an older version of dask was creating wrong metadata .. (fixed some time ago), and at first I didn't understand what is was about).\r\n   \r\n   Something like: *\"Invalid metadata file for constructing a Parquet dataset: the column chunks file path should be set, but got empty file path.\"*, or something like this\n\n##########\nFile path: python/pyarrow/parquet.py\n##########\n@@ -1739,33 +1739,53 @@ def write_to_dataset(table, root_path, partition_cols=None,\n             metadata_collector[-1].set_file_path(outfile)\n \n \n-def write_metadata(schema, where, version='1.0',\n-                   use_deprecated_int96_timestamps=False,\n-                   coerce_timestamps=None):\n+def write_metadata(schema, where, metadata_collector=None, **kwargs):\n     \"\"\"\n-    Write metadata-only Parquet file from schema.\n+    Write metadata-only Parquet file from schema. This can be used with\n+    `write_to_dataset` to generate `_common_metadata` and `_metadata` sidecar\n+    files.\n \n     Parameters\n     ----------\n     schema : pyarrow.Schema\n     where: string or pyarrow.NativeFile\n-    version : {\"1.0\", \"2.0\"}, default \"1.0\"\n-        The Parquet format version, defaults to 1.0.\n-    use_deprecated_int96_timestamps : bool, default False\n-        Write nanosecond resolution timestamps to INT96 Parquet format.\n-    coerce_timestamps : str, default None\n-        Cast timestamps a particular resolution.\n-        Valid values: {None, 'ms', 'us'}.\n-    filesystem : FileSystem, default None\n-        If nothing passed, paths assumed to be found in the local on-disk\n-        filesystem.\n+    metadata_collector:\n+    **kwargs : dict,\n+        Additional kwargs for ParquetWriter class. See docstring for\n+        `ParquetWriter` for more information.\n+\n+    Examples\n+    --------\n+\n+    Write a dataset and collect metadata information.\n+\n+    >>> metadata_collector=[]\n\nReview comment:\n       ```suggestion\r\n       >>> metadata_collector = []\r\n   ```\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-19T13:01:31.511+0000",
                    "updated": "2020-05-19T13:01:31.511+0000",
                    "started": "2020-05-19T13:01:31.511+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "434930",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435089",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r427493504\n\n\n\n##########\nFile path: python/pyarrow/dataset.py\n##########\n@@ -35,11 +35,13 @@\n     Fragment,\n     HivePartitioning,\n     IpcFileFormat,\n+    ParquetDatasetFactory,\n     ParquetFileFormat,\n     ParquetFileFragment,\n     ParquetReadOptions,\n     Partitioning,\n     PartitioningFactory,\n+    RowGroupInfo,\n\nReview comment:\n       That's required for `ParquetFileFragment.row_groups`. I could change it to only return a list of integers.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-19T17:57:27.124+0000",
                    "updated": "2020-05-19T17:57:27.124+0000",
                    "started": "2020-05-19T17:57:27.124+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435089",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435090",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r427493814\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n\nReview comment:\n       Correct.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-19T17:57:50.546+0000",
                    "updated": "2020-05-19T17:57:50.546+0000",
                    "started": "2020-05-19T17:57:50.545+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435090",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435091",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r427493814\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n\nReview comment:\n       Correct. It could be filtered, but only if the dataset was generated via the `_metadata` file (or any explicit RowGroupInfo).\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-19T17:58:24.903+0000",
                    "updated": "2020-05-19T17:58:24.903+0000",
                    "started": "2020-05-19T17:58:24.903+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435091",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435093",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#issuecomment-630985072\n\n\n   I don't get a segault for the test you added, just a wrong exception being throw.\r\n   \r\n   ```python\r\n   >   raise IOError(errno, message)\r\n   E   FileNotFoundError: [Errno 2] Failed to open local file '/tmp/pytest-of-fsaintjacques/pytest-44/test_parquet_dataset_factory_i0/test_parquet_dataset/43bd0bd1002048e0b9bbc730f7614d18.parquet'. Detail: [errno 2] No such file or directory\r\n   \r\n   pyarrow/error.pxi:98: FileNotFoundError\r\n   \r\n   ```\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-19T18:00:15.429+0000",
                    "updated": "2020-05-19T18:00:15.429+0000",
                    "started": "2020-05-19T18:00:15.428+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435093",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435097",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#issuecomment-630991816\n\n\n   > I don't get a segault for the test you added, just a wrong exception being throw.\r\n   \r\n   A FileNotFoundError sounds good (the ValueError I added in the tests was just a bit random). Will rebuild locally to see if I still get this\r\n   \r\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-19T18:13:05.215+0000",
                    "updated": "2020-05-19T18:13:05.215+0000",
                    "started": "2020-05-19T18:13:05.215+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435097",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435103",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#issuecomment-631002289\n\n\n   I'm curious about the exception/segfault. If you can reproduce, feel free to share.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-19T18:32:19.195+0000",
                    "updated": "2020-05-19T18:32:19.195+0000",
                    "started": "2020-05-19T18:32:19.195+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435103",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435170",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#issuecomment-631059751\n\n\n   It seems this failure doesn't happen all the time for me. Running it a few times, I see also see the FileNotFoundError, but in 1 out of 2 cases, approximately. \r\n   \r\n   Now, when running it on an actual example in the interactive terminal (from a small dataset where I actually deleted one of the files), I consistently see the segfault:\r\n   \r\n   ```\r\n   In [1]: import pyarrow.dataset as ds                                                                                                                                                                               \r\n   \r\n   In [2]: dataset = ds.parquet_dataset(\"/tmp/tmp9qt6cph5/_metadata\")                                                                                                                                                 \r\n   \r\n   In [3]: dataset.to_table()                                                                                                                                                                                         \r\n   terminate called after throwing an instance of 'std::system_error'\r\n     what():  Invalid argument\r\n   Aborted (core dumped)\r\n   ```\r\n   \r\n   I get a different stacktrace when running the tests, something about the \"unlink\", so it might be that the way I remove the file in the test is not very robust / is not similar to deleting a file in the file browser.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-19T20:19:31.110+0000",
                    "updated": "2020-05-19T20:19:31.110+0000",
                    "started": "2020-05-19T20:19:31.110+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435170",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435463",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428026117\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n\nReview comment:\n       I think it would be good to at least try to avoid IO if the statistics are already available in the RowGroupInfo's (at least, from my understanding how this is used in RAPIDS, can check with them), but certainly not critical to do in this PR.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-20T13:49:20.093+0000",
                    "updated": "2020-05-20T13:49:20.093+0000",
                    "started": "2020-05-20T13:49:20.092+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435463",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435590",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428183533\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n\nReview comment:\n       please restrict the scope of try blocks to wrap just the function calls which may throw\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -315,20 +233,28 @@ class ParquetScanTaskIterator {\n   }\n \n   ParquetScanTaskIterator(std::shared_ptr<ScanOptions> options,\n-                          std::shared_ptr<ScanContext> context,\n-                          std::vector<int> column_projection, RowGroupSkipper skipper,\n-                          std::unique_ptr<parquet::arrow::FileReader> reader)\n+                          std::shared_ptr<ScanContext> context, FileSource source,\n+                          std::unique_ptr<parquet::arrow::FileReader> reader,\n+                          std::vector<int> column_projection,\n+                          std::vector<RowGroupInfo> row_groups)\n       : options_(std::move(options)),\n         context_(std::move(context)),\n+        source_(std::move(source)),\n+        reader_(std::move(reader)),\n         column_projection_(std::move(column_projection)),\n-        skipper_(std::move(skipper)),\n-        reader_(std::move(reader)) {}\n+        row_groups_(std::move(row_groups)) {}\n \n   std::shared_ptr<ScanOptions> options_;\n   std::shared_ptr<ScanContext> context_;\n-  std::vector<int> column_projection_;\n-  RowGroupSkipper skipper_;\n+\n+  FileSource source_;\n\nReview comment:\n       What is this used for?\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n\nReview comment:\n       ```suggestion\r\n       row_groups = FilterRowGroups(std::move(row_groups), *options->filter);\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n\nReview comment:\n       ```suggestion\r\n         auto row_group = metadata->RowGroup(id);\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n+    auto n_columns = row_group.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"RowGroup must have a least one columns to extract path\");\n+    }\n+\n+    auto first_column = row_group.ColumnChunk(0);\n+    auto path = first_column->file_path();\n+    if (path == \"\") {\n+      return Status::Invalid(\"Got empty file path\");\n+    }\n+\n+    for (int i = 1; i < n_columns; i++) {\n+      auto column = row_group.ColumnChunk(i);\n+      auto column_path = column->file_path();\n+      if (column_path != path) {\n+        return Status::Invalid(\"Path '\", column_path, \"' not equal to path '\", path,\n+                               \", for ColumnChunk at index \", i);\n+      }\n+    }\n+\n+    return fs::internal::ConcatAbstractPath(base_path, path);\n+  } catch (const ::parquet::ParquetException& e) {\n+    return Status::Invalid(\"Could not infer file path from RowGroup :\", e.what());\n+  }\n }\n \n-const ParquetFileFormat& ParquetFileFragment::parquet_format() const {\n-  return internal::checked_cast<const ParquetFileFormat&>(*format_);\n+Result<std::vector<std::shared_ptr<FileFragment>>>\n+ParquetDatasetFactory::CollectParquetFragments(\n+    const parquet::FileMetaData& metadata,\n+    const parquet::ArrowReaderProperties& properties) {\n+  try {\n+    auto n_columns = metadata.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"ParquetDatasetFactory at least one column\");\n+    }\n+\n+    std::unordered_map<std::string, std::vector<RowGroupInfo>> paths_and_row_group_size;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto manifest, GetSchemaManifest(metadata, properties));\n+\n+    for (int i = 0; i < metadata.num_row_groups(); i++) {\n+      auto row_group = metadata.RowGroup(i);\n+      ARROW_ASSIGN_OR_RAISE(auto path, FileFromRowGroup(base_path_, *row_group));\n+      auto stats = RowGroupStatisticsAsExpression(*row_group, manifest);\n+      auto num_rows = row_group->num_rows();\n+\n+      // Insert the path, or increase the count of row groups. It will be\n+      // assumed that the RowGroup of a file are ordered exactly like in\n+      // the metadata file.\n+      auto elem_and_inserted =\n+          paths_and_row_group_size.insert({path, {{0, num_rows, stats}}});\n+      if (!elem_and_inserted.second) {\n+        auto& path_and_count = *elem_and_inserted.first;\n+        auto& row_groups = path_and_count.second;\n+        path_and_count.second.emplace_back(row_groups.size(), num_rows, stats);\n+      }\n+    }\n+\n+    std::vector<std::shared_ptr<FileFragment>> fragments;\n+    for (const auto& elem : paths_and_row_group_size) {\n\nReview comment:\n       ```suggestion\r\n       for (auto&& elem : path_to_row_group_info) {\r\n   ```\r\n   std::move doesn't forward to a move cosntructor if the argument is const\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -163,124 +158,47 @@ static std::shared_ptr<Expression> ColumnChunkStatisticsAsExpression(\n               less_equal(field_expr, scalar(max)));\n }\n \n-static Result<std::shared_ptr<Expression>> RowGroupStatisticsAsExpression(\n-    const parquet::RowGroupMetaData& metadata,\n-    const parquet::ArrowReaderProperties& properties) {\n-  ARROW_ASSIGN_OR_RAISE(auto manifest, GetSchemaManifest(metadata, properties));\n-\n-  ExpressionVector expressions;\n-  for (const auto& schema_field : manifest.schema_fields) {\n-    expressions.emplace_back(ColumnChunkStatisticsAsExpression(schema_field, metadata));\n+static std::shared_ptr<Expression> RowGroupStatisticsAsExpression(\n+    const parquet::RowGroupMetaData& metadata, const SchemaManifest& manifest) {\n+  const auto& fields = manifest.schema_fields;\n+  ExpressionVector expressions{fields.size()};\n+  for (const auto& field : fields) {\n+    expressions.emplace_back(ColumnChunkStatisticsAsExpression(field, metadata));\n   }\n \n   return expressions.empty() ? scalar(true) : and_(expressions);\n }\n \n-// Skip RowGroups with a filter and metadata\n-class RowGroupSkipper {\n- public:\n-  static constexpr int kIterationDone = -1;\n-\n-  RowGroupSkipper(std::shared_ptr<parquet::FileMetaData> metadata,\n-                  parquet::ArrowReaderProperties arrow_properties,\n-                  std::shared_ptr<Expression> filter, std::vector<int> row_groups)\n-      : metadata_(std::move(metadata)),\n-        arrow_properties_(std::move(arrow_properties)),\n-        filter_(std::move(filter)),\n-        row_group_idx_(0),\n-        row_groups_(std::move(row_groups)),\n-        num_row_groups_(row_groups_.empty() ? metadata_->num_row_groups()\n-                                            : static_cast<int>(row_groups_.size())) {}\n-\n-  int Next() {\n-    while (row_group_idx_ < num_row_groups_) {\n-      const int row_group =\n-          row_groups_.empty() ? row_group_idx_++ : row_groups_[row_group_idx_++];\n-\n-      const auto row_group_metadata = metadata_->RowGroup(row_group);\n-\n-      const int64_t num_rows = row_group_metadata->num_rows();\n-      if (CanSkip(*row_group_metadata)) {\n-        rows_skipped_ += num_rows;\n-        continue;\n-      }\n-\n-      return row_group;\n-    }\n-\n-    return kIterationDone;\n-  }\n-\n- private:\n-  bool CanSkip(const parquet::RowGroupMetaData& metadata) const {\n-    auto maybe_stats_expr = RowGroupStatisticsAsExpression(metadata, arrow_properties_);\n-    // Errors with statistics are ignored and post-filtering will apply.\n-    if (!maybe_stats_expr.ok()) {\n-      return false;\n-    }\n-\n-    auto stats_expr = maybe_stats_expr.ValueOrDie();\n-    return !filter_->Assume(stats_expr)->IsSatisfiable();\n-  }\n-\n-  std::shared_ptr<parquet::FileMetaData> metadata_;\n-  parquet::ArrowReaderProperties arrow_properties_;\n-  std::shared_ptr<Expression> filter_;\n-  int row_group_idx_;\n-  std::vector<int> row_groups_;\n-  int num_row_groups_;\n-  int64_t rows_skipped_;\n-};\n-\n class ParquetScanTaskIterator {\n  public:\n   static Result<ScanTaskIterator> Make(std::shared_ptr<ScanOptions> options,\n\nReview comment:\n       Nothing here can fail so we can just make the constructor public\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n+    auto n_columns = row_group.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"RowGroup must have a least one columns to extract path\");\n+    }\n+\n+    auto first_column = row_group.ColumnChunk(0);\n+    auto path = first_column->file_path();\n+    if (path == \"\") {\n+      return Status::Invalid(\"Got empty file path\");\n+    }\n+\n+    for (int i = 1; i < n_columns; i++) {\n+      auto column = row_group.ColumnChunk(i);\n+      auto column_path = column->file_path();\n+      if (column_path != path) {\n+        return Status::Invalid(\"Path '\", column_path, \"' not equal to path '\", path,\n+                               \", for ColumnChunk at index \", i);\n+      }\n+    }\n+\n+    return fs::internal::ConcatAbstractPath(base_path, path);\n+  } catch (const ::parquet::ParquetException& e) {\n+    return Status::Invalid(\"Could not infer file path from RowGroup :\", e.what());\n+  }\n }\n \n-const ParquetFileFormat& ParquetFileFragment::parquet_format() const {\n-  return internal::checked_cast<const ParquetFileFormat&>(*format_);\n+Result<std::vector<std::shared_ptr<FileFragment>>>\n+ParquetDatasetFactory::CollectParquetFragments(\n+    const parquet::FileMetaData& metadata,\n+    const parquet::ArrowReaderProperties& properties) {\n+  try {\n+    auto n_columns = metadata.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"ParquetDatasetFactory at least one column\");\n+    }\n+\n+    std::unordered_map<std::string, std::vector<RowGroupInfo>> paths_and_row_group_size;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto manifest, GetSchemaManifest(metadata, properties));\n+\n+    for (int i = 0; i < metadata.num_row_groups(); i++) {\n+      auto row_group = metadata.RowGroup(i);\n+      ARROW_ASSIGN_OR_RAISE(auto path, FileFromRowGroup(base_path_, *row_group));\n+      auto stats = RowGroupStatisticsAsExpression(*row_group, manifest);\n+      auto num_rows = row_group->num_rows();\n+\n+      // Insert the path, or increase the count of row groups. It will be\n+      // assumed that the RowGroup of a file are ordered exactly like in\n+      // the metadata file.\n+      auto elem_and_inserted =\n+          paths_and_row_group_size.insert({path, {{0, num_rows, stats}}});\n+      if (!elem_and_inserted.second) {\n+        auto& path_and_count = *elem_and_inserted.first;\n+        auto& row_groups = path_and_count.second;\n+        path_and_count.second.emplace_back(row_groups.size(), num_rows, stats);\n+      }\n\nReview comment:\n       You can avoid constructing the vector for failed insertion:\r\n   ```suggestion\r\n         auto it = path_to_row_group_info.insert({path, {{}}}).first;\r\n         auto& row_groups = it->second;\r\n         row_groups.emplace_back(row_groups.size(), num_rows, std::move(stats));\r\n   ```\n\n##########\nFile path: cpp/src/parquet/arrow/reader_internal.cc\n##########\n@@ -747,8 +747,7 @@ Status TypedIntegralStatisticsAsScalars(const Statistics& statistics,\n       using CType = typename StatisticsType::T;\n       return MakeMinMaxScalar<CType, StatisticsType>(statistics, min, max);\n     default:\n-      return Status::NotImplemented(\"Cannot extract statistics for type \",\n-                                    logical_type->ToString());\n+      return Status::NotImplemented(\"Cannot extract statistics for type \");\n\nReview comment:\n       Maybe this should return `optional<pair<shared_ptr<Scalar>, shared_ptr<Scalar>>>` instead so that callers can decide whether a verbose error is appropriate? Follow up, in any case\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -163,124 +158,47 @@ static std::shared_ptr<Expression> ColumnChunkStatisticsAsExpression(\n               less_equal(field_expr, scalar(max)));\n }\n \n-static Result<std::shared_ptr<Expression>> RowGroupStatisticsAsExpression(\n-    const parquet::RowGroupMetaData& metadata,\n-    const parquet::ArrowReaderProperties& properties) {\n-  ARROW_ASSIGN_OR_RAISE(auto manifest, GetSchemaManifest(metadata, properties));\n-\n-  ExpressionVector expressions;\n-  for (const auto& schema_field : manifest.schema_fields) {\n-    expressions.emplace_back(ColumnChunkStatisticsAsExpression(schema_field, metadata));\n+static std::shared_ptr<Expression> RowGroupStatisticsAsExpression(\n+    const parquet::RowGroupMetaData& metadata, const SchemaManifest& manifest) {\n+  const auto& fields = manifest.schema_fields;\n+  ExpressionVector expressions{fields.size()};\n\nReview comment:\n       ```suggestion\r\n     ExpressionVector expressions;\r\n     expressions.reserve(fields.size());\r\n   ```\r\n   (otherwise the first `fields.size()` entries will be null)\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n\nReview comment:\n       std algorithms could be used here\r\n   ```suggestion\r\n     auto new_end = std::filter(row_groups.begin(), row_groups.end(), [&](const RowGroupInfo& info) {\r\n       return info.Satisfy(predicate);\r\n     });\r\n     row_groups.erase(new_end, row_groups.end());\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -163,124 +158,47 @@ static std::shared_ptr<Expression> ColumnChunkStatisticsAsExpression(\n               less_equal(field_expr, scalar(max)));\n }\n \n-static Result<std::shared_ptr<Expression>> RowGroupStatisticsAsExpression(\n-    const parquet::RowGroupMetaData& metadata,\n-    const parquet::ArrowReaderProperties& properties) {\n-  ARROW_ASSIGN_OR_RAISE(auto manifest, GetSchemaManifest(metadata, properties));\n-\n-  ExpressionVector expressions;\n-  for (const auto& schema_field : manifest.schema_fields) {\n-    expressions.emplace_back(ColumnChunkStatisticsAsExpression(schema_field, metadata));\n+static std::shared_ptr<Expression> RowGroupStatisticsAsExpression(\n+    const parquet::RowGroupMetaData& metadata, const SchemaManifest& manifest) {\n+  const auto& fields = manifest.schema_fields;\n+  ExpressionVector expressions{fields.size()};\n+  for (const auto& field : fields) {\n+    expressions.emplace_back(ColumnChunkStatisticsAsExpression(field, metadata));\n   }\n \n   return expressions.empty() ? scalar(true) : and_(expressions);\n }\n \n-// Skip RowGroups with a filter and metadata\n-class RowGroupSkipper {\n- public:\n-  static constexpr int kIterationDone = -1;\n-\n-  RowGroupSkipper(std::shared_ptr<parquet::FileMetaData> metadata,\n-                  parquet::ArrowReaderProperties arrow_properties,\n-                  std::shared_ptr<Expression> filter, std::vector<int> row_groups)\n-      : metadata_(std::move(metadata)),\n-        arrow_properties_(std::move(arrow_properties)),\n-        filter_(std::move(filter)),\n-        row_group_idx_(0),\n-        row_groups_(std::move(row_groups)),\n-        num_row_groups_(row_groups_.empty() ? metadata_->num_row_groups()\n-                                            : static_cast<int>(row_groups_.size())) {}\n-\n-  int Next() {\n-    while (row_group_idx_ < num_row_groups_) {\n-      const int row_group =\n-          row_groups_.empty() ? row_group_idx_++ : row_groups_[row_group_idx_++];\n-\n-      const auto row_group_metadata = metadata_->RowGroup(row_group);\n-\n-      const int64_t num_rows = row_group_metadata->num_rows();\n-      if (CanSkip(*row_group_metadata)) {\n-        rows_skipped_ += num_rows;\n-        continue;\n-      }\n-\n-      return row_group;\n-    }\n-\n-    return kIterationDone;\n-  }\n-\n- private:\n-  bool CanSkip(const parquet::RowGroupMetaData& metadata) const {\n-    auto maybe_stats_expr = RowGroupStatisticsAsExpression(metadata, arrow_properties_);\n-    // Errors with statistics are ignored and post-filtering will apply.\n-    if (!maybe_stats_expr.ok()) {\n-      return false;\n-    }\n-\n-    auto stats_expr = maybe_stats_expr.ValueOrDie();\n-    return !filter_->Assume(stats_expr)->IsSatisfiable();\n-  }\n-\n-  std::shared_ptr<parquet::FileMetaData> metadata_;\n-  parquet::ArrowReaderProperties arrow_properties_;\n-  std::shared_ptr<Expression> filter_;\n-  int row_group_idx_;\n-  std::vector<int> row_groups_;\n-  int num_row_groups_;\n-  int64_t rows_skipped_;\n-};\n-\n class ParquetScanTaskIterator {\n  public:\n   static Result<ScanTaskIterator> Make(std::shared_ptr<ScanOptions> options,\n                                        std::shared_ptr<ScanContext> context,\n-                                       std::unique_ptr<parquet::ParquetFileReader> reader,\n-                                       parquet::ArrowReaderProperties arrow_properties,\n-                                       const std::vector<int>& row_groups) {\n-    auto metadata = reader->metadata();\n-\n-    auto column_projection = InferColumnProjection(*metadata, arrow_properties, options);\n-\n-    std::unique_ptr<parquet::arrow::FileReader> arrow_reader;\n-    RETURN_NOT_OK(parquet::arrow::FileReader::Make(context->pool, std::move(reader),\n-                                                   arrow_properties, &arrow_reader));\n-\n-    RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                            options->filter, row_groups);\n-\n-    return ScanTaskIterator(ParquetScanTaskIterator(\n-        std::move(options), std::move(context), std::move(column_projection),\n-        std::move(skipper), std::move(arrow_reader)));\n+                                       FileSource source,\n+                                       std::unique_ptr<parquet::arrow::FileReader> reader,\n+                                       std::vector<RowGroupInfo> row_groups) {\n+    auto column_projection = InferColumnProjection(*reader, *options);\n+    return static_cast<ScanTaskIterator>(ParquetScanTaskIterator(\n+        std::move(options), std::move(context), std::move(source), std::move(reader),\n+        std::move(column_projection), std::move(row_groups)));\n   }\n \n   Result<std::shared_ptr<ScanTask>> Next() {\n-    auto row_group = skipper_.Next();\n-\n-    // Iteration is done.\n-    if (row_group == RowGroupSkipper::kIterationDone) {\n+    if (idx_ >= row_groups_.size()) {\n       return nullptr;\n     }\n \n+    auto row_group = row_groups_[idx_++];\n     return std::shared_ptr<ScanTask>(\n         new ParquetScanTask(row_group, column_projection_, reader_, options_, context_));\n   }\n \n  private:\n   // Compute the column projection out of an optional arrow::Schema\n-  static std::vector<int> InferColumnProjection(\n-      const parquet::FileMetaData& metadata,\n-      const parquet::ArrowReaderProperties& arrow_properties,\n-      const std::shared_ptr<ScanOptions>& options) {\n-    auto maybe_manifest = GetSchemaManifest(metadata, arrow_properties);\n-    if (!maybe_manifest.ok()) {\n-      return internal::Iota(metadata.num_columns());\n-    }\n-    auto manifest = std::move(maybe_manifest).ValueOrDie();\n-\n+  static std::vector<int> InferColumnProjection(const parquet::arrow::FileReader& reader,\n+                                                const ScanOptions& options) {\n+    auto manifest = reader.manifest();\n     // Checks if the field is needed in either the projection or the filter.\n-    auto fields_name = options->MaterializedFields();\n+    auto fields_name = options.MaterializedFields();\n\nReview comment:\n       ```suggestion\r\n       auto field_names = options.MaterializedFields();\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n\nReview comment:\n       ```suggestion\r\n                           AugmentAndFilter(std::move(row_groups), *options->filter, reader.get()));\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n+    auto n_columns = row_group.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"RowGroup must have a least one columns to extract path\");\n+    }\n+\n+    auto first_column = row_group.ColumnChunk(0);\n+    auto path = first_column->file_path();\n+    if (path == \"\") {\n+      return Status::Invalid(\"Got empty file path\");\n+    }\n+\n+    for (int i = 1; i < n_columns; i++) {\n+      auto column = row_group.ColumnChunk(i);\n+      auto column_path = column->file_path();\n+      if (column_path != path) {\n+        return Status::Invalid(\"Path '\", column_path, \"' not equal to path '\", path,\n+                               \", for ColumnChunk at index \", i);\n+      }\n+    }\n+\n+    return fs::internal::ConcatAbstractPath(base_path, path);\n+  } catch (const ::parquet::ParquetException& e) {\n+    return Status::Invalid(\"Could not infer file path from RowGroup :\", e.what());\n+  }\n }\n \n-const ParquetFileFormat& ParquetFileFragment::parquet_format() const {\n-  return internal::checked_cast<const ParquetFileFormat&>(*format_);\n+Result<std::vector<std::shared_ptr<FileFragment>>>\n+ParquetDatasetFactory::CollectParquetFragments(\n+    const parquet::FileMetaData& metadata,\n+    const parquet::ArrowReaderProperties& properties) {\n+  try {\n+    auto n_columns = metadata.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"ParquetDatasetFactory at least one column\");\n\nReview comment:\n       ```suggestion\r\n         return Status::Invalid(\"ParquetDatasetFactory must contain a schema with at least one column\");\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.h\n##########\n@@ -97,53 +103,167 @@ class ARROW_DS_EXPORT ParquetFileFormat : public FileFormat {\n   Result<ScanTaskIterator> ScanFile(const FileSource& source,\n                                     std::shared_ptr<ScanOptions> options,\n                                     std::shared_ptr<ScanContext> context,\n-                                    const std::vector<int>& row_groups) const;\n+                                    std::vector<RowGroupInfo> row_groups) const;\n \n   using FileFormat::MakeFragment;\n \n+  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n-      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+      FileSource source, std::shared_ptr<Expression> partition_expression,\n+      std::vector<RowGroupInfo> row_groups);\n \n-  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n       FileSource source, std::shared_ptr<Expression> partition_expression,\n       std::vector<int> row_groups);\n \n-  /// \\brief Split a ParquetFileFragment into a Fragment for each row group.\n+  /// \\brief Create a Fragment targeting all RowGroups.\n+  Result<std::shared_ptr<FileFragment>> MakeFragment(\n+      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+\n+  /// \\brief Return a FileReader on the given source.\n+  Result<std::unique_ptr<parquet::arrow::FileReader>> GetReader(\n+      const FileSource& source, ScanOptions* = NULLPTR, ScanContext* = NULLPTR) const;\n+};\n+\n+/// \\brief Represents a parquet's RowGroup with extra information.\n+class ARROW_DS_EXPORT RowGroupInfo : public util::EqualityComparable<RowGroupInfo> {\n+ public:\n+  RowGroupInfo() : RowGroupInfo(-1) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier.\n+  explicit RowGroupInfo(int id) : RowGroupInfo(id, -1, NULLPTR) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier with statistics.\n+  RowGroupInfo(int id, int64_t num_rows, std::shared_ptr<Expression> statistics)\n+      : id_(id), num_rows_(num_rows), statistics_(std::move(statistics)) {}\n+\n+  /// \\brief Transform a vector of identifiers into a vector of RowGroupInfos\n+  static std::vector<RowGroupInfo> FromIdentifiers(const std::vector<int> ids);\n+  static std::vector<RowGroupInfo> FromCount(int count);\n+\n+  /// \\brief Return the RowGroup's identifier (index in the file).\n+  int id() const { return id_; }\n+\n+  /// \\brief Return the RowGroup's number of rows.\n   ///\n-  /// \\param[in] fragment to split\n-  /// \\param[in] filter expression that will ignore RowGroup that can't satisfy\n-  ///            the filter.\n+  /// If statistics are not provided, return 0.\n+  int64_t num_rows() const { return num_rows_; }\n+  void set_num_rows(int64_t num_rows) { num_rows_ = num_rows; }\n+\n+  /// \\brief Return the RowGroup's statistics\n+  const std::shared_ptr<Expression>& statistics() const { return statistics_; }\n+  void set_statistics(std::shared_ptr<Expression> statistics) {\n+    statistics_ = std::move(statistics);\n+  }\n+\n+  /// \\brief Indicate if statistics are set.\n+  bool HasStatistics() const { return statistics_ != NULLPTR; }\n+\n+  /// \\brief Indicate if the RowGroup's statistics satisfy the predicate.\n   ///\n-  /// \\return An iterator of fragment.\n-  Result<FragmentIterator> GetRowGroupFragments(\n-      const ParquetFileFragment& fragment,\n-      std::shared_ptr<Expression> filter = scalar(true));\n+  /// If the RowGroup was not initialized with statistics, it is deemd\n+  bool Satisfy(const Expression& predicate) const;\n+\n+  /// \\brief Indicate if the other RowGroup points to the same RowGroup.\n+  bool Equals(const RowGroupInfo& other) const { return id() == other.id(); }\n+\n+ private:\n+  int id_;\n+  int64_t num_rows_;\n+  std::shared_ptr<Expression> statistics_;\n };\n \n+/// \\brief A FileFragment with parquet logic.\n+///\n+/// ParquetFileFragment provides a lazy (with respect to IO) interface to\n+/// scan parquet files. Any heavy IO calls is deferred in the Scan() method.\n+///\n+/// The caller can provide an optional list of selected RowGroups to limit the\n+/// number of scanned RowGroups, or control parallelism partitioning.\n\nReview comment:\n       \"or control parallelism partitioning\" is not very clear, could you change this into an example use case for limiting the number of scanned RowGroups? (I think that matches your intent here)\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n\nReview comment:\n       ```suggestion\r\n     for (auto&& row_group : row_groups) {\r\n   ```\n\n##########\nFile path: python/pyarrow/_dataset.pyx\n##########\n@@ -770,31 +806,43 @@ cdef class ParquetFileFragment(FileFragment):\n \n     @property\n     def row_groups(self):\n-        row_groups = set(self.parquet_file_fragment.row_groups())\n-        if len(row_groups) != 0:\n-            return row_groups\n-        return None\n+        cdef:\n+            vector[CRowGroupInfo] c_row_groups\n+        c_row_groups = self.parquet_file_fragment.row_groups()\n+        if c_row_groups.empty():\n+            return None\n+        return [RowGroupInfo.wrap(row_group) for row_group in c_row_groups]\n \n-    def get_row_group_fragments(self, Expression extra_filter=None):\n+    def split_by_row_group(self, Expression predicate=None):\n         \"\"\"\n+        Split the fragment in multiple fragments.\n\nReview comment:\n       ```suggestion\r\n           Split the fragment into multiple fragments.\r\n   ```\r\n   Could you replicate this comment in c++?\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n+    auto n_columns = row_group.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"RowGroup must have a least one columns to extract path\");\n+    }\n+\n+    auto first_column = row_group.ColumnChunk(0);\n+    auto path = first_column->file_path();\n+    if (path == \"\") {\n+      return Status::Invalid(\"Got empty file path\");\n+    }\n+\n+    for (int i = 1; i < n_columns; i++) {\n+      auto column = row_group.ColumnChunk(i);\n+      auto column_path = column->file_path();\n+      if (column_path != path) {\n+        return Status::Invalid(\"Path '\", column_path, \"' not equal to path '\", path,\n+                               \", for ColumnChunk at index \", i);\n+      }\n+    }\n+\n+    return fs::internal::ConcatAbstractPath(base_path, path);\n+  } catch (const ::parquet::ParquetException& e) {\n+    return Status::Invalid(\"Could not infer file path from RowGroup :\", e.what());\n+  }\n }\n \n-const ParquetFileFormat& ParquetFileFragment::parquet_format() const {\n-  return internal::checked_cast<const ParquetFileFormat&>(*format_);\n+Result<std::vector<std::shared_ptr<FileFragment>>>\n+ParquetDatasetFactory::CollectParquetFragments(\n+    const parquet::FileMetaData& metadata,\n+    const parquet::ArrowReaderProperties& properties) {\n+  try {\n+    auto n_columns = metadata.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"ParquetDatasetFactory at least one column\");\n+    }\n+\n+    std::unordered_map<std::string, std::vector<RowGroupInfo>> paths_and_row_group_size;\n\nReview comment:\n       ```suggestion\r\n       std::unordered_map<std::string, std::vector<RowGroupInfo>> path_to_row_group_info;\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.h\n##########\n@@ -97,53 +103,167 @@ class ARROW_DS_EXPORT ParquetFileFormat : public FileFormat {\n   Result<ScanTaskIterator> ScanFile(const FileSource& source,\n                                     std::shared_ptr<ScanOptions> options,\n                                     std::shared_ptr<ScanContext> context,\n-                                    const std::vector<int>& row_groups) const;\n+                                    std::vector<RowGroupInfo> row_groups) const;\n \n   using FileFormat::MakeFragment;\n \n+  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n-      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+      FileSource source, std::shared_ptr<Expression> partition_expression,\n+      std::vector<RowGroupInfo> row_groups);\n \n-  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n       FileSource source, std::shared_ptr<Expression> partition_expression,\n       std::vector<int> row_groups);\n \n-  /// \\brief Split a ParquetFileFragment into a Fragment for each row group.\n+  /// \\brief Create a Fragment targeting all RowGroups.\n+  Result<std::shared_ptr<FileFragment>> MakeFragment(\n+      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+\n+  /// \\brief Return a FileReader on the given source.\n+  Result<std::unique_ptr<parquet::arrow::FileReader>> GetReader(\n+      const FileSource& source, ScanOptions* = NULLPTR, ScanContext* = NULLPTR) const;\n+};\n+\n+/// \\brief Represents a parquet's RowGroup with extra information.\n+class ARROW_DS_EXPORT RowGroupInfo : public util::EqualityComparable<RowGroupInfo> {\n+ public:\n+  RowGroupInfo() : RowGroupInfo(-1) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier.\n+  explicit RowGroupInfo(int id) : RowGroupInfo(id, -1, NULLPTR) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier with statistics.\n+  RowGroupInfo(int id, int64_t num_rows, std::shared_ptr<Expression> statistics)\n+      : id_(id), num_rows_(num_rows), statistics_(std::move(statistics)) {}\n+\n+  /// \\brief Transform a vector of identifiers into a vector of RowGroupInfos\n+  static std::vector<RowGroupInfo> FromIdentifiers(const std::vector<int> ids);\n+  static std::vector<RowGroupInfo> FromCount(int count);\n+\n+  /// \\brief Return the RowGroup's identifier (index in the file).\n+  int id() const { return id_; }\n+\n+  /// \\brief Return the RowGroup's number of rows.\n   ///\n-  /// \\param[in] fragment to split\n-  /// \\param[in] filter expression that will ignore RowGroup that can't satisfy\n-  ///            the filter.\n+  /// If statistics are not provided, return 0.\n+  int64_t num_rows() const { return num_rows_; }\n+  void set_num_rows(int64_t num_rows) { num_rows_ = num_rows; }\n+\n+  /// \\brief Return the RowGroup's statistics\n+  const std::shared_ptr<Expression>& statistics() const { return statistics_; }\n+  void set_statistics(std::shared_ptr<Expression> statistics) {\n+    statistics_ = std::move(statistics);\n+  }\n+\n+  /// \\brief Indicate if statistics are set.\n+  bool HasStatistics() const { return statistics_ != NULLPTR; }\n+\n+  /// \\brief Indicate if the RowGroup's statistics satisfy the predicate.\n   ///\n-  /// \\return An iterator of fragment.\n-  Result<FragmentIterator> GetRowGroupFragments(\n-      const ParquetFileFragment& fragment,\n-      std::shared_ptr<Expression> filter = scalar(true));\n+  /// If the RowGroup was not initialized with statistics, it is deemd\n+  bool Satisfy(const Expression& predicate) const;\n+\n+  /// \\brief Indicate if the other RowGroup points to the same RowGroup.\n+  bool Equals(const RowGroupInfo& other) const { return id() == other.id(); }\n+\n+ private:\n+  int id_;\n+  int64_t num_rows_;\n+  std::shared_ptr<Expression> statistics_;\n };\n \n+/// \\brief A FileFragment with parquet logic.\n+///\n+/// ParquetFileFragment provides a lazy (with respect to IO) interface to\n+/// scan parquet files. Any heavy IO calls is deferred in the Scan() method.\n+///\n+/// The caller can provide an optional list of selected RowGroups to limit the\n+/// number of scanned RowGroups, or control parallelism partitioning.\n+///\n+/// It can also attach optional statistics with each RowGroups, providing\n+/// pushdown predicate benefits before invoking any heavy IO. This can induce\n+/// significant performance boost when scanning high latency file systems.\n class ARROW_DS_EXPORT ParquetFileFragment : public FileFragment {\n  public:\n   Result<ScanTaskIterator> Scan(std::shared_ptr<ScanOptions> options,\n                                 std::shared_ptr<ScanContext> context) override;\n \n-  /// \\brief The row groups viewed by this Fragment. This may be empty which signifies all\n-  /// row groups are selected.\n-  const std::vector<int>& row_groups() const { return row_groups_; }\n+  Result<FragmentVector> SplitByRowGroup(const std::shared_ptr<Expression>& predicate);\n+\n+  /// \\brief Return the RowGroups selected by this fragment. An empty list\n+  /// represents all RowGroups in the parquet file.\n+  const std::vector<RowGroupInfo>& row_groups() const { return row_groups_; }\n \n  private:\n   ParquetFileFragment(FileSource source, std::shared_ptr<FileFormat> format,\n                       std::shared_ptr<Expression> partition_expression,\n-                      std::vector<int> row_groups)\n-      : FileFragment(std::move(source), std::move(format),\n-                     std::move(partition_expression)),\n-        row_groups_(std::move(row_groups)) {}\n-\n-  const ParquetFileFormat& parquet_format() const;\n+                      std::vector<RowGroupInfo> row_groups);\n \n-  std::vector<int> row_groups_;\n+  std::vector<RowGroupInfo> row_groups_;\n+  ParquetFileFormat& parquet_format_;\n \n   friend class ParquetFileFormat;\n };\n \n+/// \\brief Create FileSystemDataset from custom `_metadata` cache file.\n+///\n+/// Dask and other systems will generate a cache metadata file by concatenating\n+/// the RowGroupMetaData of multiple parquet files in a single parquet file.\n\nReview comment:\n       ```suggestion\r\n   /// the RowGroupMetaData of multiple parquet files into a single (otherwise\r\n   /// empty) parquet file.\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.h\n##########\n@@ -97,53 +103,167 @@ class ARROW_DS_EXPORT ParquetFileFormat : public FileFormat {\n   Result<ScanTaskIterator> ScanFile(const FileSource& source,\n                                     std::shared_ptr<ScanOptions> options,\n                                     std::shared_ptr<ScanContext> context,\n-                                    const std::vector<int>& row_groups) const;\n+                                    std::vector<RowGroupInfo> row_groups) const;\n \n   using FileFormat::MakeFragment;\n \n+  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n-      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+      FileSource source, std::shared_ptr<Expression> partition_expression,\n+      std::vector<RowGroupInfo> row_groups);\n \n-  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n       FileSource source, std::shared_ptr<Expression> partition_expression,\n       std::vector<int> row_groups);\n \n-  /// \\brief Split a ParquetFileFragment into a Fragment for each row group.\n+  /// \\brief Create a Fragment targeting all RowGroups.\n+  Result<std::shared_ptr<FileFragment>> MakeFragment(\n+      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+\n+  /// \\brief Return a FileReader on the given source.\n+  Result<std::unique_ptr<parquet::arrow::FileReader>> GetReader(\n+      const FileSource& source, ScanOptions* = NULLPTR, ScanContext* = NULLPTR) const;\n+};\n+\n+/// \\brief Represents a parquet's RowGroup with extra information.\n+class ARROW_DS_EXPORT RowGroupInfo : public util::EqualityComparable<RowGroupInfo> {\n+ public:\n+  RowGroupInfo() : RowGroupInfo(-1) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier.\n+  explicit RowGroupInfo(int id) : RowGroupInfo(id, -1, NULLPTR) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier with statistics.\n+  RowGroupInfo(int id, int64_t num_rows, std::shared_ptr<Expression> statistics)\n+      : id_(id), num_rows_(num_rows), statistics_(std::move(statistics)) {}\n+\n+  /// \\brief Transform a vector of identifiers into a vector of RowGroupInfos\n+  static std::vector<RowGroupInfo> FromIdentifiers(const std::vector<int> ids);\n+  static std::vector<RowGroupInfo> FromCount(int count);\n+\n+  /// \\brief Return the RowGroup's identifier (index in the file).\n+  int id() const { return id_; }\n+\n+  /// \\brief Return the RowGroup's number of rows.\n   ///\n-  /// \\param[in] fragment to split\n-  /// \\param[in] filter expression that will ignore RowGroup that can't satisfy\n-  ///            the filter.\n+  /// If statistics are not provided, return 0.\n+  int64_t num_rows() const { return num_rows_; }\n+  void set_num_rows(int64_t num_rows) { num_rows_ = num_rows; }\n+\n+  /// \\brief Return the RowGroup's statistics\n+  const std::shared_ptr<Expression>& statistics() const { return statistics_; }\n+  void set_statistics(std::shared_ptr<Expression> statistics) {\n+    statistics_ = std::move(statistics);\n+  }\n+\n+  /// \\brief Indicate if statistics are set.\n+  bool HasStatistics() const { return statistics_ != NULLPTR; }\n+\n+  /// \\brief Indicate if the RowGroup's statistics satisfy the predicate.\n   ///\n-  /// \\return An iterator of fragment.\n-  Result<FragmentIterator> GetRowGroupFragments(\n-      const ParquetFileFragment& fragment,\n-      std::shared_ptr<Expression> filter = scalar(true));\n+  /// If the RowGroup was not initialized with statistics, it is deemd\n\nReview comment:\n       ```suggestion\r\n     /// This will return true if the RowGroup was not initialized with statistics\r\n     /// (rather than silently reading metadata for a complete check).\r\n   ```\r\n   Probably overkill: this could return `optional<bool>`, with `nullopt` indicating that we have not performed a complete check?\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n+    auto n_columns = row_group.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"RowGroup must have a least one columns to extract path\");\n+    }\n+\n+    auto first_column = row_group.ColumnChunk(0);\n+    auto path = first_column->file_path();\n+    if (path == \"\") {\n+      return Status::Invalid(\"Got empty file path\");\n+    }\n+\n+    for (int i = 1; i < n_columns; i++) {\n+      auto column = row_group.ColumnChunk(i);\n+      auto column_path = column->file_path();\n+      if (column_path != path) {\n+        return Status::Invalid(\"Path '\", column_path, \"' not equal to path '\", path,\n+                               \", for ColumnChunk at index \", i);\n+      }\n+    }\n+\n+    return fs::internal::ConcatAbstractPath(base_path, path);\n+  } catch (const ::parquet::ParquetException& e) {\n+    return Status::Invalid(\"Could not infer file path from RowGroup :\", e.what());\n+  }\n }\n \n-const ParquetFileFormat& ParquetFileFragment::parquet_format() const {\n-  return internal::checked_cast<const ParquetFileFormat&>(*format_);\n+Result<std::vector<std::shared_ptr<FileFragment>>>\n+ParquetDatasetFactory::CollectParquetFragments(\n+    const parquet::FileMetaData& metadata,\n+    const parquet::ArrowReaderProperties& properties) {\n+  try {\n+    auto n_columns = metadata.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"ParquetDatasetFactory at least one column\");\n+    }\n+\n+    std::unordered_map<std::string, std::vector<RowGroupInfo>> paths_and_row_group_size;\n+\n+    ARROW_ASSIGN_OR_RAISE(auto manifest, GetSchemaManifest(metadata, properties));\n+\n+    for (int i = 0; i < metadata.num_row_groups(); i++) {\n+      auto row_group = metadata.RowGroup(i);\n+      ARROW_ASSIGN_OR_RAISE(auto path, FileFromRowGroup(base_path_, *row_group));\n+      auto stats = RowGroupStatisticsAsExpression(*row_group, manifest);\n+      auto num_rows = row_group->num_rows();\n+\n+      // Insert the path, or increase the count of row groups. It will be\n+      // assumed that the RowGroup of a file are ordered exactly like in\n+      // the metadata file.\n+      auto elem_and_inserted =\n+          paths_and_row_group_size.insert({path, {{0, num_rows, stats}}});\n+      if (!elem_and_inserted.second) {\n+        auto& path_and_count = *elem_and_inserted.first;\n+        auto& row_groups = path_and_count.second;\n+        path_and_count.second.emplace_back(row_groups.size(), num_rows, stats);\n+      }\n+    }\n+\n+    std::vector<std::shared_ptr<FileFragment>> fragments;\n+    for (const auto& elem : paths_and_row_group_size) {\n+      ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                            format_->MakeFragment({std::move(elem.first), filesystem_},\n+                                                  scalar(true), std::move(elem.second)));\n+      fragments.push_back(std::move(fragment));\n+    }\n+\n+    return fragments;\n+  } catch (const ::parquet::ParquetException& e) {\n+    return Status::Invalid(\"Could not infer file paths from FileMetaData:\", e.what());\n+  }\n+}\n+\n+Result<std::shared_ptr<Dataset>> ParquetDatasetFactory::Finish(FinishOptions options) {\n+  std::shared_ptr<Schema> schema = options.schema;\n+  bool schema_missing = schema == nullptr;\n+  if (schema_missing) {\n+    ARROW_ASSIGN_OR_RAISE(schema, Inspect(options.inspect_options));\n+  }\n+\n+  auto properties = MakeArrowReaderProperties(*format_, *metadata_);\n+  ARROW_ASSIGN_OR_RAISE(auto fragments, CollectParquetFragments(*metadata_, properties));\n+  return FileSystemDataset::Make(std::move(schema), scalar(true), format_, fragments);\n\nReview comment:\n       ```suggestion\r\n     return FileSystemDataset::Make(std::move(schema), scalar(true), format_, std::move(fragments));\r\n   ```\n\n##########\nFile path: cpp/src/arrow/dataset/scanner.cc\n##########\n@@ -177,32 +177,38 @@ static inline RecordBatchVector FlattenRecordBatchVector(\n   return flattened;\n }\n \n+struct TableAssemblyState {\n+  /// Protecting mutating accesses to batches\n+  std::mutex mutex{};\n+  std::vector<RecordBatchVector> batches{};\n+\n+  void Emplace(RecordBatchVector b, size_t position) {\n+    std::lock_guard<std::mutex> lock(mutex);\n+    if (batches.size() <= position) {\n+      batches.resize(position + 1);\n+    }\n+    batches[position] = std::move(b);\n+  }\n+};\n+\n Result<std::shared_ptr<Table>> Scanner::ToTable() {\n   ARROW_ASSIGN_OR_RAISE(auto scan_task_it, Scan());\n   auto task_group = scan_context_->TaskGroup();\n \n-  // Protecting mutating accesses to batches\n-  std::mutex mutex;\n-  std::vector<RecordBatchVector> batches;\n+  /// Wraps the state in a shared_ptr to ensure that a failing ScanTask don't\n+  /// invalidate the concurrent running tasks because Finish() early returns\n+  /// and the mutex/batches may got out of scope.\n\nReview comment:\n       ```suggestion\r\n     /// Wraps the state in a shared_ptr to ensure that failing ScanTasks don't\r\n     /// invalidate concurrently running tasks when Finish() early returns\r\n     /// and the mutex/batches fall out of scope.\r\n   ```\r\n   nice catch\n\n##########\nFile path: python/pyarrow/_dataset.pyx\n##########\n@@ -770,31 +806,43 @@ cdef class ParquetFileFragment(FileFragment):\n \n     @property\n     def row_groups(self):\n-        row_groups = set(self.parquet_file_fragment.row_groups())\n-        if len(row_groups) != 0:\n-            return row_groups\n-        return None\n+        cdef:\n+            vector[CRowGroupInfo] c_row_groups\n+        c_row_groups = self.parquet_file_fragment.row_groups()\n+        if c_row_groups.empty():\n+            return None\n+        return [RowGroupInfo.wrap(row_group) for row_group in c_row_groups]\n \n-    def get_row_group_fragments(self, Expression extra_filter=None):\n+    def split_by_row_group(self, Expression predicate=None):\n         \"\"\"\n+        Split the fragment in multiple fragments.\n+\n         Yield a Fragment wrapping each row group in this ParquetFileFragment.\n-        Row groups will be excluded whose metadata contradicts the either the\n-        filter provided on construction of this Fragment or the extra_filter\n-        argument.\n+        Row groups will be excluded whose metadata contradicts the optional\n+        predicate.\n+\n+        Parameters\n+        ----------\n+        predicate : Expression, default None\n+            Exclude RowGroups whose statistics contradicts the predicate.\n+\n+        Returns\n+        -------\n+        A generator of Fragment.\n         \"\"\"\n         cdef:\n-            CParquetFileFormat* c_format\n-            CFragmentIterator c_fragments\n-            shared_ptr[CExpression] c_extra_filter\n+            vector[shared_ptr[CFragment]] c_fragments\n+            shared_ptr[CExpression] c_predicate\n+            shared_ptr[CFragment] c_fragment\n \n         schema = self.physical_schema\n-        c_extra_filter = _insert_implicit_casts(extra_filter, schema)\n-        c_format = <CParquetFileFormat*> self.file_fragment.format().get()\n-        c_fragments = move(GetResultValue(c_format.GetRowGroupFragments(deref(\n-            self.parquet_file_fragment), move(c_extra_filter))))\n+        c_predicate = _insert_implicit_casts(predicate, schema)\n+        with nogil:\n+            c_fragments = move(GetResultValue(\n+                self.parquet_file_fragment.SplitByRowGroup(move(c_predicate))))\n \n-        for maybe_fragment in c_fragments:\n-            yield Fragment.wrap(GetResultValue(move(maybe_fragment)))\n+        for c_fragment in c_fragments:\n+            yield Fragment.wrap(c_fragment)\n\nReview comment:\n       agreed\r\n   ```suggestion\r\n           return [Fragment.wrap(c_fragment) for c_fragment in c_fragments]\r\n   ```\n\n##########\nFile path: python/pyarrow/_dataset.pyx\n##########\n@@ -1446,6 +1494,47 @@ cdef class UnionDatasetFactory(DatasetFactory):\n         self.union_factory = <CUnionDatasetFactory*> sp.get()\n \n \n+cdef class ParquetDatasetFactory(DatasetFactory):\n+    \"\"\"\n+    Create a ParquetDatasetFactory from a Parquet `_metadata` file.\n+\n+    Parameters\n+    ----------\n+    metadata_path: str\n\nReview comment:\n       ```suggestion\r\n       metadata_path : str\r\n   ```\r\n   Not sure if the doccomment parser actually relies on that space\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.h\n##########\n@@ -97,53 +103,167 @@ class ARROW_DS_EXPORT ParquetFileFormat : public FileFormat {\n   Result<ScanTaskIterator> ScanFile(const FileSource& source,\n                                     std::shared_ptr<ScanOptions> options,\n                                     std::shared_ptr<ScanContext> context,\n-                                    const std::vector<int>& row_groups) const;\n+                                    std::vector<RowGroupInfo> row_groups) const;\n \n   using FileFormat::MakeFragment;\n \n+  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n-      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+      FileSource source, std::shared_ptr<Expression> partition_expression,\n+      std::vector<RowGroupInfo> row_groups);\n \n-  /// \\brief Create a Fragment, restricted to the specified row groups.\n   Result<std::shared_ptr<FileFragment>> MakeFragment(\n       FileSource source, std::shared_ptr<Expression> partition_expression,\n       std::vector<int> row_groups);\n \n-  /// \\brief Split a ParquetFileFragment into a Fragment for each row group.\n+  /// \\brief Create a Fragment targeting all RowGroups.\n+  Result<std::shared_ptr<FileFragment>> MakeFragment(\n+      FileSource source, std::shared_ptr<Expression> partition_expression) override;\n+\n+  /// \\brief Return a FileReader on the given source.\n+  Result<std::unique_ptr<parquet::arrow::FileReader>> GetReader(\n+      const FileSource& source, ScanOptions* = NULLPTR, ScanContext* = NULLPTR) const;\n+};\n+\n+/// \\brief Represents a parquet's RowGroup with extra information.\n+class ARROW_DS_EXPORT RowGroupInfo : public util::EqualityComparable<RowGroupInfo> {\n+ public:\n+  RowGroupInfo() : RowGroupInfo(-1) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier.\n+  explicit RowGroupInfo(int id) : RowGroupInfo(id, -1, NULLPTR) {}\n+\n+  /// \\brief Construct a RowGroup from an identifier with statistics.\n+  RowGroupInfo(int id, int64_t num_rows, std::shared_ptr<Expression> statistics)\n+      : id_(id), num_rows_(num_rows), statistics_(std::move(statistics)) {}\n+\n+  /// \\brief Transform a vector of identifiers into a vector of RowGroupInfos\n+  static std::vector<RowGroupInfo> FromIdentifiers(const std::vector<int> ids);\n+  static std::vector<RowGroupInfo> FromCount(int count);\n+\n+  /// \\brief Return the RowGroup's identifier (index in the file).\n+  int id() const { return id_; }\n+\n+  /// \\brief Return the RowGroup's number of rows.\n   ///\n-  /// \\param[in] fragment to split\n-  /// \\param[in] filter expression that will ignore RowGroup that can't satisfy\n-  ///            the filter.\n+  /// If statistics are not provided, return 0.\n+  int64_t num_rows() const { return num_rows_; }\n+  void set_num_rows(int64_t num_rows) { num_rows_ = num_rows; }\n+\n+  /// \\brief Return the RowGroup's statistics\n+  const std::shared_ptr<Expression>& statistics() const { return statistics_; }\n+  void set_statistics(std::shared_ptr<Expression> statistics) {\n+    statistics_ = std::move(statistics);\n+  }\n+\n+  /// \\brief Indicate if statistics are set.\n+  bool HasStatistics() const { return statistics_ != NULLPTR; }\n+\n+  /// \\brief Indicate if the RowGroup's statistics satisfy the predicate.\n   ///\n-  /// \\return An iterator of fragment.\n-  Result<FragmentIterator> GetRowGroupFragments(\n-      const ParquetFileFragment& fragment,\n-      std::shared_ptr<Expression> filter = scalar(true));\n+  /// If the RowGroup was not initialized with statistics, it is deemd\n+  bool Satisfy(const Expression& predicate) const;\n+\n+  /// \\brief Indicate if the other RowGroup points to the same RowGroup.\n+  bool Equals(const RowGroupInfo& other) const { return id() == other.id(); }\n+\n+ private:\n+  int id_;\n+  int64_t num_rows_;\n+  std::shared_ptr<Expression> statistics_;\n };\n \n+/// \\brief A FileFragment with parquet logic.\n+///\n+/// ParquetFileFragment provides a lazy (with respect to IO) interface to\n+/// scan parquet files. Any heavy IO calls is deferred in the Scan() method.\n\nReview comment:\n       ```suggestion\r\n   /// scan parquet files. Any heavy IO calls are deferred to the Scan() method.\r\n   ```\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-20T18:34:43.960+0000",
                    "updated": "2020-05-20T18:34:43.960+0000",
                    "started": "2020-05-20T18:34:43.960+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435590",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435637",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428250038\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -163,124 +158,47 @@ static std::shared_ptr<Expression> ColumnChunkStatisticsAsExpression(\n               less_equal(field_expr, scalar(max)));\n }\n \n-static Result<std::shared_ptr<Expression>> RowGroupStatisticsAsExpression(\n-    const parquet::RowGroupMetaData& metadata,\n-    const parquet::ArrowReaderProperties& properties) {\n-  ARROW_ASSIGN_OR_RAISE(auto manifest, GetSchemaManifest(metadata, properties));\n-\n-  ExpressionVector expressions;\n-  for (const auto& schema_field : manifest.schema_fields) {\n-    expressions.emplace_back(ColumnChunkStatisticsAsExpression(schema_field, metadata));\n+static std::shared_ptr<Expression> RowGroupStatisticsAsExpression(\n+    const parquet::RowGroupMetaData& metadata, const SchemaManifest& manifest) {\n+  const auto& fields = manifest.schema_fields;\n+  ExpressionVector expressions{fields.size()};\n\nReview comment:\n       I keep getting caught by this one.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-20T19:14:46.761+0000",
                    "updated": "2020-05-20T19:14:46.761+0000",
                    "started": "2020-05-20T19:14:46.761+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435637",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435638",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428250624\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -315,20 +233,28 @@ class ParquetScanTaskIterator {\n   }\n \n   ParquetScanTaskIterator(std::shared_ptr<ScanOptions> options,\n-                          std::shared_ptr<ScanContext> context,\n-                          std::vector<int> column_projection, RowGroupSkipper skipper,\n-                          std::unique_ptr<parquet::arrow::FileReader> reader)\n+                          std::shared_ptr<ScanContext> context, FileSource source,\n+                          std::unique_ptr<parquet::arrow::FileReader> reader,\n+                          std::vector<int> column_projection,\n+                          std::vector<RowGroupInfo> row_groups)\n       : options_(std::move(options)),\n         context_(std::move(context)),\n+        source_(std::move(source)),\n+        reader_(std::move(reader)),\n         column_projection_(std::move(column_projection)),\n-        skipper_(std::move(skipper)),\n-        reader_(std::move(reader)) {}\n+        row_groups_(std::move(row_groups)) {}\n \n   std::shared_ptr<ScanOptions> options_;\n   std::shared_ptr<ScanContext> context_;\n-  std::vector<int> column_projection_;\n-  RowGroupSkipper skipper_;\n+\n+  FileSource source_;\n\nReview comment:\n       For debug purposes, this is extremely useful to introspect the object.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-20T19:15:50.243+0000",
                    "updated": "2020-05-20T19:15:50.243+0000",
                    "started": "2020-05-20T19:15:50.243+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435638",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/435999",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428664320\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n\nReview comment:\n       All parquet calls can throw, it's just simpler otherwise I have variable scope issues wrapped in multiple blocks.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-21T13:52:09.435+0000",
                    "updated": "2020-05-21T13:52:09.435+0000",
                    "started": "2020-05-21T13:52:09.435+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "435999",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/436005",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428669952\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_parquet.cc\n##########\n@@ -380,77 +316,297 @@ Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n   return ScanFile(source, std::move(options), std::move(context), {});\n }\n \n+static inline std::vector<RowGroupInfo> FilterRowGroups(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate) {\n+  // Keep the index of the last valid entry.\n+  size_t idx = 0;\n+  for (size_t i = 0; i < row_groups.size(); i++) {\n+    const auto& info = row_groups[i];\n+    if (info.Satisfy(predicate)) {\n+      row_groups[idx++] = info;\n+    }\n+  }\n+  row_groups.resize(idx);\n+  return row_groups;\n+}\n+\n+static inline Result<std::vector<RowGroupInfo>> AugmentAndFilter(\n+    std::vector<RowGroupInfo> row_groups, const Expression& predicate,\n+    parquet::arrow::FileReader* reader) {\n+  auto metadata = reader->parquet_reader()->metadata();\n+  auto manifest = reader->manifest();\n+  auto num_row_groups = metadata->num_row_groups();\n+\n+  // Augment a RowGroup with statistics if missing.\n+  auto augment = [&](RowGroupInfo& info) {\n+    auto id = info.id();\n+    if (!info.HasStatistics() && id < num_row_groups) {\n+      auto row_group = metadata->RowGroup(info.id());\n+      info.set_num_rows(row_group->num_rows());\n+      info.set_statistics(RowGroupStatisticsAsExpression(*row_group, manifest));\n+    }\n+  };\n+\n+  if (row_groups.empty()) {\n+    row_groups = RowGroupInfo::FromCount(num_row_groups);\n+  }\n+\n+  for (auto& row_group : row_groups) {\n+    augment(row_group);\n+  }\n+\n+  return FilterRowGroups(std::move(row_groups), predicate);\n+}\n+\n Result<ScanTaskIterator> ParquetFileFormat::ScanFile(\n     const FileSource& source, std::shared_ptr<ScanOptions> options,\n-    std::shared_ptr<ScanContext> context, const std::vector<int>& row_groups) const {\n-  auto properties = MakeReaderProperties(*this, context->pool);\n-  ARROW_ASSIGN_OR_RAISE(auto reader, OpenReader(source, std::move(properties)));\n+    std::shared_ptr<ScanContext> context, std::vector<RowGroupInfo> row_groups) const {\n+  // The following block is required to avoid any IO if all RowGroups are\n+  // excluded due to prior statistics knowledge.\n+  if (!row_groups.empty()) {\n+    // Apply a pre-filtering if the user requested an explicit sub-set of\n+    // row-groups. In the case where a RowGroup doesn't have statistics\n+    // metdata, it will not be excluded.\n+    row_groups = FilterRowGroups(row_groups, *options->filter);\n+    if (row_groups.empty()) {\n+      return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+    }\n+  }\n \n-  for (int i : row_groups) {\n-    if (i >= reader->metadata()->num_row_groups()) {\n-      return Status::IndexError(\"trying to scan row group \", i, \" but \", source.path(),\n-                                \" only has \", reader->metadata()->num_row_groups(),\n+  // Open the reader and pay the real IO cost.\n+  ARROW_ASSIGN_OR_RAISE(auto reader, GetReader(source, options.get(), context.get()));\n+\n+  // Ensure RowGroups are indexing valid RowGroups before augmenting.\n+  auto num_row_groups = reader->num_row_groups();\n+  for (const auto& row_group : row_groups) {\n+    if (row_group.id() >= num_row_groups) {\n+      return Status::IndexError(\"Trying to scan row group \", row_group.id(), \" but \",\n+                                source.path(), \" only has \", num_row_groups,\n                                 \" row groups\");\n     }\n   }\n \n-  auto arrow_properties = MakeArrowReaderProperties(*this, options->batch_size, *reader);\n-  return ParquetScanTaskIterator::Make(std::move(options), std::move(context),\n-                                       std::move(reader), std::move(arrow_properties),\n-                                       row_groups);\n+  ARROW_ASSIGN_OR_RAISE(row_groups,\n+                        AugmentAndFilter(row_groups, *options->filter, reader.get()));\n+\n+  if (row_groups.empty()) {\n+    return MakeEmptyIterator<std::shared_ptr<ScanTask>>();\n+  }\n+\n+  return ParquetScanTaskIterator::Make(std::move(options), std::move(context), source,\n+                                       std::move(reader), std::move(row_groups));\n }\n \n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression,\n-    std::vector<int> row_groups) {\n+    std::vector<RowGroupInfo> row_groups) {\n   return std::shared_ptr<FileFragment>(\n       new ParquetFileFragment(std::move(source), shared_from_this(),\n                               std::move(partition_expression), std::move(row_groups)));\n }\n \n+Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n+    FileSource source, std::shared_ptr<Expression> partition_expression,\n+    std::vector<int> row_groups) {\n+  return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n+      std::move(source), shared_from_this(), std::move(partition_expression),\n+      RowGroupInfo::FromIdentifiers(row_groups)));\n+}\n+\n Result<std::shared_ptr<FileFragment>> ParquetFileFormat::MakeFragment(\n     FileSource source, std::shared_ptr<Expression> partition_expression) {\n   return std::shared_ptr<FileFragment>(new ParquetFileFragment(\n       std::move(source), shared_from_this(), std::move(partition_expression), {}));\n }\n \n-Result<FragmentIterator> ParquetFileFormat::GetRowGroupFragments(\n-    const ParquetFileFragment& fragment, std::shared_ptr<Expression> filter) {\n-  auto properties = MakeReaderProperties(*this);\n-  ARROW_ASSIGN_OR_RAISE(auto reader,\n-                        OpenReader(fragment.source(), std::move(properties)));\n-\n-  auto arrow_properties =\n-      MakeArrowReaderProperties(*this, parquet::kArrowDefaultBatchSize, *reader);\n-  auto metadata = reader->metadata();\n+///\n+/// RowGroupInfo\n+///\n \n-  auto row_groups = fragment.row_groups();\n-  if (row_groups.empty()) {\n-    row_groups = internal::Iota(metadata->num_row_groups());\n+std::vector<RowGroupInfo> RowGroupInfo::FromIdentifiers(const std::vector<int> ids) {\n+  std::vector<RowGroupInfo> results;\n+  results.reserve(ids.size());\n+  for (auto i : ids) {\n+    results.emplace_back(i);\n   }\n-  FragmentVector fragments(row_groups.size());\n-\n-  RowGroupSkipper skipper(std::move(metadata), std::move(arrow_properties),\n-                          std::move(filter), std::move(row_groups));\n+  return results;\n+}\n \n-  for (int i = 0, row_group = skipper.Next();\n-       row_group != RowGroupSkipper::kIterationDone; row_group = skipper.Next()) {\n-    ARROW_ASSIGN_OR_RAISE(\n-        fragments[i++],\n-        MakeFragment(fragment.source(), fragment.partition_expression(), {row_group}));\n+std::vector<RowGroupInfo> RowGroupInfo::FromCount(int count) {\n+  std::vector<RowGroupInfo> result;\n+  result.reserve(count);\n+  for (int i = 0; i < count; i++) {\n+    result.emplace_back(i);\n   }\n+  return result;\n+}\n \n-  return MakeVectorIterator(std::move(fragments));\n+bool RowGroupInfo::Satisfy(const Expression& predicate) const {\n+  return !HasStatistics() || predicate.IsSatisfiableWith(statistics_);\n }\n \n+///\n+/// ParquetFileFragment\n+///\n+\n+ParquetFileFragment::ParquetFileFragment(FileSource source,\n+                                         std::shared_ptr<FileFormat> format,\n+                                         std::shared_ptr<Expression> partition_expression,\n+                                         std::vector<RowGroupInfo> row_groups)\n+    : FileFragment(std::move(source), std::move(format), std::move(partition_expression)),\n+      row_groups_(std::move(row_groups)),\n+      parquet_format_(internal::checked_cast<ParquetFileFormat&>(*format_)) {}\n+\n Result<ScanTaskIterator> ParquetFileFragment::Scan(std::shared_ptr<ScanOptions> options,\n                                                    std::shared_ptr<ScanContext> context) {\n-  return parquet_format().ScanFile(source_, std::move(options), std::move(context),\n-                                   row_groups_);\n+  return parquet_format_.ScanFile(source_, std::move(options), std::move(context),\n+                                  row_groups_);\n+}\n+\n+Result<FragmentVector> ParquetFileFragment::SplitByRowGroup(\n+    const std::shared_ptr<Expression>& predicate) {\n+  ARROW_ASSIGN_OR_RAISE(auto reader, parquet_format_.GetReader(source_));\n+  ARROW_ASSIGN_OR_RAISE(auto row_groups,\n+                        AugmentAndFilter(row_groups_, *predicate, reader.get()));\n+\n+  FragmentVector fragments;\n+  for (auto row_group : row_groups) {\n+    ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                          parquet_format_.MakeFragment(source_, partition_expression(),\n+                                                       {std::move(row_group)}));\n+    fragments.push_back(std::move(fragment));\n+  }\n+\n+  return fragments;\n+}\n+\n+///\n+/// ParquetDatasetFactory\n+///\n+\n+ParquetDatasetFactory::ParquetDatasetFactory(\n+    std::shared_ptr<fs::FileSystem> filesystem, std::shared_ptr<ParquetFileFormat> format,\n+    std::shared_ptr<parquet::FileMetaData> metadata, std::string base_path)\n+    : filesystem_(std::move(filesystem)),\n+      format_(std::move(format)),\n+      metadata_(std::move(metadata)),\n+      base_path_(std::move(base_path)) {}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const std::string& metadata_path, std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  // Paths in ColumnChunk are relative to the `_metadata` file. Thus, the base\n+  // directory of all parquet files is `dirname(metadata_path)`.\n+  auto dirname = arrow::fs::internal::GetAbstractPathParent(metadata_path).first;\n+  return Make({metadata_path, filesystem}, dirname, filesystem, format);\n+}\n+\n+Result<std::shared_ptr<DatasetFactory>> ParquetDatasetFactory::Make(\n+    const FileSource& metadata_source, const std::string& base_path,\n+    std::shared_ptr<fs::FileSystem> filesystem,\n+    std::shared_ptr<ParquetFileFormat> format) {\n+  DCHECK_NE(filesystem, nullptr);\n+  DCHECK_NE(format, nullptr);\n+\n+  ARROW_ASSIGN_OR_RAISE(auto reader, format->GetReader(metadata_source));\n+  auto metadata = reader->parquet_reader()->metadata();\n+\n+  return std::shared_ptr<DatasetFactory>(new ParquetDatasetFactory(\n+      std::move(filesystem), std::move(format), std::move(metadata), base_path));\n+}\n+\n+Result<std::vector<std::shared_ptr<Schema>>> ParquetDatasetFactory::InspectSchemas(\n+    InspectOptions options) {\n+  std::shared_ptr<Schema> schema;\n+  RETURN_NOT_OK(parquet::arrow::FromParquetSchema(metadata_->schema(), &schema));\n+  return std::vector<std::shared_ptr<Schema>>{schema};\n+}\n+\n+static Result<std::string> FileFromRowGroup(const std::string& base_path,\n+                                            const parquet::RowGroupMetaData& row_group) {\n+  try {\n+    auto n_columns = row_group.num_columns();\n+    if (n_columns == 0) {\n+      return Status::Invalid(\"RowGroup must have a least one columns to extract path\");\n+    }\n+\n+    auto first_column = row_group.ColumnChunk(0);\n+    auto path = first_column->file_path();\n+    if (path == \"\") {\n+      return Status::Invalid(\"Got empty file path\");\n\nReview comment:\n       Improved it for all message with the prefix: \"Extracting file path from RowGroup failed.\"\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-21T14:01:00.884+0000",
                    "updated": "2020-05-21T14:01:00.884+0000",
                    "started": "2020-05-21T14:01:00.884+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "436005",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/436014",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428676760\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/scanner.cc\n##########\n@@ -177,32 +177,38 @@ static inline RecordBatchVector FlattenRecordBatchVector(\n   return flattened;\n }\n \n+struct TableAssemblyState {\n+  /// Protecting mutating accesses to batches\n+  std::mutex mutex{};\n+  std::vector<RecordBatchVector> batches{};\n+\n+  void Emplace(RecordBatchVector b, size_t position) {\n+    std::lock_guard<std::mutex> lock(mutex);\n+    if (batches.size() <= position) {\n+      batches.resize(position + 1);\n+    }\n+    batches[position] = std::move(b);\n+  }\n+};\n+\n Result<std::shared_ptr<Table>> Scanner::ToTable() {\n   ARROW_ASSIGN_OR_RAISE(auto scan_task_it, Scan());\n   auto task_group = scan_context_->TaskGroup();\n \n-  // Protecting mutating accesses to batches\n-  std::mutex mutex;\n-  std::vector<RecordBatchVector> batches;\n+  /// Wraps the state in a shared_ptr to ensure that a failing ScanTask don't\n+  /// invalidate the concurrent running tasks because Finish() early returns\n+  /// and the mutex/batches may got out of scope.\n\nReview comment:\n       credits goes to @jorisvandenbossche  :)\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-21T14:12:36.302+0000",
                    "updated": "2020-05-21T14:12:36.302+0000",
                    "started": "2020-05-21T14:12:36.302+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "436014",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/436015",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428677725\n\n\n\n##########\nFile path: cpp/src/parquet/arrow/reader_internal.cc\n##########\n@@ -747,8 +747,7 @@ Status TypedIntegralStatisticsAsScalars(const Statistics& statistics,\n       using CType = typename StatisticsType::T;\n       return MakeMinMaxScalar<CType, StatisticsType>(statistics, min, max);\n     default:\n-      return Status::NotImplemented(\"Cannot extract statistics for type \",\n-                                    logical_type->ToString());\n+      return Status::NotImplemented(\"Cannot extract statistics for type \");\n\nReview comment:\n       KISS, we should just extend MakeMinMaxScalar to support the other types.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-21T14:14:13.597+0000",
                    "updated": "2020-05-21T14:14:13.597+0000",
                    "started": "2020-05-21T14:14:13.597+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "436015",
                    "issueId": "13290885"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/worklog/436018",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on a change in pull request #7180:\nURL: https://github.com/apache/arrow/pull/7180#discussion_r428688278\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/scanner.cc\n##########\n@@ -177,32 +177,38 @@ static inline RecordBatchVector FlattenRecordBatchVector(\n   return flattened;\n }\n \n+struct TableAssemblyState {\n+  /// Protecting mutating accesses to batches\n+  std::mutex mutex{};\n+  std::vector<RecordBatchVector> batches{};\n+\n+  void Emplace(RecordBatchVector b, size_t position) {\n+    std::lock_guard<std::mutex> lock(mutex);\n+    if (batches.size() <= position) {\n+      batches.resize(position + 1);\n+    }\n+    batches[position] = std::move(b);\n+  }\n+};\n+\n Result<std::shared_ptr<Table>> Scanner::ToTable() {\n   ARROW_ASSIGN_OR_RAISE(auto scan_task_it, Scan());\n   auto task_group = scan_context_->TaskGroup();\n \n-  // Protecting mutating accesses to batches\n-  std::mutex mutex;\n-  std::vector<RecordBatchVector> batches;\n+  /// Wraps the state in a shared_ptr to ensure that a failing ScanTask don't\n+  /// invalidate the concurrent running tasks because Finish() early returns\n+  /// and the mutex/batches may got out of scope.\n\nReview comment:\n       Well, the credit to find the cause is not mine, all I did was deleting a file ;)\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-05-21T14:31:07.854+0000",
                    "updated": "2020-05-21T14:31:07.854+0000",
                    "started": "2020-05-21T14:31:07.854+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "436018",
                    "issueId": "13290885"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 14400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@6619619[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5f9b775f[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7879f4a7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@7cdc5578[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@233e5354[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@42d00dd2[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@75bb15e3[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@6bd2e9b9[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3a27a364[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@785b976f[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5562818a[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@298135be[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 14400,
        "customfield_12312520": null,
        "customfield_12312521": "Mon May 25 19:20:30 UTC 2020",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2020-05-25T19:20:30.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-8062/watchers",
            "watchCount": 5,
            "isWatching": false
        },
        "created": "2020-03-10T18:15:51.000+0000",
        "updated": "2020-05-27T03:50:56.000+0000",
        "timeoriginalestimate": null,
        "description": "Partitioned parquet datasets sometimes come with {{_metadata}} / {{_common_metadata}} files. Those files include information about the schema of the full dataset and potentially all RowGroup metadata as well (for {{_metadata}}).\r\n\r\nUsing those files during the creation of a parquet {{Dataset}} can give a more efficient factory (using the stored schema instead of inferring the schema from unioning the schemas of all files + using the paths to individual parquet files instead of crawling the directory).\r\n\r\nBasically, based those files, the schema, list of paths and partition expressions (the information that is needed to create a Dataset) could be constructed.   \r\nSuch logic could be put in a different factory class, eg {{ParquetManifestFactory}} (as suggestetd by [~fsaintjacques]).",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "4h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 14400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Dataset] Parquet Dataset factory from a _metadata/_common_metadata file",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290885/comment/17116190",
                    "id": "17116190",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=fsaintjacques",
                        "name": "fsaintjacques",
                        "key": "fsaintjacques",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=fsaintjacques&avatarId=37276",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=fsaintjacques&avatarId=37276",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=fsaintjacques&avatarId=37276",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=fsaintjacques&avatarId=37276"
                        },
                        "displayName": "Francois Saint-Jacques",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 7180\n[https://github.com/apache/arrow/pull/7180]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=fsaintjacques",
                        "name": "fsaintjacques",
                        "key": "fsaintjacques",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=fsaintjacques&avatarId=37276",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=fsaintjacques&avatarId=37276",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=fsaintjacques&avatarId=37276",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=fsaintjacques&avatarId=37276"
                        },
                        "displayName": "Francois Saint-Jacques",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2020-05-25T19:20:30.646+0000",
                    "updated": "2020-05-25T19:20:30.646+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0cdzs:",
        "customfield_12314139": null
    }
}