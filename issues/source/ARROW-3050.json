{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13178759",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759",
    "key": "ARROW-3050",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12343066",
                "id": "12343066",
                "description": "",
                "name": "0.11.0",
                "archived": false,
                "released": true,
                "releaseDate": "2018-10-08"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 10200,
            "total": 10200,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 10200,
            "total": 10200,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-3050/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 17,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/135721",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm opened a new pull request #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444\n \n \n   I have forked the C++ part of http://github.com/cloudera/hs2client (which I collaborated on some time ago) and modified it to work with the Arrow CMake build system. It can be built with `-DARROW_HIVESERVER2=ON`. This also adds Thrift to our thirdparty toolchain as an optional EP (the configuration was lifted from parquet-cpp)\r\n   \r\n   Open questions\r\n   \r\n   - Make sure unit tests work\r\n   - I have included the extensions to HS2 that are available in Impala, so this should work with both Hive and Impala\r\n   - Attribute IP lineage in LICENSE.txt\r\n   \r\n   Follow up work:\r\n   \r\n   - Dockerized testing setup (it wouldn't really make sense to test this in Travis CI)\r\n   - Nightly testing of docker tests\r\n   - Convert HS2 columnar row sets to Arrow record batches\r\n   - Add Python bindings\r\n   - Add DBABI wrapper for Python\r\n   - Add SASL support for Kerberos-enabled HS2\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-17T15:25:18.770+0000",
                    "updated": "2018-08-17T15:25:18.770+0000",
                    "started": "2018-08-17T15:25:18.769+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135721",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/135722",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on a change in pull request #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#discussion_r210946779\n \n \n\n ##########\n File path: cpp/src/arrow/status.h\n ##########\n @@ -84,6 +84,7 @@ enum class StatusCode : char {\n   PlasmaObjectNonexistent = 21,\n   PlasmaStoreFull = 22,\n   PlasmaObjectAlreadySealed = 23,\n+  StillExecuting = 24\n \n Review comment:\n   Wasn't sure about this. We might call this a more generic \"Not ready\" status code\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-17T15:27:55.459+0000",
                    "updated": "2018-08-17T15:27:55.459+0000",
                    "started": "2018-08-17T15:27:55.458+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135722",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/135723",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-413903963\n \n \n   I'm also thinking whether we want to create a new `arrow/dbi` subdirectory rather than `adapters` since we may implement other Arrow native DB interfaces in the future (e.g. PostgreSQL would be of interest to me)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-17T15:35:42.732+0000",
                    "updated": "2018-08-17T15:35:42.732+0000",
                    "started": "2018-08-17T15:35:42.732+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135723",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/135757",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-413937425\n \n \n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=h1) Report\n   > Merging [#2444](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/170dc75468efbad2286c630b9103d1aacdb6bada?src=pr&el=desc) will **increase** coverage by `1.24%`.\n   > The diff coverage is `n/a`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/2444/graphs/tree.svg?token=LpTCFbqVT1&src=pr&height=150&width=650)](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #2444      +/-   ##\n   ==========================================\n   + Coverage   85.49%   86.73%   +1.24%     \n   ==========================================\n     Files         300      239      -61     \n     Lines       45848    42603    -3245     \n   ==========================================\n   - Hits        39199    36953    -2246     \n   + Misses       6579     5650     -929     \n   + Partials       70        0      -70\n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [cpp/src/arrow/status.h](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9zdGF0dXMuaA==) | `95.29% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [rust/src/record\\_batch.rs](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-cnVzdC9zcmMvcmVjb3JkX2JhdGNoLnJz) | | |\n   | [go/arrow/memory/memory.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWVtb3J5L21lbW9yeS5nbw==) | | |\n   | [go/arrow/math/uint64\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC91aW50NjRfYXZ4Ml9hbWQ2NC5nbw==) | | |\n   | [go/arrow/array/binarybuilder.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvYXJyYXkvYmluYXJ5YnVpbGRlci5nbw==) | | |\n   | [go/arrow/datatype\\_numeric.gen.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvZGF0YXR5cGVfbnVtZXJpYy5nZW4uZ28=) | | |\n   | [go/arrow/math/uint64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC91aW50NjQuZ28=) | | |\n   | [rust/src/datatypes.rs](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-cnVzdC9zcmMvZGF0YXR5cGVzLnJz) | | |\n   | [go/arrow/array/bufferbuilder\\_byte.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvYXJyYXkvYnVmZmVyYnVpbGRlcl9ieXRlLmdv) | | |\n   | [go/arrow/array/booleanbuilder.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvYXJyYXkvYm9vbGVhbmJ1aWxkZXIuZ28=) | | |\n   | ... and [53 more](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree-more) | |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=footer). Last update [170dc75...1257411](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-17T17:36:55.226+0000",
                    "updated": "2018-08-17T17:36:55.226+0000",
                    "started": "2018-08-17T17:36:55.225+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135757",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/135887",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io edited a comment on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-413937425\n \n \n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=h1) Report\n   > Merging [#2444](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/170dc75468efbad2286c630b9103d1aacdb6bada?src=pr&el=desc) will **increase** coverage by `<.01%`.\n   > The diff coverage is `n/a`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/2444/graphs/tree.svg?src=pr&token=LpTCFbqVT1&width=650&height=150)](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #2444      +/-   ##\n   ==========================================\n   + Coverage   85.49%   85.49%   +<.01%     \n   ==========================================\n     Files         300      301       +1     \n     Lines       45848    46273     +425     \n   ==========================================\n   + Hits        39199    39563     +364     \n   - Misses       6579     6636      +57     \n   - Partials       70       74       +4\n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [cpp/src/arrow/status.h](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9zdGF0dXMuaA==) | `95.29% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [go/arrow/math/int64\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC9pbnQ2NF9hdngyX2FtZDY0Lmdv) | `0% <0%> (-100%)` | :arrow_down: |\n   | [go/arrow/memory/memory\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWVtb3J5L21lbW9yeV9hdngyX2FtZDY0Lmdv) | `0% <0%> (-100%)` | :arrow_down: |\n   | [go/arrow/math/float64\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC9mbG9hdDY0X2F2eDJfYW1kNjQuZ28=) | `0% <0%> (-100%)` | :arrow_down: |\n   | [go/arrow/math/uint64\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC91aW50NjRfYXZ4Ml9hbWQ2NC5nbw==) | `0% <0%> (-100%)` | :arrow_down: |\n   | [go/arrow/memory/memory\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWVtb3J5L21lbW9yeV9hbWQ2NC5nbw==) | `28.57% <0%> (-14.29%)` | :arrow_down: |\n   | [go/arrow/math/math\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC9tYXRoX2FtZDY0Lmdv) | `31.57% <0%> (-5.27%)` | :arrow_down: |\n   | [rust/src/bitmap.rs](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-cnVzdC9zcmMvYml0bWFwLnJz) | `94.87% <0%> (-5.13%)` | :arrow_down: |\n   | [rust/src/record\\_batch.rs](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-cnVzdC9zcmMvcmVjb3JkX2JhdGNoLnJz) | `97.77% <0%> (-2.23%)` | :arrow_down: |\n   | [rust/src/array.rs](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-cnVzdC9zcmMvYXJyYXkucnM=) | `83.89% <0%> (-1.69%)` | :arrow_down: |\n   | ... and [17 more](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree-more) | |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=footer). Last update [170dc75...cd961e8](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-17T22:12:41.528+0000",
                    "updated": "2018-08-17T22:12:41.528+0000",
                    "started": "2018-08-17T22:12:41.527+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135887",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/135907",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414007882\n \n \n   The unit tests pass fine when you have Impala running on localhost. I added a docker-compose setup but wasn't able to get the dependent Impala container to start up properly. If @cpcloud, @xhochy, or @kszucs or another Docker expert could take a look it would be much appreciated\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-17T22:58:32.897+0000",
                    "updated": "2018-08-17T22:58:32.897+0000",
                    "started": "2018-08-17T22:58:32.897+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135907",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/135920",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io edited a comment on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-413937425\n \n \n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=h1) Report\n   > Merging [#2444](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/1209a80fc6384428f43eac20804a01e0196638d3?src=pr&el=desc) will **decrease** coverage by `0.11%`.\n   > The diff coverage is `n/a`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/2444/graphs/tree.svg?width=650&src=pr&token=LpTCFbqVT1&height=150)](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #2444      +/-   ##\n   ==========================================\n   - Coverage   85.61%   85.49%   -0.12%     \n   ==========================================\n     Files         301      301              \n     Lines       46273    46273              \n   ==========================================\n   - Hits        39616    39563      -53     \n   - Misses       6585     6636      +51     \n   - Partials       72       74       +2\n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [cpp/src/arrow/status.h](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9zdGF0dXMuaA==) | `95.29% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [go/arrow/memory/memory\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWVtb3J5L21lbW9yeV9hdngyX2FtZDY0Lmdv) | `0% <0%> (-100%)` | :arrow_down: |\n   | [go/arrow/math/uint64\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC91aW50NjRfYXZ4Ml9hbWQ2NC5nbw==) | `0% <0%> (-100%)` | :arrow_down: |\n   | [go/arrow/math/int64\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC9pbnQ2NF9hdngyX2FtZDY0Lmdv) | `0% <0%> (-100%)` | :arrow_down: |\n   | [go/arrow/math/float64\\_avx2\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC9mbG9hdDY0X2F2eDJfYW1kNjQuZ28=) | `0% <0%> (-100%)` | :arrow_down: |\n   | [go/arrow/math/int64\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC9pbnQ2NF9hbWQ2NC5nbw==) | `33.33% <0%> (-33.34%)` | :arrow_down: |\n   | [go/arrow/math/float64\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC9mbG9hdDY0X2FtZDY0Lmdv) | `33.33% <0%> (-33.34%)` | :arrow_down: |\n   | [go/arrow/math/uint64\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC91aW50NjRfYW1kNjQuZ28=) | `33.33% <0%> (-33.34%)` | :arrow_down: |\n   | [go/arrow/math/math\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC9tYXRoX2FtZDY0Lmdv) | `31.57% <0%> (-31.58%)` | :arrow_down: |\n   | [go/arrow/memory/memory\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2444/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWVtb3J5L21lbW9yeV9hbWQ2NC5nbw==) | `28.57% <0%> (-28.58%)` | :arrow_down: |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=footer). Last update [1209a80...5d95864](https://codecov.io/gh/apache/arrow/pull/2444?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-17T23:25:24.574+0000",
                    "updated": "2018-08-17T23:25:24.574+0000",
                    "started": "2018-08-17T23:25:24.574+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135920",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136147",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414344564\n \n \n   I know that this PR is big. I'd like to proceed with the follow up work and would prefer to do so in smaller PRs. Let me know if you have any strong opinions about what you see so far \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-20T14:53:07.895+0000",
                    "updated": "2018-08-20T14:53:07.895+0000",
                    "started": "2018-08-20T14:53:07.894+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136147",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136369",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pcmoritz commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414517178\n \n \n   No objection to this PR, but I wonder if longer term we can standardize on one RPC framework for arrow. This would make it easier to tie together the various services that live in the repo. My personal preference would be gRPC (but this is based off limited experience) and I'd be interested in exposing plasma as a gRPC service.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T01:13:57.102+0000",
                    "updated": "2018-08-21T01:13:57.102+0000",
                    "started": "2018-08-21T01:13:57.102+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136369",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136371",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414518383\n \n \n   @pcmoritz this isn't an RPC framework. The client-server protocol for talking with Hive and Impala is Thrift\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T01:21:13.106+0000",
                    "updated": "2018-08-21T01:21:13.106+0000",
                    "started": "2018-08-21T01:21:13.106+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136371",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136526",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "xhochy commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414687654\n \n \n   I'm a bit -1 on merging it directly merging it. As far as I could see from a quick glance, it currently only uses Arrow's CMake infrastructure and Status infrastructure. \r\n   \r\n   @wesm some questions (feel free to answer them on the ML)\r\n   \r\n   - Is the intention to then discontinue the hs2client project and instead evolve everything here?\r\n   - Would you also try to get rid of some of the hs2client classes in favour of the similar ones in Arrow?\r\n   - We have some DBABI extensions in `turbodbc` for retrieving and submitting Arrow. Would it make sense to propose a PEP that extends the DBAPI spec so that they are standardized?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T14:09:58.111+0000",
                    "updated": "2018-08-21T14:09:58.111+0000",
                    "started": "2018-08-21T14:09:58.110+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136526",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136528",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414692323\n \n \n   @xhochy on your questions\r\n   \r\n   * Yes\r\n   * Yes, or at least to develop standard abstract interfaces where it makes sense\r\n   * Yes, I want to define an Arrow DBABI protocol. \r\n   \r\n   I know no better place to develop this software than Apache Arrow. Would it be OK if I merged this once the build is green? If I am unable to push it where it needs to go and the community deems its maintenance too burdensome, then we can address this (e.g. splitting off into a new project) at a future point.\r\n   \r\n   FWIW, I believe that Turbodbc would benefit from being a part of Apache Arrow\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T14:23:51.260+0000",
                    "updated": "2018-08-21T14:23:51.260+0000",
                    "started": "2018-08-21T14:23:51.259+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136528",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136529",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm edited a comment on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414692323\n \n \n   @xhochy on your questions\r\n   \r\n   * Yes\r\n   * Yes, or at least to develop standard abstract interfaces where it makes sense\r\n   * Yes, I want to define an Arrow DBABI interface. \r\n   \r\n   I know no better place to develop this software than Apache Arrow. Would it be OK if I merged this once the build is green? If I am unable to push it where it needs to go and the community deems its maintenance too burdensome, then we can address this (e.g. splitting off into a new project) at a future point.\r\n   \r\n   FWIW, I believe that Turbodbc would benefit from being a part of Apache Arrow\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T14:24:04.232+0000",
                    "updated": "2018-08-21T14:24:04.232+0000",
                    "started": "2018-08-21T14:24:04.232+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136529",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136530",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414692880\n \n \n   Ah the build is green except for Rust, restarted that build\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T14:25:18.175+0000",
                    "updated": "2018-08-21T14:25:18.175+0000",
                    "started": "2018-08-21T14:25:18.174+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136530",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136531",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414693133\n \n \n   See related discussion on cloudera/hs2client https://github.com/cloudera/hs2client/issues/27\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T14:25:52.944+0000",
                    "updated": "2018-08-21T14:25:52.944+0000",
                    "started": "2018-08-21T14:25:52.943+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136531",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136543",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "xhochy commented on issue #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444#issuecomment-414701063\n \n \n   +1, feel free to merge then.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T14:48:10.059+0000",
                    "updated": "2018-08-21T14:48:10.059+0000",
                    "started": "2018-08-21T14:48:10.058+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136543",
                    "issueId": "13178759"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/worklog/136545",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm closed pull request #2444: ARROW-3050: [C++] Adopt HiveServer2 client codebase from cloudera/hs2client. Add Thrift to thirdparty toolchain\nURL: https://github.com/apache/arrow/pull/2444\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/LICENSE.txt b/LICENSE.txt\nindex 9a7ad1f05f..e557b8048f 100644\n--- a/LICENSE.txt\n+++ b/LICENSE.txt\n@@ -630,3 +630,45 @@ THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n \n --------------------------------------------------------------------------------\n+\n+This project includes code from the hs2client\n+\n+https://github.com/cloudera/hs2client\n+\n+Copyright 2016 Cloudera Inc.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+--------------------------------------------------------------------------------\n+\n+The script dev/docker_common/wait-for-it.sh has the following license\n+\n+Copyright (c) 2016 Giles Hall\n+\n+Permission is hereby granted, free of charge, to any person obtaining a copy of\n+this software and associated documentation files (the \"Software\"), to deal in\n+the Software without restriction, including without limitation the rights to\n+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n+of the Software, and to permit persons to whom the Software is furnished to do\n+so, subject to the following conditions:\n+\n+The above copyright notice and this permission notice shall be included in all\n+copies or substantial portions of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n+SOFTWARE.\ndiff --git a/cpp/CMakeLists.txt b/cpp/CMakeLists.txt\nindex bc080aaee4..9f41142492 100644\n--- a/cpp/CMakeLists.txt\n+++ b/cpp/CMakeLists.txt\n@@ -203,6 +203,10 @@ Pass multiple labels by dividing with semicolons\")\n     \"Build Arrow libraries with install_name set to @rpath\"\n     ON)\n \n+  option(ARROW_HIVESERVER2\n+    \"Build the HiveServer2 client and Arrow adapter\"\n+    OFF)\n+\n   option(ARROW_PLASMA\n     \"Build the plasma object store along with Arrow\"\n     OFF)\n@@ -721,3 +725,7 @@ add_subdirectory(src/arrow)\n if(ARROW_PYTHON)\n   add_subdirectory(src/arrow/python)\n endif()\n+\n+if(ARROW_HIVESERVER2)\n+  add_subdirectory(src/arrow/dbi/hiveserver2)\n+endif()\ndiff --git a/cpp/cmake_modules/ThirdpartyToolchain.cmake b/cpp/cmake_modules/ThirdpartyToolchain.cmake\nindex dd81ee5169..124a8d3ade 100644\n--- a/cpp/cmake_modules/ThirdpartyToolchain.cmake\n+++ b/cpp/cmake_modules/ThirdpartyToolchain.cmake\n@@ -31,6 +31,7 @@ if (NOT \"$ENV{ARROW_BUILD_TOOLCHAIN}\" STREQUAL \"\")\n   set(BROTLI_HOME \"$ENV{ARROW_BUILD_TOOLCHAIN}\")\n   set(LZ4_HOME \"$ENV{ARROW_BUILD_TOOLCHAIN}\")\n   set(ZSTD_HOME \"$ENV{ARROW_BUILD_TOOLCHAIN}\")\n+  set(THRIFT_HOME \"$ENV{ARROW_BUILD_TOOLCHAIN}\")\n \n   if (NOT DEFINED ENV{BOOST_ROOT})\n     # Since we have to set this in the environment, we check whether\n@@ -79,6 +80,10 @@ if (DEFINED ENV{PROTOBUF_HOME})\n   set(PROTOBUF_HOME \"$ENV{PROTOBUF_HOME}\")\n endif()\n \n+if (DEFINED ENV{THRIFT_HOME})\n+  set(THRIFT_HOME \"$ENV{THRIFT_HOME}\")\n+endif()\n+\n # ----------------------------------------------------------------------\n # Versions and URLs for toolchain builds, which also can be used to configure\n # offline builds\n@@ -192,6 +197,12 @@ else()\n   set(ORC_SOURCE_URL \"https://github.com/apache/orc/archive/rel/release-${ORC_VERSION}.tar.gz\")\n endif()\n \n+if (DEFINED ENV{ARROW_THRIFT_URL})\n+  set(THRIFT_SOURCE_URL \"$ENV{ARROW_THRIFT_URL}\")\n+else()\n+  set(THRIFT_SOURCE_URL \"http://archive.apache.org/dist/thrift/${THRIFT_VERSION}/thrift-${THRIFT_VERSION}.tar.gz\")\n+endif()\n+\n # ----------------------------------------------------------------------\n # ExternalProject options\n \n@@ -1084,3 +1095,136 @@ if (ARROW_ORC)\n   endif()\n \n endif()\n+\n+# ----------------------------------------------------------------------\n+# Thrift\n+\n+if (ARROW_HIVESERVER2)\n+\n+# find thrift headers and libs\n+find_package(Thrift)\n+\n+if (NOT THRIFT_FOUND)\n+  set(ZLIB_PREFIX \"${CMAKE_CURRENT_BINARY_DIR}/zlib_ep/src/zlib_ep-install\")\n+  set(ZLIB_HOME \"${ZLIB_PREFIX}\")\n+  set(ZLIB_INCLUDE_DIR \"${ZLIB_PREFIX}/include\")\n+  if (MSVC)\n+    if (${UPPERCASE_BUILD_TYPE} STREQUAL \"DEBUG\")\n+      set(ZLIB_STATIC_LIB_NAME zlibstaticd.lib)\n+    else()\n+      set(ZLIB_STATIC_LIB_NAME zlibstatic.lib)\n+    endif()\n+  else()\n+    set(ZLIB_STATIC_LIB_NAME libz.a)\n+  endif()\n+  set(ZLIB_STATIC_LIB \"${ZLIB_PREFIX}/lib/${ZLIB_STATIC_LIB_NAME}\")\n+  set(ZLIB_CMAKE_ARGS -DCMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}\n+    -DCMAKE_INSTALL_PREFIX=${ZLIB_PREFIX}\n+    -DCMAKE_C_FLAGS=${EP_C_FLAGS}\n+    -DBUILD_SHARED_LIBS=OFF)\n+  ExternalProject_Add(zlib_ep\n+    URL \"http://zlib.net/fossils/zlib-1.2.8.tar.gz\"\n+    BUILD_BYPRODUCTS \"${ZLIB_STATIC_LIB}\"\n+    ${ZLIB_BUILD_BYPRODUCTS}\n+    ${EP_LOG_OPTIONS}\n+    CMAKE_ARGS ${ZLIB_CMAKE_ARGS})\n+\n+  set(THRIFT_PREFIX \"${CMAKE_CURRENT_BINARY_DIR}/thrift_ep/src/thrift_ep-install\")\n+  set(THRIFT_HOME \"${THRIFT_PREFIX}\")\n+  set(THRIFT_INCLUDE_DIR \"${THRIFT_PREFIX}/include\")\n+  set(THRIFT_COMPILER \"${THRIFT_PREFIX}/bin/thrift\")\n+  set(THRIFT_CMAKE_ARGS \"-DCMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}\"\n+                        \"-DCMAKE_CXX_FLAGS=${EP_CXX_FLAGS}\"\n+                        \"-DCMAKE_C_FLAGS=${EP_C_FLAGS}\"\n+                        \"-DCMAKE_INSTALL_PREFIX=${THRIFT_PREFIX}\"\n+                        \"-DCMAKE_INSTALL_RPATH=${THRIFT_PREFIX}/lib\"\n+                        \"-DBUILD_SHARED_LIBS=OFF\"\n+                        \"-DBUILD_TESTING=OFF\"\n+                        \"-DBUILD_EXAMPLES=OFF\"\n+                        \"-DBUILD_TUTORIALS=OFF\"\n+                        \"-DWITH_QT4=OFF\"\n+                        \"-DWITH_C_GLIB=OFF\"\n+                        \"-DWITH_JAVA=OFF\"\n+                        \"-DWITH_PYTHON=OFF\"\n+                        \"-DWITH_HASKELL=OFF\"\n+                        \"-DWITH_CPP=ON\"\n+                        \"-DWITH_STATIC_LIB=ON\"\n+                        \"-DWITH_LIBEVENT=OFF\"\n+                        )\n+\n+  # Thrift also uses boost. Forward important boost settings if there were ones passed.\n+  if (DEFINED BOOST_ROOT)\n+    set(THRIFT_CMAKE_ARGS ${THRIFT_CMAKE_ARGS} \"-DBOOST_ROOT=${BOOST_ROOT}\")\n+  endif()\n+  if (DEFINED Boost_NAMESPACE)\n+    set(THRIFT_CMAKE_ARGS ${THRIFT_CMAKE_ARGS} \"-DBoost_NAMESPACE=${Boost_NAMESPACE}\")\n+  endif()\n+\n+  set(THRIFT_STATIC_LIB_NAME \"${CMAKE_STATIC_LIBRARY_PREFIX}thrift\")\n+  if (MSVC)\n+    if (ARROW_USE_STATIC_CRT)\n+      set(THRIFT_STATIC_LIB_NAME \"${THRIFT_STATIC_LIB_NAME}mt\")\n+      set(THRIFT_CMAKE_ARGS ${THRIFT_CMAKE_ARGS} \"-DWITH_MT=ON\")\n+    else()\n+      set(THRIFT_STATIC_LIB_NAME \"${THRIFT_STATIC_LIB_NAME}md\")\n+      set(THRIFT_CMAKE_ARGS ${THRIFT_CMAKE_ARGS} \"-DWITH_MT=OFF\")\n+    endif()\n+  endif()\n+  if (${UPPERCASE_BUILD_TYPE} STREQUAL \"DEBUG\")\n+    set(THRIFT_STATIC_LIB_NAME \"${THRIFT_STATIC_LIB_NAME}d\")\n+  endif()\n+  set(THRIFT_STATIC_LIB \"${THRIFT_PREFIX}/lib/${THRIFT_STATIC_LIB_NAME}${CMAKE_STATIC_LIBRARY_SUFFIX}\")\n+\n+  if (MSVC)\n+    set(WINFLEXBISON_VERSION 2.4.9)\n+    set(WINFLEXBISON_PREFIX \"${CMAKE_CURRENT_BINARY_DIR}/winflexbison_ep/src/winflexbison_ep-install\")\n+    ExternalProject_Add(winflexbison_ep\n+      URL https://github.com/lexxmark/winflexbison/releases/download/v.${WINFLEXBISON_VERSION}/win_flex_bison-${WINFLEXBISON_VERSION}.zip\n+      URL_HASH MD5=a2e979ea9928fbf8567e995e9c0df765\n+      SOURCE_DIR ${WINFLEXBISON_PREFIX}\n+      CONFIGURE_COMMAND \"\"\n+      BUILD_COMMAND \"\"\n+      INSTALL_COMMAND \"\"\n+      ${EP_LOG_OPTIONS})\n+    set(THRIFT_DEPENDENCIES ${THRIFT_DEPENDENCIES} winflexbison_ep)\n+\n+    set(THRIFT_CMAKE_ARGS \"-DFLEX_EXECUTABLE=${WINFLEXBISON_PREFIX}/win_flex.exe\"\n+                          \"-DBISON_EXECUTABLE=${WINFLEXBISON_PREFIX}/win_bison.exe\"\n+                          \"-DZLIB_INCLUDE_DIR=${ZLIB_INCLUDE_DIR}\"\n+                          \"-DZLIB_LIBRARY=${ZLIB_STATIC_LIB}\"\n+                          \"-DWITH_SHARED_LIB=OFF\"\n+                          \"-DWITH_PLUGIN=OFF\"\n+                          ${THRIFT_CMAKE_ARGS})\n+    set(THRIFT_DEPENDENCIES ${THRIFT_DEPENDENCIES} zlib_ep)\n+  elseif (APPLE)\n+    if (DEFINED BISON_EXECUTABLE)\n+      set(THRIFT_CMAKE_ARGS \"-DBISON_EXECUTABLE=${BISON_EXECUTABLE}\"\n+                            ${THRIFT_CMAKE_ARGS})\n+    endif()\n+  endif()\n+\n+  ExternalProject_Add(thrift_ep\n+    URL ${THRIFT_SOURCE_URL}\n+    BUILD_BYPRODUCTS \"${THRIFT_STATIC_LIB}\" \"${THRIFT_COMPILER}\"\n+    CMAKE_ARGS ${THRIFT_CMAKE_ARGS}\n+    DEPENDS ${THRIFT_DEPENDENCIES}\n+    ${EP_LOG_OPTIONS})\n+\n+  set(THRIFT_VENDORED 1)\n+else()\n+  set(THRIFT_VENDORED 0)\n+endif()\n+\n+include_directories(SYSTEM ${THRIFT_INCLUDE_DIR} ${THRIFT_INCLUDE_DIR}/thrift)\n+message(STATUS \"Thrift include dir: ${THRIFT_INCLUDE_DIR}\")\n+message(STATUS \"Thrift static library: ${THRIFT_STATIC_LIB}\")\n+message(STATUS \"Thrift compiler: ${THRIFT_COMPILER}\")\n+message(STATUS \"Thrift version: ${THRIFT_VERSION}\")\n+add_library(thriftstatic STATIC IMPORTED)\n+set_target_properties(thriftstatic PROPERTIES IMPORTED_LOCATION ${THRIFT_STATIC_LIB})\n+\n+if (THRIFT_VENDORED)\n+  add_dependencies(thriftstatic thrift_ep)\n+endif()\n+\n+endif()  # ARROW_HIVESERVER2\ndiff --git a/cpp/src/arrow/dbi/README.md b/cpp/src/arrow/dbi/README.md\nnew file mode 100644\nindex 0000000000..d73666c372\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/README.md\n@@ -0,0 +1,24 @@\n+<!---\n+  Licensed to the Apache Software Foundation (ASF) under one\n+  or more contributor license agreements.  See the NOTICE file\n+  distributed with this work for additional information\n+  regarding copyright ownership.  The ASF licenses this file\n+  to you under the Apache License, Version 2.0 (the\n+  \"License\"); you may not use this file except in compliance\n+  with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing,\n+  software distributed under the License is distributed on an\n+  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  KIND, either express or implied.  See the License for the\n+  specific language governing permissions and limitations\n+  under the License.\n+-->\n+\n+# Arrow Database Interfaces\n+\n+## HiveServer2\n+\n+For Apache Hive and Apache Impala. See `hiveserver2/` directory\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/CMakeLists.txt b/cpp/src/arrow/dbi/hiveserver2/CMakeLists.txt\nnew file mode 100644\nindex 0000000000..60d88daad9\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/CMakeLists.txt\n@@ -0,0 +1,110 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# Headers: top level\n+install(FILES\n+  api.h\n+  columnar-row-set.h\n+  operation.h\n+  service.h\n+  session.h\n+  types.h\n+  util.h\n+  DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}/arrow/dbi/hiveserver2\")\n+\n+set(ARROW_HIVESERVER2_SRCS\n+  columnar-row-set.cc\n+  service.cc\n+  session.cc\n+  operation.cc\n+  sample-usage.cc\n+  thrift-internal.cc\n+  types.cc\n+  util.cc\n+)\n+\n+add_subdirectory(thrift)\n+\n+set(HIVESERVER2_THRIFT_SRC\n+  ErrorCodes_constants.cpp\n+  ErrorCodes_types.cpp\n+  ImpalaService.cpp\n+  ImpalaService_constants.cpp\n+  ImpalaService_types.cpp\n+  ImpalaHiveServer2Service.cpp\n+  beeswax_constants.cpp\n+  beeswax_types.cpp\n+  BeeswaxService.cpp\n+  TCLIService.cpp\n+  TCLIService_constants.cpp\n+  TCLIService_types.cpp\n+  ExecStats_constants.cpp\n+  ExecStats_types.cpp\n+  hive_metastore_constants.cpp\n+  hive_metastore_types.cpp\n+  Status_constants.cpp\n+  Status_types.cpp\n+  Types_constants.cpp\n+  Types_types.cpp\n+)\n+\n+set_source_files_properties(${HIVESERVER2_THRIFT_SRC} PROPERTIES\n+  COMPILE_FLAGS \"-Wno-unused-variable -Wno-shadow-field\"\n+  GENERATED TRUE)\n+\n+# keep everything in one library, the object files reference\n+# each other\n+add_library(arrow_hiveserver2_thrift STATIC ${HIVESERVER2_THRIFT_SRC})\n+\n+# Setting these files as code-generated lets make clean and incremental builds work\n+# correctly\n+\n+# TODO(wesm): Something is broken with the dependency chain with\n+# ImpalaService.cpp and others. Couldn't figure out what is different between\n+# this setup and Impala.\n+\n+add_dependencies(arrow_hiveserver2_thrift hs2-thrift-cpp)\n+\n+set_target_properties(arrow_hiveserver2_thrift\n+  PROPERTIES\n+  LIBRARY_OUTPUT_DIRECTORY \"${BUILD_OUTPUT_ROOT_DIRECTORY}\")\n+\n+ADD_ARROW_LIB(arrow_hiveserver2\n+  SOURCES ${ARROW_HIVESERVER2_SRCS}\n+  DEPENDENCIES arrow_hiveserver2_thrift\n+  SHARED_LINK_FLAGS \"\"\n+  SHARED_LINK_LIBS ${ARROW_PYTHON_SHARED_LINK_LIBS}\n+)\n+\n+set_property(SOURCE ${ARROW_HIVESERVER2_SRCS}\n+  APPEND_STRING PROPERTY COMPILE_FLAGS\n+  \" -Wno-shadow-field\")\n+\n+set(ARROW_HIVESERVER2_TEST_LINK_LIBS\n+  ${ARROW_TEST_LINK_LIBS}\n+  arrow_hiveserver2_static\n+  arrow_hiveserver2_thrift\n+  thriftstatic)\n+\n+ADD_ARROW_TEST(hiveserver2-test\n+  STATIC_LINK_LIBS \"${ARROW_HIVESERVER2_TEST_LINK_LIBS}\"\n+  LABELS \"hiveserver2\"\n+)\n+\n+set_property(TARGET hiveserver2-test\n+  APPEND_STRING PROPERTY COMPILE_FLAGS\n+  \" -Wno-shadow-field\")\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/api.h b/cpp/src/arrow/dbi/hiveserver2/api.h\nnew file mode 100644\nindex 0000000000..6ac849ef87\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/api.h\n@@ -0,0 +1,27 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include \"arrow/dbi/hiveserver2/columnar-row-set.h\"\n+#include \"arrow/dbi/hiveserver2/operation.h\"\n+#include \"arrow/dbi/hiveserver2/service.h\"\n+#include \"arrow/dbi/hiveserver2/session.h\"\n+#include \"arrow/dbi/hiveserver2/types.h\"\n+#include \"arrow/dbi/hiveserver2/util.h\"\n+\n+#include \"arrow/status.h\"\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/columnar-row-set.cc b/cpp/src/arrow/dbi/hiveserver2/columnar-row-set.cc\nnew file mode 100644\nindex 0000000000..c3d654e31c\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/columnar-row-set.cc\n@@ -0,0 +1,100 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dbi/hiveserver2/columnar-row-set.h\"\n+\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/dbi/hiveserver2/TCLIService.h\"\n+#include \"arrow/dbi/hiveserver2/thrift-internal.h\"\n+\n+#include \"arrow/util/logging.h\"\n+\n+namespace hs2 = apache::hive::service::cli::thrift;\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+Column::Column(const std::string* nulls) {\n+  DCHECK(nulls);\n+  nulls_ = reinterpret_cast<const uint8_t*>(nulls->c_str());\n+  nulls_size_ = static_cast<int64_t>(nulls->size());\n+}\n+\n+ColumnarRowSet::ColumnarRowSet(ColumnarRowSetImpl* impl) : impl_(impl) {}\n+\n+ColumnarRowSet::~ColumnarRowSet() = default;\n+\n+template <typename T>\n+struct type_helpers {};\n+\n+#define VALUE_GETTER(COLUMN_TYPE, VALUE_TYPE, ATTR_NAME)                       \\\n+  template <>                                                                  \\\n+  struct type_helpers<COLUMN_TYPE> {                                           \\\n+    static const std::vector<VALUE_TYPE>* GetValues(const hs2::TColumn& col) { \\\n+      return &col.ATTR_NAME.values;                                            \\\n+    }                                                                          \\\n+                                                                               \\\n+    static const std::string* GetNulls(const hs2::TColumn& col) {              \\\n+      return &col.ATTR_NAME.nulls;                                             \\\n+    }                                                                          \\\n+  };\n+\n+VALUE_GETTER(BoolColumn, bool, boolVal);\n+VALUE_GETTER(ByteColumn, int8_t, byteVal);\n+VALUE_GETTER(Int16Column, int16_t, i16Val);\n+VALUE_GETTER(Int32Column, int32_t, i32Val);\n+VALUE_GETTER(Int64Column, int64_t, i64Val);\n+VALUE_GETTER(DoubleColumn, double, doubleVal);\n+VALUE_GETTER(StringColumn, std::string, stringVal);\n+\n+#undef VALUE_GETTER\n+\n+template <typename T>\n+std::unique_ptr<T> ColumnarRowSet::GetCol(int i) const {\n+  using helper = type_helpers<T>;\n+\n+  DCHECK_LT(i, static_cast<int>(impl_->resp.results.columns.size()));\n+\n+  const hs2::TColumn& col = impl_->resp.results.columns[i];\n+  return std::unique_ptr<T>(new T(helper::GetNulls(col), helper::GetValues(col)));\n+}\n+\n+#define TYPED_GETTER(FUNC_NAME, TYPE)                            \\\n+  std::unique_ptr<TYPE> ColumnarRowSet::FUNC_NAME(int i) const { \\\n+    return GetCol<TYPE>(i);                                      \\\n+  }                                                              \\\n+  template std::unique_ptr<TYPE> ColumnarRowSet::GetCol<TYPE>(int i) const;\n+\n+TYPED_GETTER(GetBoolCol, BoolColumn);\n+TYPED_GETTER(GetByteCol, ByteColumn);\n+TYPED_GETTER(GetInt16Col, Int16Column);\n+TYPED_GETTER(GetInt32Col, Int32Column);\n+TYPED_GETTER(GetInt64Col, Int64Column);\n+TYPED_GETTER(GetDoubleCol, DoubleColumn);\n+TYPED_GETTER(GetStringCol, StringColumn);\n+\n+#undef TYPED_GETTER\n+\n+// BinaryColumn is an alias for StringColumn\n+std::unique_ptr<BinaryColumn> ColumnarRowSet::GetBinaryCol(int i) const {\n+  return GetCol<BinaryColumn>(i);\n+}\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/columnar-row-set.h b/cpp/src/arrow/dbi/hiveserver2/columnar-row-set.h\nnew file mode 100644\nindex 0000000000..a62c738020\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/columnar-row-set.h\n@@ -0,0 +1,155 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/util/macros.h\"\n+#include \"arrow/util/visibility.h\"\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+// The Column class is used to access data that was fetched in columnar format.\n+// The contents of the data can be accessed through the data() fn, which returns\n+// a ptr to a vector containing the contents of this column in the fetched\n+// results, avoiding copies. This vector will be of size length().\n+//\n+// If any of the values are null, they will be represented in the data vector as\n+// default values, i.e. 0 for numeric types. The nulls() fn returns a ptr to a\n+// bit array representing which values are null, and the IsNull() fn is provided\n+// for convenience when working with this bit array. The user should check\n+// IsNull() to distinguish between actual instances of the default values and nulls.\n+//\n+// A Column object is returned from a ColumnarRowSet and is only valid as long\n+// as that ColumnarRowSet still exists.\n+//\n+// Example:\n+// unique_ptr<Int32Column> col = columnar_row_set->GetInt32Col();\n+// for (int i = 0; i < col->length(); i++) {\n+//   if (col->IsNull(i)) {\n+//     cout << \"NULL\\n\";\n+//   } else {\n+//     cout << col->data()[i] << \"\\n\";\n+//   }\n+// }\n+class ARROW_EXPORT Column {\n+ public:\n+  virtual ~Column() {}\n+\n+  virtual int64_t length() const = 0;\n+\n+  const uint8_t* nulls() const { return nulls_; }\n+  int64_t nulls_size() const { return nulls_size_; }\n+\n+  // Returns true iff the value for the i-th row within this set of data for this\n+  // column is null.\n+  bool IsNull(int64_t i) const { return (nulls_[i / 8] & (1 << (i % 8))) != 0; }\n+\n+ protected:\n+  explicit Column(const std::string* nulls);\n+\n+  // The memory for these ptrs is owned by the ColumnarRowSet that\n+  // created this Column.\n+  //\n+  // Due to the issue described in HUE-2722, the null bitmap may have fewer\n+  // bytes than expected for some versions of Hive, so we retain the ability to\n+  // check the buffer size in case this happens.\n+  const uint8_t* nulls_;\n+  int64_t nulls_size_;\n+};\n+\n+template <class T>\n+class ARROW_EXPORT TypedColumn : public Column {\n+ public:\n+  const std::vector<T>& data() const { return *data_; }\n+  int64_t length() const { return data().size(); }\n+\n+  // Returns the value for the i-th row within this set of data for this column.\n+  const T& GetData(int64_t i) const { return data()[i]; }\n+\n+ private:\n+  // For access to the c'tor.\n+  friend class ColumnarRowSet;\n+\n+  TypedColumn(const std::string* nulls, const std::vector<T>* data)\n+      : Column(nulls), data_(data) {}\n+\n+  const std::vector<T>* data_;\n+};\n+\n+typedef TypedColumn<bool> BoolColumn;\n+typedef TypedColumn<int8_t> ByteColumn;\n+typedef TypedColumn<int16_t> Int16Column;\n+typedef TypedColumn<int32_t> Int32Column;\n+typedef TypedColumn<int64_t> Int64Column;\n+typedef TypedColumn<double> DoubleColumn;\n+typedef TypedColumn<std::string> StringColumn;\n+typedef TypedColumn<std::string> BinaryColumn;\n+\n+// A ColumnarRowSet represents the full results returned by a call to\n+// Operation::Fetch() when a columnar format is being used.\n+//\n+// ColumnarRowSet provides access to specific columns by their type and index in\n+// the results. All Column objects returned from a given ColumnarRowSet will have\n+// the same length(). A Column object returned by a ColumnarRowSet is only valid\n+// as long as the ColumnarRowSet still exists.\n+//\n+// Example:\n+// unique_ptr<Operation> op;\n+// session->ExecuteStatement(\"select int_col, string_col from tbl\", &op);\n+// unique_ptr<ColumnarRowSet> columnar_row_set;\n+// if (op->Fetch(&columnar_row_set).ok()) {\n+//   unique_ptr<Int32Column> int32_col = columnar_row_set->GetInt32Col(0);\n+//   unique_ptr<StringColumn> string_col = columnar_row_set->GetStringCol(1);\n+// }\n+class ARROW_EXPORT ColumnarRowSet {\n+ public:\n+  ~ColumnarRowSet();\n+\n+  std::unique_ptr<BoolColumn> GetBoolCol(int i) const;\n+  std::unique_ptr<ByteColumn> GetByteCol(int i) const;\n+  std::unique_ptr<Int16Column> GetInt16Col(int i) const;\n+  std::unique_ptr<Int32Column> GetInt32Col(int i) const;\n+  std::unique_ptr<Int64Column> GetInt64Col(int i) const;\n+  std::unique_ptr<DoubleColumn> GetDoubleCol(int i) const;\n+  std::unique_ptr<StringColumn> GetStringCol(int i) const;\n+  std::unique_ptr<BinaryColumn> GetBinaryCol(int i) const;\n+\n+  template <typename T>\n+  std::unique_ptr<T> GetCol(int i) const;\n+\n+ private:\n+  // Hides Thrift objects from the header.\n+  struct ColumnarRowSetImpl;\n+\n+  ARROW_DISALLOW_COPY_AND_ASSIGN(ColumnarRowSet);\n+\n+  // For access to the c'tor.\n+  friend class Operation;\n+\n+  explicit ColumnarRowSet(ColumnarRowSetImpl* impl);\n+\n+  std::unique_ptr<ColumnarRowSetImpl> impl_;\n+};\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/hiveserver2-test.cc b/cpp/src/arrow/dbi/hiveserver2/hiveserver2-test.cc\nnew file mode 100644\nindex 0000000000..7022ff017f\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/hiveserver2-test.cc\n@@ -0,0 +1,462 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dbi/hiveserver2/operation.h\"\n+\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/dbi/hiveserver2/service.h\"\n+#include \"arrow/dbi/hiveserver2/session.h\"\n+#include \"arrow/dbi/hiveserver2/thrift-internal.h\"\n+\n+#include \"arrow/status.h\"\n+#include \"arrow/test-util.h\"\n+\n+using std::string;\n+using std::unique_ptr;\n+using std::vector;\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+static std::string GetTestHost() {\n+  const char* host = std::getenv(\"ARROW_HIVESERVER2_TEST_HOST\");\n+  return host == nullptr ? \"localhost\" : std::string(host);\n+}\n+\n+// Convenience functions for finding a row of values given several columns.\n+template <typename VType, typename CType>\n+bool FindRow(VType value, CType* column) {\n+  for (int i = 0; i < column->length(); ++i) {\n+    if (column->data()[i] == value) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+template <typename V1Type, typename V2Type, typename C1Type, typename C2Type>\n+bool FindRow(V1Type value1, V2Type value2, C1Type* column1, C2Type* column2) {\n+  EXPECT_EQ(column1->length(), column2->length());\n+  for (int i = 0; i < column1->length(); ++i) {\n+    if (column1->data()[i] == value1 && column2->data()[i] == value2) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+template <typename V1Type, typename V2Type, typename V3Type, typename C1Type,\n+          typename C2Type, typename C3Type>\n+bool FindRow(V1Type value1, V2Type value2, V3Type value3, C1Type* column1,\n+             C2Type* column2, C3Type column3) {\n+  EXPECT_EQ(column1->length(), column2->length());\n+  EXPECT_EQ(column1->length(), column3->length());\n+  for (int i = 0; i < column1->length(); ++i) {\n+    if (column1->data()[i] == value1 && column2->data()[i] == value2 &&\n+        column3->data()[i] == value3) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+// Waits for this operation to reach the given state, sleeping for sleep microseconds\n+// between checks, and failing after max_retries checks.\n+Status Wait(const std::unique_ptr<Operation>& op,\n+            Operation::State state = Operation::State::FINISHED, int sleep_us = 10000,\n+            int max_retries = 100) {\n+  int retries = 0;\n+  Operation::State op_state;\n+  RETURN_NOT_OK(op->GetState(&op_state));\n+  while (op_state != state && retries < max_retries) {\n+    usleep(sleep_us);\n+    RETURN_NOT_OK(op->GetState(&op_state));\n+    ++retries;\n+  }\n+\n+  if (op_state == state) {\n+    return Status::OK();\n+  } else {\n+    std::stringstream ss;\n+    ss << \"Failed to reach state '\" << OperationStateToString(state) << \"' after \"\n+       << retries << \" retries.\";\n+    return Status::IOError(ss.str());\n+  }\n+}\n+\n+// Creates a service, session, and database for use in tests.\n+class HS2ClientTest : public ::testing::Test {\n+ protected:\n+  virtual void SetUp() {\n+    hostname_ = GetTestHost();\n+\n+    int conn_timeout = 0;\n+    ProtocolVersion protocol_version = ProtocolVersion::PROTOCOL_V7;\n+    ASSERT_OK(\n+        Service::Connect(hostname_, port, conn_timeout, protocol_version, &service_));\n+\n+    std::string user = \"user\";\n+    HS2ClientConfig config;\n+    ASSERT_OK(service_->OpenSession(user, config, &session_));\n+\n+    std::unique_ptr<Operation> drop_db_op;\n+    ASSERT_OK(session_->ExecuteStatement(\n+        \"drop database if exists \" + TEST_DB + \" cascade\", &drop_db_op));\n+    ASSERT_OK(drop_db_op->Close());\n+\n+    std::unique_ptr<Operation> create_db_op;\n+    ASSERT_OK(session_->ExecuteStatement(\"create database \" + TEST_DB, &create_db_op));\n+    ASSERT_OK(create_db_op->Close());\n+\n+    std::unique_ptr<Operation> use_db_op;\n+    ASSERT_OK(session_->ExecuteStatement(\"use \" + TEST_DB, &use_db_op));\n+    ASSERT_OK(use_db_op->Close());\n+  }\n+\n+  virtual void TearDown() {\n+    std::unique_ptr<Operation> use_db_op;\n+    if (session_) {\n+      // We were able to create a session and service\n+      ASSERT_OK(session_->ExecuteStatement(\"use default\", &use_db_op));\n+      ASSERT_OK(use_db_op->Close());\n+\n+      std::unique_ptr<Operation> drop_db_op;\n+      ASSERT_OK(session_->ExecuteStatement(\"drop database \" + TEST_DB + \" cascade\",\n+                                           &drop_db_op));\n+      ASSERT_OK(drop_db_op->Close());\n+\n+      ASSERT_OK(session_->Close());\n+      ASSERT_OK(service_->Close());\n+    }\n+  }\n+\n+  void CreateTestTable() {\n+    std::unique_ptr<Operation> create_table_op;\n+    ASSERT_OK(session_->ExecuteStatement(\n+        \"create table \" + TEST_TBL + \" (\" + TEST_COL1 + \" int, \" + TEST_COL2 + \" string)\",\n+        &create_table_op));\n+    ASSERT_OK(create_table_op->Close());\n+  }\n+\n+  void InsertIntoTestTable(std::vector<int> int_col_data,\n+                           std::vector<std::string> string_col_data) {\n+    ASSERT_EQ(int_col_data.size(), string_col_data.size());\n+\n+    std::stringstream query;\n+    query << \"insert into \" << TEST_TBL << \" VALUES \";\n+    for (size_t i = 0; i < int_col_data.size(); i++) {\n+      if (int_col_data[i] == NULL_INT_VALUE) {\n+        query << \" (NULL, \";\n+      } else {\n+        query << \" (\" << int_col_data[i] << \", \";\n+      }\n+\n+      if (string_col_data[i] == \"NULL\") {\n+        query << \"NULL)\";\n+      } else {\n+        query << \"'\" << string_col_data[i] << \"')\";\n+      }\n+\n+      if (i != int_col_data.size() - 1) {\n+        query << \", \";\n+      }\n+    }\n+\n+    std::unique_ptr<Operation> insert_op;\n+    ASSERT_OK(session_->ExecuteStatement(query.str(), &insert_op));\n+    ASSERT_OK(Wait(insert_op));\n+    Operation::State insert_op_state;\n+    ASSERT_OK(insert_op->GetState(&insert_op_state));\n+    ASSERT_EQ(insert_op_state, Operation::State::FINISHED);\n+    ASSERT_OK(insert_op->Close());\n+  }\n+  std::string hostname_;\n+\n+  int port = 21050;\n+\n+  const std::string TEST_DB = \"hs2client_test_db\";\n+  const std::string TEST_TBL = \"hs2client_test_table\";\n+  const std::string TEST_COL1 = \"int_col\";\n+  const std::string TEST_COL2 = \"string_col\";\n+\n+  const int NULL_INT_VALUE = -1;\n+\n+  std::unique_ptr<Service> service_;\n+  std::unique_ptr<Session> session_;\n+};\n+\n+class OperationTest : public HS2ClientTest {};\n+\n+TEST_F(OperationTest, TestFetch) {\n+  CreateTestTable();\n+  InsertIntoTestTable(vector<int>({1, 2, 3, 4}), vector<string>({\"a\", \"b\", \"c\", \"d\"}));\n+\n+  unique_ptr<Operation> select_op;\n+  ASSERT_OK(session_->ExecuteStatement(\"select * from \" + TEST_TBL + \" order by int_col\",\n+                                       &select_op));\n+\n+  unique_ptr<ColumnarRowSet> results;\n+  bool has_more_rows = false;\n+  // Impala only supports NEXT and FIRST.\n+  ASSERT_RAISES(IOError,\n+                select_op->Fetch(2, FetchOrientation::LAST, &results, &has_more_rows));\n+\n+  // Fetch the results in two batches by passing max_rows to Fetch.\n+  ASSERT_OK(select_op->Fetch(2, FetchOrientation::NEXT, &results, &has_more_rows));\n+  ASSERT_OK(Wait(select_op));\n+  ASSERT_TRUE(select_op->HasResultSet());\n+  unique_ptr<Int32Column> int_col = results->GetInt32Col(0);\n+  unique_ptr<StringColumn> string_col = results->GetStringCol(1);\n+  ASSERT_EQ(int_col->data(), vector<int>({1, 2}));\n+  ASSERT_EQ(string_col->data(), vector<string>({\"a\", \"b\"}));\n+  ASSERT_TRUE(has_more_rows);\n+\n+  ASSERT_OK(select_op->Fetch(2, FetchOrientation::NEXT, &results, &has_more_rows));\n+  int_col = results->GetInt32Col(0);\n+  string_col = results->GetStringCol(1);\n+  ASSERT_EQ(int_col->data(), vector<int>({3, 4}));\n+  ASSERT_EQ(string_col->data(), vector<string>({\"c\", \"d\"}));\n+\n+  ASSERT_OK(select_op->Fetch(2, FetchOrientation::NEXT, &results, &has_more_rows));\n+  int_col = results->GetInt32Col(0);\n+  string_col = results->GetStringCol(1);\n+  ASSERT_EQ(int_col->length(), 0);\n+  ASSERT_EQ(string_col->length(), 0);\n+  ASSERT_FALSE(has_more_rows);\n+\n+  ASSERT_OK(select_op->Fetch(2, FetchOrientation::NEXT, &results, &has_more_rows));\n+  int_col = results->GetInt32Col(0);\n+  string_col = results->GetStringCol(1);\n+  ASSERT_EQ(int_col->length(), 0);\n+  ASSERT_EQ(string_col->length(), 0);\n+  ASSERT_FALSE(has_more_rows);\n+\n+  ASSERT_OK(select_op->Close());\n+}\n+\n+TEST_F(OperationTest, TestIsNull) {\n+  CreateTestTable();\n+  // Insert some NULLs and ensure Column::IsNull() is correct.\n+  InsertIntoTestTable(vector<int>({1, 2, 3, 4, 5, NULL_INT_VALUE}),\n+                      vector<string>({\"a\", \"b\", \"NULL\", \"d\", \"NULL\", \"f\"}));\n+\n+  unique_ptr<Operation> select_nulls_op;\n+  ASSERT_OK(session_->ExecuteStatement(\"select * from \" + TEST_TBL + \" order by int_col\",\n+                                       &select_nulls_op));\n+\n+  unique_ptr<ColumnarRowSet> nulls_results;\n+  bool has_more_rows = false;\n+  ASSERT_OK(select_nulls_op->Fetch(&nulls_results, &has_more_rows));\n+  unique_ptr<Int32Column> int_col = nulls_results->GetInt32Col(0);\n+  unique_ptr<StringColumn> string_col = nulls_results->GetStringCol(1);\n+  ASSERT_EQ(int_col->length(), 6);\n+  ASSERT_EQ(int_col->length(), string_col->length());\n+\n+  bool int_nulls[] = {false, false, false, false, false, true};\n+  for (int i = 0; i < int_col->length(); i++) {\n+    ASSERT_EQ(int_col->IsNull(i), int_nulls[i]);\n+  }\n+  bool string_nulls[] = {false, false, true, false, true, false};\n+  for (int i = 0; i < string_col->length(); i++) {\n+    ASSERT_EQ(string_col->IsNull(i), string_nulls[i]);\n+  }\n+\n+  ASSERT_OK(select_nulls_op->Close());\n+}\n+\n+TEST_F(OperationTest, TestCancel) {\n+  CreateTestTable();\n+  InsertIntoTestTable(vector<int>({1, 2, 3, 4}), vector<string>({\"a\", \"b\", \"c\", \"d\"}));\n+\n+  unique_ptr<Operation> op;\n+  ASSERT_OK(session_->ExecuteStatement(\"select count(*) from \" + TEST_TBL, &op));\n+  ASSERT_OK(op->Cancel());\n+  // Impala currently returns ERROR and not CANCELED for canceled queries\n+  // due to the use of beeswax states, which don't support a canceled state.\n+  ASSERT_OK(Wait(op, Operation::State::ERROR));\n+\n+  string profile;\n+  ASSERT_OK(op->GetProfile(&profile));\n+  ASSERT_TRUE(profile.find(\"Cancelled\") != string::npos);\n+\n+  ASSERT_OK(op->Close());\n+}\n+\n+TEST_F(OperationTest, TestGetLog) {\n+  CreateTestTable();\n+\n+  unique_ptr<Operation> op;\n+  ASSERT_OK(session_->ExecuteStatement(\"select count(*) from \" + TEST_TBL, &op));\n+  string log;\n+  ASSERT_OK(op->GetLog(&log));\n+  ASSERT_NE(log, \"\");\n+\n+  ASSERT_OK(op->Close());\n+}\n+\n+TEST_F(OperationTest, TestGetResultSetMetadata) {\n+  const string TEST_COL1 = \"int_col\";\n+  const string TEST_COL2 = \"varchar_col\";\n+  const int MAX_LENGTH = 10;\n+  const string TEST_COL3 = \"decimal_cal\";\n+  const int PRECISION = 5;\n+  const int SCALE = 3;\n+  std::stringstream create_query;\n+  create_query << \"create table \" << TEST_TBL << \" (\" << TEST_COL1 << \" int, \"\n+               << TEST_COL2 << \" varchar(\" << MAX_LENGTH << \"), \" << TEST_COL3\n+               << \" decimal(\" << PRECISION << \", \" << SCALE << \"))\";\n+  unique_ptr<Operation> create_table_op;\n+  ASSERT_OK(session_->ExecuteStatement(create_query.str(), &create_table_op));\n+  ASSERT_OK(create_table_op->Close());\n+\n+  // Perform a select, and check that we get the right metadata back.\n+  unique_ptr<Operation> select_op;\n+  ASSERT_OK(session_->ExecuteStatement(\"select * from \" + TEST_TBL, &select_op));\n+  vector<ColumnDesc> column_descs;\n+  ASSERT_OK(select_op->GetResultSetMetadata(&column_descs));\n+  ASSERT_EQ(column_descs.size(), 3);\n+\n+  ASSERT_EQ(column_descs[0].column_name(), TEST_COL1);\n+  ASSERT_EQ(column_descs[0].type()->ToString(), \"INT\");\n+  ASSERT_EQ(column_descs[0].type()->type_id(), ColumnType::TypeId::INT);\n+  ASSERT_EQ(column_descs[0].position(), 0);\n+\n+  ASSERT_EQ(column_descs[1].column_name(), TEST_COL2);\n+  ASSERT_EQ(column_descs[1].type()->ToString(), \"VARCHAR\");\n+  ASSERT_EQ(column_descs[1].type()->type_id(), ColumnType::TypeId::VARCHAR);\n+  ASSERT_EQ(column_descs[1].position(), 1);\n+  ASSERT_EQ(column_descs[1].GetCharacterType()->max_length(), MAX_LENGTH);\n+\n+  ASSERT_EQ(column_descs[2].column_name(), TEST_COL3);\n+  ASSERT_EQ(column_descs[2].type()->ToString(), \"DECIMAL\");\n+  ASSERT_EQ(column_descs[2].type()->type_id(), ColumnType::TypeId::DECIMAL);\n+  ASSERT_EQ(column_descs[2].position(), 2);\n+  ASSERT_EQ(column_descs[2].GetDecimalType()->precision(), PRECISION);\n+  ASSERT_EQ(column_descs[2].GetDecimalType()->scale(), SCALE);\n+\n+  ASSERT_OK(select_op->Close());\n+\n+  // Insert ops don't have result sets.\n+  std::stringstream insert_query;\n+  insert_query << \"insert into \" << TEST_TBL << \" VALUES (1, cast('a' as varchar(\"\n+               << MAX_LENGTH << \")), cast(1 as decimal(\" << PRECISION << \", \" << SCALE\n+               << \")))\";\n+  unique_ptr<Operation> insert_op;\n+  ASSERT_OK(session_->ExecuteStatement(insert_query.str(), &insert_op));\n+  vector<ColumnDesc> insert_column_descs;\n+  ASSERT_OK(insert_op->GetResultSetMetadata(&insert_column_descs));\n+  ASSERT_EQ(insert_column_descs.size(), 0);\n+  ASSERT_OK(insert_op->Close());\n+}\n+\n+class SessionTest : public HS2ClientTest {};\n+\n+TEST_F(SessionTest, TestSessionConfig) {\n+  // Create a table in TEST_DB.\n+  const string& TEST_TBL = \"hs2client_test_table\";\n+  unique_ptr<Operation> create_table_op;\n+  ASSERT_OK(session_->ExecuteStatement(\n+      \"create table \" + TEST_TBL + \" (int_col int, string_col string)\",\n+      &create_table_op));\n+  ASSERT_OK(create_table_op->Close());\n+\n+  // Start a new session with the use:database session option.\n+  string user = \"user\";\n+  HS2ClientConfig config_use;\n+  config_use.SetOption(\"use:database\", TEST_DB);\n+  unique_ptr<Session> session_ok;\n+  ASSERT_OK(service_->OpenSession(user, config_use, &session_ok));\n+\n+  // Ensure the use:database worked and we can access the table.\n+  unique_ptr<Operation> select_op;\n+  ASSERT_OK(session_ok->ExecuteStatement(\"select * from \" + TEST_TBL, &select_op));\n+  ASSERT_OK(select_op->Close());\n+  ASSERT_OK(session_ok->Close());\n+\n+  // Start another session without use:database.\n+  HS2ClientConfig config_no_use;\n+  unique_ptr<Session> session_error;\n+  ASSERT_OK(service_->OpenSession(user, config_no_use, &session_error));\n+\n+  // Ensure the we can't access the table.\n+  unique_ptr<Operation> select_op_error;\n+  ASSERT_RAISES(IOError, session_error->ExecuteStatement(\"select * from \" + TEST_TBL,\n+                                                         &select_op_error));\n+  ASSERT_OK(session_error->Close());\n+}\n+\n+TEST(ServiceTest, TestConnect) {\n+  // Open a connection.\n+  string host = GetTestHost();\n+  int port = 21050;\n+  int conn_timeout = 0;\n+  ProtocolVersion protocol_version = ProtocolVersion::PROTOCOL_V7;\n+  unique_ptr<Service> service;\n+  ASSERT_OK(Service::Connect(host, port, conn_timeout, protocol_version, &service));\n+  ASSERT_TRUE(service->IsConnected());\n+\n+  // Check that we can start a session.\n+  string user = \"user\";\n+  HS2ClientConfig config;\n+  unique_ptr<Session> session1;\n+  ASSERT_OK(service->OpenSession(user, config, &session1));\n+  ASSERT_OK(session1->Close());\n+\n+  // Close the service. We should not be able to open a session.\n+  ASSERT_OK(service->Close());\n+  ASSERT_FALSE(service->IsConnected());\n+  ASSERT_OK(service->Close());\n+  unique_ptr<Session> session3;\n+  ASSERT_RAISES(IOError, service->OpenSession(user, config, &session3));\n+  ASSERT_OK(session3->Close());\n+\n+  // We should be able to call Close again without errors.\n+  ASSERT_OK(service->Close());\n+  ASSERT_FALSE(service->IsConnected());\n+}\n+\n+TEST(ServiceTest, TestFailedConnect) {\n+  string host = GetTestHost();\n+  int port = 21050;\n+\n+  // Set 100ms timeout so these return quickly\n+  int conn_timeout = 100;\n+\n+  ProtocolVersion protocol_version = ProtocolVersion::PROTOCOL_V7;\n+  unique_ptr<Service> service;\n+\n+  string invalid_host = \"does_not_exist\";\n+  ASSERT_RAISES(IOError, Service::Connect(invalid_host, port, conn_timeout,\n+                                          protocol_version, &service));\n+\n+  int invalid_port = -1;\n+  ASSERT_RAISES(IOError, Service::Connect(host, invalid_port, conn_timeout,\n+                                          protocol_version, &service));\n+\n+  ProtocolVersion invalid_protocol_version = ProtocolVersion::PROTOCOL_V2;\n+  ASSERT_RAISES(NotImplemented, Service::Connect(host, port, conn_timeout,\n+                                                 invalid_protocol_version, &service));\n+}\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/operation.cc b/cpp/src/arrow/dbi/hiveserver2/operation.cc\nnew file mode 100644\nindex 0000000000..09e6514bd0\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/operation.cc\n@@ -0,0 +1,150 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dbi/hiveserver2/operation.h\"\n+\n+#include \"arrow/dbi/hiveserver2/thrift-internal.h\"\n+\n+#include \"arrow/dbi/hiveserver2/ImpalaService_types.h\"\n+#include \"arrow/dbi/hiveserver2/TCLIService.h\"\n+\n+#include \"arrow/status.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/macros.h\"\n+\n+namespace hs2 = apache::hive::service::cli::thrift;\n+using std::unique_ptr;\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+// Max rows to fetch, if not specified.\n+constexpr int kDefaultMaxRows = 1024;\n+\n+Operation::Operation(const std::shared_ptr<ThriftRPC>& rpc)\n+    : impl_(new OperationImpl()), rpc_(rpc), open_(false) {}\n+\n+Operation::~Operation() { DCHECK(!open_); }\n+\n+Status Operation::GetState(Operation::State* out) const {\n+  hs2::TGetOperationStatusReq req;\n+  req.__set_operationHandle(impl_->handle);\n+  hs2::TGetOperationStatusResp resp;\n+  TRY_RPC_OR_RETURN(rpc_->client->GetOperationStatus(resp, req));\n+  THRIFT_RETURN_NOT_OK(resp.status);\n+  *out = TOperationStateToOperationState(resp.operationState);\n+  return TStatusToStatus(resp.status);\n+}\n+\n+Status Operation::GetLog(std::string* out) const {\n+  hs2::TGetLogReq req;\n+  req.__set_operationHandle(impl_->handle);\n+  hs2::TGetLogResp resp;\n+  TRY_RPC_OR_RETURN(rpc_->client->GetLog(resp, req));\n+  THRIFT_RETURN_NOT_OK(resp.status);\n+  *out = resp.log;\n+  return TStatusToStatus(resp.status);\n+}\n+\n+Status Operation::GetProfile(std::string* out) const {\n+  impala::TGetRuntimeProfileReq req;\n+  req.__set_operationHandle(impl_->handle);\n+  req.__set_sessionHandle(impl_->session_handle);\n+  impala::TGetRuntimeProfileResp resp;\n+  TRY_RPC_OR_RETURN(rpc_->client->GetRuntimeProfile(resp, req));\n+  THRIFT_RETURN_NOT_OK(resp.status);\n+  *out = resp.profile;\n+  return TStatusToStatus(resp.status);\n+}\n+\n+Status Operation::GetResultSetMetadata(std::vector<ColumnDesc>* column_descs) const {\n+  hs2::TGetResultSetMetadataReq req;\n+  req.__set_operationHandle(impl_->handle);\n+  hs2::TGetResultSetMetadataResp resp;\n+  TRY_RPC_OR_RETURN(rpc_->client->GetResultSetMetadata(resp, req));\n+  THRIFT_RETURN_NOT_OK(resp.status);\n+\n+  column_descs->clear();\n+  column_descs->reserve(resp.schema.columns.size());\n+  for (const hs2::TColumnDesc& tcolumn_desc : resp.schema.columns) {\n+    column_descs->emplace_back(tcolumn_desc.columnName,\n+                               TTypeDescToColumnType(tcolumn_desc.typeDesc),\n+                               tcolumn_desc.position, tcolumn_desc.comment);\n+  }\n+\n+  return TStatusToStatus(resp.status);\n+}\n+\n+Status Operation::Fetch(unique_ptr<ColumnarRowSet>* results, bool* has_more_rows) const {\n+  return Fetch(kDefaultMaxRows, FetchOrientation::NEXT, results, has_more_rows);\n+}\n+\n+Status Operation::Fetch(int max_rows, FetchOrientation orientation,\n+                        unique_ptr<ColumnarRowSet>* results, bool* has_more_rows) const {\n+  hs2::TFetchResultsReq req;\n+  req.__set_operationHandle(impl_->handle);\n+  req.__set_orientation(FetchOrientationToTFetchOrientation(orientation));\n+  req.__set_maxRows(max_rows);\n+  std::unique_ptr<ColumnarRowSet::ColumnarRowSetImpl> row_set_impl(\n+      new ColumnarRowSet::ColumnarRowSetImpl());\n+  TRY_RPC_OR_RETURN(rpc_->client->FetchResults(row_set_impl->resp, req));\n+  THRIFT_RETURN_NOT_OK(row_set_impl->resp.status);\n+\n+  if (has_more_rows != NULL) {\n+    *has_more_rows = row_set_impl->resp.hasMoreRows;\n+  }\n+  Status status = TStatusToStatus(row_set_impl->resp.status);\n+  DCHECK(status.ok());\n+  results->reset(new ColumnarRowSet(row_set_impl.release()));\n+  return status;\n+}\n+\n+Status Operation::Cancel() const {\n+  hs2::TCancelOperationReq req;\n+  req.__set_operationHandle(impl_->handle);\n+  hs2::TCancelOperationResp resp;\n+  TRY_RPC_OR_RETURN(rpc_->client->CancelOperation(resp, req));\n+  return TStatusToStatus(resp.status);\n+}\n+\n+Status Operation::Close() {\n+  if (!open_) return Status::OK();\n+\n+  hs2::TCloseOperationReq req;\n+  req.__set_operationHandle(impl_->handle);\n+  hs2::TCloseOperationResp resp;\n+  TRY_RPC_OR_RETURN(rpc_->client->CloseOperation(resp, req));\n+  THRIFT_RETURN_NOT_OK(resp.status);\n+\n+  open_ = false;\n+  return TStatusToStatus(resp.status);\n+}\n+\n+bool Operation::HasResultSet() const {\n+  State op_state;\n+  Status s = GetState(&op_state);\n+  if (!s.ok()) return false;\n+  return op_state == State::FINISHED;\n+}\n+\n+bool Operation::IsColumnar() const {\n+  // We currently only support the columnar hs2 protocols.\n+  return true;\n+}\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/operation.h b/cpp/src/arrow/dbi/hiveserver2/operation.h\nnew file mode 100644\nindex 0000000000..f275592e23\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/operation.h\n@@ -0,0 +1,127 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/dbi/hiveserver2/columnar-row-set.h\"\n+#include \"arrow/dbi/hiveserver2/types.h\"\n+\n+#include \"arrow/util/macros.h\"\n+#include \"arrow/util/visibility.h\"\n+\n+namespace arrow {\n+\n+class Status;\n+\n+namespace hiveserver2 {\n+\n+struct ThriftRPC;\n+\n+// Maps directly to TFetchOrientation in the HiveServer2 interface.\n+enum class FetchOrientation {\n+  NEXT,      // supported\n+  PRIOR,     // not supported\n+  RELATIVE,  // not supported\n+  ABSOLUTE,  // not supported\n+  FIRST,     // supported if query result caching is enabled in Impala\n+  LAST       // not supported\n+};\n+\n+// Represents a single HiveServer2 operation. Used to monitor the status of an operation\n+// and to retrieve its results. The only Operation function that will block is Fetch,\n+// which blocks if there aren't any results ready yet.\n+//\n+// Operations are created using Session functions, eg. ExecuteStatement. They must\n+// have Close called on them before they can be deleted.\n+//\n+// This class is not thread-safe.\n+class ARROW_EXPORT Operation {\n+ public:\n+  // Maps directly to TOperationState in the HiveServer2 interface.\n+  enum class State {\n+    INITIALIZED,\n+    RUNNING,\n+    FINISHED,\n+    CANCELED,\n+    CLOSED,\n+    ERROR,\n+    UNKNOWN,\n+    PENDING,\n+  };\n+\n+  ~Operation();\n+\n+  // Fetches the current state of this operation. If successful, sets the operation state\n+  // in 'out' and returns an OK status, otherwise an error status is returned. May be\n+  // called after successfully creating the operation and before calling Close.\n+  Status GetState(Operation::State* out) const;\n+\n+  // May be called after successfully creating the operation and before calling Close.\n+  Status GetLog(std::string* out) const;\n+\n+  // May be called after successfully creating the operation and before calling Close.\n+  Status GetProfile(std::string* out) const;\n+\n+  // Fetches metadata for the columns in the output of this operation, such as the\n+  // names and types of the columns, and returns it as a list of column descriptions.\n+  // May be called after successfully creating the operation and before calling Close.\n+  Status GetResultSetMetadata(std::vector<ColumnDesc>* column_descs) const;\n+\n+  // Fetches a batch of results, stores them in 'results', and sets has_more_rows.\n+  // Fetch will block if there aren't any results that are ready.\n+  Status Fetch(std::unique_ptr<ColumnarRowSet>* results, bool* has_more_rows) const;\n+  Status Fetch(int max_rows, FetchOrientation orientation,\n+               std::unique_ptr<ColumnarRowSet>* results, bool* has_more_rows) const;\n+\n+  // May be called after successfully creating the operation and before calling Close.\n+  Status Cancel() const;\n+\n+  // Closes the operation. Must be called before the operation is deleted. May be safely\n+  // called on an invalid or already closed operation - will only return an error if the\n+  // operation is open but the close rpc fails.\n+  Status Close();\n+\n+  // May be called after successfully creating the operation and before calling Close.\n+  bool HasResultSet() const;\n+\n+  // Returns true iff this operation's results will be returned in a columnar format.\n+  // May be called at any time.\n+  bool IsColumnar() const;\n+\n+ protected:\n+  // Hides Thrift objects from the header.\n+  struct OperationImpl;\n+\n+  explicit Operation(const std::shared_ptr<ThriftRPC>& rpc);\n+\n+  std::unique_ptr<OperationImpl> impl_;\n+  std::shared_ptr<ThriftRPC> rpc_;\n+\n+  // True iff this operation has been successfully created and has not been closed yet,\n+  // corresponding to when the operation has a valid operation handle.\n+  bool open_;\n+\n+ private:\n+  ARROW_DISALLOW_COPY_AND_ASSIGN(Operation);\n+};\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/public-api-test.cc b/cpp/src/arrow/dbi/hiveserver2/public-api-test.cc\nnew file mode 100644\nindex 0000000000..833ad02ead\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/public-api-test.cc\n@@ -0,0 +1,26 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/adapters/hiveserver2/api.h\"\n+\n+TEST(TestPublicAPI, DoesNotIncludeThrift) {\n+#ifdef _THRIFT_THRIFT_H_\n+  FAIL() << \"Thrift headers should not be in the public API\";\n+#endif\n+}\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/sample-usage.cc b/cpp/src/arrow/dbi/hiveserver2/sample-usage.cc\nnew file mode 100644\nindex 0000000000..f16a81b07b\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/sample-usage.cc\n@@ -0,0 +1,139 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <cassert>\n+#include <iostream>\n+#include <memory>\n+#include <string>\n+\n+#include \"arrow/dbi/hiveserver2/api.h\"\n+\n+namespace hs2 = arrow::hiveserver2;\n+\n+using arrow::Status;\n+using std::string;\n+using std::unique_ptr;\n+\n+using hs2::Operation;\n+using hs2::Service;\n+using hs2::Session;\n+\n+#define ABORT_NOT_OK(s)                  \\\n+  do {                                   \\\n+    ::arrow::Status _s = (s);            \\\n+    if (ARROW_PREDICT_FALSE(!_s.ok())) { \\\n+      std::cerr << s.ToString() << \"\\n\"; \\\n+      std::abort();                      \\\n+    }                                    \\\n+  } while (false);\n+\n+int main(int argc, char** argv) {\n+  // Connect to the server.\n+  string host = \"localhost\";\n+  int port = 21050;\n+  int conn_timeout = 0;\n+  hs2::ProtocolVersion protocol = hs2::ProtocolVersion::PROTOCOL_V7;\n+  unique_ptr<Service> service;\n+  Status status = Service::Connect(host, port, conn_timeout, protocol, &service);\n+  if (!status.ok()) {\n+    std::cout << \"Failed to connect to service: \" << status.ToString();\n+    ABORT_NOT_OK(service->Close());\n+    return 1;\n+  }\n+\n+  // Open a session.\n+  string user = \"user\";\n+  hs2::HS2ClientConfig config;\n+  unique_ptr<Session> session;\n+  status = service->OpenSession(user, config, &session);\n+  if (!status.ok()) {\n+    std::cout << \"Failed to open session: \" << status.ToString();\n+    ABORT_NOT_OK(session->Close());\n+    ABORT_NOT_OK(service->Close());\n+    return 1;\n+  }\n+\n+  // Execute a statement.\n+  string statement = \"SELECT int_col, string_col FROM test order by int_col\";\n+  unique_ptr<hs2::Operation> execute_op;\n+  status = session->ExecuteStatement(statement, &execute_op);\n+  if (!status.ok()) {\n+    std::cout << \"Failed to execute select: \" << status.ToString();\n+    ABORT_NOT_OK(execute_op->Close());\n+    ABORT_NOT_OK(session->Close());\n+    ABORT_NOT_OK(service->Close());\n+    return 1;\n+  }\n+\n+  unique_ptr<hs2::ColumnarRowSet> execute_results;\n+  bool has_more_rows = true;\n+  int total_retrieved = 0;\n+  std::cout << \"Contents of test:\\n\";\n+  while (has_more_rows) {\n+    status = execute_op->Fetch(&execute_results, &has_more_rows);\n+    if (!status.ok()) {\n+      std::cout << \"Failed to fetch results: \" << status.ToString();\n+      ABORT_NOT_OK(execute_op->Close());\n+      ABORT_NOT_OK(session->Close());\n+      ABORT_NOT_OK(service->Close());\n+      return 1;\n+    }\n+\n+    unique_ptr<hs2::Int32Column> int_col = execute_results->GetInt32Col(0);\n+    unique_ptr<hs2::StringColumn> string_col = execute_results->GetStringCol(1);\n+    assert(int_col->length() == string_col->length());\n+    total_retrieved += int_col->length();\n+    for (int64_t i = 0; i < int_col->length(); ++i) {\n+      if (int_col->IsNull(i)) {\n+        std::cout << \"NULL\";\n+      } else {\n+        std::cout << int_col->GetData(i);\n+      }\n+      std::cout << \":\";\n+\n+      if (string_col->IsNull(i)) {\n+        std::cout << \"NULL\";\n+      } else {\n+        std::cout << \"'\" << string_col->GetData(i) << \"'\";\n+      }\n+      std::cout << \"\\n\";\n+    }\n+  }\n+  std::cout << \"retrieved \" << total_retrieved << \" rows\\n\";\n+  std::cout << \"\\n\";\n+  ABORT_NOT_OK(execute_op->Close());\n+\n+  unique_ptr<Operation> show_tables_op;\n+  status = session->ExecuteStatement(\"show tables\", &show_tables_op);\n+  if (!status.ok()) {\n+    std::cout << \"Failed to execute GetTables: \" << status.ToString();\n+    ABORT_NOT_OK(show_tables_op->Close());\n+    ABORT_NOT_OK(session->Close());\n+    ABORT_NOT_OK(service->Close());\n+    return 1;\n+  }\n+\n+  std::cout << \"Show tables:\\n\";\n+  hs2::Util::PrintResults(show_tables_op.get(), std::cout);\n+  ABORT_NOT_OK(show_tables_op->Close());\n+\n+  // Shut down.\n+  ABORT_NOT_OK(session->Close());\n+  ABORT_NOT_OK(service->Close());\n+\n+  return 0;\n+}\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/service.cc b/cpp/src/arrow/dbi/hiveserver2/service.cc\nnew file mode 100644\nindex 0000000000..e2d3f2a21b\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/service.cc\n@@ -0,0 +1,113 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dbi/hiveserver2/service.h\"\n+\n+#include <thrift/protocol/TBinaryProtocol.h>\n+#include <thrift/transport/TSocket.h>\n+#include <thrift/transport/TTransportUtils.h>\n+#include <sstream>\n+\n+#include \"arrow/dbi/hiveserver2/session.h\"\n+#include \"arrow/dbi/hiveserver2/thrift-internal.h\"\n+\n+#include \"arrow/dbi/hiveserver2/ImpalaHiveServer2Service.h\"\n+#include \"arrow/dbi/hiveserver2/TCLIService.h\"\n+\n+#include \"arrow/status.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace hs2 = apache::hive::service::cli::thrift;\n+\n+using apache::thrift::TException;\n+using apache::thrift::protocol::TBinaryProtocol;\n+using apache::thrift::protocol::TProtocol;\n+using apache::thrift::transport::TBufferedTransport;\n+using apache::thrift::transport::TSocket;\n+using apache::thrift::transport::TTransport;\n+using std::string;\n+using std::unique_ptr;\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+struct Service::ServiceImpl {\n+  hs2::TProtocolVersion::type protocol_version;\n+  std::shared_ptr<TSocket> socket;\n+  std::shared_ptr<TTransport> transport;\n+  std::shared_ptr<TProtocol> protocol;\n+};\n+\n+Status Service::Connect(const string& host, int port, int conn_timeout,\n+                        ProtocolVersion protocol_version, unique_ptr<Service>* service) {\n+  service->reset(new Service(host, port, conn_timeout, protocol_version));\n+  return (*service)->Open();\n+}\n+\n+Service::~Service() { DCHECK(!IsConnected()); }\n+\n+Status Service::Close() {\n+  if (!IsConnected()) return Status::OK();\n+  TRY_RPC_OR_RETURN(impl_->transport->close());\n+  return Status::OK();\n+}\n+\n+bool Service::IsConnected() const {\n+  return impl_->transport && impl_->transport->isOpen();\n+}\n+\n+void Service::SetRecvTimeout(int timeout) { impl_->socket->setRecvTimeout(timeout); }\n+\n+void Service::SetSendTimeout(int timeout) { impl_->socket->setSendTimeout(timeout); }\n+\n+Status Service::OpenSession(const string& user, const HS2ClientConfig& config,\n+                            unique_ptr<Session>* session) const {\n+  session->reset(new Session(rpc_));\n+  return (*session)->Open(config, user);\n+}\n+\n+Service::Service(const string& host, int port, int conn_timeout,\n+                 ProtocolVersion protocol_version)\n+    : host_(host),\n+      port_(port),\n+      conn_timeout_(conn_timeout),\n+      impl_(new ServiceImpl()),\n+      rpc_(new ThriftRPC()) {\n+  impl_->protocol_version = ProtocolVersionToTProtocolVersion(protocol_version);\n+}\n+\n+Status Service::Open() {\n+  if (impl_->protocol_version < hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V6) {\n+    std::stringstream ss;\n+    ss << \"Unsupported protocol: \" << impl_->protocol_version;\n+    return Status::NotImplemented(ss.str());\n+  }\n+\n+  impl_->socket.reset(new TSocket(host_, port_));\n+  impl_->socket->setConnTimeout(conn_timeout_);\n+  impl_->transport.reset(new TBufferedTransport(impl_->socket));\n+  impl_->protocol.reset(new TBinaryProtocol(impl_->transport));\n+\n+  rpc_->client.reset(new impala::ImpalaHiveServer2ServiceClient(impl_->protocol));\n+\n+  TRY_RPC_OR_RETURN(impl_->transport->open());\n+\n+  return Status::OK();\n+}\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/service.h b/cpp/src/arrow/dbi/hiveserver2/service.h\nnew file mode 100644\nindex 0000000000..bfa7a97db3\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/service.h\n@@ -0,0 +1,140 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <map>\n+#include <memory>\n+#include <string>\n+\n+#include \"arrow/util/macros.h\"\n+#include \"arrow/util/visibility.h\"\n+\n+namespace arrow {\n+\n+class Status;\n+\n+namespace hiveserver2 {\n+\n+class Session;\n+struct ThriftRPC;\n+\n+// Stores per-session or per-operation configuration parameters.\n+class HS2ClientConfig {\n+ public:\n+  void SetOption(const std::string& key, const std::string& value) {\n+    config_[key] = value;\n+  }\n+\n+  bool GetOption(const std::string& key, std::string* value_out) {\n+    if (config_.find(key) != config_.end() && value_out) {\n+      *value_out = config_[key];\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  const std::map<std::string, std::string>& GetConfig() const { return config_; }\n+\n+ private:\n+  std::map<std::string, std::string> config_;\n+};\n+\n+// Maps directly to TProtocolVersion in the HiveServer2 interface.\n+enum class ProtocolVersion {\n+  PROTOCOL_V1,  // not supported\n+  PROTOCOL_V2,  // not supported\n+  PROTOCOL_V3,  // not supported\n+  PROTOCOL_V4,  // not supported\n+  PROTOCOL_V5,  // not supported\n+  PROTOCOL_V6,  // supported\n+  PROTOCOL_V7,  // supported\n+};\n+\n+// Manages a connection to a HiveServer2 server. Primarily used to create\n+// new sessions via OpenSession.\n+//\n+// Service objects are created using Service::Connect(). They must\n+// have Close called on them before they can be deleted.\n+//\n+// This class is not thread-safe.\n+//\n+// Example:\n+// unique_ptr<Service> service;\n+// if (Service::Connect(host, port, protocol_version, &service).ok()) {\n+//   // do some work\n+//   service->Close();\n+// }\n+class ARROW_EXPORT Service {\n+ public:\n+  // Creates a new connection to a HS2 service at the given host and port. If\n+  // conn_timeout > 0, connection attempts will timeout after conn_timeout ms, otherwise\n+  // no timeout is used. protocol_version is the HiveServer2 protocol to use, and\n+  // determines whether the results returned by operations from this service are row or\n+  // column oriented. Only column oriented protocols are currently supported.\n+  //\n+  // The client calling Connect has ownership of the new Service that is created.\n+  // Executing RPCs with an Session or Operation corresponding to a particular\n+  // Service after that Service has been closed or deleted in undefined.\n+  static Status Connect(const std::string& host, int port, int conn_timeout,\n+                        ProtocolVersion protocol_version,\n+                        std::unique_ptr<Service>* service);\n+\n+  ~Service();\n+\n+  // Closes the connection. Must be called before the service is deleted. May be\n+  // safely called on an invalid or already closed service - will only return an\n+  // error if the service is open but the close rpc fails.\n+  Status Close();\n+\n+  // Returns true iff this service has an active connection to the HiveServer2 server.\n+  bool IsConnected() const;\n+\n+  // Set the send and receive timeout for Thrift RPCs in ms. 0 indicates no timeout,\n+  // negative values are ignored.\n+  void SetRecvTimeout(int timeout);\n+  void SetSendTimeout(int timeout);\n+\n+  // Opens a new HS2 session using this service.\n+  // The client calling OpenSession has ownership of the Session that is created.\n+  // Operations on the Session are undefined once it is closed.\n+  Status OpenSession(const std::string& user, const HS2ClientConfig& config,\n+                     std::unique_ptr<Session>* session) const;\n+\n+ private:\n+  ARROW_DISALLOW_COPY_AND_ASSIGN(Service);\n+\n+  // Hides Thrift objects from the header.\n+  struct ServiceImpl;\n+\n+  Service(const std::string& host, int port, int conn_timeout,\n+          ProtocolVersion protocol_version);\n+\n+  // Opens the connection to the server. Called by Connect before new service is returned\n+  // to the user. Must be called before OpenSession.\n+  Status Open();\n+\n+  std::string host_;\n+  int port_;\n+  int conn_timeout_;\n+\n+  std::unique_ptr<ServiceImpl> impl_;\n+  std::shared_ptr<ThriftRPC> rpc_;\n+};\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/session.cc b/cpp/src/arrow/dbi/hiveserver2/session.cc\nnew file mode 100644\nindex 0000000000..0d3784829a\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/session.cc\n@@ -0,0 +1,105 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dbi/hiveserver2/session.h\"\n+\n+#include \"arrow/dbi/hiveserver2/TCLIService.h\"\n+#include \"arrow/dbi/hiveserver2/thrift-internal.h\"\n+\n+#include \"arrow/status.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace hs2 = apache::hive::service::cli::thrift;\n+using apache::thrift::TException;\n+using std::string;\n+using std::unique_ptr;\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+struct Session::SessionImpl {\n+  hs2::TSessionHandle handle;\n+};\n+\n+Session::Session(const std::shared_ptr<ThriftRPC>& rpc)\n+    : impl_(new SessionImpl()), rpc_(rpc), open_(false) {}\n+\n+Session::~Session() { DCHECK(!open_); }\n+\n+Status Session::Close() {\n+  if (!open_) return Status::OK();\n+\n+  hs2::TCloseSessionReq req;\n+  req.__set_sessionHandle(impl_->handle);\n+  hs2::TCloseSessionResp resp;\n+  TRY_RPC_OR_RETURN(rpc_->client->CloseSession(resp, req));\n+  THRIFT_RETURN_NOT_OK(resp.status);\n+\n+  open_ = false;\n+  return TStatusToStatus(resp.status);\n+}\n+\n+Status Session::Open(const HS2ClientConfig& config, const string& user) {\n+  hs2::TOpenSessionReq req;\n+  req.__set_configuration(config.GetConfig());\n+  req.__set_username(user);\n+  hs2::TOpenSessionResp resp;\n+  TRY_RPC_OR_RETURN(rpc_->client->OpenSession(resp, req));\n+  THRIFT_RETURN_NOT_OK(resp.status);\n+\n+  impl_->handle = resp.sessionHandle;\n+  open_ = true;\n+  return TStatusToStatus(resp.status);\n+}\n+\n+class ExecuteStatementOperation : public Operation {\n+ public:\n+  explicit ExecuteStatementOperation(const std::shared_ptr<ThriftRPC>& rpc)\n+      : Operation(rpc) {}\n+\n+  Status Open(hs2::TSessionHandle session_handle, const string& statement,\n+              const HS2ClientConfig& config) {\n+    hs2::TExecuteStatementReq req;\n+    req.__set_sessionHandle(session_handle);\n+    req.__set_statement(statement);\n+    req.__set_confOverlay(config.GetConfig());\n+    hs2::TExecuteStatementResp resp;\n+    TRY_RPC_OR_RETURN(rpc_->client->ExecuteStatement(resp, req));\n+    THRIFT_RETURN_NOT_OK(resp.status);\n+\n+    impl_->handle = resp.operationHandle;\n+    impl_->session_handle = session_handle;\n+    open_ = true;\n+    return TStatusToStatus(resp.status);\n+  }\n+};\n+\n+Status Session::ExecuteStatement(const string& statement,\n+                                 unique_ptr<Operation>* operation) const {\n+  return ExecuteStatement(statement, HS2ClientConfig(), operation);\n+}\n+\n+Status Session::ExecuteStatement(const string& statement,\n+                                 const HS2ClientConfig& conf_overlay,\n+                                 unique_ptr<Operation>* operation) const {\n+  ExecuteStatementOperation* op = new ExecuteStatementOperation(rpc_);\n+  operation->reset(op);\n+  return op->Open(impl_->handle, statement, conf_overlay);\n+}\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/session.h b/cpp/src/arrow/dbi/hiveserver2/session.h\nnew file mode 100644\nindex 0000000000..4e223de6c1\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/session.h\n@@ -0,0 +1,84 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <string>\n+\n+#include \"arrow/dbi/hiveserver2/operation.h\"\n+#include \"arrow/dbi/hiveserver2/service.h\"\n+\n+#include \"arrow/util/visibility.h\"\n+\n+namespace arrow {\n+\n+class Status;\n+\n+namespace hiveserver2 {\n+\n+struct ThriftRPC;\n+\n+// Manages a single HiveServer2 session - stores the session handle returned by\n+// the OpenSession RPC and uses it to create and return operations.\n+//\n+// Sessions are created with Service::OpenSession(). They must have Close\n+// called on them before they can be deleted.\n+//\n+// Executing RPCs with an Operation corresponding to a particular Session after\n+// that Session has been closed or deleted is undefined.\n+//\n+// This class is not thread-safe.\n+class ARROW_EXPORT Session {\n+ public:\n+  ~Session();\n+\n+  // Closes the session. Must be called before the session is deleted. May be safely\n+  // called on an invalid or already closed session - will only return an error if the\n+  // session is open but the close rpc fails.\n+  Status Close();\n+\n+  Status ExecuteStatement(const std::string& statement,\n+                          std::unique_ptr<Operation>* operation) const;\n+  Status ExecuteStatement(const std::string& statement,\n+                          const HS2ClientConfig& conf_overlay,\n+                          std::unique_ptr<Operation>* operation) const;\n+\n+ private:\n+  ARROW_DISALLOW_COPY_AND_ASSIGN(Session);\n+\n+  // Hides Thrift objects from the header.\n+  struct SessionImpl;\n+\n+  // For access to the c'tor.\n+  friend class Service;\n+\n+  explicit Session(const std::shared_ptr<ThriftRPC>& rpc);\n+\n+  // Performs the RPC that initiates the session and stores the returned handle.\n+  // Must be called before operations can be executed.\n+  Status Open(const HS2ClientConfig& config, const std::string& user);\n+\n+  std::unique_ptr<SessionImpl> impl_;\n+  std::shared_ptr<ThriftRPC> rpc_;\n+\n+  // True if Open has been called and Close has not.\n+  bool open_;\n+};\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift-internal.cc b/cpp/src/arrow/dbi/hiveserver2/thrift-internal.cc\nnew file mode 100644\nindex 0000000000..d154e143ba\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift-internal.cc\n@@ -0,0 +1,303 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dbi/hiveserver2/thrift-internal.h\"\n+\n+#include <map>\n+#include <sstream>\n+\n+#include \"arrow/dbi/hiveserver2/TCLIService_constants.h\"\n+#include \"arrow/dbi/hiveserver2/service.h\"\n+\n+#include \"arrow/status.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace hs2 = apache::hive::service::cli::thrift;\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+namespace {\n+\n+// Convert an \"enum class\" value to an integer equivalent, for outputting.\n+template <typename ENUM>\n+typename std::underlying_type<ENUM>::type EnumToInt(const ENUM& value) {\n+  return static_cast<typename std::underlying_type<ENUM>::type>(value);\n+}\n+\n+}  // namespace\n+\n+const std::string OperationStateToString(const Operation::State& state) {\n+  switch (state) {\n+    case Operation::State::INITIALIZED:\n+      return \"INITIALIZED\";\n+    case Operation::State::RUNNING:\n+      return \"RUNNING\";\n+    case Operation::State::FINISHED:\n+      return \"FINISHED\";\n+    case Operation::State::CANCELED:\n+      return \"CANCELED\";\n+    case Operation::State::CLOSED:\n+      return \"CLOSED\";\n+    case Operation::State::ERROR:\n+      return \"ERROR\";\n+    case Operation::State::UNKNOWN:\n+      return \"UNKNOWN\";\n+    case Operation::State::PENDING:\n+      return \"PENDING\";\n+    default:\n+      std::stringstream ss;\n+      ss << \"Unknown Operation::State \" << EnumToInt(state);\n+      return ss.str();\n+  }\n+}\n+\n+const std::string TypeIdToString(const ColumnType::TypeId& type_id) {\n+  switch (type_id) {\n+    case ColumnType::TypeId::BOOLEAN:\n+      return \"BOOLEAN\";\n+    case ColumnType::TypeId::TINYINT:\n+      return \"TINYINT\";\n+    case ColumnType::TypeId::SMALLINT:\n+      return \"SMALLINT\";\n+    case ColumnType::TypeId::INT:\n+      return \"INT\";\n+    case ColumnType::TypeId::BIGINT:\n+      return \"BIGINT\";\n+    case ColumnType::TypeId::FLOAT:\n+      return \"FLOAT\";\n+    case ColumnType::TypeId::DOUBLE:\n+      return \"DOUBLE\";\n+    case ColumnType::TypeId::STRING:\n+      return \"STRING\";\n+    case ColumnType::TypeId::TIMESTAMP:\n+      return \"TIMESTAMP\";\n+    case ColumnType::TypeId::BINARY:\n+      return \"BINARY\";\n+    case ColumnType::TypeId::ARRAY:\n+      return \"ARRAY\";\n+    case ColumnType::TypeId::MAP:\n+      return \"MAP\";\n+    case ColumnType::TypeId::STRUCT:\n+      return \"STRUCT\";\n+    case ColumnType::TypeId::UNION:\n+      return \"UNION\";\n+    case ColumnType::TypeId::USER_DEFINED:\n+      return \"USER_DEFINED\";\n+    case ColumnType::TypeId::DECIMAL:\n+      return \"DECIMAL\";\n+    case ColumnType::TypeId::NULL_TYPE:\n+      return \"NULL_TYPE\";\n+    case ColumnType::TypeId::DATE:\n+      return \"DATE\";\n+    case ColumnType::TypeId::VARCHAR:\n+      return \"VARCHAR\";\n+    case ColumnType::TypeId::CHAR:\n+      return \"CHAR\";\n+    case ColumnType::TypeId::INVALID:\n+      return \"INVALID\";\n+    default: {\n+      std::stringstream ss;\n+      ss << \"Unknown ColumnType::TypeId \" << EnumToInt(type_id);\n+      return ss.str();\n+    }\n+  }\n+}\n+\n+hs2::TFetchOrientation::type FetchOrientationToTFetchOrientation(\n+    FetchOrientation orientation) {\n+  switch (orientation) {\n+    case FetchOrientation::NEXT:\n+      return hs2::TFetchOrientation::FETCH_NEXT;\n+    case FetchOrientation::PRIOR:\n+      return hs2::TFetchOrientation::FETCH_PRIOR;\n+    case FetchOrientation::RELATIVE:\n+      return hs2::TFetchOrientation::FETCH_RELATIVE;\n+    case FetchOrientation::ABSOLUTE:\n+      return hs2::TFetchOrientation::FETCH_ABSOLUTE;\n+    case FetchOrientation::FIRST:\n+      return hs2::TFetchOrientation::FETCH_FIRST;\n+    case FetchOrientation::LAST:\n+      return hs2::TFetchOrientation::FETCH_LAST;\n+    default:\n+      DCHECK(false) << \"Unknown FetchOrientation \" << EnumToInt(orientation);\n+      return hs2::TFetchOrientation::FETCH_NEXT;\n+  }\n+}\n+\n+hs2::TProtocolVersion::type ProtocolVersionToTProtocolVersion(ProtocolVersion protocol) {\n+  switch (protocol) {\n+    case ProtocolVersion::PROTOCOL_V1:\n+      return hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V1;\n+    case ProtocolVersion::PROTOCOL_V2:\n+      return hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V2;\n+    case ProtocolVersion::PROTOCOL_V3:\n+      return hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V3;\n+    case ProtocolVersion::PROTOCOL_V4:\n+      return hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V4;\n+    case ProtocolVersion::PROTOCOL_V5:\n+      return hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V5;\n+    case ProtocolVersion::PROTOCOL_V6:\n+      return hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V6;\n+    case ProtocolVersion::PROTOCOL_V7:\n+      return hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V7;\n+    default:\n+      DCHECK(false) << \"Unknown ProtocolVersion \" << EnumToInt(protocol);\n+      return hs2::TProtocolVersion::HIVE_CLI_SERVICE_PROTOCOL_V7;\n+  }\n+}\n+\n+Operation::State TOperationStateToOperationState(\n+    const hs2::TOperationState::type& tstate) {\n+  switch (tstate) {\n+    case hs2::TOperationState::INITIALIZED_STATE:\n+      return Operation::State::INITIALIZED;\n+    case hs2::TOperationState::RUNNING_STATE:\n+      return Operation::State::RUNNING;\n+    case hs2::TOperationState::FINISHED_STATE:\n+      return Operation::State::FINISHED;\n+    case hs2::TOperationState::CANCELED_STATE:\n+      return Operation::State::CANCELED;\n+    case hs2::TOperationState::CLOSED_STATE:\n+      return Operation::State::CLOSED;\n+    case hs2::TOperationState::ERROR_STATE:\n+      return Operation::State::ERROR;\n+    case hs2::TOperationState::UKNOWN_STATE:\n+      return Operation::State::UNKNOWN;\n+    case hs2::TOperationState::PENDING_STATE:\n+      return Operation::State::PENDING;\n+    default:\n+      ARROW_LOG(WARNING) << \"Unknown TOperationState \" << tstate;\n+      return Operation::State::UNKNOWN;\n+  }\n+}\n+\n+Status TStatusToStatus(const hs2::TStatus& tstatus) {\n+  switch (tstatus.statusCode) {\n+    case hs2::TStatusCode::SUCCESS_STATUS:\n+      return Status::OK();\n+    case hs2::TStatusCode::SUCCESS_WITH_INFO_STATUS: {\n+      std::stringstream ss;\n+      for (size_t i = 0; i < tstatus.infoMessages.size(); i++) {\n+        if (i != 0) ss << \",\";\n+        ss << tstatus.infoMessages[i];\n+      }\n+      return Status::OK(ss.str());\n+    }\n+    case hs2::TStatusCode::STILL_EXECUTING_STATUS:\n+      return Status::StillExecuting();\n+    case hs2::TStatusCode::ERROR_STATUS:\n+      return Status::IOError(tstatus.errorMessage);\n+    case hs2::TStatusCode::INVALID_HANDLE_STATUS:\n+      return Status::Invalid(\"Invalid handle\");\n+    default: {\n+      std::stringstream ss;\n+      ss << \"Unknown TStatusCode \" << tstatus.statusCode;\n+      return Status::UnknownError(ss.str());\n+    }\n+  }\n+}\n+\n+std::unique_ptr<ColumnType> TTypeDescToColumnType(const hs2::TTypeDesc& ttype_desc) {\n+  if (ttype_desc.types.size() != 1 || !ttype_desc.types[0].__isset.primitiveEntry) {\n+    ARROW_LOG(WARNING) << \"TTypeDescToColumnType only supports primitive types.\";\n+    return std::unique_ptr<ColumnType>(new PrimitiveType(ColumnType::TypeId::INVALID));\n+  }\n+\n+  ColumnType::TypeId type_id = TTypeIdToTypeId(ttype_desc.types[0].primitiveEntry.type);\n+  if (type_id == ColumnType::TypeId::CHAR || type_id == ColumnType::TypeId::VARCHAR) {\n+    const std::map<std::string, hs2::TTypeQualifierValue>& qualifiers =\n+        ttype_desc.types[0].primitiveEntry.typeQualifiers.qualifiers;\n+    DCHECK_EQ(qualifiers.count(hs2::g_TCLIService_constants.CHARACTER_MAXIMUM_LENGTH), 1);\n+\n+    try {\n+      return std::unique_ptr<ColumnType>(new CharacterType(\n+          type_id,\n+          qualifiers.at(hs2::g_TCLIService_constants.CHARACTER_MAXIMUM_LENGTH).i32Value));\n+    } catch (std::out_of_range e) {\n+      ARROW_LOG(ERROR) << \"Character type qualifiers invalid: \" << e.what();\n+      return std::unique_ptr<ColumnType>(new PrimitiveType(ColumnType::TypeId::INVALID));\n+    }\n+  } else if (type_id == ColumnType::TypeId::DECIMAL) {\n+    const std::map<std::string, hs2::TTypeQualifierValue>& qualifiers =\n+        ttype_desc.types[0].primitiveEntry.typeQualifiers.qualifiers;\n+    DCHECK_EQ(qualifiers.count(hs2::g_TCLIService_constants.PRECISION), 1);\n+    DCHECK_EQ(qualifiers.count(hs2::g_TCLIService_constants.SCALE), 1);\n+\n+    try {\n+      return std::unique_ptr<ColumnType>(new DecimalType(\n+          type_id, qualifiers.at(hs2::g_TCLIService_constants.PRECISION).i32Value,\n+          qualifiers.at(hs2::g_TCLIService_constants.SCALE).i32Value));\n+    } catch (std::out_of_range e) {\n+      ARROW_LOG(ERROR) << \"Decimal type qualifiers invalid: \" << e.what();\n+      return std::unique_ptr<ColumnType>(new PrimitiveType(ColumnType::TypeId::INVALID));\n+    }\n+  } else {\n+    return std::unique_ptr<ColumnType>(new PrimitiveType(type_id));\n+  }\n+}\n+\n+ColumnType::TypeId TTypeIdToTypeId(const hs2::TTypeId::type& type_id) {\n+  switch (type_id) {\n+    case hs2::TTypeId::BOOLEAN_TYPE:\n+      return ColumnType::TypeId::BOOLEAN;\n+    case hs2::TTypeId::TINYINT_TYPE:\n+      return ColumnType::TypeId::TINYINT;\n+    case hs2::TTypeId::SMALLINT_TYPE:\n+      return ColumnType::TypeId::SMALLINT;\n+    case hs2::TTypeId::INT_TYPE:\n+      return ColumnType::TypeId::INT;\n+    case hs2::TTypeId::BIGINT_TYPE:\n+      return ColumnType::TypeId::BIGINT;\n+    case hs2::TTypeId::FLOAT_TYPE:\n+      return ColumnType::TypeId::FLOAT;\n+    case hs2::TTypeId::DOUBLE_TYPE:\n+      return ColumnType::TypeId::DOUBLE;\n+    case hs2::TTypeId::STRING_TYPE:\n+      return ColumnType::TypeId::STRING;\n+    case hs2::TTypeId::TIMESTAMP_TYPE:\n+      return ColumnType::TypeId::TIMESTAMP;\n+    case hs2::TTypeId::BINARY_TYPE:\n+      return ColumnType::TypeId::BINARY;\n+    case hs2::TTypeId::ARRAY_TYPE:\n+      return ColumnType::TypeId::ARRAY;\n+    case hs2::TTypeId::MAP_TYPE:\n+      return ColumnType::TypeId::MAP;\n+    case hs2::TTypeId::STRUCT_TYPE:\n+      return ColumnType::TypeId::STRUCT;\n+    case hs2::TTypeId::UNION_TYPE:\n+      return ColumnType::TypeId::UNION;\n+    case hs2::TTypeId::USER_DEFINED_TYPE:\n+      return ColumnType::TypeId::USER_DEFINED;\n+    case hs2::TTypeId::DECIMAL_TYPE:\n+      return ColumnType::TypeId::DECIMAL;\n+    case hs2::TTypeId::NULL_TYPE:\n+      return ColumnType::TypeId::NULL_TYPE;\n+    case hs2::TTypeId::DATE_TYPE:\n+      return ColumnType::TypeId::DATE;\n+    case hs2::TTypeId::VARCHAR_TYPE:\n+      return ColumnType::TypeId::VARCHAR;\n+    case hs2::TTypeId::CHAR_TYPE:\n+      return ColumnType::TypeId::CHAR;\n+    default:\n+      ARROW_LOG(WARNING) << \"Unknown TTypeId \" << type_id;\n+      return ColumnType::TypeId::INVALID;\n+  }\n+}\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift-internal.h b/cpp/src/arrow/dbi/hiveserver2/thrift-internal.h\nnew file mode 100644\nindex 0000000000..aad535fc1f\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift-internal.h\n@@ -0,0 +1,91 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <string>\n+\n+#include \"arrow/dbi/hiveserver2/columnar-row-set.h\"\n+#include \"arrow/dbi/hiveserver2/operation.h\"\n+#include \"arrow/dbi/hiveserver2/service.h\"\n+#include \"arrow/dbi/hiveserver2/types.h\"\n+\n+#include \"arrow/dbi/hiveserver2/ImpalaHiveServer2Service.h\"\n+#include \"arrow/dbi/hiveserver2/TCLIService.h\"\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+// PIMPL structs.\n+struct ColumnarRowSet::ColumnarRowSetImpl {\n+  apache::hive::service::cli::thrift::TFetchResultsResp resp;\n+};\n+\n+struct Operation::OperationImpl {\n+  apache::hive::service::cli::thrift::TOperationHandle handle;\n+  apache::hive::service::cli::thrift::TSessionHandle session_handle;\n+};\n+\n+struct ThriftRPC {\n+  std::unique_ptr<impala::ImpalaHiveServer2ServiceClient> client;\n+};\n+\n+const std::string OperationStateToString(const Operation::State& state);\n+\n+const std::string TypeIdToString(const ColumnType::TypeId& type_id);\n+\n+// Functions for converting Thrift object to hs2client objects and vice-versa.\n+apache::hive::service::cli::thrift::TFetchOrientation::type\n+FetchOrientationToTFetchOrientation(FetchOrientation orientation);\n+\n+apache::hive::service::cli::thrift::TProtocolVersion::type\n+ProtocolVersionToTProtocolVersion(ProtocolVersion protocol);\n+\n+Operation::State TOperationStateToOperationState(\n+    const apache::hive::service::cli::thrift::TOperationState::type& tstate);\n+\n+Status TStatusToStatus(const apache::hive::service::cli::thrift::TStatus& tstatus);\n+\n+// Converts a TTypeDesc to a ColumnType. Currently only primitive types are supported.\n+// The converted type is returned as a pointer to allow for polymorphism with ColumnType\n+// and its subclasses.\n+std::unique_ptr<ColumnType> TTypeDescToColumnType(\n+    const apache::hive::service::cli::thrift::TTypeDesc& ttype_desc);\n+\n+ColumnType::TypeId TTypeIdToTypeId(\n+    const apache::hive::service::cli::thrift::TTypeId::type& type_id);\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\n+\n+#define TRY_RPC_OR_RETURN(rpc)                  \\\n+  do {                                          \\\n+    try {                                       \\\n+      (rpc);                                    \\\n+    } catch (apache::thrift::TException & tx) { \\\n+      return Status::IOError(tx.what());        \\\n+    }                                           \\\n+  } while (0)\n+\n+#define THRIFT_RETURN_NOT_OK(tstatus)                                       \\\n+  do {                                                                      \\\n+    if (tstatus.statusCode != hs2::TStatusCode::SUCCESS_STATUS &&           \\\n+        tstatus.statusCode != hs2::TStatusCode::SUCCESS_WITH_INFO_STATUS) { \\\n+      return TStatusToStatus(tstatus);                                      \\\n+    }                                                                       \\\n+  } while (0)\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/.gitignore b/cpp/src/arrow/dbi/hiveserver2/thrift/.gitignore\nnew file mode 100644\nindex 0000000000..f510e7c958\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/.gitignore\n@@ -0,0 +1 @@\n+ErrorCodes.thrift\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/CMakeLists.txt b/cpp/src/arrow/dbi/hiveserver2/thrift/CMakeLists.txt\nnew file mode 100644\nindex 0000000000..c59fd5a049\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/CMakeLists.txt\n@@ -0,0 +1,110 @@\n+# Copyright 2012 Cloudera Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Helper function to generate build rules.  For each input thrift file, this function will\n+# generate a rule that maps the input file to the output c++ file.\n+# Thrift will generate multiple output files for each input (including java files) and\n+# ideally, we'd specify all of the outputs for dependency tracking.\n+# Unfortunately, it's not easy to figure out all the output files without parsing the\n+# thrift input. (TODO: can thrift tells us what the java output files will be?)\n+# The list of output files is used for build dependency tracking so it's not necessary to\n+# capture all the output files.\n+#\n+# To call this function, pass it the output file list followed by the input thrift files:\n+#    i.e. HS2_THRIFT_GEN(OUTPUT_FILES, ${THRIFT_FILES})\n+#\n+# cmake seems to be case sensitive for some keywords. Changing the first IF check to lower\n+# case makes it not work.  TODO: investigate this\n+function(HS2_THRIFT_GEN VAR)\n+  IF (NOT ARGN)\n+    MESSAGE(SEND_ERROR \"Error: THRIFT_GEN called without any src files\")\n+    RETURN()\n+  ENDIF(NOT ARGN)\n+\n+  set(${VAR})\n+  foreach(FIL ${ARGN})\n+    # Get full path\n+    get_filename_component(ABS_FIL ${FIL} ABSOLUTE)\n+    # Get basename\n+    get_filename_component(FIL_WE ${FIL} NAME_WE)\n+\n+    set(GEN_DIR \"${OUTPUT_DIR}/arrow/dbi/hiveserver2\")\n+\n+    # All the output files we can determine based on filename.\n+    #   - Does not include .skeleton.cpp files\n+    #   - Does not include java output files\n+    set(OUTPUT_BE_FILE \"${GEN_DIR}/${FIL_WE}_types.cpp\")\n+    set(OUTPUT_BE_FILE ${OUTPUT_BE_FILE} \" ${GEN_DIR}/${FIL_WE}_types.h\")\n+    set(OUTPUT_BE_FILE ${OUTPUT_BE_FILE} \" ${GEN_DIR}/${FIL_WE}_constants.cpp\")\n+    set(OUTPUT_BE_FILE ${OUTPUT_BE_FILE} \" ${GEN_DIR}/${FIL_WE}_constants.h\")\n+    list(APPEND ${VAR} ${OUTPUT_BE_FILE})\n+\n+    # BeeswaxService thrift generation\n+    # It depends on hive_meta_store, which in turn depends on fb303.\n+    # The java dependency is handled by maven.\n+    # We need to generate C++ src file for the parent dependencies using the \"-r\" option.\n+    set(CPP_ARGS -nowarn --gen cpp -out ${GEN_DIR})\n+    IF (FIL STREQUAL \"beeswax.thrift\")\n+      set(CPP_ARGS -r -nowarn --gen cpp -out ${GEN_DIR})\n+    ENDIF(FIL STREQUAL \"beeswax.thrift\")\n+\n+    # Be able to include generated ErrorCodes.thrift file\n+    set(CPP_ARGS ${CPP_ARGS} -I ${CMAKE_CURRENT_BINARY_DIR})\n+\n+    add_custom_command(\n+      OUTPUT ${OUTPUT_BE_FILE}\n+      COMMAND ${THRIFT_COMPILER} ${CPP_ARGS} ${FIL}\n+      DEPENDS ${ABS_FIL}\n+      COMMENT \"Running thrift compiler on ${FIL}\"\n+\t  WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n+      VERBATIM\n+    )\n+  endforeach(FIL)\n+\n+  set(${VAR} ${${VAR}} PARENT_SCOPE)\n+endfunction(HS2_THRIFT_GEN)\n+\n+message(\"Using Thrift compiler: ${THRIFT_COMPILER}\")\n+\n+set(OUTPUT_DIR ${CMAKE_BINARY_DIR}/src)\n+file(MAKE_DIRECTORY ${OUTPUT_DIR})\n+\n+add_custom_command(OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/ErrorCodes.thrift\n+  COMMAND python generate_error_codes.py ${CMAKE_CURRENT_BINARY_DIR}\n+  DEPENDS generate_error_codes.py\n+  WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR})\n+\n+set(SRC_FILES\n+  ${CMAKE_CURRENT_BINARY_DIR}/ErrorCodes.thrift\n+  beeswax.thrift\n+  TCLIService.thrift\n+  ExecStats.thrift\n+  ImpalaService.thrift\n+  Status.thrift\n+  Types.thrift\n+)\n+\n+SET_SOURCE_FILES_PROPERTIES(Status.thrift PROPERTIES OBJECT_DEPENDS\n+  ${CMAKE_CURRENT_BINARY_DIR}/ErrorCodes.thrift)\n+\n+# Create a build command for each of the thrift src files and generate\n+# a list of files they produce\n+HS2_THRIFT_GEN(THRIFT_ALL_FILES ${SRC_FILES})\n+\n+# Add a custom target that generates all the thrift files\n+add_custom_target(hs2-thrift-cpp ALL DEPENDS ${THRIFT_ALL_FILES})\n+\n+add_custom_target(hs2-thrift-generated-files-error DEPENDS\n+  ${CMAKE_CURRENT_BINARY_DIR}/ErrorCodes.thrift)\n+add_dependencies(hs2-thrift-cpp hs2-thrift-generated-files-error)\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/ExecStats.thrift b/cpp/src/arrow/dbi/hiveserver2/thrift/ExecStats.thrift\nnew file mode 100644\nindex 0000000000..bcf5c4c6ae\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/ExecStats.thrift\n@@ -0,0 +1,103 @@\n+// Copyright 2012 Cloudera Inc.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+namespace cpp impala\n+namespace java com.cloudera.impala.thrift\n+\n+include \"Status.thrift\"\n+include \"Types.thrift\"\n+\n+enum TExecState {\n+  REGISTERED = 0,\n+  PLANNING = 1,\n+  QUEUED = 2,\n+  RUNNING = 3,\n+  FINISHED = 4,\n+\n+  CANCELLED = 5,\n+  FAILED = 6,\n+}\n+\n+// Execution stats for a single plan node.\n+struct TExecStats {\n+  // The wall clock time spent on the \"main\" thread. This is the user perceived\n+  // latency. This value indicates the current bottleneck.\n+  // Note: anywhere we have a queue between operators, this time can fluctuate\n+  // significantly without the overall query time changing much (i.e. the bottleneck\n+  // moved to another operator). This is unavoidable though.\n+  1: optional i64 latency_ns\n+\n+  // Total CPU time spent across all threads. For operators that have an async\n+  // component (e.g. multi-threaded) this will be >= latency_ns.\n+  2: optional i64 cpu_time_ns\n+\n+  // Number of rows returned.\n+  3: optional i64 cardinality\n+\n+  // Peak memory used (in bytes).\n+  4: optional i64 memory_used\n+}\n+\n+// Summary for a single plan node. This includes labels for how to display the\n+// node as well as per instance stats.\n+struct TPlanNodeExecSummary {\n+  1: required Types.TPlanNodeId node_id\n+  2: required i32 fragment_id\n+  3: required string label\n+  4: optional string label_detail\n+  5: required i32 num_children\n+\n+  // Estimated stats generated by the planner\n+  6: optional TExecStats estimated_stats\n+\n+  // One entry for each BE executing this plan node.\n+  7: optional list<TExecStats> exec_stats\n+\n+  // One entry for each BE executing this plan node. True if this plan node is still\n+  // running.\n+  8: optional list<bool> is_active\n+\n+  // If true, this plan node is an exchange node that is the receiver of a broadcast.\n+  9: optional bool is_broadcast\n+}\n+\n+// Progress counters for an in-flight query.\n+struct TExecProgress {\n+  1: optional i64 total_scan_ranges\n+  2: optional i64 num_completed_scan_ranges\n+}\n+\n+// Execution summary of an entire query.\n+struct TExecSummary {\n+  // State of the query.\n+  1: required TExecState state\n+\n+  // Contains the error if state is FAILED.\n+  2: optional Status.TStatus status\n+\n+  // Flattened execution summary of the plan tree.\n+  3: optional list<TPlanNodeExecSummary> nodes\n+\n+  // For each exch node in 'nodes', contains the index to the root node of the sending\n+  // fragment for this exch. Both the key and value are indices into 'nodes'.\n+  4: optional map<i32, i32> exch_to_sender_map\n+\n+  // List of errors that were encountered during execution. This can be non-empty\n+  // even if status is okay, in which case it contains errors that impala skipped\n+  // over.\n+  5: optional list<string> error_logs\n+\n+  // Optional record indicating the query progress\n+  6: optional TExecProgress progress\n+}\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/ImpalaService.thrift b/cpp/src/arrow/dbi/hiveserver2/thrift/ImpalaService.thrift\nnew file mode 100644\nindex 0000000000..33f049f441\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/ImpalaService.thrift\n@@ -0,0 +1,300 @@\n+// Copyright 2012 Cloudera Inc.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+namespace cpp impala\n+namespace java com.cloudera.impala.thrift\n+\n+include \"ExecStats.thrift\"\n+include \"Status.thrift\"\n+include \"Types.thrift\"\n+include \"beeswax.thrift\"\n+include \"TCLIService.thrift\"\n+\n+// ImpalaService accepts query execution options through beeswax.Query.configuration in\n+// key:value form. For example, the list of strings could be:\n+//     \"num_nodes:1\", \"abort_on_error:false\"\n+// The valid keys are listed in this enum. They map to TQueryOptions.\n+// Note: If you add an option or change the default, you also need to update:\n+// - ImpalaInternalService.thrift: TQueryOptions\n+// - SetQueryOption(), SetQueryOptions()\n+// - TQueryOptionsToMap()\n+enum TImpalaQueryOptions {\n+  // if true, abort execution on the first error\n+  ABORT_ON_ERROR,\n+\n+  // maximum # of errors to be reported; Unspecified or 0 indicates backend default\n+  MAX_ERRORS,\n+\n+  // if true, disable llvm codegen\n+  DISABLE_CODEGEN,\n+\n+  // batch size to be used by backend; Unspecified or a size of 0 indicates backend\n+  // default\n+  BATCH_SIZE,\n+\n+  // a per-machine approximate limit on the memory consumption of this query;\n+  // unspecified or a limit of 0 means no limit;\n+  // otherwise specified either as:\n+  // a) an int (= number of bytes);\n+  // b) a float followed by \"M\" (MB) or \"G\" (GB)\n+  MEM_LIMIT,\n+\n+  // specifies the degree of parallelism with which to execute the query;\n+  // 1: single-node execution\n+  // NUM_NODES_ALL: executes on all nodes that contain relevant data\n+  // NUM_NODES_ALL_RACKS: executes on one node per rack that holds relevant data\n+  // > 1: executes on at most that many nodes at any point in time (ie, there can be\n+  //      more nodes than numNodes with plan fragments for this query, but at most\n+  //      numNodes would be active at any point in time)\n+  // Constants (NUM_NODES_ALL, NUM_NODES_ALL_RACKS) are defined in JavaConstants.thrift.\n+  NUM_NODES,\n+\n+  // maximum length of the scan range; only applicable to HDFS scan range; Unspecified or\n+  // a length of 0 indicates backend default;\n+  MAX_SCAN_RANGE_LENGTH,\n+\n+  // Maximum number of io buffers (per disk)\n+  MAX_IO_BUFFERS,\n+\n+  // Number of scanner threads.\n+  NUM_SCANNER_THREADS,\n+\n+  // If true, Impala will try to execute on file formats that are not fully supported yet\n+  ALLOW_UNSUPPORTED_FORMATS,\n+\n+  // if set and > -1, specifies the default limit applied to a top-level SELECT statement\n+  // with an ORDER BY but without a LIMIT clause (ie, if the SELECT statement also has\n+  // a LIMIT clause, this default is ignored)\n+  DEFAULT_ORDER_BY_LIMIT,\n+\n+  // DEBUG ONLY:\n+  // If set to\n+  //   \"[<backend number>:]<node id>:<TExecNodePhase>:<TDebugAction>\",\n+  // the exec node with the given id will perform the specified action in the given\n+  // phase. If the optional backend number (starting from 0) is specified, only that\n+  // backend instance will perform the debug action, otherwise all backends will behave\n+  // in that way.\n+  // If the string doesn't have the required format or if any of its components is\n+  // invalid, the option is ignored.\n+  DEBUG_ACTION,\n+\n+  // If true, raise an error when the DEFAULT_ORDER_BY_LIMIT has been reached.\n+  ABORT_ON_DEFAULT_LIMIT_EXCEEDED,\n+\n+  // Compression codec when inserting into tables.\n+  // Valid values are \"snappy\", \"gzip\", \"bzip2\" and \"none\"\n+  // Leave blank to use default.\n+  COMPRESSION_CODEC,\n+\n+  // Mode for compressing sequence files; either BLOCK, RECORD, or DEFAULT\n+  SEQ_COMPRESSION_MODE,\n+\n+  // HBase scan query option. If set and > 0, HBASE_CACHING is the value for\n+  // \"hbase.client.Scan.setCaching()\" when querying HBase table. Otherwise, use backend\n+  // default.\n+  // If the value is too high, then the hbase region server will have a hard time (GC\n+  // pressure and long response times). If the value is too small, then there will be\n+  // extra trips to the hbase region server.\n+  HBASE_CACHING,\n+\n+  // HBase scan query option. If set, HBase scan will always set\n+  // \"hbase.client.setCacheBlocks\" to CACHE_BLOCKS. Default is false.\n+  // If the table is large and the query is doing big scan, set it to false to\n+  // avoid polluting the cache in the hbase region server.\n+  // If the table is small and the table is used several time, set it to true to improve\n+  // performance.\n+  HBASE_CACHE_BLOCKS,\n+\n+  // Target file size for inserts into parquet tables. 0 uses the default.\n+  PARQUET_FILE_SIZE,\n+\n+  // Level of detail for explain output (NORMAL, VERBOSE).\n+  EXPLAIN_LEVEL,\n+\n+  // If true, waits for the result of all catalog operations to be processed by all\n+  // active impalad in the cluster before completing.\n+  SYNC_DDL,\n+\n+  // Request pool this request should be submitted to. If not set\n+  // the pool is determined based on the user.\n+  REQUEST_POOL,\n+\n+  // Per-host virtual CPU cores required for query (only relevant with RM).\n+  V_CPU_CORES,\n+\n+  // Max time in milliseconds the resource broker should wait for\n+  // a resource request to be granted by Llama/Yarn (only relevant with RM).\n+  RESERVATION_REQUEST_TIMEOUT,\n+\n+  // if true, disables cached reads. This option has no effect if REPLICA_PREFERENCE is\n+  // configured.\n+  // TODO: Retire in C6\n+  DISABLE_CACHED_READS,\n+\n+  // Temporary testing flag\n+  DISABLE_OUTERMOST_TOPN,\n+\n+  // Size of initial memory reservation when RM is enabled\n+  RM_INITIAL_MEM,\n+\n+  // Time, in s, before a query will be timed out if it is inactive. May not exceed\n+  // --idle_query_timeout if that flag > 0.\n+  QUERY_TIMEOUT_S,\n+\n+  // Test hook for spill to disk operators\n+  MAX_BLOCK_MGR_MEMORY,\n+\n+  // Transforms all count(distinct) aggregations into NDV()\n+  APPX_COUNT_DISTINCT,\n+\n+  // If true, allows Impala to internally disable spilling for potentially\n+  // disastrous query plans. Impala will excercise this option if a query\n+  // has no plan hints, and at least one table is missing relevant stats.\n+  DISABLE_UNSAFE_SPILLS,\n+\n+  // If the number of rows that are processed for a single query is below the\n+  // threshold, it will be executed on the coordinator only with codegen disabled\n+  EXEC_SINGLE_NODE_ROWS_THRESHOLD,\n+\n+  // If true, use the table's metadata to produce the partition columns instead of table\n+  // scans whenever possible. This option is opt-in by default as this optimization may\n+  // produce different results than the scan based approach in some edge cases.\n+  OPTIMIZE_PARTITION_KEY_SCANS,\n+\n+  // Prefered memory distance of replicas. This parameter determines the pool of replicas\n+  // among which scans will be scheduled in terms of the distance of the replica storage\n+  // from the impalad.\n+  REPLICA_PREFERENCE,\n+\n+  // Determines tie breaking policy when picking locations.\n+  RANDOM_REPLICA,\n+\n+  // For scan nodes with any conjuncts, use codegen to evaluate the conjuncts if\n+  // the number of rows * number of operators in the conjuncts exceeds this threshold.\n+  SCAN_NODE_CODEGEN_THRESHOLD,\n+\n+  // If true, the planner will not generate plans with streaming preaggregations.\n+  DISABLE_STREAMING_PREAGGREGATIONS,\n+\n+  RUNTIME_FILTER_MODE,\n+\n+  // Size (in bytes) of a runtime Bloom Filter. Will be rounded up to nearest power of\n+  // two.\n+  RUNTIME_BLOOM_FILTER_SIZE,\n+\n+  // Time (in ms) to wait in scans for partition filters to arrive.\n+  RUNTIME_FILTER_WAIT_TIME_MS,\n+\n+  // If true, disable application of runtime filters to individual rows.\n+  DISABLE_ROW_RUNTIME_FILTERING,\n+\n+  // Maximum number of runtime filters allowed per query.\n+  MAX_NUM_RUNTIME_FILTERS\n+}\n+\n+// The summary of an insert.\n+struct TInsertResult {\n+  // Number of appended rows per modified partition. Only applies to HDFS tables.\n+  // The keys represent partitions to create, coded as k1=v1/k2=v2/k3=v3..., with the\n+  // root in an unpartitioned table being the empty string.\n+  1: required map<string, i64> rows_appended\n+}\n+\n+// Response from a call to PingImpalaService\n+struct TPingImpalaServiceResp {\n+  // The Impala service's version string.\n+  1: string version\n+}\n+\n+// Parameters for a ResetTable request which will invalidate a table's metadata.\n+// DEPRECATED.\n+struct TResetTableReq {\n+  // Name of the table's parent database.\n+  1: required string db_name\n+\n+  // Name of the table.\n+  2: required string table_name\n+}\n+\n+// For all rpc that return a TStatus as part of their result type,\n+// if the status_code field is set to anything other than OK, the contents\n+// of the remainder of the result type is undefined (typically not set)\n+service ImpalaService extends beeswax.BeeswaxService {\n+  // Cancel execution of query. Returns RUNTIME_ERROR if query_id\n+  // unknown.\n+  // This terminates all threads running on behalf of this query at\n+  // all nodes that were involved in the execution.\n+  // Throws BeeswaxException if the query handle is invalid (this doesn't\n+  // necessarily indicate an error: the query might have finished).\n+  Status.TStatus Cancel(1:beeswax.QueryHandle query_id)\n+      throws(1:beeswax.BeeswaxException error);\n+\n+  // Invalidates all catalog metadata, forcing a reload\n+  // DEPRECATED; execute query \"invalidate metadata\" to refresh metadata\n+  Status.TStatus ResetCatalog();\n+\n+  // Invalidates a specific table's catalog metadata, forcing a reload on the next access\n+  // DEPRECATED; execute query \"refresh <table>\" to refresh metadata\n+  Status.TStatus ResetTable(1:TResetTableReq request)\n+\n+  // Returns the runtime profile string for the given query handle.\n+  string GetRuntimeProfile(1:beeswax.QueryHandle query_id)\n+      throws(1:beeswax.BeeswaxException error);\n+\n+  // Closes the query handle and return the result summary of the insert.\n+  TInsertResult CloseInsert(1:beeswax.QueryHandle handle)\n+      throws(1:beeswax.QueryNotFoundException error, 2:beeswax.BeeswaxException error2);\n+\n+  // Client calls this RPC to verify that the server is an ImpalaService. Returns the\n+  // server version.\n+  TPingImpalaServiceResp PingImpalaService();\n+\n+  // Returns the summary of the current execution.\n+  ExecStats.TExecSummary GetExecSummary(1:beeswax.QueryHandle handle)\n+      throws(1:beeswax.QueryNotFoundException error, 2:beeswax.BeeswaxException error2);\n+}\n+\n+// Impala HiveServer2 service\n+\n+struct TGetExecSummaryReq {\n+  1: optional TCLIService.TOperationHandle operationHandle\n+\n+  2: optional TCLIService.TSessionHandle sessionHandle\n+}\n+\n+struct TGetExecSummaryResp {\n+  1: required TCLIService.TStatus status\n+\n+  2: optional ExecStats.TExecSummary summary\n+}\n+\n+struct TGetRuntimeProfileReq {\n+  1: optional TCLIService.TOperationHandle operationHandle\n+\n+  2: optional TCLIService.TSessionHandle sessionHandle\n+}\n+\n+struct TGetRuntimeProfileResp {\n+  1: required TCLIService.TStatus status\n+\n+  2: optional string profile\n+}\n+\n+service ImpalaHiveServer2Service extends TCLIService.TCLIService {\n+  // Returns the exec summary for the given query\n+  TGetExecSummaryResp GetExecSummary(1:TGetExecSummaryReq req);\n+\n+  // Returns the runtime profile string for the given query\n+  TGetRuntimeProfileResp GetRuntimeProfile(1:TGetRuntimeProfileReq req);\n+}\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/Status.thrift b/cpp/src/arrow/dbi/hiveserver2/thrift/Status.thrift\nnew file mode 100644\nindex 0000000000..db9518e02a\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/Status.thrift\n@@ -0,0 +1,23 @@\n+// Copyright 2012 Cloudera Inc.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+include \"ErrorCodes.thrift\"\n+\n+namespace cpp impala\n+namespace java com.cloudera.impala.thrift\n+\n+struct TStatus {\n+  1: required ErrorCodes.TErrorCode status_code\n+  2: list<string> error_msgs\n+}\n\\ No newline at end of file\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/TCLIService.thrift b/cpp/src/arrow/dbi/hiveserver2/thrift/TCLIService.thrift\nnew file mode 100644\nindex 0000000000..f95e2f82bb\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/TCLIService.thrift\n@@ -0,0 +1,1180 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+// Coding Conventions for this file:\n+//\n+// Structs/Enums/Unions\n+// * Struct, Enum, and Union names begin with a \"T\",\n+//   and use a capital letter for each new word, with no underscores.\n+// * All fields should be declared as either optional or required.\n+//\n+// Functions\n+// * Function names start with a capital letter and have a capital letter for\n+//   each new word, with no underscores.\n+// * Each function should take exactly one parameter, named TFunctionNameReq,\n+//   and should return either void or TFunctionNameResp. This convention allows\n+//   incremental updates.\n+//\n+// Services\n+// * Service names begin with the letter \"T\", use a capital letter for each\n+//   new word (with no underscores), and end with the word \"Service\".\n+\n+namespace java org.apache.hive.service.cli.thrift\n+namespace cpp apache.hive.service.cli.thrift\n+\n+// List of protocol versions. A new token should be\n+// added to the end of this list every time a change is made.\n+enum TProtocolVersion {\n+  HIVE_CLI_SERVICE_PROTOCOL_V1,\n+\n+  // V2 adds support for asynchronous execution\n+  HIVE_CLI_SERVICE_PROTOCOL_V2\n+\n+  // V3 add varchar type, primitive type qualifiers\n+  HIVE_CLI_SERVICE_PROTOCOL_V3\n+\n+  // V4 add decimal precision/scale, char type\n+  HIVE_CLI_SERVICE_PROTOCOL_V4\n+\n+  // V5 adds error details when GetOperationStatus returns in error state\n+  HIVE_CLI_SERVICE_PROTOCOL_V5\n+\n+  // V6 uses binary type for binary payload (was string) and uses columnar result set\n+  HIVE_CLI_SERVICE_PROTOCOL_V6\n+\n+  // V7 adds support for delegation token based connection\n+  HIVE_CLI_SERVICE_PROTOCOL_V7\n+}\n+\n+enum TTypeId {\n+  BOOLEAN_TYPE,\n+  TINYINT_TYPE,\n+  SMALLINT_TYPE,\n+  INT_TYPE,\n+  BIGINT_TYPE,\n+  FLOAT_TYPE,\n+  DOUBLE_TYPE,\n+  STRING_TYPE,\n+  TIMESTAMP_TYPE,\n+  BINARY_TYPE,\n+  ARRAY_TYPE,\n+  MAP_TYPE,\n+  STRUCT_TYPE,\n+  UNION_TYPE,\n+  USER_DEFINED_TYPE,\n+  DECIMAL_TYPE,\n+  NULL_TYPE,\n+  DATE_TYPE,\n+  VARCHAR_TYPE,\n+  CHAR_TYPE\n+}\n+\n+const set<TTypeId> PRIMITIVE_TYPES = [\n+  TTypeId.BOOLEAN_TYPE,\n+  TTypeId.TINYINT_TYPE,\n+  TTypeId.SMALLINT_TYPE,\n+  TTypeId.INT_TYPE,\n+  TTypeId.BIGINT_TYPE,\n+  TTypeId.FLOAT_TYPE,\n+  TTypeId.DOUBLE_TYPE,\n+  TTypeId.STRING_TYPE,\n+  TTypeId.TIMESTAMP_TYPE,\n+  TTypeId.BINARY_TYPE,\n+  TTypeId.DECIMAL_TYPE,\n+  TTypeId.NULL_TYPE,\n+  TTypeId.DATE_TYPE,\n+  TTypeId.VARCHAR_TYPE,\n+  TTypeId.CHAR_TYPE\n+]\n+\n+const set<TTypeId> COMPLEX_TYPES = [\n+  TTypeId.ARRAY_TYPE\n+  TTypeId.MAP_TYPE\n+  TTypeId.STRUCT_TYPE\n+  TTypeId.UNION_TYPE\n+  TTypeId.USER_DEFINED_TYPE\n+]\n+\n+const set<TTypeId> COLLECTION_TYPES = [\n+  TTypeId.ARRAY_TYPE\n+  TTypeId.MAP_TYPE\n+]\n+\n+const map<TTypeId,string> TYPE_NAMES = {\n+  TTypeId.BOOLEAN_TYPE: \"BOOLEAN\",\n+  TTypeId.TINYINT_TYPE: \"TINYINT\",\n+  TTypeId.SMALLINT_TYPE: \"SMALLINT\",\n+  TTypeId.INT_TYPE: \"INT\",\n+  TTypeId.BIGINT_TYPE: \"BIGINT\",\n+  TTypeId.FLOAT_TYPE: \"FLOAT\",\n+  TTypeId.DOUBLE_TYPE: \"DOUBLE\",\n+  TTypeId.STRING_TYPE: \"STRING\",\n+  TTypeId.TIMESTAMP_TYPE: \"TIMESTAMP\",\n+  TTypeId.BINARY_TYPE: \"BINARY\",\n+  TTypeId.ARRAY_TYPE: \"ARRAY\",\n+  TTypeId.MAP_TYPE: \"MAP\",\n+  TTypeId.STRUCT_TYPE: \"STRUCT\",\n+  TTypeId.UNION_TYPE: \"UNIONTYPE\",\n+  TTypeId.DECIMAL_TYPE: \"DECIMAL\",\n+  TTypeId.NULL_TYPE: \"NULL\"\n+  TTypeId.DATE_TYPE: \"DATE\"\n+  TTypeId.VARCHAR_TYPE: \"VARCHAR\"\n+  TTypeId.CHAR_TYPE: \"CHAR\"\n+}\n+\n+// Thrift does not support recursively defined types or forward declarations,\n+// which makes it difficult to represent Hive's nested types.\n+// To get around these limitations TTypeDesc employs a type list that maps\n+// integer \"pointers\" to TTypeEntry objects. The following examples show\n+// how different types are represented using this scheme:\n+//\n+// \"INT\":\n+// TTypeDesc {\n+//   types = [\n+//     TTypeEntry.primitive_entry {\n+//       type = INT_TYPE\n+//     }\n+//   ]\n+// }\n+//\n+// \"ARRAY<INT>\":\n+// TTypeDesc {\n+//   types = [\n+//     TTypeEntry.array_entry {\n+//       object_type_ptr = 1\n+//     },\n+//     TTypeEntry.primitive_entry {\n+//       type = INT_TYPE\n+//     }\n+//   ]\n+// }\n+//\n+// \"MAP<INT,STRING>\":\n+// TTypeDesc {\n+//   types = [\n+//     TTypeEntry.map_entry {\n+//       key_type_ptr = 1\n+//       value_type_ptr = 2\n+//     },\n+//     TTypeEntry.primitive_entry {\n+//       type = INT_TYPE\n+//     },\n+//     TTypeEntry.primitive_entry {\n+//       type = STRING_TYPE\n+//     }\n+//   ]\n+// }\n+\n+typedef i32 TTypeEntryPtr\n+\n+// Valid TTypeQualifiers key names\n+const string CHARACTER_MAXIMUM_LENGTH = \"characterMaximumLength\"\n+\n+// Type qualifier key name for decimal\n+const string PRECISION = \"precision\"\n+const string SCALE = \"scale\"\n+\n+union TTypeQualifierValue {\n+  1: optional i32 i32Value\n+  2: optional string stringValue\n+}\n+\n+// Type qualifiers for primitive type.\n+struct TTypeQualifiers {\n+  1: required map <string, TTypeQualifierValue> qualifiers\n+}\n+\n+// Type entry for a primitive type.\n+struct TPrimitiveTypeEntry {\n+  // The primitive type token. This must satisfy the condition\n+  // that type is in the PRIMITIVE_TYPES set.\n+  1: required TTypeId type\n+  2: optional TTypeQualifiers typeQualifiers\n+}\n+\n+// Type entry for an ARRAY type.\n+struct TArrayTypeEntry {\n+  1: required TTypeEntryPtr objectTypePtr\n+}\n+\n+// Type entry for a MAP type.\n+struct TMapTypeEntry {\n+  1: required TTypeEntryPtr keyTypePtr\n+  2: required TTypeEntryPtr valueTypePtr\n+}\n+\n+// Type entry for a STRUCT type.\n+struct TStructTypeEntry {\n+  1: required map<string, TTypeEntryPtr> nameToTypePtr\n+}\n+\n+// Type entry for a UNIONTYPE type.\n+struct TUnionTypeEntry {\n+  1: required map<string, TTypeEntryPtr> nameToTypePtr\n+}\n+\n+struct TUserDefinedTypeEntry {\n+  // The fully qualified name of the class implementing this type.\n+  1: required string typeClassName\n+}\n+\n+// We use a union here since Thrift does not support inheritance.\n+union TTypeEntry {\n+  1: TPrimitiveTypeEntry primitiveEntry\n+  2: TArrayTypeEntry arrayEntry\n+  3: TMapTypeEntry mapEntry\n+  4: TStructTypeEntry structEntry\n+  5: TUnionTypeEntry unionEntry\n+  6: TUserDefinedTypeEntry userDefinedTypeEntry\n+}\n+\n+// Type descriptor for columns.\n+struct TTypeDesc {\n+  // The \"top\" type is always the first element of the list.\n+  // If the top type is an ARRAY, MAP, STRUCT, or UNIONTYPE\n+  // type, then subsequent elements represent nested types.\n+  1: required list<TTypeEntry> types\n+}\n+\n+// A result set column descriptor.\n+struct TColumnDesc {\n+  // The name of the column\n+  1: required string columnName\n+\n+  // The type descriptor for this column\n+  2: required TTypeDesc typeDesc\n+\n+  // The ordinal position of this column in the schema\n+  3: required i32 position\n+\n+  4: optional string comment\n+}\n+\n+// Metadata used to describe the schema (column names, types, comments)\n+// of result sets.\n+struct TTableSchema {\n+  1: required list<TColumnDesc> columns\n+}\n+\n+// A Boolean column value.\n+struct TBoolValue {\n+  // NULL if value is unset.\n+  1: optional bool value\n+}\n+\n+// A Byte column value.\n+struct TByteValue {\n+  // NULL if value is unset.\n+  1: optional byte value\n+}\n+\n+// A signed, 16 bit column value.\n+struct TI16Value {\n+  // NULL if value is unset\n+  1: optional i16 value\n+}\n+\n+// A signed, 32 bit column value\n+struct TI32Value {\n+  // NULL if value is unset\n+  1: optional i32 value\n+}\n+\n+// A signed 64 bit column value\n+struct TI64Value {\n+  // NULL if value is unset\n+  1: optional i64 value\n+}\n+\n+// A floating point 64 bit column value\n+struct TDoubleValue {\n+  // NULL if value is unset\n+  1: optional double value\n+}\n+\n+struct TStringValue {\n+  // NULL if value is unset\n+  1: optional string value\n+}\n+\n+// A single column value in a result set.\n+// Note that Hive's type system is richer than Thrift's,\n+// so in some cases we have to map multiple Hive types\n+// to the same Thrift type. On the client-side this is\n+// disambiguated by looking at the Schema of the\n+// result set.\n+union TColumnValue {\n+  1: TBoolValue   boolVal      // BOOLEAN\n+  2: TByteValue   byteVal      // TINYINT\n+  3: TI16Value    i16Val       // SMALLINT\n+  4: TI32Value    i32Val       // INT\n+  5: TI64Value    i64Val       // BIGINT, TIMESTAMP\n+  6: TDoubleValue doubleVal    // FLOAT, DOUBLE\n+  7: TStringValue stringVal    // STRING, LIST, MAP, STRUCT, UNIONTYPE, BINARY, DECIMAL, NULL\n+}\n+\n+// Represents a row in a rowset.\n+struct TRow {\n+  1: required list<TColumnValue> colVals\n+}\n+\n+struct TBoolColumn {\n+  1: required list<bool> values\n+  2: required binary nulls\n+}\n+\n+struct TByteColumn {\n+  1: required list<byte> values\n+  2: required binary nulls\n+}\n+\n+struct TI16Column {\n+  1: required list<i16> values\n+  2: required binary nulls\n+}\n+\n+struct TI32Column {\n+  1: required list<i32> values\n+  2: required binary nulls\n+}\n+\n+struct TI64Column {\n+  1: required list<i64> values\n+  2: required binary nulls\n+}\n+\n+struct TDoubleColumn {\n+  1: required list<double> values\n+  2: required binary nulls\n+}\n+\n+struct TStringColumn {\n+  1: required list<string> values\n+  2: required binary nulls\n+}\n+\n+struct TBinaryColumn {\n+  1: required list<binary> values\n+  2: required binary nulls\n+}\n+\n+// Note that Hive's type system is richer than Thrift's,\n+// so in some cases we have to map multiple Hive types\n+// to the same Thrift type. On the client-side this is\n+// disambiguated by looking at the Schema of the\n+// result set.\n+union TColumn {\n+  1: TBoolColumn   boolVal      // BOOLEAN\n+  2: TByteColumn   byteVal      // TINYINT\n+  3: TI16Column    i16Val       // SMALLINT\n+  4: TI32Column    i32Val       // INT\n+  5: TI64Column    i64Val       // BIGINT, TIMESTAMP\n+  6: TDoubleColumn doubleVal    // FLOAT, DOUBLE\n+  7: TStringColumn stringVal    // STRING, LIST, MAP, STRUCT, UNIONTYPE, DECIMAL, NULL\n+  8: TBinaryColumn binaryVal    // BINARY\n+}\n+\n+// Represents a rowset\n+struct TRowSet {\n+  // The starting row offset of this rowset.\n+  1: required i64 startRowOffset\n+  2: required list<TRow> rows\n+  3: optional list<TColumn> columns\n+}\n+\n+// The return status code contained in each response.\n+enum TStatusCode {\n+  SUCCESS_STATUS,\n+  SUCCESS_WITH_INFO_STATUS,\n+  STILL_EXECUTING_STATUS,\n+  ERROR_STATUS,\n+  INVALID_HANDLE_STATUS\n+}\n+\n+// The return status of a remote request\n+struct TStatus {\n+  1: required TStatusCode statusCode\n+\n+  // If status is SUCCESS_WITH_INFO, info_msgs may be populated with\n+  // additional diagnostic information.\n+  2: optional list<string> infoMessages\n+\n+  // If status is ERROR, then the following fields may be set\n+  3: optional string sqlState  // as defined in the ISO/IEF CLI specification\n+  4: optional i32 errorCode    // internal error code\n+  5: optional string errorMessage\n+}\n+\n+// The state of an operation (i.e. a query or other\n+// asynchronous operation that generates a result set)\n+// on the server.\n+enum TOperationState {\n+  // The operation has been initialized\n+  INITIALIZED_STATE,\n+\n+  // The operation is running. In this state the result\n+  // set is not available.\n+  RUNNING_STATE,\n+\n+  // The operation has completed. When an operation is in\n+  // this state its result set may be fetched.\n+  FINISHED_STATE,\n+\n+  // The operation was canceled by a client\n+  CANCELED_STATE,\n+\n+  // The operation was closed by a client\n+  CLOSED_STATE,\n+\n+  // The operation failed due to an error\n+  ERROR_STATE,\n+\n+  // The operation is in an unrecognized state\n+  UKNOWN_STATE,\n+\n+  // The operation is in an pending state\n+  PENDING_STATE,\n+}\n+\n+// A string identifier. This is interpreted literally.\n+typedef string TIdentifier\n+\n+// A search pattern.\n+//\n+// Valid search pattern characters:\n+// '_': Any single character.\n+// '%': Any sequence of zero or more characters.\n+// '\\': Escape character used to include special characters,\n+//      e.g. '_', '%', '\\'. If a '\\' precedes a non-special\n+//      character it has no special meaning and is interpreted\n+//      literally.\n+typedef string TPattern\n+\n+\n+// A search pattern or identifier. Used as input\n+// parameter for many of the catalog functions.\n+typedef string TPatternOrIdentifier\n+\n+struct THandleIdentifier {\n+  // 16 byte globally unique identifier\n+  // This is the public ID of the handle and\n+  // can be used for reporting.\n+  1: required binary guid,\n+\n+  // 16 byte secret generated by the server\n+  // and used to verify that the handle is not\n+  // being hijacked by another user.\n+  2: required binary secret,\n+}\n+\n+// Client-side handle to persistent\n+// session information on the server-side.\n+struct TSessionHandle {\n+  1: required THandleIdentifier sessionId\n+}\n+\n+// The subtype of an OperationHandle.\n+enum TOperationType {\n+  EXECUTE_STATEMENT,\n+  GET_TYPE_INFO,\n+  GET_CATALOGS,\n+  GET_SCHEMAS,\n+  GET_TABLES,\n+  GET_TABLE_TYPES,\n+  GET_COLUMNS,\n+  GET_FUNCTIONS,\n+  UNKNOWN,\n+}\n+\n+// Client-side reference to a task running\n+// asynchronously on the server.\n+struct TOperationHandle {\n+  1: required THandleIdentifier operationId\n+  2: required TOperationType operationType\n+\n+  // If hasResultSet = TRUE, then this operation\n+  // generates a result set that can be fetched.\n+  // Note that the result set may be empty.\n+  //\n+  // If hasResultSet = FALSE, then this operation\n+  // does not generate a result set, and calling\n+  // GetResultSetMetadata or FetchResults against\n+  // this OperationHandle will generate an error.\n+  3: required bool hasResultSet\n+\n+  // For operations that don't generate result sets,\n+  // modifiedRowCount is either:\n+  //\n+  // 1) The number of rows that were modified by\n+  //    the DML operation (e.g. number of rows inserted,\n+  //    number of rows deleted, etc).\n+  //\n+  // 2) 0 for operations that don't modify or add rows.\n+  //\n+  // 3) < 0 if the operation is capable of modifiying rows,\n+  //    but Hive is unable to determine how many rows were\n+  //    modified. For example, Hive's LOAD DATA command\n+  //    doesn't generate row count information because\n+  //    Hive doesn't inspect the data as it is loaded.\n+  //\n+  // modifiedRowCount is unset if the operation generates\n+  // a result set.\n+  4: optional double modifiedRowCount\n+}\n+\n+\n+// OpenSession()\n+//\n+// Open a session (connection) on the server against\n+// which operations may be executed.\n+struct TOpenSessionReq {\n+  // The version of the HiveServer2 protocol that the client is using.\n+  1: required TProtocolVersion client_protocol = TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V6\n+\n+  // Username and password for authentication.\n+  // Depending on the authentication scheme being used,\n+  // this information may instead be provided by a lower\n+  // protocol layer, in which case these fields may be\n+  // left unset.\n+  2: optional string username\n+  3: optional string password\n+\n+  // Configuration overlay which is applied when the session is\n+  // first created.\n+  4: optional map<string, string> configuration\n+}\n+\n+struct TOpenSessionResp {\n+  1: required TStatus status\n+\n+  // The protocol version that the server is using.\n+  2: required TProtocolVersion serverProtocolVersion = TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V6\n+\n+  // Session Handle\n+  3: optional TSessionHandle sessionHandle\n+\n+  // The configuration settings for this session.\n+  4: optional map<string, string> configuration\n+}\n+\n+\n+// CloseSession()\n+//\n+// Closes the specified session and frees any resources\n+// currently allocated to that session. Any open\n+// operations in that session will be canceled.\n+struct TCloseSessionReq {\n+  1: required TSessionHandle sessionHandle\n+}\n+\n+struct TCloseSessionResp {\n+  1: required TStatus status\n+}\n+\n+\n+\n+enum TGetInfoType {\n+  CLI_MAX_DRIVER_CONNECTIONS =           0,\n+  CLI_MAX_CONCURRENT_ACTIVITIES =        1,\n+  CLI_DATA_SOURCE_NAME =                 2,\n+  CLI_FETCH_DIRECTION =                  8,\n+  CLI_SERVER_NAME =                      13,\n+  CLI_SEARCH_PATTERN_ESCAPE =            14,\n+  CLI_DBMS_NAME =                        17,\n+  CLI_DBMS_VER =                         18,\n+  CLI_ACCESSIBLE_TABLES =                19,\n+  CLI_ACCESSIBLE_PROCEDURES =            20,\n+  CLI_CURSOR_COMMIT_BEHAVIOR =           23,\n+  CLI_DATA_SOURCE_READ_ONLY =            25,\n+  CLI_DEFAULT_TXN_ISOLATION =            26,\n+  CLI_IDENTIFIER_CASE =                  28,\n+  CLI_IDENTIFIER_QUOTE_CHAR =            29,\n+  CLI_MAX_COLUMN_NAME_LEN =              30,\n+  CLI_MAX_CURSOR_NAME_LEN =              31,\n+  CLI_MAX_SCHEMA_NAME_LEN =              32,\n+  CLI_MAX_CATALOG_NAME_LEN =             34,\n+  CLI_MAX_TABLE_NAME_LEN =               35,\n+  CLI_SCROLL_CONCURRENCY =               43,\n+  CLI_TXN_CAPABLE =                      46,\n+  CLI_USER_NAME =                        47,\n+  CLI_TXN_ISOLATION_OPTION =             72,\n+  CLI_INTEGRITY =                        73,\n+  CLI_GETDATA_EXTENSIONS =               81,\n+  CLI_NULL_COLLATION =                   85,\n+  CLI_ALTER_TABLE =                      86,\n+  CLI_ORDER_BY_COLUMNS_IN_SELECT =       90,\n+  CLI_SPECIAL_CHARACTERS =               94,\n+  CLI_MAX_COLUMNS_IN_GROUP_BY =          97,\n+  CLI_MAX_COLUMNS_IN_INDEX =             98,\n+  CLI_MAX_COLUMNS_IN_ORDER_BY =          99,\n+  CLI_MAX_COLUMNS_IN_SELECT =            100,\n+  CLI_MAX_COLUMNS_IN_TABLE =             101,\n+  CLI_MAX_INDEX_SIZE =                   102,\n+  CLI_MAX_ROW_SIZE =                     104,\n+  CLI_MAX_STATEMENT_LEN =                105,\n+  CLI_MAX_TABLES_IN_SELECT =             106,\n+  CLI_MAX_USER_NAME_LEN =                107,\n+  CLI_OJ_CAPABILITIES =                  115,\n+\n+  CLI_XOPEN_CLI_YEAR =                   10000,\n+  CLI_CURSOR_SENSITIVITY =               10001,\n+  CLI_DESCRIBE_PARAMETER =               10002,\n+  CLI_CATALOG_NAME =                     10003,\n+  CLI_COLLATION_SEQ =                    10004,\n+  CLI_MAX_IDENTIFIER_LEN =               10005,\n+}\n+\n+union TGetInfoValue {\n+  1: string stringValue\n+  2: i16 smallIntValue\n+  3: i32 integerBitmask\n+  4: i32 integerFlag\n+  5: i32 binaryValue\n+  6: i64 lenValue\n+}\n+\n+// GetInfo()\n+//\n+// This function is based on ODBC's CLIGetInfo() function.\n+// The function returns general information about the data source\n+// using the same keys as ODBC.\n+struct TGetInfoReq {\n+  // The sesssion to run this request against\n+  1: required TSessionHandle sessionHandle\n+\n+  2: required TGetInfoType infoType\n+}\n+\n+struct TGetInfoResp {\n+  1: required TStatus status\n+\n+  2: required TGetInfoValue infoValue\n+}\n+\n+\n+// ExecuteStatement()\n+//\n+// Execute a statement.\n+// The returned OperationHandle can be used to check on the\n+// status of the statement, and to fetch results once the\n+// statement has finished executing.\n+struct TExecuteStatementReq {\n+  // The session to execute the statement against\n+  1: required TSessionHandle sessionHandle\n+\n+  // The statement to be executed (DML, DDL, SET, etc)\n+  2: required string statement\n+\n+  // Configuration properties that are overlayed on top of the\n+  // the existing session configuration before this statement\n+  // is executed. These properties apply to this statement\n+  // only and will not affect the subsequent state of the Session.\n+  3: optional map<string, string> confOverlay\n+\n+  // Execute asynchronously when runAsync is true\n+  4: optional bool runAsync = false\n+}\n+\n+struct TExecuteStatementResp {\n+  1: required TStatus status\n+  2: optional TOperationHandle operationHandle\n+}\n+\n+// GetTypeInfo()\n+//\n+// Get information about types supported by the HiveServer instance.\n+// The information is returned as a result set which can be fetched\n+// using the OperationHandle provided in the response.\n+//\n+// Refer to the documentation for ODBC's CLIGetTypeInfo function for\n+// the format of the result set.\n+struct TGetTypeInfoReq {\n+  // The session to run this request against.\n+  1: required TSessionHandle sessionHandle\n+}\n+\n+struct TGetTypeInfoResp {\n+  1: required TStatus status\n+  2: optional TOperationHandle operationHandle\n+}\n+\n+\n+// GetCatalogs()\n+//\n+// Returns the list of catalogs (databases)\n+// Results are ordered by TABLE_CATALOG\n+//\n+// Resultset columns :\n+// col1\n+// name: TABLE_CAT\n+// type: STRING\n+// desc: Catalog name. NULL if not applicable.\n+//\n+struct TGetCatalogsReq {\n+  // Session to run this request against\n+  1: required TSessionHandle sessionHandle\n+}\n+\n+struct TGetCatalogsResp {\n+  1: required TStatus status\n+  2: optional TOperationHandle operationHandle\n+}\n+\n+\n+// GetSchemas()\n+//\n+// Retrieves the schema names available in this database.\n+// The results are ordered by TABLE_CATALOG and TABLE_SCHEM.\n+// col1\n+// name: TABLE_SCHEM\n+// type: STRING\n+// desc: schema name\n+// col2\n+// name: TABLE_CATALOG\n+// type: STRING\n+// desc: catalog name\n+struct TGetSchemasReq {\n+  // Session to run this request against\n+  1: required TSessionHandle sessionHandle\n+\n+  // Name of the catalog. Must not contain a search pattern.\n+  2: optional TIdentifier catalogName\n+\n+  // schema name or pattern\n+  3: optional TPatternOrIdentifier schemaName\n+}\n+\n+struct TGetSchemasResp {\n+  1: required TStatus status\n+  2: optional TOperationHandle operationHandle\n+}\n+\n+\n+// GetTables()\n+//\n+// Returns a list of tables with catalog, schema, and table\n+// type information. The information is returned as a result\n+// set which can be fetched using the OperationHandle\n+// provided in the response.\n+// Results are ordered by TABLE_TYPE, TABLE_CAT, TABLE_SCHEM, and TABLE_NAME\n+//\n+// Result Set Columns:\n+//\n+// col1\n+// name: TABLE_CAT\n+// type: STRING\n+// desc: Catalog name. NULL if not applicable.\n+//\n+// col2\n+// name: TABLE_SCHEM\n+// type: STRING\n+// desc: Schema name.\n+//\n+// col3\n+// name: TABLE_NAME\n+// type: STRING\n+// desc: Table name.\n+//\n+// col4\n+// name: TABLE_TYPE\n+// type: STRING\n+// desc: The table type, e.g. \"TABLE\", \"VIEW\", etc.\n+//\n+// col5\n+// name: REMARKS\n+// type: STRING\n+// desc: Comments about the table\n+//\n+struct TGetTablesReq {\n+  // Session to run this request against\n+  1: required TSessionHandle sessionHandle\n+\n+  // Name of the catalog or a search pattern.\n+  2: optional TPatternOrIdentifier catalogName\n+\n+  // Name of the schema or a search pattern.\n+  3: optional TPatternOrIdentifier schemaName\n+\n+  // Name of the table or a search pattern.\n+  4: optional TPatternOrIdentifier tableName\n+\n+  // List of table types to match\n+  // e.g. \"TABLE\", \"VIEW\", \"SYSTEM TABLE\", \"GLOBAL TEMPORARY\",\n+  // \"LOCAL TEMPORARY\", \"ALIAS\", \"SYNONYM\", etc.\n+  5: optional list<string> tableTypes\n+}\n+\n+struct TGetTablesResp {\n+  1: required TStatus status\n+  2: optional TOperationHandle operationHandle\n+}\n+\n+\n+// GetTableTypes()\n+//\n+// Returns the table types available in this database.\n+// The results are ordered by table type.\n+//\n+// col1\n+// name: TABLE_TYPE\n+// type: STRING\n+// desc: Table type name.\n+struct TGetTableTypesReq {\n+  // Session to run this request against\n+  1: required TSessionHandle sessionHandle\n+}\n+\n+struct TGetTableTypesResp {\n+  1: required TStatus status\n+  2: optional TOperationHandle operationHandle\n+}\n+\n+\n+// GetColumns()\n+//\n+// Returns a list of columns in the specified tables.\n+// The information is returned as a result set which can be fetched\n+// using the OperationHandle provided in the response.\n+// Results are ordered by TABLE_CAT, TABLE_SCHEM, TABLE_NAME,\n+// and ORDINAL_POSITION.\n+//\n+// Result Set Columns are the same as those for the ODBC CLIColumns\n+// function.\n+//\n+struct TGetColumnsReq {\n+  // Session to run this request against\n+  1: required TSessionHandle sessionHandle\n+\n+  // Name of the catalog. Must not contain a search pattern.\n+  2: optional TIdentifier catalogName\n+\n+  // Schema name or search pattern\n+  3: optional TPatternOrIdentifier schemaName\n+\n+  // Table name or search pattern\n+  4: optional TPatternOrIdentifier tableName\n+\n+  // Column name or search pattern\n+  5: optional TPatternOrIdentifier columnName\n+}\n+\n+struct TGetColumnsResp {\n+  1: required TStatus status\n+  2: optional TOperationHandle operationHandle\n+}\n+\n+\n+// GetFunctions()\n+//\n+// Returns a list of functions supported by the data source. The\n+// behavior of this function matches\n+// java.sql.DatabaseMetaData.getFunctions() both in terms of\n+// inputs and outputs.\n+//\n+// Result Set Columns:\n+//\n+// col1\n+// name: FUNCTION_CAT\n+// type: STRING\n+// desc: Function catalog (may be null)\n+//\n+// col2\n+// name: FUNCTION_SCHEM\n+// type: STRING\n+// desc: Function schema (may be null)\n+//\n+// col3\n+// name: FUNCTION_NAME\n+// type: STRING\n+// desc: Function name. This is the name used to invoke the function.\n+//\n+// col4\n+// name: REMARKS\n+// type: STRING\n+// desc: Explanatory comment on the function.\n+//\n+// col5\n+// name: FUNCTION_TYPE\n+// type: SMALLINT\n+// desc: Kind of function. One of:\n+//       * functionResultUnknown - Cannot determine if a return value or a table\n+//                                 will be returned.\n+//       * functionNoTable       - Does not a return a table.\n+//       * functionReturnsTable  - Returns a table.\n+//\n+// col6\n+// name: SPECIFIC_NAME\n+// type: STRING\n+// desc: The name which uniquely identifies this function within its schema.\n+//       In this case this is the fully qualified class name of the class\n+//       that implements this function.\n+//\n+struct TGetFunctionsReq {\n+  // Session to run this request against\n+  1: required TSessionHandle sessionHandle\n+\n+  // A catalog name; must match the catalog name as it is stored in the\n+  // database; \"\" retrieves those without a catalog; null means\n+  // that the catalog name should not be used to narrow the search.\n+  2: optional TIdentifier catalogName\n+\n+  // A schema name pattern; must match the schema name as it is stored\n+  // in the database; \"\" retrieves those without a schema; null means\n+  // that the schema name should not be used to narrow the search.\n+  3: optional TPatternOrIdentifier schemaName\n+\n+  // A function name pattern; must match the function name as it is stored\n+  // in the database.\n+  4: required TPatternOrIdentifier functionName\n+}\n+\n+struct TGetFunctionsResp {\n+  1: required TStatus status\n+  2: optional TOperationHandle operationHandle\n+}\n+\n+\n+// GetOperationStatus()\n+//\n+// Get the status of an operation running on the server.\n+struct TGetOperationStatusReq {\n+  // Session to run this request against\n+  1: required TOperationHandle operationHandle\n+}\n+\n+struct TGetOperationStatusResp {\n+  1: required TStatus status\n+  2: optional TOperationState operationState\n+\n+  // If operationState is ERROR_STATE, then the following fields may be set\n+  // sqlState as defined in the ISO/IEF CLI specification\n+  3: optional string sqlState\n+\n+  // Internal error code\n+  4: optional i32 errorCode\n+\n+  // Error message\n+  5: optional string errorMessage\n+}\n+\n+\n+// CancelOperation()\n+//\n+// Cancels processing on the specified operation handle and\n+// frees any resources which were allocated.\n+struct TCancelOperationReq {\n+  // Operation to cancel\n+  1: required TOperationHandle operationHandle\n+}\n+\n+struct TCancelOperationResp {\n+  1: required TStatus status\n+}\n+\n+\n+// CloseOperation()\n+//\n+// Given an operation in the FINISHED, CANCELED,\n+// or ERROR states, CloseOperation() will free\n+// all of the resources which were allocated on\n+// the server to service the operation.\n+struct TCloseOperationReq {\n+  1: required TOperationHandle operationHandle\n+}\n+\n+struct TCloseOperationResp {\n+  1: required TStatus status\n+}\n+\n+\n+// GetResultSetMetadata()\n+//\n+// Retrieves schema information for the specified operation\n+struct TGetResultSetMetadataReq {\n+  // Operation for which to fetch result set schema information\n+  1: required TOperationHandle operationHandle\n+}\n+\n+struct TGetResultSetMetadataResp {\n+  1: required TStatus status\n+  2: optional TTableSchema schema\n+}\n+\n+\n+enum TFetchOrientation {\n+  // Get the next rowset. The fetch offset is ignored.\n+  FETCH_NEXT,\n+\n+  // Get the previous rowset. The fetch offset is ignored.\n+  // NOT SUPPORTED\n+  FETCH_PRIOR,\n+\n+  // Return the rowset at the given fetch offset relative\n+  // to the curren rowset.\n+  // NOT SUPPORTED\n+  FETCH_RELATIVE,\n+\n+  // Return the rowset at the specified fetch offset.\n+  // NOT SUPPORTED\n+  FETCH_ABSOLUTE,\n+\n+  // Get the first rowset in the result set.\n+  FETCH_FIRST,\n+\n+  // Get the last rowset in the result set.\n+  // NOT SUPPORTED\n+  FETCH_LAST\n+}\n+\n+// FetchResults()\n+//\n+// Fetch rows from the server corresponding to\n+// a particular OperationHandle.\n+struct TFetchResultsReq {\n+  // Operation from which to fetch results.\n+  1: required TOperationHandle operationHandle\n+\n+  // The fetch orientation. For V1 this must be either\n+  // FETCH_NEXT or FETCH_FIRST. Defaults to FETCH_NEXT.\n+  2: required TFetchOrientation orientation = TFetchOrientation.FETCH_NEXT\n+\n+  // Max number of rows that should be returned in\n+  // the rowset.\n+  3: required i64 maxRows\n+}\n+\n+struct TFetchResultsResp {\n+  1: required TStatus status\n+\n+  // TRUE if there are more rows left to fetch from the server.\n+  2: optional bool hasMoreRows\n+\n+  // The rowset. This is optional so that we have the\n+  // option in the future of adding alternate formats for\n+  // representing result set data, e.g. delimited strings,\n+  // binary encoded, etc.\n+  3: optional TRowSet results\n+}\n+\n+// GetDelegationToken()\n+// Retrieve delegation token for the current user\n+struct  TGetDelegationTokenReq {\n+  // session handle\n+  1: required TSessionHandle sessionHandle\n+\n+  // userid for the proxy user\n+  2: required string owner\n+\n+  // designated renewer userid\n+  3: required string renewer\n+}\n+\n+struct TGetDelegationTokenResp {\n+  // status of the request\n+  1: required TStatus status\n+\n+  // delegation token string\n+  2: optional string delegationToken\n+}\n+\n+// CancelDelegationToken()\n+// Cancel the given delegation token\n+struct TCancelDelegationTokenReq {\n+  // session handle\n+  1: required TSessionHandle sessionHandle\n+\n+  // delegation token to cancel\n+  2: required string delegationToken\n+}\n+\n+struct TCancelDelegationTokenResp {\n+  // status of the request\n+  1: required TStatus status\n+}\n+\n+// RenewDelegationToken()\n+// Renew the given delegation token\n+struct TRenewDelegationTokenReq {\n+  // session handle\n+  1: required TSessionHandle sessionHandle\n+\n+  // delegation token to renew\n+  2: required string delegationToken\n+}\n+\n+struct TRenewDelegationTokenResp {\n+  // status of the request\n+  1: required TStatus status\n+}\n+\n+// GetLog()\n+// Not present in Hive 0.13, re-added for backwards compatibility.\n+//\n+// Fetch operation log from the server corresponding to\n+// a particular OperationHandle.\n+struct TGetLogReq {\n+  // Operation whose log is requested\n+  1: required TOperationHandle operationHandle\n+}\n+\n+struct TGetLogResp {\n+  1: required TStatus status\n+  2: required string log\n+}\n+\n+service TCLIService {\n+\n+  TOpenSessionResp OpenSession(1:TOpenSessionReq req);\n+\n+  TCloseSessionResp CloseSession(1:TCloseSessionReq req);\n+\n+  TGetInfoResp GetInfo(1:TGetInfoReq req);\n+\n+  TExecuteStatementResp ExecuteStatement(1:TExecuteStatementReq req);\n+\n+  TGetTypeInfoResp GetTypeInfo(1:TGetTypeInfoReq req);\n+\n+  TGetCatalogsResp GetCatalogs(1:TGetCatalogsReq req);\n+\n+  TGetSchemasResp GetSchemas(1:TGetSchemasReq req);\n+\n+  TGetTablesResp GetTables(1:TGetTablesReq req);\n+\n+  TGetTableTypesResp GetTableTypes(1:TGetTableTypesReq req);\n+\n+  TGetColumnsResp GetColumns(1:TGetColumnsReq req);\n+\n+  TGetFunctionsResp GetFunctions(1:TGetFunctionsReq req);\n+\n+  TGetOperationStatusResp GetOperationStatus(1:TGetOperationStatusReq req);\n+\n+  TCancelOperationResp CancelOperation(1:TCancelOperationReq req);\n+\n+  TCloseOperationResp CloseOperation(1:TCloseOperationReq req);\n+\n+  TGetResultSetMetadataResp GetResultSetMetadata(1:TGetResultSetMetadataReq req);\n+\n+  TFetchResultsResp FetchResults(1:TFetchResultsReq req);\n+\n+  TGetDelegationTokenResp GetDelegationToken(1:TGetDelegationTokenReq req);\n+\n+  TCancelDelegationTokenResp CancelDelegationToken(1:TCancelDelegationTokenReq req);\n+\n+  TRenewDelegationTokenResp RenewDelegationToken(1:TRenewDelegationTokenReq req);\n+\n+  // Not present in Hive 0.13, re-added for backwards compatibility.\n+  TGetLogResp GetLog(1:TGetLogReq req);\n+}\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/Types.thrift b/cpp/src/arrow/dbi/hiveserver2/thrift/Types.thrift\nnew file mode 100644\nindex 0000000000..4238f9c26b\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/Types.thrift\n@@ -0,0 +1,218 @@\n+// Copyright 2012 Cloudera Inc.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+namespace cpp impala\n+namespace java com.cloudera.impala.thrift\n+\n+typedef i64 TTimestamp\n+typedef i32 TPlanNodeId\n+typedef i32 TTupleId\n+typedef i32 TSlotId\n+typedef i32 TTableId\n+\n+// TODO: Consider moving unrelated enums to better locations.\n+\n+enum TPrimitiveType {\n+  INVALID_TYPE,\n+  NULL_TYPE,\n+  BOOLEAN,\n+  TINYINT,\n+  SMALLINT,\n+  INT,\n+  BIGINT,\n+  FLOAT,\n+  DOUBLE,\n+  DATE,\n+  DATETIME,\n+  TIMESTAMP,\n+  STRING,\n+  // Unsupported types\n+  BINARY,\n+  DECIMAL,\n+  // CHAR(n). Currently only supported in UDAs\n+  CHAR,\n+  VARCHAR\n+}\n+\n+enum TTypeNodeType {\n+  SCALAR,\n+  ARRAY,\n+  MAP,\n+  STRUCT\n+}\n+\n+struct TScalarType {\n+  1: required TPrimitiveType type\n+\n+  // Only set if type == CHAR or type == VARCHAR\n+  2: optional i32 len\n+\n+  // Only set for DECIMAL\n+  3: optional i32 precision\n+  4: optional i32 scale\n+}\n+\n+// Represents a field in a STRUCT type.\n+// TODO: Model column stats for struct fields.\n+struct TStructField {\n+  1: required string name\n+  2: optional string comment\n+}\n+\n+struct TTypeNode {\n+  1: required TTypeNodeType type\n+\n+  // only set for scalar types\n+  2: optional TScalarType scalar_type\n+\n+  // only used for structs; has struct_fields.size() corresponding child types\n+  3: optional list<TStructField> struct_fields\n+}\n+\n+// A flattened representation of a tree of column types obtained by depth-first\n+// traversal. Complex types such as map, array and struct have child types corresponding\n+// to the map key/value, array item type, and struct fields, respectively.\n+// For scalar types the list contains only a single node.\n+// Note: We cannot rename this to TType because it conflicts with Thrift's internal TType\n+// and the generated Python thrift files will not work.\n+struct TColumnType {\n+  1: list<TTypeNode> types\n+}\n+\n+enum TStmtType {\n+  QUERY,\n+  DDL, // Data definition, e.g. CREATE TABLE (includes read-only functions e.g. SHOW)\n+  DML, // Data modification e.g. INSERT\n+  EXPLAIN,\n+  LOAD, // Statement type for LOAD commands\n+  SET\n+}\n+\n+// Level of verboseness for \"explain\" output.\n+enum TExplainLevel {\n+  MINIMAL,\n+  STANDARD,\n+  EXTENDED,\n+  VERBOSE\n+}\n+\n+enum TRuntimeFilterMode {\n+  // No filters are computed in the FE or the BE.\n+  OFF,\n+\n+  // Only broadcast filters are computed in the BE, and are only published to the local\n+  // fragment.\n+  LOCAL,\n+\n+  // All fiters are computed in the BE, and are published globally.\n+  GLOBAL\n+}\n+\n+// A TNetworkAddress is the standard host, port representation of a\n+// network address. The hostname field must be resolvable to an IPv4\n+// address.\n+struct TNetworkAddress {\n+  1: required string hostname\n+  2: required i32 port\n+}\n+\n+// Wire format for UniqueId\n+struct TUniqueId {\n+  1: required i64 hi\n+  2: required i64 lo\n+}\n+\n+enum TFunctionCategory {\n+  SCALAR,\n+  AGGREGATE,\n+  ANALYTIC\n+}\n+\n+enum TFunctionBinaryType {\n+  // Impala builtin. We can either run this interpreted or via codegen\n+  // depending on the query option.\n+  BUILTIN,\n+\n+  // Java UDFs, loaded from *.jar\n+  JAVA,\n+\n+  // Native-interface, precompiled UDFs loaded from *.so\n+  NATIVE,\n+\n+  // Native-interface, precompiled to IR; loaded from *.ll\n+  IR,\n+}\n+\n+// Represents a fully qualified function name.\n+struct TFunctionName {\n+  // Name of the function's parent database. Not set if in global\n+  // namespace (e.g. builtins)\n+  1: optional string db_name\n+\n+  // Name of the function\n+  2: required string function_name\n+}\n+\n+struct TScalarFunction {\n+  1: required string symbol;\n+  2: optional string prepare_fn_symbol\n+  3: optional string close_fn_symbol\n+}\n+\n+struct TAggregateFunction {\n+  1: required TColumnType intermediate_type\n+  2: required string update_fn_symbol\n+  3: required string init_fn_symbol\n+  4: optional string serialize_fn_symbol\n+  5: optional string merge_fn_symbol\n+  6: optional string finalize_fn_symbol\n+  8: optional string get_value_fn_symbol\n+  9: optional string remove_fn_symbol\n+\n+  7: optional bool ignores_distinct\n+}\n+\n+// Represents a function in the Catalog.\n+struct TFunction {\n+  // Fully qualified function name.\n+  1: required TFunctionName name\n+\n+  // Type of the udf. e.g. hive, native, ir\n+  2: required TFunctionBinaryType binary_type\n+\n+  // The types of the arguments to the function\n+  3: required list<TColumnType> arg_types\n+\n+  // Return type for the function.\n+  4: required TColumnType ret_type\n+\n+  // If true, this function takes var args.\n+  5: required bool has_var_args\n+\n+  // Optional comment to attach to the function\n+  6: optional string comment\n+\n+  7: optional string signature\n+\n+  // HDFS path for the function binary. This binary must exist at the time the\n+  // function is created.\n+  8: optional string hdfs_location\n+\n+  // One of these should be set.\n+  9: optional TScalarFunction scalar_fn\n+  10: optional TAggregateFunction aggregate_fn\n+\n+  // True for builtins or user-defined functions persisted by the catalog\n+  11: required bool is_persistent\n+}\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/beeswax.thrift b/cpp/src/arrow/dbi/hiveserver2/thrift/beeswax.thrift\nnew file mode 100644\nindex 0000000000..a0ca5a7469\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/beeswax.thrift\n@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to Cloudera, Inc. under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  Cloudera, Inc. licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ * Interface for interacting with Beeswax Server\n+ */\n+\n+namespace java com.cloudera.beeswax.api\n+namespace py beeswaxd\n+namespace cpp beeswax\n+\n+include \"hive_metastore.thrift\"\n+\n+// A Query\n+struct Query {\n+  1: string query;\n+  // A list of HQL commands to execute before the query.\n+  // This is typically defining UDFs, setting settings, and loading resources.\n+  3: list<string> configuration;\n+\n+  // User and groups to \"act as\" for purposes of Hadoop.\n+  4: string hadoop_user;\n+}\n+\n+typedef string LogContextId\n+\n+enum QueryState {\n+  CREATED,\n+  INITIALIZED,\n+  COMPILED,\n+  RUNNING,\n+  FINISHED,\n+  EXCEPTION\n+}\n+\n+struct QueryHandle {\n+  1: string id;\n+  2: LogContextId log_context;\n+}\n+\n+struct QueryExplanation {\n+  1: string textual\n+}\n+\n+struct Results {\n+  // If set, data is valid.  Otherwise, results aren't ready yet.\n+  1: bool ready,\n+  // Columns for the results\n+  2: list<string> columns,\n+  // A set of results\n+  3: list<string> data,\n+  // The starting row of the results\n+  4: i64 start_row,\n+  // Whether there are more results to fetch\n+  5: bool has_more\n+}\n+\n+/**\n+ * Metadata information about the results.\n+ * Applicable only for SELECT.\n+ */\n+struct ResultsMetadata {\n+  /** The schema of the results */\n+  1: hive_metastore.Schema schema,\n+  /** The directory containing the results. Not applicable for partition table. */\n+  2: string table_dir,\n+  /** If the results are straight from an existing table, the table name. */\n+  3: string in_tablename,\n+  /** Field delimiter */\n+  4: string delim,\n+}\n+\n+exception BeeswaxException {\n+  1: string message,\n+  // Use get_log(log_context) to retrieve any log related to this exception\n+  2: LogContextId log_context,\n+  // (Optional) The QueryHandle that caused this exception\n+  3: QueryHandle handle,\n+  4: optional i32 errorCode = 0,\n+  5: optional string SQLState = \"     \"\n+}\n+\n+exception QueryNotFoundException {\n+} \n+\n+/** Represents a Hadoop-style configuration variable. */\n+struct ConfigVariable {\n+  1: string key,\n+  2: string value,\n+  3: string description\n+}\n+\n+service BeeswaxService {\n+  /**\n+   * Submit a query and return a handle (QueryHandle). The query runs asynchronously.\n+   */\n+  QueryHandle query(1:Query query) throws(1:BeeswaxException error),\n+\n+  /**\n+   * run a query synchronously and return a handle (QueryHandle).\n+   */\n+  QueryHandle executeAndWait(1:Query query, 2:LogContextId clientCtx) \n+                        throws(1:BeeswaxException error),\n+\n+  /**\n+   * Get the query plan for a query.\n+   */\n+  QueryExplanation explain(1:Query query)\n+                        throws(1:BeeswaxException error),\n+\n+  /**\n+   * Get the results of a query. This is non-blocking. Caller should check\n+   * Results.ready to determine if the results are in yet. The call requests\n+   * the batch size of fetch.\n+   */\n+  Results fetch(1:QueryHandle query_id, 2:bool start_over, 3:i32 fetch_size=-1) \n+              throws(1:QueryNotFoundException error, 2:BeeswaxException error2),\n+\n+  /**\n+   * Get the state of the query\n+   */\n+  QueryState get_state(1:QueryHandle handle) throws(1:QueryNotFoundException error),\n+\n+  /**\n+   * Get the result metadata\n+   */\n+  ResultsMetadata get_results_metadata(1:QueryHandle handle)\n+                                    throws(1:QueryNotFoundException error),\n+\n+  /**\n+   * Used to test connection to server.  A \"noop\" command.\n+   */\n+  string echo(1:string s)\n+\n+  /**\n+   * Returns a string representation of the configuration object being used.\n+   * Handy for debugging.\n+   */\n+  string dump_config()\n+\n+  /**\n+   * Get the log messages related to the given context.\n+   */\n+  string get_log(1:LogContextId context) throws(1:QueryNotFoundException error)\n+\n+  /*\n+   * Returns \"default\" configuration.\n+   */\n+  list<ConfigVariable> get_default_configuration(1:bool include_hadoop)\n+\n+  /*\n+   * closes the query with given handle\n+   */\n+  void close(1:QueryHandle handle) throws(1:QueryNotFoundException error, \n+                            2:BeeswaxException error2)\n+\n+  /*\n+   * clean the log context for given id \n+   */\n+  void clean(1:LogContextId log_context)\n+}\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/fb303.thrift b/cpp/src/arrow/dbi/hiveserver2/thrift/fb303.thrift\nnew file mode 100644\nindex 0000000000..66c8315274\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/fb303.thrift\n@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership. The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License. You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/**\n+ * fb303.thrift\n+ */\n+\n+namespace java com.facebook.fb303\n+namespace cpp facebook.fb303\n+namespace perl Facebook.FB303\n+\n+/**\n+ * Common status reporting mechanism across all services\n+ */\n+enum fb_status {\n+  DEAD = 0,\n+  STARTING = 1,\n+  ALIVE = 2,\n+  STOPPING = 3,\n+  STOPPED = 4,\n+  WARNING = 5,\n+}\n+\n+/**\n+ * Standard base service\n+ */\n+service FacebookService {\n+\n+  /**\n+   * Returns a descriptive name of the service\n+   */\n+  string getName(),\n+\n+  /**\n+   * Returns the version of the service\n+   */\n+  string getVersion(),\n+\n+  /**\n+   * Gets the status of this service\n+   */\n+  fb_status getStatus(),\n+\n+  /**\n+   * User friendly description of status, such as why the service is in\n+   * the dead or warning state, or what is being started or stopped.\n+   */\n+  string getStatusDetails(),\n+\n+  /**\n+   * Gets the counters for this service\n+   */\n+  map<string, i64> getCounters(),\n+\n+  /**\n+   * Gets the value of a single counter\n+   */\n+  i64 getCounter(1: string key),\n+\n+  /**\n+   * Sets an option\n+   */\n+  void setOption(1: string key, 2: string value),\n+\n+  /**\n+   * Gets an option\n+   */\n+  string getOption(1: string key),\n+\n+  /**\n+   * Gets all options\n+   */\n+  map<string, string> getOptions(),\n+\n+  /**\n+   * Returns a CPU profile over the given time interval (client and server\n+   * must agree on the profile format).\n+   */\n+  string getCpuProfile(1: i32 profileDurationInSec),\n+\n+  /**\n+   * Returns the unix time that the server has been running since\n+   */\n+  i64 aliveSince(),\n+\n+  /**\n+   * Tell the server to reload its configuration, reopen log files, etc\n+   */\n+  oneway void reinitialize(),\n+\n+  /**\n+   * Suggest a shutdown to the server\n+   */\n+  oneway void shutdown(),\n+\n+}\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/generate_error_codes.py b/cpp/src/arrow/dbi/hiveserver2/thrift/generate_error_codes.py\nnew file mode 100644\nindex 0000000000..3790057d25\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/generate_error_codes.py\n@@ -0,0 +1,293 @@\n+#!/usr/bin/env python\n+# Copyright 2015 Cloudera Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import sys\n+import os\n+\n+\n+# For readability purposes we define the error codes and messages at the top of the\n+# file. New codes and messages must be added here. Old error messages MUST NEVER BE\n+# DELETED, but can be renamed. The tuple layout for a new entry is: error code enum name,\n+# numeric error code, format string of the message.\n+#\n+# TODO Add support for SQL Error Codes\n+#      https://msdn.microsoft.com/en-us/library/ms714687%28v=vs.85%29.aspx\n+error_codes = (\n+  (\"OK\", 0, \"\"),\n+\n+  (\"UNUSED\", 1, \"<UNUSED>\"),\n+\n+  (\"GENERAL\", 2, \"$0\"),\n+\n+  (\"CANCELLED\", 3, \"$0\"),\n+\n+  (\"ANALYSIS_ERROR\", 4, \"$0\"),\n+\n+  (\"NOT_IMPLEMENTED_ERROR\", 5, \"$0\"),\n+\n+  (\"RUNTIME_ERROR\", 6, \"$0\"),\n+\n+  (\"MEM_LIMIT_EXCEEDED\", 7, \"$0\"),\n+\n+  (\"INTERNAL_ERROR\", 8, \"$0\"),\n+\n+  (\"RECOVERABLE_ERROR\", 9, \"$0\"),\n+\n+  (\"PARQUET_MULTIPLE_BLOCKS\", 10,\n+   \"Parquet files should not be split into multiple hdfs-blocks. file=$0\"),\n+\n+  (\"PARQUET_COLUMN_METADATA_INVALID\", 11,\n+   \"Column metadata states there are $0 values, but read $1 values from column $2. \"\n+   \"file=$3\"),\n+\n+  (\"PARQUET_HEADER_PAGE_SIZE_EXCEEDED\", 12, \"(unused)\"),\n+\n+  (\"PARQUET_HEADER_EOF\", 13,\n+    \"ParquetScanner: reached EOF while deserializing data page header. file=$0\"),\n+\n+  (\"PARQUET_GROUP_ROW_COUNT_ERROR\", 14,\n+    \"Metadata states that in group $0($1) there are $2 rows, but $3 rows were read.\"),\n+\n+  (\"PARQUET_GROUP_ROW_COUNT_OVERFLOW\", 15, \"(unused)\"),\n+\n+  (\"PARQUET_MISSING_PRECISION\", 16,\n+   \"File '$0' column '$1' does not have the decimal precision set.\"),\n+\n+  (\"PARQUET_WRONG_PRECISION\", 17,\n+    \"File '$0' column '$1' has a precision that does not match the table metadata \"\n+    \" precision. File metadata precision: $2, table metadata precision: $3.\"),\n+\n+  (\"PARQUET_BAD_CONVERTED_TYPE\", 18,\n+   \"File '$0' column '$1' does not have converted type set to DECIMAL\"),\n+\n+  (\"PARQUET_INCOMPATIBLE_DECIMAL\", 19,\n+   \"File '$0' column '$1' contains decimal data but the table metadata has type $2\"),\n+\n+  (\"SEQUENCE_SCANNER_PARSE_ERROR\", 20,\n+   \"Problem parsing file $0 at $1$2\"),\n+\n+  (\"SNAPPY_DECOMPRESS_INVALID_BLOCK_SIZE\", 21,\n+   \"Decompressor: block size is too big.  Data is likely corrupt. Size: $0\"),\n+\n+  (\"SNAPPY_DECOMPRESS_INVALID_COMPRESSED_LENGTH\", 22,\n+   \"Decompressor: invalid compressed length.  Data is likely corrupt.\"),\n+\n+  (\"SNAPPY_DECOMPRESS_UNCOMPRESSED_LENGTH_FAILED\", 23,\n+   \"Snappy: GetUncompressedLength failed\"),\n+\n+  (\"SNAPPY_DECOMPRESS_RAW_UNCOMPRESS_FAILED\", 24,\n+   \"SnappyBlock: RawUncompress failed\"),\n+\n+  (\"SNAPPY_DECOMPRESS_DECOMPRESS_SIZE_INCORRECT\", 25,\n+   \"Snappy: Decompressed size is not correct.\"),\n+\n+  (\"HDFS_SCAN_NODE_UNKNOWN_DISK\", 26, \"Unknown disk id.  \"\n+   \"This will negatively affect performance. \"\n+   \"Check your hdfs settings to enable block location metadata.\"),\n+\n+  (\"FRAGMENT_EXECUTOR\", 27, \"Reserved resource size ($0) is larger than \"\n+    \"query mem limit ($1), and will be restricted to $1. Configure the reservation \"\n+    \"size by setting RM_INITIAL_MEM.\"),\n+\n+  (\"PARTITIONED_HASH_JOIN_MAX_PARTITION_DEPTH\", 28,\n+   \"Cannot perform join at hash join node with id $0.\"\n+   \" The input data was partitioned the maximum number of $1 times.\"\n+   \" This could mean there is significant skew in the data or the memory limit is\"\n+   \" set too low.\"),\n+\n+  (\"PARTITIONED_AGG_MAX_PARTITION_DEPTH\", 29,\n+   \"Cannot perform aggregation at hash aggregation node with id $0.\"\n+   \" The input data was partitioned the maximum number of $1 times.\"\n+   \" This could mean there is significant skew in the data or the memory limit is\"\n+   \" set too low.\"),\n+\n+  (\"MISSING_BUILTIN\", 30, \"Builtin '$0' with symbol '$1' does not exist. \"\n+   \"Verify that all your impalads are the same version.\"),\n+\n+  (\"RPC_GENERAL_ERROR\", 31, \"RPC Error: $0\"),\n+  (\"RPC_TIMEOUT\", 32, \"RPC timed out\"),\n+\n+  (\"UDF_VERIFY_FAILED\", 33,\n+   \"Failed to verify function $0 from LLVM module $1, see log for more details.\"),\n+\n+  (\"PARQUET_CORRUPT_VALUE\", 34, \"File $0 corrupt. RLE level data bytes = $1\"),\n+\n+  (\"AVRO_DECIMAL_RESOLUTION_ERROR\", 35, \"Column '$0' has conflicting Avro decimal types. \"\n+   \"Table schema $1: $2, file schema $1: $3\"),\n+\n+  (\"AVRO_DECIMAL_METADATA_MISMATCH\", 36, \"Column '$0' has conflicting Avro decimal types. \"\n+   \"Declared $1: $2, $1 in table's Avro schema: $3\"),\n+\n+  (\"AVRO_SCHEMA_RESOLUTION_ERROR\", 37, \"Unresolvable types for column '$0': \"\n+   \"table type: $1, file type: $2\"),\n+\n+  (\"AVRO_SCHEMA_METADATA_MISMATCH\", 38, \"Unresolvable types for column '$0': \"\n+   \"declared column type: $1, table's Avro schema type: $2\"),\n+\n+  (\"AVRO_UNSUPPORTED_DEFAULT_VALUE\", 39, \"Field $0 is missing from file and default \"\n+   \"values of type $1 are not yet supported.\"),\n+\n+  (\"AVRO_MISSING_FIELD\", 40, \"Inconsistent table metadata. Mismatch between column \"\n+   \"definition and Avro schema: cannot read field $0 because there are only $1 fields.\"),\n+\n+  (\"AVRO_MISSING_DEFAULT\", 41,\n+   \"Field $0 is missing from file and does not have a default value.\"),\n+\n+  (\"AVRO_NULLABILITY_MISMATCH\", 42,\n+   \"Field $0 is nullable in the file schema but not the table schema.\"),\n+\n+  (\"AVRO_NOT_A_RECORD\", 43,\n+   \"Inconsistent table metadata. Field $0 is not a record in the Avro schema.\"),\n+\n+  (\"PARQUET_DEF_LEVEL_ERROR\", 44, \"Could not read definition level, even though metadata\"\n+   \" states there are $0 values remaining in data page. file=$1\"),\n+\n+  (\"PARQUET_NUM_COL_VALS_ERROR\", 45, \"Mismatched number of values in column index $0 \"\n+   \"($1 vs. $2). file=$3\"),\n+\n+  (\"PARQUET_DICT_DECODE_FAILURE\", 46, \"Failed to decode dictionary-encoded value. \"\n+   \"file=$0\"),\n+\n+  (\"SSL_PASSWORD_CMD_FAILED\", 47,\n+   \"SSL private-key password command ('$0') failed with error: $1\"),\n+\n+  (\"SSL_CERTIFICATE_PATH_BLANK\", 48, \"The SSL certificate path is blank\"),\n+  (\"SSL_PRIVATE_KEY_PATH_BLANK\", 49, \"The SSL private key path is blank\"),\n+\n+  (\"SSL_CERTIFICATE_NOT_FOUND\", 50, \"The SSL certificate file does not exist at path $0\"),\n+  (\"SSL_PRIVATE_KEY_NOT_FOUND\", 51, \"The SSL private key file does not exist at path $0\"),\n+\n+  (\"SSL_SOCKET_CREATION_FAILED\", 52, \"SSL socket creation failed: $0\"),\n+\n+  (\"MEM_ALLOC_FAILED\", 53, \"Memory allocation of $0 bytes failed\"),\n+\n+  (\"PARQUET_REP_LEVEL_ERROR\", 54, \"Could not read repetition level, even though metadata\"\n+   \" states there are $0 values remaining in data page. file=$1\"),\n+\n+  (\"PARQUET_UNRECOGNIZED_SCHEMA\", 55, \"File '$0' has an incompatible Parquet schema for \"\n+   \"column '$1'. Column type: $2, Parquet schema:\\\\n$3\"),\n+\n+  (\"COLLECTION_ALLOC_FAILED\", 56, \"Failed to allocate buffer for collection '$0'.\"),\n+\n+  (\"TMP_DEVICE_BLACKLISTED\", 57,\n+    \"Temporary device for directory $0 is blacklisted from a previous error and cannot \"\n+    \"be used.\"),\n+\n+  (\"TMP_FILE_BLACKLISTED\", 58,\n+    \"Temporary file $0 is blacklisted from a previous error and cannot be expanded.\"),\n+\n+  (\"RPC_CLIENT_CONNECT_FAILURE\", 59,\n+    \"RPC client failed to connect: $0\"),\n+\n+  (\"STALE_METADATA_FILE_TOO_SHORT\", 60, \"Metadata for file '$0' appears stale. \"\n+   \"Try running \\\\\\\"refresh $1\\\\\\\" to reload the file metadata.\"),\n+\n+  (\"PARQUET_BAD_VERSION_NUMBER\", 61, \"File '$0' has an invalid version number: $1\\\\n\"\n+   \"This could be due to stale metadata. Try running \\\\\\\"refresh $2\\\\\\\".\"),\n+\n+  (\"SCANNER_INCOMPLETE_READ\", 62, \"Tried to read $0 bytes but could only read $1 bytes. \"\n+   \"This may indicate data file corruption. (file $2, byte offset: $3)\"),\n+\n+  (\"SCANNER_INVALID_READ\", 63, \"Invalid read of $0 bytes. This may indicate data file \"\n+   \"corruption. (file $1, byte offset: $2)\"),\n+\n+  (\"AVRO_BAD_VERSION_HEADER\", 64, \"File '$0' has an invalid version header: $1\\\\n\"\n+   \"Make sure the file is an Avro data file.\"),\n+\n+  (\"UDF_MEM_LIMIT_EXCEEDED\", 65, \"$0's allocations exceeded memory limits.\"),\n+\n+  (\"BTS_BLOCK_OVERFLOW\", 66, \"Cannot process row that is bigger than the IO size \"\n+   \"(row_size=$0, null_indicators_size=$1). To run this query, increase the IO size \"\n+   \"(--read_size option).\"),\n+\n+  (\"COMPRESSED_FILE_MULTIPLE_BLOCKS\", 67,\n+   \"For better performance, snappy-, gzip-, and bzip-compressed files \"\n+   \"should not be split into multiple HDFS blocks. file=$0 offset $1\"),\n+\n+  (\"COMPRESSED_FILE_BLOCK_CORRUPTED\", 68,\n+   \"$0 Data error, likely data corrupted in this block.\"),\n+\n+  (\"COMPRESSED_FILE_DECOMPRESSOR_ERROR\", 69, \"$0 Decompressor error at $1, code=$2\"),\n+\n+  (\"COMPRESSED_FILE_DECOMPRESSOR_NO_PROGRESS\", 70,\n+   \"Decompression failed to make progress, but end of input is not reached. \"\n+   \"File appears corrupted. file=$0\"),\n+\n+  (\"COMPRESSED_FILE_TRUNCATED\", 71,\n+   \"Unexpected end of compressed file. File may be truncated. file=$0\")\n+)\n+\n+# Verifies the uniqueness of the error constants and numeric error codes.\n+# Numeric codes must start from 0, be in order and have no gaps\n+def check_duplicates(codes):\n+  constants = {}\n+  next_num_code = 0\n+  for row in codes:\n+    if row[0] in constants:\n+      print(\"Constant %s already used, please check definition of '%s'!\" % \\\n+            (row[0], constants[row[0]]))\n+      exit(1)\n+    if row[1] != next_num_code:\n+      print(\"Numeric error codes must start from 0, be in order, and not have any gaps: \"\n+            \"got %d, expected %d\" % (row[1], next_num_code))\n+      exit(1)\n+    next_num_code += 1\n+    constants[row[0]] = row[2]\n+\n+preamble = \"\"\"\n+// Copyright 2015 Cloudera Inc.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+//\n+//\n+// THIS FILE IS AUTO GENERATED BY generated_error_codes.py DO NOT MODIFY\n+// IT BY HAND.\n+//\n+\n+namespace cpp impala\n+namespace java com.cloudera.impala.thrift\n+\n+\"\"\"\n+# The script will always generate the file, CMake will take care of running it only if\n+# necessary.\n+target_file = os.path.join(sys.argv[1], \"ErrorCodes.thrift\")\n+\n+# Check uniqueness of error constants and numeric codes\n+check_duplicates(error_codes)\n+\n+fid = open(target_file, \"w+\")\n+try:\n+  fid.write(preamble)\n+  fid.write(\"\"\"\\nenum TErrorCode {\\n\"\"\")\n+  fid.write(\",\\n\".join(map(lambda x: \"  %s = %d\" % (x[0], x[1]), error_codes)))\n+  fid.write(\"\\n}\")\n+  fid.write(\"\\n\")\n+  fid.write(\"const list<string> TErrorMessage = [\\n\")\n+  fid.write(\",\\n\".join(map(lambda x: \"  // %s\\n  \\\"%s\\\"\" %(x[0], x[2]), error_codes)))\n+  fid.write(\"\\n]\")\n+finally:\n+  fid.close()\n+\n+print(\"%s created.\" % target_file)\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/thrift/hive_metastore.thrift b/cpp/src/arrow/dbi/hiveserver2/thrift/hive_metastore.thrift\nnew file mode 100644\nindex 0000000000..f7c2693f48\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/thrift/hive_metastore.thrift\n@@ -0,0 +1,1214 @@\n+#!/usr/local/bin/thrift -java\n+\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+#\n+# Thrift Service that the MetaStore is built on\n+#\n+\n+include \"fb303.thrift\"\n+\n+namespace java org.apache.hadoop.hive.metastore.api\n+namespace php metastore\n+namespace cpp Apache.Hadoop.Hive\n+\n+const string DDL_TIME = \"transient_lastDdlTime\"\n+\n+struct Version {\n+  1: string version,\n+  2: string comments\n+}\n+\n+struct FieldSchema {\n+  1: string name, // name of the field\n+  2: string type, // type of the field. primitive types defined above, specify list<TYPE_NAME>, map<TYPE_NAME, TYPE_NAME> for lists & maps\n+  3: string comment\n+}\n+\n+struct Type {\n+  1: string          name,             // one of the types in PrimitiveTypes or CollectionTypes or User defined types\n+  2: optional string type1,            // object type if the name is 'list' (LIST_TYPE), key type if the name is 'map' (MAP_TYPE)\n+  3: optional string type2,            // val type if the name is 'map' (MAP_TYPE)\n+  4: optional list<FieldSchema> fields // if the name is one of the user defined types\n+}\n+\n+enum HiveObjectType {\n+  GLOBAL = 1,\n+  DATABASE = 2,\n+  TABLE = 3,\n+  PARTITION = 4,\n+  COLUMN = 5,\n+}\n+\n+enum PrincipalType {\n+  USER = 1,\n+  ROLE = 2,\n+  GROUP = 3,\n+}\n+\n+const string HIVE_FILTER_FIELD_OWNER = \"hive_filter_field_owner__\"\n+const string HIVE_FILTER_FIELD_PARAMS = \"hive_filter_field_params__\"\n+const string HIVE_FILTER_FIELD_LAST_ACCESS = \"hive_filter_field_last_access__\"\n+\n+enum PartitionEventType {\n+  LOAD_DONE = 1,\n+}\n+\n+// Enums for transaction and lock management \n+enum TxnState {\n+    COMMITTED = 1,\n+    ABORTED = 2,\n+    OPEN = 3,\n+}\n+\n+enum LockLevel {\n+    DB = 1,\n+    TABLE = 2,\n+    PARTITION = 3,\n+}\n+\n+enum LockState {\n+    ACQUIRED = 1,       // requester has the lock\n+    WAITING = 2,        // requester is waiting for the lock and should call checklock at a later point to see if the lock has been obtained.\n+    ABORT = 3,          // the lock has been aborted, most likely due to timeout\n+    NOT_ACQUIRED = 4,   // returned only with lockNoWait, indicates the lock was not available and was not acquired\n+}\n+\n+enum LockType {\n+    SHARED_READ = 1,\n+    SHARED_WRITE = 2,\n+    EXCLUSIVE = 3,\n+}\n+\n+enum CompactionType {\n+    MINOR = 1,\n+    MAJOR = 2,\n+}\n+\n+enum GrantRevokeType {\n+    GRANT = 1,\n+    REVOKE = 2,\n+}\n+\n+struct HiveObjectRef{\n+  1: HiveObjectType objectType,\n+  2: string dbName,\n+  3: string objectName,\n+  4: list<string> partValues,\n+  5: string columnName,\n+}\n+\n+struct PrivilegeGrantInfo {\n+  1: string privilege,\n+  2: i32 createTime,\n+  3: string grantor,\n+  4: PrincipalType grantorType,\n+  5: bool grantOption,\n+}\n+\n+struct HiveObjectPrivilege {\n+  1: HiveObjectRef  hiveObject,\n+  2: string principalName,\n+  3: PrincipalType principalType,\n+  4: PrivilegeGrantInfo grantInfo,\n+}\n+\n+struct PrivilegeBag {\n+  1: list<HiveObjectPrivilege> privileges,\n+}\n+\n+struct PrincipalPrivilegeSet {\n+  1: map<string, list<PrivilegeGrantInfo>> userPrivileges, // user name -> privilege grant info\n+  2: map<string, list<PrivilegeGrantInfo>> groupPrivileges, // group name -> privilege grant info\n+  3: map<string, list<PrivilegeGrantInfo>> rolePrivileges, //role name -> privilege grant info\n+}\n+\n+struct GrantRevokePrivilegeRequest {\n+  1: GrantRevokeType requestType;\n+  2: PrivilegeBag privileges;\n+  3: optional bool revokeGrantOption;  // Only for revoke request\n+}\n+\n+struct GrantRevokePrivilegeResponse {\n+  1: optional bool success;\n+}\n+\n+struct Role {\n+  1: string roleName,\n+  2: i32 createTime,\n+  3: string ownerName,\n+}\n+\n+// Representation of a grant for a principal to a role\n+struct RolePrincipalGrant {\n+  1: string roleName,\n+  2: string principalName,\n+  3: PrincipalType principalType,\n+  4: bool grantOption,\n+  5: i32 grantTime,\n+  6: string grantorName,\n+  7: PrincipalType grantorPrincipalType\n+}\n+\n+struct GetRoleGrantsForPrincipalRequest {\n+  1: required string principal_name,\n+  2: required PrincipalType principal_type\n+}\n+\n+struct GetRoleGrantsForPrincipalResponse {\n+  1: required list<RolePrincipalGrant> principalGrants;\n+}\n+\n+struct GetPrincipalsInRoleRequest {\n+  1: required string roleName;\n+}\n+\n+struct GetPrincipalsInRoleResponse {\n+  1: required list<RolePrincipalGrant> principalGrants;\n+}\n+\n+struct GrantRevokeRoleRequest {\n+  1: GrantRevokeType requestType;\n+  2: string roleName;\n+  3: string principalName;\n+  4: PrincipalType principalType;\n+  5: optional string grantor;            // Needed for grant\n+  6: optional PrincipalType grantorType; // Needed for grant\n+  7: optional bool grantOption;\n+}\n+\n+struct GrantRevokeRoleResponse {\n+  1: optional bool success;\n+}\n+\n+// namespace for tables\n+struct Database {\n+  1: string name,\n+  2: string description,\n+  3: string locationUri,\n+  4: map<string, string> parameters, // properties associated with the database\n+  5: optional PrincipalPrivilegeSet privileges,\n+  6: optional string ownerName,\n+  7: optional PrincipalType ownerType\n+}\n+\n+// This object holds the information needed by SerDes\n+struct SerDeInfo {\n+  1: string name,                   // name of the serde, table name by default\n+  2: string serializationLib,       // usually the class that implements the extractor & loader\n+  3: map<string, string> parameters // initialization parameters\n+}\n+\n+// sort order of a column (column name along with asc(1)/desc(0))\n+struct Order {\n+  1: string col,  // sort column name\n+  2: i32    order // asc(1) or desc(0)\n+}\n+\n+// this object holds all the information about skewed table\n+struct SkewedInfo {\n+  1: list<string> skewedColNames, // skewed column names\n+  2: list<list<string>> skewedColValues, //skewed values\n+  3: map<list<string>, string> skewedColValueLocationMaps, //skewed value to location mappings\n+}\n+\n+// this object holds all the information about physical storage of the data belonging to a table\n+struct StorageDescriptor {\n+  1: list<FieldSchema> cols,  // required (refer to types defined above)\n+  2: string location,         // defaults to <warehouse loc>/<db loc>/tablename\n+  3: string inputFormat,      // SequenceFileInputFormat (binary) or TextInputFormat`  or custom format\n+  4: string outputFormat,     // SequenceFileOutputFormat (binary) or IgnoreKeyTextOutputFormat or custom format\n+  5: bool   compressed,       // compressed or not\n+  6: i32    numBuckets,       // this must be specified if there are any dimension columns\n+  7: SerDeInfo    serdeInfo,  // serialization and deserialization information\n+  8: list<string> bucketCols, // reducer grouping columns and clustering columns and bucketing columns`\n+  9: list<Order>  sortCols,   // sort order of the data in each bucket\n+  10: map<string, string> parameters, // any user supplied key value hash\n+  11: optional SkewedInfo skewedInfo, // skewed information\n+  12: optional bool   storedAsSubDirectories       // stored as subdirectories or not\n+}\n+\n+// table information\n+struct Table {\n+  1: string tableName,                // name of the table\n+  2: string dbName,                   // database name ('default')\n+  3: string owner,                    // owner of this table\n+  4: i32    createTime,               // creation time of the table\n+  5: i32    lastAccessTime,           // last access time (usually this will be filled from HDFS and shouldn't be relied on)\n+  6: i32    retention,                // retention time\n+  7: StorageDescriptor sd,            // storage descriptor of the table\n+  8: list<FieldSchema> partitionKeys, // partition keys of the table. only primitive types are supported\n+  9: map<string, string> parameters,   // to store comments or any other user level parameters\n+  10: string viewOriginalText,         // original view text, null for non-view\n+  11: string viewExpandedText,         // expanded view text, null for non-view\n+  12: string tableType,                 // table type enum, e.g. EXTERNAL_TABLE\n+  13: optional PrincipalPrivilegeSet privileges,\n+  14: optional bool temporary=false\n+}\n+\n+struct Partition {\n+  1: list<string> values // string value is converted to appropriate partition key type\n+  2: string       dbName,\n+  3: string       tableName,\n+  4: i32          createTime,\n+  5: i32          lastAccessTime,\n+  6: StorageDescriptor   sd,\n+  7: map<string, string> parameters,\n+  8: optional PrincipalPrivilegeSet privileges\n+}\n+\n+struct PartitionWithoutSD {\n+  1: list<string> values // string value is converted to appropriate partition key type\n+  2: i32          createTime,\n+  3: i32          lastAccessTime,\n+  4: string       relativePath,\n+  5: map<string, string> parameters,\n+  6: optional PrincipalPrivilegeSet privileges\n+}\n+\n+struct PartitionSpecWithSharedSD {\n+  1: list<PartitionWithoutSD> partitions,\n+  2: StorageDescriptor sd,\n+}\n+\n+struct PartitionListComposingSpec {\n+  1: list<Partition> partitions\n+}\n+\n+struct PartitionSpec {\n+  1: string dbName,\n+  2: string tableName,\n+  3: string rootPath,\n+  4: optional PartitionSpecWithSharedSD sharedSDPartitionSpec,\n+  5: optional PartitionListComposingSpec partitionList\n+}\n+\n+struct Index {\n+  1: string       indexName, // unique with in the whole database namespace\n+  2: string       indexHandlerClass, // reserved\n+  3: string       dbName,\n+  4: string       origTableName,\n+  5: i32          createTime,\n+  6: i32          lastAccessTime,\n+  7: string       indexTableName,\n+  8: StorageDescriptor   sd,\n+  9: map<string, string> parameters,\n+  10: bool         deferredRebuild\n+}\n+\n+// column statistics\n+struct BooleanColumnStatsData {\n+1: required i64 numTrues,\n+2: required i64 numFalses,\n+3: required i64 numNulls\n+}\n+\n+struct DoubleColumnStatsData {\n+1: optional double lowValue,\n+2: optional double highValue,\n+3: required i64 numNulls,\n+4: required i64 numDVs\n+}\n+\n+struct LongColumnStatsData {\n+1: optional i64 lowValue,\n+2: optional i64 highValue,\n+3: required i64 numNulls,\n+4: required i64 numDVs\n+}\n+\n+struct StringColumnStatsData {\n+1: required i64 maxColLen,\n+2: required double avgColLen,\n+3: required i64 numNulls,\n+4: required i64 numDVs\n+}\n+\n+struct BinaryColumnStatsData {\n+1: required i64 maxColLen,\n+2: required double avgColLen,\n+3: required i64 numNulls\n+}\n+\n+\n+struct Decimal {\n+1: required binary unscaled,\n+3: required i16 scale\n+}\n+\n+struct DecimalColumnStatsData {\n+1: optional Decimal lowValue,\n+2: optional Decimal highValue,\n+3: required i64 numNulls,\n+4: required i64 numDVs\n+}\n+\n+union ColumnStatisticsData {\n+1: BooleanColumnStatsData booleanStats,\n+2: LongColumnStatsData longStats,\n+3: DoubleColumnStatsData doubleStats,\n+4: StringColumnStatsData stringStats,\n+5: BinaryColumnStatsData binaryStats,\n+6: DecimalColumnStatsData decimalStats\n+}\n+\n+struct ColumnStatisticsObj {\n+1: required string colName,\n+2: required string colType,\n+3: required ColumnStatisticsData statsData\n+}\n+\n+struct ColumnStatisticsDesc {\n+1: required bool isTblLevel,\n+2: required string dbName,\n+3: required string tableName,\n+4: optional string partName,\n+5: optional i64 lastAnalyzed\n+}\n+\n+struct ColumnStatistics {\n+1: required ColumnStatisticsDesc statsDesc,\n+2: required list<ColumnStatisticsObj> statsObj;\n+}\n+\n+struct AggrStats {\n+1: required list<ColumnStatisticsObj> colStats,\n+2: required i64 partsFound // number of partitions for which stats were found\n+}\n+\n+struct SetPartitionsStatsRequest {\n+1: required list<ColumnStatistics> colStats\n+}\n+\n+// schema of the table/query results etc.\n+struct Schema {\n+ // column names, types, comments\n+ 1: list<FieldSchema> fieldSchemas,  // delimiters etc\n+ 2: map<string, string> properties\n+}\n+\n+// Key-value store to be used with selected\n+// Metastore APIs (create, alter methods).\n+// The client can pass environment properties / configs that can be\n+// accessed in hooks.\n+struct EnvironmentContext {\n+  1: map<string, string> properties\n+}\n+\n+// Return type for get_partitions_by_expr\n+struct PartitionsByExprResult {\n+  1: required list<Partition> partitions,\n+  // Whether the results has any (currently, all) partitions which may or may not match\n+  2: required bool hasUnknownPartitions\n+}\n+\n+struct PartitionsByExprRequest {\n+  1: required string dbName,\n+  2: required string tblName,\n+  3: required binary expr,\n+  4: optional string defaultPartitionName,\n+  5: optional i16 maxParts=-1\n+}\n+\n+struct TableStatsResult {\n+  1: required list<ColumnStatisticsObj> tableStats\n+}\n+\n+struct PartitionsStatsResult {\n+  1: required map<string, list<ColumnStatisticsObj>> partStats\n+}\n+\n+struct TableStatsRequest {\n+ 1: required string dbName,\n+ 2: required string tblName,\n+ 3: required list<string> colNames\n+}\n+\n+struct PartitionsStatsRequest {\n+ 1: required string dbName,\n+ 2: required string tblName,\n+ 3: required list<string> colNames,\n+ 4: required list<string> partNames\n+}\n+\n+// Return type for add_partitions_req\n+struct AddPartitionsResult {\n+  1: optional list<Partition> partitions,\n+}\n+\n+// Request type for add_partitions_req\n+struct AddPartitionsRequest {\n+  1: required string dbName,\n+  2: required string tblName,\n+  3: required list<Partition> parts,\n+  4: required bool ifNotExists,\n+  5: optional bool needResult=true\n+}\n+\n+// Return type for drop_partitions_req\n+struct DropPartitionsResult {\n+  1: optional list<Partition> partitions,\n+}\n+\n+struct DropPartitionsExpr {\n+  1: required binary expr;\n+  2: optional i32 partArchiveLevel;\n+}\n+\n+union RequestPartsSpec {\n+  1: list<string> names;\n+  2: list<DropPartitionsExpr> exprs;\n+}\n+\n+// Request type for drop_partitions_req\n+// TODO: we might want to add \"bestEffort\" flag; where a subset can fail\n+struct DropPartitionsRequest {\n+  1: required string dbName,\n+  2: required string tblName,\n+  3: required RequestPartsSpec parts,\n+  4: optional bool deleteData,\n+  5: optional bool ifExists=true, // currently verified on client\n+  6: optional bool ignoreProtection,\n+  7: optional EnvironmentContext environmentContext,\n+  8: optional bool needResult=true\n+}\n+\n+enum FunctionType {\n+  JAVA = 1,\n+}\n+\n+enum ResourceType {\n+  JAR     = 1,\n+  FILE    = 2,\n+  ARCHIVE = 3,\n+}\n+\n+struct ResourceUri {\n+  1: ResourceType resourceType,\n+  2: string       uri,\n+}\n+\n+// User-defined function\n+struct Function {\n+  1: string           functionName,\n+  2: string           dbName,\n+  3: string           className,\n+  4: string           ownerName,\n+  5: PrincipalType    ownerType,\n+  6: i32              createTime,\n+  7: FunctionType     functionType,\n+  8: list<ResourceUri> resourceUris,\n+}\n+\n+// Structs for transaction and locks\n+struct TxnInfo {\n+    1: required i64 id,\n+    2: required TxnState state,\n+    3: required string user,        // used in 'show transactions' to help admins find who has open transactions\n+    4: required string hostname,    // used in 'show transactions' to help admins find who has open transactions\n+}\n+\n+struct GetOpenTxnsInfoResponse {\n+    1: required i64 txn_high_water_mark,\n+    2: required list<TxnInfo> open_txns,\n+}\n+\n+struct GetOpenTxnsResponse {\n+    1: required i64 txn_high_water_mark,\n+    2: required set<i64> open_txns,\n+}\n+\n+struct OpenTxnRequest {\n+    1: required i32 num_txns,\n+    2: required string user,\n+    3: required string hostname,\n+}\n+\n+struct OpenTxnsResponse {\n+    1: required list<i64> txn_ids,\n+}\n+\n+struct AbortTxnRequest {\n+    1: required i64 txnid,\n+}\n+\n+struct CommitTxnRequest {\n+    1: required i64 txnid,\n+}\n+\n+struct LockComponent {\n+    1: required LockType type,\n+    2: required LockLevel level,\n+    3: required string dbname,\n+    4: optional string tablename,\n+    5: optional string partitionname,\n+}\n+\n+struct LockRequest {\n+    1: required list<LockComponent> component,\n+    2: optional i64 txnid,\n+    3: required string user,     // used in 'show locks' to help admins find who has open locks\n+    4: required string hostname, // used in 'show locks' to help admins find who has open locks\n+}\n+\n+struct LockResponse {\n+    1: required i64 lockid,\n+    2: required LockState state,\n+}\n+\n+struct CheckLockRequest {\n+    1: required i64 lockid,\n+}\n+\n+struct UnlockRequest {\n+    1: required i64 lockid,\n+}\n+\n+struct ShowLocksRequest {\n+}\n+\n+struct ShowLocksResponseElement {\n+    1: required i64 lockid,\n+    2: required string dbname,\n+    3: optional string tablename,\n+    4: optional string partname,\n+    5: required LockState state,\n+    6: required LockType type,\n+    7: optional i64 txnid,\n+    8: required i64 lastheartbeat,\n+    9: optional i64 acquiredat,\n+    10: required string user,\n+    11: required string hostname,\n+}\n+\n+struct ShowLocksResponse {\n+    1: list<ShowLocksResponseElement> locks,\n+}\n+\n+struct HeartbeatRequest {\n+    1: optional i64 lockid,\n+    2: optional i64 txnid\n+}\n+\n+struct HeartbeatTxnRangeRequest {\n+    1: required i64 min,\n+    2: required i64 max\n+}\n+\n+struct HeartbeatTxnRangeResponse {\n+    1: required set<i64> aborted,\n+    2: required set<i64> nosuch\n+}\n+\n+struct CompactionRequest {\n+    1: required string dbname,\n+    2: required string tablename,\n+    3: optional string partitionname,\n+    4: required CompactionType type,\n+    5: optional string runas,\n+}\n+\n+struct ShowCompactRequest {\n+}\n+\n+struct ShowCompactResponseElement {\n+    1: required string dbname,\n+    2: required string tablename,\n+    3: optional string partitionname,\n+    4: required CompactionType type,\n+    5: required string state,\n+    6: optional string workerid,\n+    7: optional i64 start,\n+    8: optional string runAs,\n+}\n+\n+struct ShowCompactResponse {\n+    1: required list<ShowCompactResponseElement> compacts,\n+}\n+\n+struct NotificationEventRequest {\n+    1: required i64 lastEvent,\n+    2: optional i32 maxEvents,\n+}\n+\n+struct NotificationEvent {\n+    1: required i64 eventId,\n+    2: required i32 eventTime,\n+    3: required string eventType,\n+    4: optional string dbName,\n+    5: optional string tableName,\n+    6: required string message,\n+}\n+\n+struct NotificationEventResponse {\n+    1: required list<NotificationEvent> events,\n+}\n+\n+struct CurrentNotificationEventId {\n+    1: required i64 eventId,\n+}\n+\n+struct InsertEventRequestData {\n+    1: required list<string> filesAdded\n+}\n+\n+union FireEventRequestData {\n+    1: InsertEventRequestData insertData\n+}\n+\n+struct FireEventRequest {\n+    1: required bool successful,\n+    2: required FireEventRequestData data\n+    // dbname, tablename, and partition vals are included as optional in the top level event rather than placed in each type of\n+    // subevent as I assume they'll be used across most event types.\n+    3: optional string dbName,\n+    4: optional string tableName,\n+    5: optional list<string> partitionVals,\n+}\n+\n+struct FireEventResponse {\n+    // NOP for now, this is just a place holder for future responses\n+}\n+    \n+\n+struct GetAllFunctionsResponse {\n+  1: optional list<Function> functions\n+}\n+\n+struct TableMeta {\n+  1: required string dbName;\n+  2: required string tableName;\n+  3: required string tableType;\n+  4: optional string comments;\n+}\n+\n+exception MetaException {\n+  1: string message\n+}\n+\n+exception UnknownTableException {\n+  1: string message\n+}\n+\n+exception UnknownDBException {\n+  1: string message\n+}\n+\n+exception AlreadyExistsException {\n+  1: string message\n+}\n+\n+exception InvalidPartitionException {\n+  1: string message\n+}\n+\n+exception UnknownPartitionException {\n+  1: string message\n+}\n+\n+exception InvalidObjectException {\n+  1: string message\n+}\n+\n+exception NoSuchObjectException {\n+  1: string message\n+}\n+\n+exception IndexAlreadyExistsException {\n+  1: string message\n+}\n+\n+exception InvalidOperationException {\n+  1: string message\n+}\n+\n+exception ConfigValSecurityException {\n+  1: string message\n+}\n+\n+exception InvalidInputException {\n+  1: string message\n+}\n+\n+// Transaction and lock exceptions\n+exception NoSuchTxnException {\n+    1: string message\n+}\n+\n+exception TxnAbortedException {\n+    1: string message\n+}\n+\n+exception TxnOpenException {\n+    1: string message\n+}\n+\n+exception NoSuchLockException {\n+    1: string message\n+}\n+\n+/**\n+* This interface is live.\n+*/\n+service ThriftHiveMetastore extends fb303.FacebookService\n+{\n+  string getMetaConf(1:string key) throws(1:MetaException o1)\n+  void setMetaConf(1:string key, 2:string value) throws(1:MetaException o1)\n+\n+  void create_database(1:Database database) throws(1:AlreadyExistsException o1, 2:InvalidObjectException o2, 3:MetaException o3)\n+  Database get_database(1:string name) throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  void drop_database(1:string name, 2:bool deleteData, 3:bool cascade) throws(1:NoSuchObjectException o1, 2:InvalidOperationException o2, 3:MetaException o3)\n+  list<string> get_databases(1:string pattern) throws(1:MetaException o1)\n+  list<string> get_all_databases() throws(1:MetaException o1)\n+  void alter_database(1:string dbname, 2:Database db) throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  // returns the type with given name (make seperate calls for the dependent types if needed)\n+  Type get_type(1:string name)  throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+  bool create_type(1:Type type) throws(1:AlreadyExistsException o1, 2:InvalidObjectException o2, 3:MetaException o3)\n+  bool drop_type(1:string type) throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+  map<string, Type> get_type_all(1:string name)\n+                                throws(1:MetaException o2)\n+\n+  // Gets a list of FieldSchemas describing the columns of a particular table\n+  list<FieldSchema> get_fields(1: string db_name, 2: string table_name) throws (1: MetaException o1, 2: UnknownTableException o2, 3: UnknownDBException o3),\n+  list<FieldSchema> get_fields_with_environment_context(1: string db_name, 2: string table_name, 3:EnvironmentContext environment_context) throws (1: MetaException o1, 2: UnknownTableException o2, 3: UnknownDBException o3)\n+\n+  // Gets a list of FieldSchemas describing both the columns and the partition keys of a particular table\n+  list<FieldSchema> get_schema(1: string db_name, 2: string table_name) throws (1: MetaException o1, 2: UnknownTableException o2, 3: UnknownDBException o3)\n+  list<FieldSchema> get_schema_with_environment_context(1: string db_name, 2: string table_name, 3:EnvironmentContext environment_context) throws (1: MetaException o1, 2: UnknownTableException o2, 3: UnknownDBException o3)\n+\n+  // create a Hive table. Following fields must be set\n+  // tableName\n+  // database        (only 'default' for now until Hive QL supports databases)\n+  // owner           (not needed, but good to have for tracking purposes)\n+  // sd.cols         (list of field schemas)\n+  // sd.inputFormat  (SequenceFileInputFormat (binary like falcon tables or u_full) or TextInputFormat)\n+  // sd.outputFormat (SequenceFileInputFormat (binary) or TextInputFormat)\n+  // sd.serdeInfo.serializationLib (SerDe class name eg org.apache.hadoop.hive.serde.simple_meta.MetadataTypedColumnsetSerDe\n+  // * See notes on DDL_TIME\n+  void create_table(1:Table tbl) throws(1:AlreadyExistsException o1, 2:InvalidObjectException o2, 3:MetaException o3, 4:NoSuchObjectException o4)\n+  void create_table_with_environment_context(1:Table tbl,\n+      2:EnvironmentContext environment_context)\n+      throws (1:AlreadyExistsException o1,\n+              2:InvalidObjectException o2, 3:MetaException o3,\n+              4:NoSuchObjectException o4)\n+  // drops the table and all the partitions associated with it if the table has partitions\n+  // delete data (including partitions) if deleteData is set to true\n+  void drop_table(1:string dbname, 2:string name, 3:bool deleteData)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o3)\n+  void drop_table_with_environment_context(1:string dbname, 2:string name, 3:bool deleteData,\n+      4:EnvironmentContext environment_context)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o3)\n+  list<string> get_tables(1: string db_name, 2: string pattern) throws (1: MetaException o1)\n+  list<TableMeta> get_table_meta(1: string db_patterns, 2: string tbl_patterns, 3: list<string> tbl_types)\n+                       throws (1: MetaException o1)\n+  list<string> get_all_tables(1: string db_name) throws (1: MetaException o1)\n+\n+  Table get_table(1:string dbname, 2:string tbl_name)\n+                       throws (1:MetaException o1, 2:NoSuchObjectException o2)\n+  list<Table> get_table_objects_by_name(1:string dbname, 2:list<string> tbl_names)\n+\t\t\t\t   throws (1:MetaException o1, 2:InvalidOperationException o2, 3:UnknownDBException o3)\n+\n+  // Get a list of table names that match a filter.\n+  // The filter operators are LIKE, <, <=, >, >=, =, <>\n+  //\n+  // In the filter statement, values interpreted as strings must be enclosed in quotes,\n+  // while values interpreted as integers should not be.  Strings and integers are the only\n+  // supported value types.\n+  //\n+  // The currently supported key names in the filter are:\n+  // Constants.HIVE_FILTER_FIELD_OWNER, which filters on the tables' owner's name\n+  //   and supports all filter operators\n+  // Constants.HIVE_FILTER_FIELD_LAST_ACCESS, which filters on the last access times\n+  //   and supports all filter operators except LIKE\n+  // Constants.HIVE_FILTER_FIELD_PARAMS, which filters on the tables' parameter keys and values\n+  //   and only supports the filter operators = and <>.\n+  //   Append the parameter key name to HIVE_FILTER_FIELD_PARAMS in the filter statement.\n+  //   For example, to filter on parameter keys called \"retention\", the key name in the filter\n+  //   statement should be Constants.HIVE_FILTER_FIELD_PARAMS + \"retention\"\n+  //   Also, = and <> only work for keys that exist\n+  //   in the tables. E.g., if you are looking for tables where key1 <> value, it will only\n+  //   look at tables that have a value for the parameter key1.\n+  // Some example filter statements include:\n+  // filter = Constants.HIVE_FILTER_FIELD_OWNER + \" like \\\".*test.*\\\" and \" +\n+  //   Constants.HIVE_FILTER_FIELD_LAST_ACCESS + \" = 0\";\n+  // filter = Constants.HIVE_FILTER_FIELD_PARAMS + \"retention = \\\"30\\\" or \" +\n+  //   Constants.HIVE_FILTER_FIELD_PARAMS + \"retention = \\\"90\\\"\"\n+  // @param dbName\n+  //          The name of the database from which you will retrieve the table names\n+  // @param filterType\n+  //          The type of filter\n+  // @param filter\n+  //          The filter string\n+  // @param max_tables\n+  //          The maximum number of tables returned\n+  // @return  A list of table names that match the desired filter\n+  list<string> get_table_names_by_filter(1:string dbname, 2:string filter, 3:i16 max_tables=-1)\n+                       throws (1:MetaException o1, 2:InvalidOperationException o2, 3:UnknownDBException o3)\n+\n+  // alter table applies to only future partitions not for existing partitions\n+  // * See notes on DDL_TIME\n+  void alter_table(1:string dbname, 2:string tbl_name, 3:Table new_tbl)\n+                       throws (1:InvalidOperationException o1, 2:MetaException o2)\n+  void alter_table_with_environment_context(1:string dbname, 2:string tbl_name,\n+      3:Table new_tbl, 4:EnvironmentContext environment_context)\n+      throws (1:InvalidOperationException o1, 2:MetaException o2)\n+  // alter table not only applies to future partitions but also cascade to existing partitions\n+  void alter_table_with_cascade(1:string dbname, 2:string tbl_name, 3:Table new_tbl, 4:bool cascade)\n+                       throws (1:InvalidOperationException o1, 2:MetaException o2)\n+  // the following applies to only tables that have partitions\n+  // * See notes on DDL_TIME\n+  Partition add_partition(1:Partition new_part)\n+                       throws(1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  Partition add_partition_with_environment_context(1:Partition new_part,\n+      2:EnvironmentContext environment_context)\n+      throws (1:InvalidObjectException o1, 2:AlreadyExistsException o2,\n+      3:MetaException o3)\n+  i32 add_partitions(1:list<Partition> new_parts)\n+                       throws(1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  i32 add_partitions_pspec(1:list<PartitionSpec> new_parts)\n+                       throws(1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  Partition append_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals)\n+                       throws (1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  AddPartitionsResult add_partitions_req(1:AddPartitionsRequest request)\n+                       throws(1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  Partition append_partition_with_environment_context(1:string db_name, 2:string tbl_name,\n+      3:list<string> part_vals, 4:EnvironmentContext environment_context)\n+                       throws (1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  Partition append_partition_by_name(1:string db_name, 2:string tbl_name, 3:string part_name)\n+                       throws (1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  Partition append_partition_by_name_with_environment_context(1:string db_name, 2:string tbl_name,\n+      3:string part_name, 4:EnvironmentContext environment_context)\n+                       throws (1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  bool drop_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals, 4:bool deleteData)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  bool drop_partition_with_environment_context(1:string db_name, 2:string tbl_name,\n+      3:list<string> part_vals, 4:bool deleteData, 5:EnvironmentContext environment_context)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  bool drop_partition_by_name(1:string db_name, 2:string tbl_name, 3:string part_name, 4:bool deleteData)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  bool drop_partition_by_name_with_environment_context(1:string db_name, 2:string tbl_name,\n+      3:string part_name, 4:bool deleteData, 5:EnvironmentContext environment_context)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  DropPartitionsResult drop_partitions_req(1: DropPartitionsRequest req)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+\n+  Partition get_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals)\n+                       throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+  Partition exchange_partition(1:map<string, string> partitionSpecs, 2:string source_db,\n+      3:string source_table_name, 4:string dest_db, 5:string dest_table_name)\n+      throws(1:MetaException o1, 2:NoSuchObjectException o2, 3:InvalidObjectException o3,\n+      4:InvalidInputException o4)\n+\n+  Partition get_partition_with_auth(1:string db_name, 2:string tbl_name, 3:list<string> part_vals,\n+      4: string user_name, 5: list<string> group_names) throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  Partition get_partition_by_name(1:string db_name 2:string tbl_name, 3:string part_name)\n+                       throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  // returns all the partitions for this table in reverse chronological order.\n+  // If max parts is given then it will return only that many.\n+  list<Partition> get_partitions(1:string db_name, 2:string tbl_name, 3:i16 max_parts=-1)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  list<Partition> get_partitions_with_auth(1:string db_name, 2:string tbl_name, 3:i16 max_parts=-1,\n+     4: string user_name, 5: list<string> group_names) throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+\n+  list<PartitionSpec> get_partitions_pspec(1:string db_name, 2:string tbl_name, 3:i32 max_parts=-1)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+\n+  list<string> get_partition_names(1:string db_name, 2:string tbl_name, 3:i16 max_parts=-1)\n+                       throws(1:MetaException o2)\n+\n+  // get_partition*_ps methods allow filtering by a partial partition specification,\n+  // as needed for dynamic partitions. The values that are not restricted should\n+  // be empty strings. Nulls were considered (instead of \"\") but caused errors in\n+  // generated Python code. The size of part_vals may be smaller than the\n+  // number of partition columns - the unspecified values are considered the same\n+  // as \"\".\n+  list<Partition> get_partitions_ps(1:string db_name 2:string tbl_name\n+  \t3:list<string> part_vals, 4:i16 max_parts=-1)\n+                       throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+  list<Partition> get_partitions_ps_with_auth(1:string db_name, 2:string tbl_name, 3:list<string> part_vals, 4:i16 max_parts=-1,\n+     5: string user_name, 6: list<string> group_names) throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+\n+  list<string> get_partition_names_ps(1:string db_name,\n+  \t2:string tbl_name, 3:list<string> part_vals, 4:i16 max_parts=-1)\n+  \t                   throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  // get the partitions matching the given partition filter\n+  list<Partition> get_partitions_by_filter(1:string db_name 2:string tbl_name\n+    3:string filter, 4:i16 max_parts=-1)\n+                       throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  // List partitions as PartitionSpec instances.\n+  list<PartitionSpec> get_part_specs_by_filter(1:string db_name 2:string tbl_name\n+    3:string filter, 4:i32 max_parts=-1)\n+                       throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  // get the partitions matching the given partition filter\n+  // unlike get_partitions_by_filter, takes serialized hive expression, and with that can work\n+  // with any filter (get_partitions_by_filter only works if the filter can be pushed down to JDOQL.\n+  PartitionsByExprResult get_partitions_by_expr(1:PartitionsByExprRequest req)\n+                       throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  // get partitions give a list of partition names\n+  list<Partition> get_partitions_by_names(1:string db_name 2:string tbl_name 3:list<string> names)\n+                       throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  // changes the partition to the new partition object. partition is identified from the part values\n+  // in the new_part\n+  // * See notes on DDL_TIME\n+  void alter_partition(1:string db_name, 2:string tbl_name, 3:Partition new_part)\n+                       throws (1:InvalidOperationException o1, 2:MetaException o2)\n+\n+  // change a list of partitions. All partitions are altered atomically and all\n+  // prehooks are fired together followed by all post hooks\n+  void alter_partitions(1:string db_name, 2:string tbl_name, 3:list<Partition> new_parts)\n+                       throws (1:InvalidOperationException o1, 2:MetaException o2)\n+\n+  void alter_partition_with_environment_context(1:string db_name,\n+      2:string tbl_name, 3:Partition new_part,\n+      4:EnvironmentContext environment_context)\n+      throws (1:InvalidOperationException o1, 2:MetaException o2)\n+\n+  // rename the old partition to the new partition object by changing old part values to the part values\n+  // in the new_part. old partition is identified from part_vals.\n+  // partition keys in new_part should be the same as those in old partition.\n+  void rename_partition(1:string db_name, 2:string tbl_name, 3:list<string> part_vals, 4:Partition new_part)\n+                       throws (1:InvalidOperationException o1, 2:MetaException o2)\n+\n+  // returns whether or not the partition name is valid based on the value of the config\n+  // hive.metastore.partition.name.whitelist.pattern\n+  bool partition_name_has_valid_characters(1:list<string> part_vals, 2:bool throw_exception)\n+ \tthrows(1: MetaException o1)\n+\n+  // gets the value of the configuration key in the metastore server. returns\n+  // defaultValue if the key does not exist. if the configuration key does not\n+  // begin with \"hive\", \"mapred\", or \"hdfs\", a ConfigValSecurityException is\n+  // thrown.\n+  string get_config_value(1:string name, 2:string defaultValue)\n+                          throws(1:ConfigValSecurityException o1)\n+\n+  // converts a partition name into a partition values array\n+  list<string> partition_name_to_vals(1: string part_name)\n+                          throws(1: MetaException o1)\n+  // converts a partition name into a partition specification (a mapping from\n+  // the partition cols to the values)\n+  map<string, string> partition_name_to_spec(1: string part_name)\n+                          throws(1: MetaException o1)\n+\n+  void markPartitionForEvent(1:string db_name, 2:string tbl_name, 3:map<string,string> part_vals,\n+                  4:PartitionEventType eventType) throws (1: MetaException o1, 2: NoSuchObjectException o2,\n+                  3: UnknownDBException o3, 4: UnknownTableException o4, 5: UnknownPartitionException o5,\n+                  6: InvalidPartitionException o6)\n+  bool isPartitionMarkedForEvent(1:string db_name, 2:string tbl_name, 3:map<string,string> part_vals,\n+                  4: PartitionEventType eventType) throws (1: MetaException o1, 2:NoSuchObjectException o2,\n+                  3: UnknownDBException o3, 4: UnknownTableException o4, 5: UnknownPartitionException o5,\n+                  6: InvalidPartitionException o6)\n+\n+  //index\n+  Index add_index(1:Index new_index, 2: Table index_table)\n+                       throws(1:InvalidObjectException o1, 2:AlreadyExistsException o2, 3:MetaException o3)\n+  void alter_index(1:string dbname, 2:string base_tbl_name, 3:string idx_name, 4:Index new_idx)\n+                       throws (1:InvalidOperationException o1, 2:MetaException o2)\n+  bool drop_index_by_name(1:string db_name, 2:string tbl_name, 3:string index_name, 4:bool deleteData)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  Index get_index_by_name(1:string db_name 2:string tbl_name, 3:string index_name)\n+                       throws(1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  list<Index> get_indexes(1:string db_name, 2:string tbl_name, 3:i16 max_indexes=-1)\n+                       throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  list<string> get_index_names(1:string db_name, 2:string tbl_name, 3:i16 max_indexes=-1)\n+                       throws(1:MetaException o2)\n+\n+  // column statistics interfaces\n+\n+  // update APIs persist the column statistics object(s) that are passed in. If statistics already\n+  // exists for one or more columns, the existing statistics will be overwritten. The update APIs\n+  // validate that the dbName, tableName, partName, colName[] passed in as part of the ColumnStatistics\n+  // struct are valid, throws InvalidInputException/NoSuchObjectException if found to be invalid\n+  bool update_table_column_statistics(1:ColumnStatistics stats_obj) throws (1:NoSuchObjectException o1,\n+              2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)\n+  bool update_partition_column_statistics(1:ColumnStatistics stats_obj) throws (1:NoSuchObjectException o1,\n+              2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)\n+\n+  // get APIs return the column statistics corresponding to db_name, tbl_name, [part_name], col_name if\n+  // such statistics exists. If the required statistics doesn't exist, get APIs throw NoSuchObjectException\n+  // For instance, if get_table_column_statistics is called on a partitioned table for which only\n+  // partition level column stats exist, get_table_column_statistics will throw NoSuchObjectException\n+  ColumnStatistics get_table_column_statistics(1:string db_name, 2:string tbl_name, 3:string col_name) throws\n+              (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidInputException o3, 4:InvalidObjectException o4)\n+  ColumnStatistics get_partition_column_statistics(1:string db_name, 2:string tbl_name, 3:string part_name,\n+               4:string col_name) throws (1:NoSuchObjectException o1, 2:MetaException o2,\n+               3:InvalidInputException o3, 4:InvalidObjectException o4)\n+  TableStatsResult get_table_statistics_req(1:TableStatsRequest request) throws\n+              (1:NoSuchObjectException o1, 2:MetaException o2)\n+  PartitionsStatsResult get_partitions_statistics_req(1:PartitionsStatsRequest request) throws\n+              (1:NoSuchObjectException o1, 2:MetaException o2)\n+  AggrStats get_aggr_stats_for(1:PartitionsStatsRequest request) throws\n+              (1:NoSuchObjectException o1, 2:MetaException o2)\n+  bool set_aggr_stats_for(1:SetPartitionsStatsRequest request) throws\n+              (1:NoSuchObjectException o1, 2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)\n+\n+\n+  // delete APIs attempt to delete column statistics, if found, associated with a given db_name, tbl_name, [part_name]\n+  // and col_name. If the delete API doesn't find the statistics record in the metastore, throws NoSuchObjectException\n+  // Delete API validates the input and if the input is invalid throws InvalidInputException/InvalidObjectException.\n+  bool delete_partition_column_statistics(1:string db_name, 2:string tbl_name, 3:string part_name, 4:string col_name) throws\n+              (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidObjectException o3,\n+               4:InvalidInputException o4)\n+  bool delete_table_column_statistics(1:string db_name, 2:string tbl_name, 3:string col_name) throws\n+              (1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidObjectException o3,\n+               4:InvalidInputException o4)\n+\n+  //\n+  // user-defined functions\n+  //\n+\n+  void create_function(1:Function func)\n+      throws (1:AlreadyExistsException o1,\n+              2:InvalidObjectException o2,\n+              3:MetaException o3,\n+              4:NoSuchObjectException o4)\n+\n+  void drop_function(1:string dbName, 2:string funcName)\n+      throws (1:NoSuchObjectException o1, 2:MetaException o3)\n+\n+  void alter_function(1:string dbName, 2:string funcName, 3:Function newFunc)\n+      throws (1:InvalidOperationException o1, 2:MetaException o2)\n+\n+  list<string> get_functions(1:string dbName, 2:string pattern)\n+      throws (1:MetaException o1)\n+  Function get_function(1:string dbName, 2:string funcName)\n+      throws (1:MetaException o1, 2:NoSuchObjectException o2)\n+\n+  GetAllFunctionsResponse get_all_functions() throws (1:MetaException o1)\n+\n+  //authorization privileges\n+\n+  bool create_role(1:Role role) throws(1:MetaException o1)\n+  bool drop_role(1:string role_name) throws(1:MetaException o1)\n+  list<string> get_role_names() throws(1:MetaException o1)\n+  // Deprecated, use grant_revoke_role()\n+  bool grant_role(1:string role_name, 2:string principal_name, 3:PrincipalType principal_type,\n+    4:string grantor, 5:PrincipalType grantorType, 6:bool grant_option) throws(1:MetaException o1)\n+  // Deprecated, use grant_revoke_role()\n+  bool revoke_role(1:string role_name, 2:string principal_name, 3:PrincipalType principal_type)\n+                        throws(1:MetaException o1)\n+  list<Role> list_roles(1:string principal_name, 2:PrincipalType principal_type) throws(1:MetaException o1)\n+  GrantRevokeRoleResponse grant_revoke_role(1:GrantRevokeRoleRequest request) throws(1:MetaException o1)\n+\n+  // get all role-grants for users/roles that have been granted the given role\n+  // Note that in the returned list of RolePrincipalGrants, the roleName is\n+  // redundant as it would match the role_name argument of this function\n+  GetPrincipalsInRoleResponse get_principals_in_role(1: GetPrincipalsInRoleRequest request) throws(1:MetaException o1)\n+\n+  // get grant information of all roles granted to the given principal\n+  // Note that in the returned list of RolePrincipalGrants, the principal name,type is\n+  // redundant as it would match the principal name,type arguments of this function\n+  GetRoleGrantsForPrincipalResponse get_role_grants_for_principal(1: GetRoleGrantsForPrincipalRequest request) throws(1:MetaException o1)\n+\n+  PrincipalPrivilegeSet get_privilege_set(1:HiveObjectRef hiveObject, 2:string user_name,\n+    3: list<string> group_names) throws(1:MetaException o1)\n+  list<HiveObjectPrivilege> list_privileges(1:string principal_name, 2:PrincipalType principal_type,\n+    3: HiveObjectRef hiveObject) throws(1:MetaException o1)\n+\n+  // Deprecated, use grant_revoke_privileges()\n+  bool grant_privileges(1:PrivilegeBag privileges) throws(1:MetaException o1)\n+  // Deprecated, use grant_revoke_privileges()\n+  bool revoke_privileges(1:PrivilegeBag privileges) throws(1:MetaException o1)\n+  GrantRevokePrivilegeResponse grant_revoke_privileges(1:GrantRevokePrivilegeRequest request) throws(1:MetaException o1);\n+\n+  // this is used by metastore client to send UGI information to metastore server immediately\n+  // after setting up a connection.\n+  list<string> set_ugi(1:string user_name, 2:list<string> group_names) throws (1:MetaException o1)\n+\n+  //Authentication (delegation token) interfaces\n+\n+  // get metastore server delegation token for use from the map/reduce tasks to authenticate\n+  // to metastore server\n+  string get_delegation_token(1:string token_owner, 2:string renewer_kerberos_principal_name)\n+    throws (1:MetaException o1)\n+\n+  // method to renew delegation token obtained from metastore server\n+  i64 renew_delegation_token(1:string token_str_form) throws (1:MetaException o1)\n+\n+  // method to cancel delegation token obtained from metastore server\n+  void cancel_delegation_token(1:string token_str_form) throws (1:MetaException o1)\n+\n+  // Transaction and lock management calls\n+  // Get just list of open transactions\n+  GetOpenTxnsResponse get_open_txns()\n+  // Get list of open transactions with state (open, aborted)\n+  GetOpenTxnsInfoResponse get_open_txns_info()\n+  OpenTxnsResponse open_txns(1:OpenTxnRequest rqst)\n+  void abort_txn(1:AbortTxnRequest rqst) throws (1:NoSuchTxnException o1)\n+  void commit_txn(1:CommitTxnRequest rqst) throws (1:NoSuchTxnException o1, 2:TxnAbortedException o2)\n+  LockResponse lock(1:LockRequest rqst) throws (1:NoSuchTxnException o1, 2:TxnAbortedException o2)\n+  LockResponse check_lock(1:CheckLockRequest rqst)\n+    throws (1:NoSuchTxnException o1, 2:TxnAbortedException o2, 3:NoSuchLockException o3)\n+  void unlock(1:UnlockRequest rqst) throws (1:NoSuchLockException o1, 2:TxnOpenException o2)\n+  ShowLocksResponse show_locks(1:ShowLocksRequest rqst)\n+  void heartbeat(1:HeartbeatRequest ids) throws (1:NoSuchLockException o1, 2:NoSuchTxnException o2, 3:TxnAbortedException o3)\n+  HeartbeatTxnRangeResponse heartbeat_txn_range(1:HeartbeatTxnRangeRequest txns)\n+  void compact(1:CompactionRequest rqst) \n+  ShowCompactResponse show_compact(1:ShowCompactRequest rqst)\n+\n+  // Notification logging calls\n+  NotificationEventResponse get_next_notification(1:NotificationEventRequest rqst) \n+  CurrentNotificationEventId get_current_notificationEventId()\n+}\n+\n+// * Note about the DDL_TIME: When creating or altering a table or a partition,\n+// if the DDL_TIME is not set, the current time will be used.\n+\n+// For storing info about archived partitions in parameters\n+\n+// Whether the partition is archived\n+const string IS_ARCHIVED = \"is_archived\",\n+// The original location of the partition, before archiving. After archiving,\n+// this directory will contain the archive. When the partition\n+// is dropped, this directory will be deleted\n+const string ORIGINAL_LOCATION = \"original_location\",\n+\n+// Whether or not the table is considered immutable - immutable tables can only be\n+// overwritten or created if unpartitioned, or if partitioned, partitions inside them\n+// can only be overwritten or created. Immutability supports write-once and replace\n+// semantics, but not append.\n+const string IS_IMMUTABLE = \"immutable\",\n+\n+// these should be needed only for backward compatibility with filestore\n+const string META_TABLE_COLUMNS   = \"columns\",\n+const string META_TABLE_COLUMN_TYPES   = \"columns.types\",\n+const string BUCKET_FIELD_NAME    = \"bucket_field_name\",\n+const string BUCKET_COUNT         = \"bucket_count\",\n+const string FIELD_TO_DIMENSION   = \"field_to_dimension\",\n+const string META_TABLE_NAME      = \"name\",\n+const string META_TABLE_DB        = \"db\",\n+const string META_TABLE_LOCATION  = \"location\",\n+const string META_TABLE_SERDE     = \"serde\",\n+const string META_TABLE_PARTITION_COLUMNS = \"partition_columns\",\n+const string META_TABLE_PARTITION_COLUMN_TYPES = \"partition_columns.types\",\n+const string FILE_INPUT_FORMAT    = \"file.inputformat\",\n+const string FILE_OUTPUT_FORMAT   = \"file.outputformat\",\n+const string META_TABLE_STORAGE   = \"storage_handler\",\n+const string TABLE_IS_TRANSACTIONAL = \"transactional\",\n+const string TABLE_NO_AUTO_COMPACT = \"no_auto_compaction\",\n+\n+\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/types.cc b/cpp/src/arrow/dbi/hiveserver2/types.cc\nnew file mode 100644\nindex 0000000000..30b9fbc942\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/types.cc\n@@ -0,0 +1,45 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dbi/hiveserver2/types.h\"\n+\n+#include \"arrow/dbi/hiveserver2/thrift-internal.h\"\n+\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+const PrimitiveType* ColumnDesc::GetPrimitiveType() const {\n+  return static_cast<PrimitiveType*>(type_.get());\n+}\n+\n+const CharacterType* ColumnDesc::GetCharacterType() const {\n+  DCHECK(type_->type_id() == ColumnType::TypeId::CHAR ||\n+         type_->type_id() == ColumnType::TypeId::VARCHAR);\n+  return static_cast<CharacterType*>(type_.get());\n+}\n+\n+const DecimalType* ColumnDesc::GetDecimalType() const {\n+  DCHECK(type_->type_id() == ColumnType::TypeId::DECIMAL);\n+  return static_cast<DecimalType*>(type_.get());\n+}\n+\n+std::string PrimitiveType::ToString() const { return TypeIdToString(type_id_); }\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/types.h b/cpp/src/arrow/dbi/hiveserver2/types.h\nnew file mode 100644\nindex 0000000000..38cebcc2ee\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/types.h\n@@ -0,0 +1,131 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+// Represents a column's type.\n+//\n+// For now only PrimitiveType is implemented, as thase are the only types Impala will\n+// currently return. In the future, nested types will be represented as other subclasses\n+// of ColumnType containing ptrs to other ColumnTypes - for example, an ArrayType subclass\n+// would contain a single ptr to another ColumnType representing the type of objects\n+// stored in the array.\n+class ColumnType {\n+ public:\n+  virtual ~ColumnType() = default;\n+\n+  // Maps directly to TTypeId in the HiveServer2 interface.\n+  enum class TypeId {\n+    BOOLEAN,\n+    TINYINT,\n+    SMALLINT,\n+    INT,\n+    BIGINT,\n+    FLOAT,\n+    DOUBLE,\n+    STRING,\n+    TIMESTAMP,\n+    BINARY,\n+    ARRAY,\n+    MAP,\n+    STRUCT,\n+    UNION,\n+    USER_DEFINED,\n+    DECIMAL,\n+    NULL_TYPE,\n+    DATE,\n+    VARCHAR,\n+    CHAR,\n+    INVALID,\n+  };\n+\n+  virtual TypeId type_id() const = 0;\n+  virtual std::string ToString() const = 0;\n+};\n+\n+class PrimitiveType : public ColumnType {\n+ public:\n+  explicit PrimitiveType(const TypeId& type_id) : type_id_(type_id) {}\n+\n+  TypeId type_id() const override { return type_id_; }\n+  std::string ToString() const override;\n+\n+ private:\n+  const TypeId type_id_;\n+};\n+\n+// Represents CHAR and VARCHAR types.\n+class CharacterType : public PrimitiveType {\n+ public:\n+  CharacterType(const TypeId& type_id, int max_length)\n+      : PrimitiveType(type_id), max_length_(max_length) {}\n+\n+  int max_length() const { return max_length_; }\n+\n+ private:\n+  const int max_length_;\n+};\n+\n+// Represents DECIMAL types.\n+class DecimalType : public PrimitiveType {\n+ public:\n+  DecimalType(const TypeId& type_id, int precision, int scale)\n+      : PrimitiveType(type_id), precision_(precision), scale_(scale) {}\n+\n+  int precision() const { return precision_; }\n+  int scale() const { return scale_; }\n+\n+ private:\n+  const int precision_;\n+  const int scale_;\n+};\n+\n+// Represents the metadata for a single column.\n+class ColumnDesc {\n+ public:\n+  ColumnDesc(const std::string& column_name, std::unique_ptr<ColumnType> type,\n+             int position, const std::string& comment)\n+      : column_name_(column_name),\n+        type_(move(type)),\n+        position_(position),\n+        comment_(comment) {}\n+\n+  const std::string& column_name() const { return column_name_; }\n+  const ColumnType* type() const { return type_.get(); }\n+  int position() const { return position_; }\n+  const std::string& comment() const { return comment_; }\n+\n+  const PrimitiveType* GetPrimitiveType() const;\n+  const CharacterType* GetCharacterType() const;\n+  const DecimalType* GetDecimalType() const;\n+\n+ private:\n+  const std::string column_name_;\n+  std::unique_ptr<ColumnType> type_;\n+  const int position_;\n+  const std::string comment_;\n+};\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/util.cc b/cpp/src/arrow/dbi/hiveserver2/util.cc\nnew file mode 100644\nindex 0000000000..8d8b593827\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/util.cc\n@@ -0,0 +1,251 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dbi/hiveserver2/util.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <sstream>\n+#include <vector>\n+\n+#include \"arrow/dbi/hiveserver2/columnar-row-set.h\"\n+#include \"arrow/dbi/hiveserver2/thrift-internal.h\"\n+\n+#include \"arrow/dbi/hiveserver2/TCLIService.h\"\n+#include \"arrow/dbi/hiveserver2/TCLIService_types.h\"\n+\n+#include \"arrow/status.h\"\n+\n+namespace hs2 = apache::hive::service::cli::thrift;\n+using std::string;\n+using std::unique_ptr;\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+// PrintResults\n+namespace {\n+\n+const char kNullSymbol[] = \"NULL\";\n+const char kTrueSymbol[] = \"true\";\n+const char kFalseSymbol[] = \"false\";\n+\n+struct PrintInfo {\n+  // The PrintInfo takes ownership of the Column ptr.\n+  PrintInfo(Column* c, size_t m) : column(c), max_size(m) {}\n+\n+  unique_ptr<Column> column;\n+  size_t max_size;\n+};\n+\n+// Adds a horizontal line of '-'s, with '+'s at the column breaks.\n+static void AddTableBreak(std::ostream& out, std::vector<PrintInfo>* columns) {\n+  for (size_t i = 0; i < columns->size(); ++i) {\n+    out << \"+\";\n+    for (size_t j = 0; j < (*columns)[i].max_size + 2; ++j) {\n+      out << \"-\";\n+    }\n+  }\n+  out << \"+\\n\";\n+}\n+\n+// Returns the number of spaces needed to display n, i.e. the number of digits n has,\n+// plus 1 if n is negative.\n+static size_t NumSpaces(int64_t n) {\n+  if (n < 0) {\n+    return 1 + NumSpaces(-n);\n+  } else if (n < 10) {\n+    return 1;\n+  } else {\n+    return 1 + NumSpaces(n / 10);\n+  }\n+}\n+\n+// Returns the max size needed to display a column of integer type.\n+template <typename T>\n+static size_t GetIntMaxSize(T* column, const string& column_name) {\n+  size_t max_size = column_name.size();\n+  for (int i = 0; i < column->length(); ++i) {\n+    if (!column->IsNull(i)) {\n+      max_size = std::max(max_size, NumSpaces(column->data()[i]));\n+    } else {\n+      max_size = std::max(max_size, sizeof(kNullSymbol));\n+    }\n+  }\n+  return max_size;\n+}\n+\n+}  // namespace\n+\n+void Util::PrintResults(const Operation* op, std::ostream& out) {\n+  unique_ptr<ColumnarRowSet> results;\n+  bool has_more_rows = true;\n+  while (has_more_rows) {\n+    Status s = op->Fetch(&results, &has_more_rows);\n+    if (!s.ok()) {\n+      out << s.ToString();\n+      return;\n+    }\n+\n+    std::vector<ColumnDesc> column_descs;\n+    s = op->GetResultSetMetadata(&column_descs);\n+\n+    if (!s.ok()) {\n+      out << s.ToString();\n+      return;\n+    } else if (column_descs.size() == 0) {\n+      out << \"No result set to print.\\n\";\n+      return;\n+    }\n+\n+    std::vector<PrintInfo> columns;\n+    for (int i = 0; i < static_cast<int>(column_descs.size()); i++) {\n+      const string column_name = column_descs[i].column_name();\n+      switch (column_descs[i].type()->type_id()) {\n+        case ColumnType::TypeId::BOOLEAN: {\n+          BoolColumn* bool_col = results->GetBoolCol(i).release();\n+\n+          // The largest symbol is length 4 unless there is a FALSE, then is it\n+          // kFalseSymbol.size() = 5.\n+          size_t max_size = std::max(column_name.size(), sizeof(kTrueSymbol));\n+          for (int j = 0; j < bool_col->length(); ++j) {\n+            if (!bool_col->IsNull(j) && !bool_col->data()[j]) {\n+              max_size = std::max(max_size, sizeof(kFalseSymbol));\n+              break;\n+            }\n+          }\n+\n+          columns.emplace_back(bool_col, max_size);\n+          break;\n+        }\n+        case ColumnType::TypeId::TINYINT: {\n+          ByteColumn* byte_col = results->GetByteCol(i).release();\n+          columns.emplace_back(byte_col, GetIntMaxSize(byte_col, column_name));\n+          break;\n+        }\n+        case ColumnType::TypeId::SMALLINT: {\n+          Int16Column* int16_col = results->GetInt16Col(i).release();\n+          columns.emplace_back(int16_col, GetIntMaxSize(int16_col, column_name));\n+          break;\n+        }\n+        case ColumnType::TypeId::INT: {\n+          Int32Column* int32_col = results->GetInt32Col(i).release();\n+          columns.emplace_back(int32_col, GetIntMaxSize(int32_col, column_name));\n+          break;\n+        }\n+        case ColumnType::TypeId::BIGINT: {\n+          Int64Column* int64_col = results->GetInt64Col(i).release();\n+          columns.emplace_back(int64_col, GetIntMaxSize(int64_col, column_name));\n+          break;\n+        }\n+        case ColumnType::TypeId::STRING: {\n+          unique_ptr<StringColumn> string_col = results->GetStringCol(i);\n+\n+          size_t max_size = column_name.size();\n+          for (int j = 0; j < string_col->length(); ++j) {\n+            if (!string_col->IsNull(j)) {\n+              max_size = std::max(max_size, string_col->data()[j].size());\n+            } else {\n+              max_size = std::max(max_size, sizeof(kNullSymbol));\n+            }\n+          }\n+\n+          columns.emplace_back(string_col.release(), max_size);\n+          break;\n+        }\n+        case ColumnType::TypeId::BINARY:\n+          columns.emplace_back(results->GetBinaryCol(i).release(), column_name.size());\n+          break;\n+        default: {\n+          out << \"Unrecognized ColumnType = \" << column_descs[i].type()->ToString();\n+        }\n+      }\n+    }\n+\n+    AddTableBreak(out, &columns);\n+    for (size_t i = 0; i < columns.size(); ++i) {\n+      out << \"| \" << column_descs[i].column_name() << \" \";\n+\n+      int padding =\n+          static_cast<int>(columns[i].max_size - column_descs[i].column_name().size());\n+      while (padding > 0) {\n+        out << \" \";\n+        --padding;\n+      }\n+    }\n+    out << \"|\\n\";\n+    AddTableBreak(out, &columns);\n+\n+    for (int i = 0; i < columns[0].column->length(); ++i) {\n+      for (size_t j = 0; j < columns.size(); ++j) {\n+        std::stringstream value;\n+\n+        if (columns[j].column->IsNull(i)) {\n+          value << kNullSymbol;\n+        } else {\n+          switch (column_descs[j].type()->type_id()) {\n+            case ColumnType::TypeId::BOOLEAN:\n+              if (reinterpret_cast<BoolColumn*>(columns[j].column.get())->data()[i]) {\n+                value << kTrueSymbol;\n+              } else {\n+                value << kFalseSymbol;\n+              }\n+              break;\n+            case ColumnType::TypeId::TINYINT:\n+              // The cast prevents us from printing this as a char.\n+              value << static_cast<int16_t>(\n+                  reinterpret_cast<ByteColumn*>(columns[j].column.get())->data()[i]);\n+              break;\n+            case ColumnType::TypeId::SMALLINT:\n+              value << reinterpret_cast<Int16Column*>(columns[j].column.get())->data()[i];\n+              break;\n+            case ColumnType::TypeId::INT:\n+              value << reinterpret_cast<Int32Column*>(columns[j].column.get())->data()[i];\n+              break;\n+            case ColumnType::TypeId::BIGINT:\n+              value << reinterpret_cast<Int64Column*>(columns[j].column.get())->data()[i];\n+              break;\n+            case ColumnType::TypeId::STRING:\n+              value\n+                  << reinterpret_cast<StringColumn*>(columns[j].column.get())->data()[i];\n+              break;\n+            case ColumnType::TypeId::BINARY:\n+              value\n+                  << reinterpret_cast<BinaryColumn*>(columns[j].column.get())->data()[i];\n+              break;\n+            default:\n+              value << \"unrecognized type\";\n+              break;\n+          }\n+        }\n+\n+        string value_str = value.str();\n+        out << \"| \" << value_str << \" \";\n+        int padding = static_cast<int>(columns[j].max_size - value_str.size());\n+        while (padding > 0) {\n+          out << \" \";\n+          --padding;\n+        }\n+      }\n+      out << \"|\\n\";\n+    }\n+    AddTableBreak(out, &columns);\n+  }\n+}\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/dbi/hiveserver2/util.h b/cpp/src/arrow/dbi/hiveserver2/util.h\nnew file mode 100644\nindex 0000000000..a17e7b2286\n--- /dev/null\n+++ b/cpp/src/arrow/dbi/hiveserver2/util.h\n@@ -0,0 +1,36 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <string>\n+\n+#include \"arrow/dbi/hiveserver2/operation.h\"\n+\n+namespace arrow {\n+namespace hiveserver2 {\n+\n+// Utility functions. Intended primary for testing purposes - clients should not\n+// rely on stability of the behavior or API of these functions.\n+class Util {\n+ public:\n+  // Fetches the operation's results and returns them in a nicely formatted string.\n+  static void PrintResults(const Operation* op, std::ostream& out);\n+};\n+\n+}  // namespace hiveserver2\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/status.h b/cpp/src/arrow/status.h\nindex 539b8d1161..4df6cc5bbd 100644\n--- a/cpp/src/arrow/status.h\n+++ b/cpp/src/arrow/status.h\n@@ -84,6 +84,7 @@ enum class StatusCode : char {\n   PlasmaObjectNonexistent = 21,\n   PlasmaStoreFull = 22,\n   PlasmaObjectAlreadySealed = 23,\n+  StillExecuting = 24\n };\n \n #if defined(__clang__)\n@@ -122,6 +123,9 @@ class ARROW_EXPORT Status {\n   // Return a success status.\n   static Status OK() { return Status(); }\n \n+  // Return a success status with extra info\n+  static Status OK(const std::string& msg) { return Status(StatusCode::OK, msg); }\n+\n   // Return error status of an appropriate type.\n   static Status OutOfMemory(const std::string& msg) {\n     return Status(StatusCode::OutOfMemory, msg);\n@@ -175,6 +179,8 @@ class ARROW_EXPORT Status {\n     return Status(StatusCode::PlasmaStoreFull, msg);\n   }\n \n+  static Status StillExecuting() { return Status(StatusCode::StillExecuting, \"\"); }\n+\n   // Returns true iff the status indicates success.\n   bool ok() const { return (state_ == NULL); }\n \n@@ -203,6 +209,8 @@ class ARROW_EXPORT Status {\n   // An object is too large to fit into the plasma store.\n   bool IsPlasmaStoreFull() const { return code() == StatusCode::PlasmaStoreFull; }\n \n+  bool IsStillExecuting() const { return code() == StatusCode::StillExecuting; }\n+\n   // Return a string representation of this status suitable for printing.\n   // Returns the string \"OK\" for success.\n   std::string ToString() const;\ndiff --git a/cpp/thirdparty/download_dependencies.sh b/cpp/thirdparty/download_dependencies.sh\nindex ab8c0b263c..1f0ffed3c1 100755\n--- a/cpp/thirdparty/download_dependencies.sh\n+++ b/cpp/thirdparty/download_dependencies.sh\n@@ -65,6 +65,8 @@ wget -c -O $_DST/grpc.tar.gz https://github.com/grpc/grpc/archive/v$GRPC_VERSION\n \n wget -c -O $_DST/orc.tar.gz https://github.com/apache/orc/archive/rel/release-$ORC_VERSION.tar.gz\n \n+wget -c -O $_DST/thrift.tar.gz http://archive.apache.org/dist/thrift/${THRIFT_VERSION}/thrift-${THRIFT_VERSION}.tar.gz\n+\n echo \"\n # Environment variables for offline Arrow build\n export ARROW_BOOST_URL=$_DST/boost.tar.gz\n@@ -81,4 +83,5 @@ export ARROW_ZSTD_URL=$_DST/zstd.tar.gz\n export ARROW_PROTOBUF_URL=$_DST/protobuf.tar.gz\n export ARROW_GRPC_URL=$_DST/grpc.tar.gz\n export ARROW_ORC_URL=$_DST/orc.tar.gz\n+export ARROW_THRIFT_URL=$_DST/thrift.tar.gz\n \"\ndiff --git a/cpp/thirdparty/versions.txt b/cpp/thirdparty/versions.txt\nindex 554c7196c2..e7ea231fec 100644\n--- a/cpp/thirdparty/versions.txt\n+++ b/cpp/thirdparty/versions.txt\n@@ -32,3 +32,4 @@ ZSTD_VERSION=1.2.0\n PROTOBUF_VERSION=2.6.0\n GRPC_VERSION=1.12.1\n ORC_VERSION=1.5.1\n+THRIFT_VERSION=0.11.0\ndiff --git a/dev/docker-compose.yml b/dev/docker-compose.yml\nindex 8f20dde2ed..c832cc3c61 100644\n--- a/dev/docker-compose.yml\n+++ b/dev/docker-compose.yml\n@@ -36,6 +36,12 @@ services:\n     links:\n       - hdfs-namenode:hdfs-namenode\n \n+  impala:\n+    image: cpcloud86/impala:java8-1\n+    ports:\n+      - \"21050\"\n+    hostname: impala\n+\n   hdfs_integration:\n     links:\n       - hdfs-namenode:hdfs-namenode\n@@ -49,6 +55,14 @@ services:\n     volumes:\n      - ../..:/apache-arrow\n \n+  hiveserver2:\n+    links:\n+      - impala\n+    build:\n+      context: hiveserver2\n+    volumes:\n+      - ../..:/apache-arrow\n+\n   spark_integration:\n     build:\n       context: spark_integration\ndiff --git a/dev/docker_common/Dockerfile.xenial.base b/dev/docker_common/Dockerfile.xenial.base\nnew file mode 100644\nindex 0000000000..b2589de6d9\n--- /dev/null\n+++ b/dev/docker_common/Dockerfile.xenial.base\n@@ -0,0 +1,61 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+FROM ubuntu:16.04\n+\n+# Basic OS utilities\n+RUN apt-get update \\\n+ && apt-get install -y \\\n+      wget \\\n+      git \\\n+      pkg-config \\\n+      build-essential \\\n+      software-properties-common \\\n+      ninja-build \\\n+ && apt-get clean\n+\n+ENV PATH=\"/opt/conda/bin:${PATH}\"\n+\n+# install conda in /home/ubuntu/miniconda\n+RUN wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O conda.sh \\\n+  && /bin/bash conda.sh -b -p /opt/conda \\\n+  && rm conda.sh \\\n+  && conda create -y -q -c conda-forge -n pyarrow-dev \\\n+      python=3.6 \\\n+      ipython \\\n+      nomkl \\\n+      numpy \\\n+      six \\\n+      setuptools \\\n+      cython \\\n+      pandas \\\n+      pytest \\\n+      cmake \\\n+      flatbuffers \\\n+      rapidjson \\\n+      boost-cpp \\\n+      thrift-cpp \\\n+      snappy \\\n+      zlib \\\n+      gflags \\\n+      brotli \\\n+      jemalloc \\\n+      lz4-c \\\n+      zstd \\\n+      setuptools \\\n+      setuptools_scm \\\n+ && conda clean --all\n\\ No newline at end of file\ndiff --git a/dev/docker_common/wait-for-it.sh b/dev/docker_common/wait-for-it.sh\nnew file mode 100755\nindex 0000000000..51ce816eb8\n--- /dev/null\n+++ b/dev/docker_common/wait-for-it.sh\n@@ -0,0 +1,199 @@\n+#!/usr/bin/env bash\n+\n+# The MIT License (MIT)\n+# Copyright (c) 2016 Giles Hall\n+#\n+# Permission is hereby granted, free of charge, to any person obtaining a copy of\n+# this software and associated documentation files (the \"Software\"), to deal in\n+# the Software without restriction, including without limitation the rights to\n+# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\n+# of the Software, and to permit persons to whom the Software is furnished to do\n+# so, subject to the following conditions:\n+#\n+# The above copyright notice and this permission notice shall be included in all\n+# copies or substantial portions of the Software.\n+#\n+# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n+# SOFTWARE.\n+#\n+#   Use this script to test if a given TCP host/port are available\n+\n+cmdname=$(basename $0)\n+\n+echoerr() { if [[ $QUIET -ne 1 ]]; then echo \"$@\" 1>&2; fi }\n+\n+usage()\n+{\n+    cat << USAGE >&2\n+Usage:\n+    $cmdname host:port [-s] [-t timeout] [-- command args]\n+    -h HOST | --host=HOST       Host or IP under test\n+    -p PORT | --port=PORT       TCP port under test\n+                                Alternatively, you specify the host and port as host:port\n+    -s | --strict               Only execute subcommand if the test succeeds\n+    -q | --quiet                Don't output any status messages\n+    -t TIMEOUT | --timeout=TIMEOUT\n+                                Timeout in seconds, zero for no timeout\n+    -- COMMAND ARGS             Execute command with args after the test finishes\n+USAGE\n+    exit 1\n+}\n+\n+wait_for()\n+{\n+    if [[ $TIMEOUT -gt 0 ]]; then\n+        echoerr \"$cmdname: waiting $TIMEOUT seconds for $HOST:$PORT\"\n+    else\n+        echoerr \"$cmdname: waiting for $HOST:$PORT without a timeout\"\n+    fi\n+    start_ts=$(date +%s)\n+    while :\n+    do\n+        if [[ $ISBUSY -eq 1 ]]; then\n+            nc -z $HOST $PORT\n+            result=$?\n+        else\n+            (echo > /dev/tcp/$HOST/$PORT) >/dev/null 2>&1\n+            result=$?\n+        fi\n+        if [[ $result -eq 0 ]]; then\n+            end_ts=$(date +%s)\n+            echoerr \"$cmdname: $HOST:$PORT is available after $((end_ts - start_ts)) seconds\"\n+            break\n+        fi\n+        sleep 1\n+    done\n+    return $result\n+}\n+\n+wait_for_wrapper()\n+{\n+    # In order to support SIGINT during timeout: http://unix.stackexchange.com/a/57692\n+    if [[ $QUIET -eq 1 ]]; then\n+        timeout $BUSYTIMEFLAG $TIMEOUT $0 --quiet --child --host=$HOST --port=$PORT --timeout=$TIMEOUT &\n+    else\n+        timeout $BUSYTIMEFLAG $TIMEOUT $0 --child --host=$HOST --port=$PORT --timeout=$TIMEOUT &\n+    fi\n+    PID=$!\n+    trap \"kill -INT -$PID\" INT\n+    wait $PID\n+    RESULT=$?\n+    if [[ $RESULT -ne 0 ]]; then\n+        echoerr \"$cmdname: timeout occurred after waiting $TIMEOUT seconds for $HOST:$PORT\"\n+    fi\n+    return $RESULT\n+}\n+\n+# process arguments\n+while [[ $# -gt 0 ]]\n+do\n+    case \"$1\" in\n+        *:* )\n+        hostport=(${1//:/ })\n+        HOST=${hostport[0]}\n+        PORT=${hostport[1]}\n+        shift 1\n+        ;;\n+        --child)\n+        CHILD=1\n+        shift 1\n+        ;;\n+        -q | --quiet)\n+        QUIET=1\n+        shift 1\n+        ;;\n+        -s | --strict)\n+        STRICT=1\n+        shift 1\n+        ;;\n+        -h)\n+        HOST=\"$2\"\n+        if [[ $HOST == \"\" ]]; then break; fi\n+        shift 2\n+        ;;\n+        --host=*)\n+        HOST=\"${1#*=}\"\n+        shift 1\n+        ;;\n+        -p)\n+        PORT=\"$2\"\n+        if [[ $PORT == \"\" ]]; then break; fi\n+        shift 2\n+        ;;\n+        --port=*)\n+        PORT=\"${1#*=}\"\n+        shift 1\n+        ;;\n+        -t)\n+        TIMEOUT=\"$2\"\n+        if [[ $TIMEOUT == \"\" ]]; then break; fi\n+        shift 2\n+        ;;\n+        --timeout=*)\n+        TIMEOUT=\"${1#*=}\"\n+        shift 1\n+        ;;\n+        --)\n+        shift\n+        CLI=(\"$@\")\n+        break\n+        ;;\n+        --help)\n+        usage\n+        ;;\n+        *)\n+        echoerr \"Unknown argument: $1\"\n+        usage\n+        ;;\n+    esac\n+done\n+\n+if [[ \"$HOST\" == \"\" || \"$PORT\" == \"\" ]]; then\n+    echoerr \"Error: you need to provide a host and port to test.\"\n+    usage\n+fi\n+\n+TIMEOUT=${TIMEOUT:-15}\n+STRICT=${STRICT:-0}\n+CHILD=${CHILD:-0}\n+QUIET=${QUIET:-0}\n+\n+# check to see if timeout is from busybox?\n+# check to see if timeout is from busybox?\n+TIMEOUT_PATH=$(realpath $(which timeout))\n+if [[ $TIMEOUT_PATH =~ \"busybox\" ]]; then\n+        ISBUSY=1\n+        BUSYTIMEFLAG=\"-t\"\n+else\n+        ISBUSY=0\n+        BUSYTIMEFLAG=\"\"\n+fi\n+\n+if [[ $CHILD -gt 0 ]]; then\n+    wait_for\n+    RESULT=$?\n+    exit $RESULT\n+else\n+    if [[ $TIMEOUT -gt 0 ]]; then\n+        wait_for_wrapper\n+        RESULT=$?\n+    else\n+        wait_for\n+        RESULT=$?\n+    fi\n+fi\n+\n+if [[ $CLI != \"\" ]]; then\n+    if [[ $RESULT -ne 0 && $STRICT -eq 1 ]]; then\n+        echoerr \"$cmdname: strict mode, refusing to execute subprocess\"\n+        exit $RESULT\n+    fi\n+    exec \"${CLI[@]}\"\n+else\n+    exit $RESULT\n+fi\ndiff --git a/dev/hiveserver2/Dockerfile b/dev/hiveserver2/Dockerfile\nnew file mode 100644\nindex 0000000000..36fa392475\n--- /dev/null\n+++ b/dev/hiveserver2/Dockerfile\n@@ -0,0 +1,23 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+FROM arrow_integration_xenial_base\n+\n+ADD . /apache-arrow\n+WORKDIR /apache-arrow\n+\n+CMD arrow/dev/hiveserver2/hiveserver2.sh\ndiff --git a/dev/hiveserver2/hiveserver2.sh b/dev/hiveserver2/hiveserver2.sh\nnew file mode 100755\nindex 0000000000..0ff649e032\n--- /dev/null\n+++ b/dev/hiveserver2/hiveserver2.sh\n@@ -0,0 +1,67 @@\n+#!/usr/bin/env bash\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# Exit on any error\n+set -e\n+\n+# cwd is mounted from host machine to\n+# and contains both arrow and parquet-cpp\n+\n+# Activate conda environment\n+source activate pyarrow-dev\n+\n+# Arrow build variables\n+export ARROW_BUILD_TYPE=debug\n+export ARROW_BUILD_TOOLCHAIN=$CONDA_PREFIX\n+export ARROW_HOME=$CONDA_PREFIX\n+\n+# For newer GCC per https://arrow.apache.org/docs/python/development.html#known-issues\n+export CXXFLAGS=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\n+export PYARROW_CXXFLAGS=$CXXFLAGS\n+export PYARROW_CMAKE_GENERATOR=Ninja\n+\n+_PWD=`pwd`\n+ARROW_CPP_BUILD_DIR=$_PWD/arrow/cpp/hiveserver2-build\n+DOCKER_COMMON_DIR=$_PWD/arrow/dev/docker_common\n+\n+function cleanup {\n+    rm -rf $ARROW_CPP_BUILD_DIR\n+}\n+\n+trap cleanup EXIT\n+\n+# Install arrow-cpp\n+mkdir -p $ARROW_CPP_BUILD_DIR\n+pushd $ARROW_CPP_BUILD_DIR\n+\n+cmake -GNinja \\\n+      -DCMAKE_BUILD_TYPE=$ARROW_BUILD_TYPE \\\n+      -DCMAKE_INSTALL_PREFIX=$ARROW_HOME \\\n+      -DARROW_HIVESERVER2=ON \\\n+      -DARROW_BUILD_TESTS=ON \\\n+      -DCMAKE_CXX_FLAGS=$CXXFLAGS \\\n+      ..\n+ninja hiveserver2-test\n+\n+$DOCKER_COMMON_DIR/wait-for-it.sh impala:21050 -t 300 -s -- echo \"impala is up\"\n+\n+# Run C++ unit tests\n+export ARROW_HIVESERVER2_TEST_HOST=impala\n+debug/hiveserver2-test\n+\n+popd\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-21T14:53:29.876+0000",
                    "updated": "2018-08-21T14:53:29.876+0000",
                    "started": "2018-08-21T14:53:29.876+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136545",
                    "issueId": "13178759"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 10200,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@78b682f0[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4e9cf5dc[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3252cbe1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@1b49ea62[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@46da69b5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@43deb7ed[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@677771b4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@33398dc2[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@26bbcdd1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@106c6734[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4cca4e5c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@33e28d9a[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 10200,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Aug 21 14:54:04 UTC 2018",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2018-08-21T14:54:04.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-3050/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2018-08-14T03:37:14.000+0000",
        "updated": "2018-08-21T14:54:07.000+0000",
        "timeoriginalestimate": null,
        "description": "I helped develop a small C++/Python library for interacting with databases like Hive and Impala via the HiveServer2 Thrift protocol and making them accessible to Python / pandas:\r\n\r\nhttps://github.com/cloudera/hs2client\r\n\r\nInternally this interfaces with HS2's own columnar representation. Arrow is a natural partner for this project, much of which could be discarded. I think Arrow would make as much sense as any place to develop this codebase further. It could be later split off into a new project if a large enough community develops\r\n\r\ncc [~twmarshall] [~mjacobs] for thoughts\r\n\r\nIf we did this, do we need to do a software grant (essentially what I'm proposing is to fork)? Can we just attribute the original Cloudera authors in LICENSE.txt?",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "2h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 10200
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Adopt HiveServer2 client C++ codebase",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/comment/16582222",
                    "id": "16582222",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=uwe",
                        "name": "uwe",
                        "key": "xhochy",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xhochy&avatarId=30652",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xhochy&avatarId=30652",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xhochy&avatarId=30652",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xhochy&avatarId=30652"
                        },
                        "displayName": "Uwe Korn",
                        "active": true,
                        "timeZone": "Europe/Berlin"
                    },
                    "body": "Hive is working on Arrow support for their connectors. Once that is in state to be developed against, this could then be used in hs2client. I think before that we should try to keep them separate to not overload the Arrow build.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=uwe",
                        "name": "uwe",
                        "key": "xhochy",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xhochy&avatarId=30652",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xhochy&avatarId=30652",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xhochy&avatarId=30652",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xhochy&avatarId=30652"
                        },
                        "displayName": "Uwe Korn",
                        "active": true,
                        "timeZone": "Europe/Berlin"
                    },
                    "created": "2018-08-16T08:52:39.703+0000",
                    "updated": "2018-08-16T08:52:39.703+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/comment/16582679",
                    "id": "16582679",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Makes sense. I think this supports the argument to bring hs2client and Arrow closer together. \r\n\r\nI'm not proposing to include this in Arrow's CI since the testing procedure (with Hive or Impala) is more complicated. \r\n\r\nI started a branch to do the integration work, when I have something worth looking at I will put up a PR",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-08-16T15:14:02.958+0000",
                    "updated": "2018-08-16T15:14:02.958+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/comment/16582681",
                    "id": "16582681",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "In progress: https://github.com/wesm/arrow/tree/hs2client-fork",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-08-16T15:14:56.296+0000",
                    "updated": "2018-08-16T15:14:56.296+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13178759/comment/16587545",
                    "id": "16587545",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Done in https://github.com/apache/arrow/commit/dbf531b17bd8f706f83192b4cf4f16be08047716",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-08-21T14:54:04.523+0000",
                    "updated": "2018-08-21T14:54:04.523+0000"
                }
            ],
            "maxResults": 4,
            "total": 4,
            "startAt": 0
        },
        "customfield_12311820": "0|i3x0jb:",
        "customfield_12314139": null
    }
}