{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13362740",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740",
    "key": "ARROW-11889",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12349983",
                "id": "12349983",
                "description": "",
                "name": "5.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-07-28"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 12600,
            "total": 12600,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 12600,
            "total": 12600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11889/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 21,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/612946",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace opened a new pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568\n\n\n   Still in draft for a moment but this converts the parser & decoder into map functions and then creates the streaming CSV reader as an async generator.  Parallel readahead is then added on top of the parser/decoder to allow for parallel reads.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-22T03:31:28.659+0000",
                    "updated": "2021-06-22T03:31:28.659+0000",
                    "started": "2021-06-22T03:31:28.659+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "612946",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/612947",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-865500987\n\n\n   https://issues.apache.org/jira/browse/ARROW-11889\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-22T03:31:55.761+0000",
                    "updated": "2021-06-22T03:31:55.761+0000",
                    "started": "2021-06-22T03:31:55.761+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "612947",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/613100",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-865500987\n\n\n   https://issues.apache.org/jira/browse/ARROW-11889\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-22T07:47:36.227+0000",
                    "updated": "2021-06-22T07:47:36.227+0000",
                    "started": "2021-06-22T07:47:36.227+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "613100",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/613158",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace opened a new pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568\n\n\n   Still in draft for a moment but this converts the parser & decoder into map functions and then creates the streaming CSV reader as an async generator.  Parallel readahead is then added on top of the parser/decoder to allow for parallel reads.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-22T07:54:26.495+0000",
                    "updated": "2021-06-22T07:54:26.495+0000",
                    "started": "2021-06-22T07:54:26.495+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "613158",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/614666",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-867874234\n\n\n   @pitrou @n3world This should probably wait until after ARROW-12996 is merged in.  I think it'd be easier to rebase ARROW-12996 into this PR than the other way around.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-24T18:50:20.253+0000",
                    "updated": "2021-06-24T18:50:20.253+0000",
                    "started": "2021-06-24T18:50:20.253+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614666",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/617469",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-872039323\n\n\n   I've rebased in the changes from #10509.  The behavior is only slightly different.  Opening the streaming CSV reader reads in the first record batch so the bytes_read will reflect that before any batch is read.  After that each time a batch is read in the next batch will be read in.  This means the read will not increment bytes_read.  If reading in parallel then bytes_read could potentially be even further ahead of the consumer since it will be doing decoding in readahead.  It should still match the spirit of the feature which is to report how many bytes have been decoded.\r\n   \r\n   @n3world @pitrou review is welcome.  The CI failure is unrelated.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-01T08:27:22.806+0000",
                    "updated": "2021-07-01T08:27:22.806+0000",
                    "started": "2021-07-01T08:27:22.805+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "617469",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/617957",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "n3world commented on a change in pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#discussion_r662678017\n\n\n\n##########\nFile path: cpp/src/arrow/csv/reader_test.cc\n##########\n@@ -227,37 +289,47 @@ TEST(StreamingReaderTest, BytesRead) {\n \n     auto read_options = ReadOptions::Defaults();\n     read_options.block_size = 20;\n+    read_options.use_threads = false;\n     ASSERT_OK_AND_ASSIGN(\n         auto streaming_reader,\n         StreamingReader::Make(io::default_io_context(), input, read_options,\n                               ParseOptions::Defaults(), ConvertOptions::Defaults()));\n     std::shared_ptr<RecordBatch> batch;\n-    int64_t bytes = 6;  // Size of header\n+    int64_t bytes = 18;  // Size of header and first batch\n     do {\n       ASSERT_EQ(bytes, streaming_reader->bytes_read());\n       ASSERT_OK(streaming_reader->ReadNext(&batch));\n       bytes += 12;  // Add size of each row\n-    } while (batch);\n+    } while (bytes <= 42);\n+    ASSERT_EQ(42, streaming_reader->bytes_read());\n+    // Should be able to read past the end without bumping bytes_read\n+    ASSERT_OK(streaming_reader->ReadNext(&batch));\n     ASSERT_EQ(42, streaming_reader->bytes_read());\n+    ASSERT_EQ(batch.get(), nullptr);\n   }\n \n   // Interaction of skip_rows and bytes_read()\n   {\n     auto input = std::make_shared<io::BufferReader>(table_buffer);\n \n     auto read_options = ReadOptions::Defaults();\n-    read_options.skip_rows = 2;\n+    read_options.skip_rows = 1;\n+    read_options.block_size = 32;\n     ASSERT_OK_AND_ASSIGN(\n         auto streaming_reader,\n         StreamingReader::Make(io::default_io_context(), input, read_options,\n                               ParseOptions::Defaults(), ConvertOptions::Defaults()));\n     std::shared_ptr<RecordBatch> batch;\n-    // first two rows and third row as header\n+    // Skip the actual header (6 bytes) and then treat first row as header (12 bytes)\n+    // and then streaming reader reads in first batch (12 bytes)\n     ASSERT_EQ(30, streaming_reader->bytes_read());\n     ASSERT_OK(streaming_reader->ReadNext(&batch));\n     ASSERT_NE(batch.get(), nullptr);\n     ASSERT_EQ(42, streaming_reader->bytes_read());\n     ASSERT_OK(streaming_reader->ReadNext(&batch));\n+    ASSERT_NE(batch.get(), nullptr);\n+    ASSERT_EQ(42, streaming_reader->bytes_read());\n+    ASSERT_OK(streaming_reader->ReadNext(&batch));\n     ASSERT_EQ(batch.get(), nullptr);\n\nReview comment:\n       I would say this changes the intent I had for `bytes_read()` when threads are used. The goal was to be able to report progress along with the batch. So that after a batch was retrieved with `ReadNext()` `bytes_read()` could be used to calculate the progress of this batch. In this example the second to last batch would be calculated as 100% complete and this can become more skewed with more read ahead a parallel processing. However with the futures you never know when the record batch is retrieved from the future making it impossible for `bytes_read()` to work that way.\r\n   \r\n   My only thought on how to solve this would be to have ReadNextAsync() or a new similar method return a Future on a pair where one of the values was the bytes read so that anybody who actually wants to associate progress with a batch will just use that API.\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -640,264 +815,112 @@ class BaseTableReader : public ReaderMixin, public csv::TableReader {\n /////////////////////////////////////////////////////////////////////////\n // Base class for streaming readers\n \n-class BaseStreamingReader : public ReaderMixin, public csv::StreamingReader {\n+class StreamingReaderImpl : public ReaderMixin,\n+                            public csv::StreamingReader,\n+                            public std::enable_shared_from_this<StreamingReaderImpl> {\n  public:\n-  BaseStreamingReader(io::IOContext io_context, Executor* cpu_executor,\n-                      std::shared_ptr<io::InputStream> input,\n+  StreamingReaderImpl(io::IOContext io_context, std::shared_ptr<io::InputStream> input,\n                       const ReadOptions& read_options, const ParseOptions& parse_options,\n                       const ConvertOptions& convert_options, bool count_rows)\n       : ReaderMixin(io_context, std::move(input), read_options, parse_options,\n                     convert_options, count_rows),\n-        cpu_executor_(cpu_executor) {}\n-\n-  virtual Future<std::shared_ptr<csv::StreamingReader>> Init() = 0;\n-\n-  std::shared_ptr<Schema> schema() const override { return schema_; }\n-\n-  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n-    auto next_fut = ReadNextAsync();\n-    auto next_result = next_fut.result();\n-    return std::move(next_result).Value(batch);\n-  }\n+        bytes_decoded_(0) {}\n \n- protected:\n-  // Make column decoders from conversion schema\n-  Status MakeColumnDecoders() {\n-    for (const auto& column : conversion_schema_.columns) {\n-      std::shared_ptr<ColumnDecoder> decoder;\n-      if (column.is_missing) {\n-        ARROW_ASSIGN_OR_RAISE(decoder, ColumnDecoder::MakeNull(io_context_.pool(),\n-                                                               column.type, task_group_));\n-      } else if (column.type != nullptr) {\n-        ARROW_ASSIGN_OR_RAISE(\n-            decoder, ColumnDecoder::Make(io_context_.pool(), column.type, column.index,\n-                                         convert_options_, task_group_));\n-      } else {\n-        ARROW_ASSIGN_OR_RAISE(decoder,\n-                              ColumnDecoder::Make(io_context_.pool(), column.index,\n-                                                  convert_options_, task_group_));\n-      }\n-      column_decoders_.push_back(std::move(decoder));\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<int64_t> ParseAndInsert(const std::shared_ptr<Buffer>& partial,\n-                                 const std::shared_ptr<Buffer>& completion,\n-                                 const std::shared_ptr<Buffer>& block,\n-                                 int64_t block_index, bool is_final) {\n-    ARROW_ASSIGN_OR_RAISE(auto result,\n-                          Parse(partial, completion, block, block_index, is_final));\n-    RETURN_NOT_OK(ProcessData(result.parser, block_index));\n-    return result.parsed_bytes;\n-  }\n-\n-  // Trigger conversion of parsed block data\n-  Status ProcessData(const std::shared_ptr<BlockParser>& parser, int64_t block_index) {\n-    for (auto& decoder : column_decoders_) {\n-      decoder->Insert(block_index, parser);\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<std::shared_ptr<RecordBatch>> DecodeNextBatch() {\n-    DCHECK(!column_decoders_.empty());\n-    ArrayVector arrays;\n-    arrays.reserve(column_decoders_.size());\n-    Status st;\n-    for (auto& decoder : column_decoders_) {\n-      auto maybe_array = decoder->NextChunk();\n-      if (!maybe_array.ok()) {\n-        // If there's an error, still fetch results from other decoders to\n-        // keep them in sync.\n-        st &= maybe_array.status();\n-      } else {\n-        arrays.push_back(*std::move(maybe_array));\n-      }\n-    }\n-    RETURN_NOT_OK(st);\n-    DCHECK_EQ(arrays.size(), column_decoders_.size());\n-    const bool is_null = (arrays[0] == nullptr);\n-#ifndef NDEBUG\n-    for (const auto& array : arrays) {\n-      DCHECK_EQ(array == nullptr, is_null);\n-    }\n-#endif\n-    if (is_null) {\n-      eof_ = true;\n-      return nullptr;\n-    }\n-\n-    if (schema_ == nullptr) {\n-      FieldVector fields(arrays.size());\n-      for (size_t i = 0; i < arrays.size(); ++i) {\n-        fields[i] = field(conversion_schema_.columns[i].name, arrays[i]->type());\n-      }\n-      schema_ = arrow::schema(std::move(fields));\n-    }\n-    const auto n_rows = arrays[0]->length();\n-    return RecordBatch::Make(schema_, n_rows, std::move(arrays));\n-  }\n-\n-  // Column decoders (in ConversionSchema order)\n-  std::vector<std::shared_ptr<ColumnDecoder>> column_decoders_;\n-  std::shared_ptr<Schema> schema_;\n-  std::shared_ptr<RecordBatch> pending_batch_;\n-  AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator_;\n-  Executor* cpu_executor_;\n-  bool eof_ = false;\n-};\n-\n-/////////////////////////////////////////////////////////////////////////\n-// Serial StreamingReader implementation\n-\n-class SerialStreamingReader : public BaseStreamingReader,\n-                              public std::enable_shared_from_this<SerialStreamingReader> {\n- public:\n-  using BaseStreamingReader::BaseStreamingReader;\n-\n-  Future<std::shared_ptr<csv::StreamingReader>> Init() override {\n+  Future<> Init(Executor* cpu_executor) {\n     ARROW_ASSIGN_OR_RAISE(auto istream_it,\n                           io::MakeInputStreamIterator(input_, read_options_.block_size));\n \n     // TODO Consider exposing readahead as a read option (ARROW-12090)\n     ARROW_ASSIGN_OR_RAISE(auto bg_it, MakeBackgroundGenerator(std::move(istream_it),\n                                                               io_context_.executor()));\n \n-    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor_);\n+    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor);\n \n-    buffer_generator_ = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n-    task_group_ = internal::TaskGroup::MakeSerial(io_context_.stop_token());\n+    auto buffer_generator = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n \n+    int max_readahead = cpu_executor->GetCapacity();\n     auto self = shared_from_this();\n-    // Read schema from first batch\n-    return ReadNextAsync(true).Then(\n-        [self](const std::shared_ptr<RecordBatch>& first_batch)\n-            -> Result<std::shared_ptr<csv::StreamingReader>> {\n-          self->pending_batch_ = first_batch;\n-          DCHECK_NE(self->schema_, nullptr);\n-          return self;\n-        });\n-  }\n \n-  Result<std::shared_ptr<RecordBatch>> DecodeBatchAndUpdateSchema() {\n-    auto maybe_batch = DecodeNextBatch();\n-    if (schema_ == nullptr && maybe_batch.ok()) {\n-      schema_ = (*maybe_batch)->schema();\n-    }\n-    return maybe_batch;\n+    return buffer_generator().Then([self, buffer_generator, max_readahead](\n+                                       const std::shared_ptr<Buffer>& first_buffer) {\n+      return self->InitAfterFirstBuffer(first_buffer, buffer_generator, max_readahead);\n+    });\n   }\n \n-  Future<std::shared_ptr<RecordBatch>> DoReadNext(\n-      std::shared_ptr<SerialStreamingReader> self) {\n-    auto batch = std::move(pending_batch_);\n-    if (batch != nullptr) {\n-      return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-    }\n+  std::shared_ptr<Schema> schema() const override { return schema_; }\n \n-    if (!source_eof_) {\n-      return block_generator_()\n-          .Then([self](const CSVBlock& maybe_block) -> Status {\n-            if (!IsIterationEnd(maybe_block)) {\n-              self->bytes_parsed_ += maybe_block.bytes_skipped;\n-              self->last_block_index_ = maybe_block.block_index;\n-              auto maybe_parsed = self->ParseAndInsert(\n-                  maybe_block.partial, maybe_block.completion, maybe_block.buffer,\n-                  maybe_block.block_index, maybe_block.is_final);\n-              if (!maybe_parsed.ok()) {\n-                // Parse error => bail out\n-                self->eof_ = true;\n-                return maybe_parsed.status();\n-              }\n-              self->bytes_parsed_ += *maybe_parsed;\n-              RETURN_NOT_OK(maybe_block.consume_bytes(*maybe_parsed));\n-            } else {\n-              self->source_eof_ = true;\n-              for (auto& decoder : self->column_decoders_) {\n-                decoder->SetEOF(self->last_block_index_ + 1);\n-              }\n-            }\n-            return Status::OK();\n-          })\n-          .Then([self]() -> Result<std::shared_ptr<RecordBatch>> {\n-            return self->DecodeBatchAndUpdateSchema();\n-          });\n-    }\n-    return Future<std::shared_ptr<RecordBatch>>::MakeFinished(\n-        DecodeBatchAndUpdateSchema());\n-  }\n+  int64_t bytes_read() const override { return bytes_decoded_.load(); }\n \n-  Future<std::shared_ptr<RecordBatch>> ReadNextSkippingEmpty(\n-      std::shared_ptr<SerialStreamingReader> self, bool internal_read) {\n-    return DoReadNext(self).Then(\n-        [self, internal_read](const std::shared_ptr<RecordBatch>& batch) {\n-          if (batch != nullptr && batch->num_rows() == 0) {\n-            return self->ReadNextSkippingEmpty(self, internal_read);\n-          }\n-          if (!internal_read) {\n-            self->bytes_decoded_ += self->bytes_parsed_;\n-            self->bytes_parsed_ = 0;\n-          }\n-          return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-        });\n+  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n+    auto next_fut = ReadNextAsync();\n+    auto next_result = next_fut.result();\n+    return std::move(next_result).Value(batch);\n   }\n \n   Future<std::shared_ptr<RecordBatch>> ReadNextAsync() override {\n-    return ReadNextAsync(false);\n-  };\n-\n-  int64_t bytes_read() const override { return bytes_decoded_; }\n+    return record_batch_gen_();\n+  }\n \n  protected:\n-  Future<> SetupReader(std::shared_ptr<SerialStreamingReader> self) {\n-    return buffer_generator_().Then([self](const std::shared_ptr<Buffer>& first_buffer) {\n-      if (first_buffer == nullptr) {\n-        return Status::Invalid(\"Empty CSV file\");\n-      }\n-      auto own_first_buffer = first_buffer;\n-      auto start = own_first_buffer->data();\n-      RETURN_NOT_OK(self->ProcessHeader(own_first_buffer, &own_first_buffer));\n-      self->bytes_decoded_ = own_first_buffer->data() - start;\n-      RETURN_NOT_OK(self->MakeColumnDecoders());\n-\n-      self->block_generator_ = SerialBlockReader::MakeAsyncIterator(\n-          std::move(self->buffer_generator_), MakeChunker(self->parse_options_),\n-          std::move(own_first_buffer), self->read_options_.skip_rows_after_names);\n-      return Status::OK();\n-    });\n+  Future<> InitAfterFirstBuffer(const std::shared_ptr<Buffer>& first_buffer,\n+                                AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator,\n+                                int max_readahead) {\n+    if (first_buffer == nullptr) {\n+      return Status::Invalid(\"Empty CSV file\");\n+    }\n+\n+    std::shared_ptr<Buffer> after_header;\n+    ARROW_ASSIGN_OR_RAISE(auto header_bytes_consumed,\n+                          ProcessHeader(first_buffer, &after_header));\n+    bytes_decoded_.fetch_add(header_bytes_consumed);\n+    auto parser_op =\n+        BlockParsingOperator(io_context_, parse_options_, num_csv_cols_, count_rows_);\n+    ARROW_ASSIGN_OR_RAISE(auto decoder_op, BlockDecodingOperator::Make(\n+                                               io_context_, convert_options_,\n+                                               conversion_schema_, &bytes_decoded_));\n+    auto block_gen = SerialBlockReader::MakeAsyncIterator(\n+        std::move(buffer_generator), MakeChunker(parse_options_), std::move(after_header),\n+        read_options_.skip_rows_after_names);\n+    auto parsed_block_gen =\n+        MakeMappedGenerator<ParsedBlock>(std::move(block_gen), std::move(parser_op));\n+    auto rb_gen = MakeMappedGenerator<std::shared_ptr<RecordBatch>>(\n+        std::move(parsed_block_gen), decoder_op);\n+    auto self = shared_from_this();\n+    return rb_gen().Then(\n+        [self, rb_gen, max_readahead](const std::shared_ptr<RecordBatch>& first_batch) {\n+          return self->InitAfterFirstBatch(first_batch, std::move(rb_gen), max_readahead);\n+        });\n   }\n \n-  Future<std::shared_ptr<RecordBatch>> ReadNextAsync(bool internal_read) {\n-    if (eof_) {\n-      return Future<std::shared_ptr<RecordBatch>>::MakeFinished(nullptr);\n-    }\n-    if (io_context_.stop_token().IsStopRequested()) {\n-      eof_ = true;\n-      return io_context_.stop_token().Poll();\n+  Status InitAfterFirstBatch(const std::shared_ptr<RecordBatch>& first_batch,\n+                             AsyncGenerator<std::shared_ptr<RecordBatch>> batch_gen,\n+                             int max_readahead) {\n+    schema_ = first_batch->schema();\n+\n+    AsyncGenerator<std::shared_ptr<RecordBatch>> readahead_gen;\n+    if (read_options_.use_threads) {\n+      readahead_gen = MakeReadaheadGenerator(std::move(batch_gen), max_readahead);\n+    } else {\n+      readahead_gen = std::move(batch_gen);\n     }\n-    auto self = shared_from_this();\n-    if (!block_generator_) {\n-      return SetupReader(self).Then(\n-          [self, internal_read]() -> Future<std::shared_ptr<RecordBatch>> {\n-            return self->ReadNextSkippingEmpty(self, internal_read);\n-          },\n-          [self](const Status& err) -> Result<std::shared_ptr<RecordBatch>> {\n-            self->eof_ = true;\n-            return err;\n-          });\n+\n+    AsyncGenerator<std::shared_ptr<RecordBatch>> restarted_gen;\n+    // Streaming reader should not emit empty record batches\n+    if (first_batch->num_rows() > 0) {\n+      restarted_gen = MakeGeneratorStartsWith({first_batch}, std::move(readahead_gen));\n     } else {\n-      return self->ReadNextSkippingEmpty(self, internal_read);\n+      restarted_gen = std::move(readahead_gen);\n     }\n+    record_batch_gen_ =\n+        MakeCancellable(std::move(restarted_gen), io_context_.stop_token());\n+    return Status::OK();\n   }\n \n-  bool source_eof_ = false;\n-  int64_t last_block_index_ = 0;\n-  AsyncGenerator<CSVBlock> block_generator_;\n-  // bytes of data parsed but not yet decoded\n-  int64_t bytes_parsed_ = 0;\n+  std::shared_ptr<Schema> schema_;\n+  AsyncGenerator<std::shared_ptr<RecordBatch>> record_batch_gen_;\n   // bytes which have been decoded for caller\n-  int64_t bytes_decoded_ = 0;\n-};\n+  std::atomic<int64_t> bytes_decoded_;\n+};  // namespace\n\nReview comment:\n       I don't think this is the end of a namespace\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -640,264 +815,112 @@ class BaseTableReader : public ReaderMixin, public csv::TableReader {\n /////////////////////////////////////////////////////////////////////////\n // Base class for streaming readers\n \n-class BaseStreamingReader : public ReaderMixin, public csv::StreamingReader {\n+class StreamingReaderImpl : public ReaderMixin,\n+                            public csv::StreamingReader,\n+                            public std::enable_shared_from_this<StreamingReaderImpl> {\n  public:\n-  BaseStreamingReader(io::IOContext io_context, Executor* cpu_executor,\n-                      std::shared_ptr<io::InputStream> input,\n+  StreamingReaderImpl(io::IOContext io_context, std::shared_ptr<io::InputStream> input,\n                       const ReadOptions& read_options, const ParseOptions& parse_options,\n                       const ConvertOptions& convert_options, bool count_rows)\n       : ReaderMixin(io_context, std::move(input), read_options, parse_options,\n                     convert_options, count_rows),\n-        cpu_executor_(cpu_executor) {}\n-\n-  virtual Future<std::shared_ptr<csv::StreamingReader>> Init() = 0;\n-\n-  std::shared_ptr<Schema> schema() const override { return schema_; }\n-\n-  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n-    auto next_fut = ReadNextAsync();\n-    auto next_result = next_fut.result();\n-    return std::move(next_result).Value(batch);\n-  }\n+        bytes_decoded_(0) {}\n \n- protected:\n-  // Make column decoders from conversion schema\n-  Status MakeColumnDecoders() {\n-    for (const auto& column : conversion_schema_.columns) {\n-      std::shared_ptr<ColumnDecoder> decoder;\n-      if (column.is_missing) {\n-        ARROW_ASSIGN_OR_RAISE(decoder, ColumnDecoder::MakeNull(io_context_.pool(),\n-                                                               column.type, task_group_));\n-      } else if (column.type != nullptr) {\n-        ARROW_ASSIGN_OR_RAISE(\n-            decoder, ColumnDecoder::Make(io_context_.pool(), column.type, column.index,\n-                                         convert_options_, task_group_));\n-      } else {\n-        ARROW_ASSIGN_OR_RAISE(decoder,\n-                              ColumnDecoder::Make(io_context_.pool(), column.index,\n-                                                  convert_options_, task_group_));\n-      }\n-      column_decoders_.push_back(std::move(decoder));\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<int64_t> ParseAndInsert(const std::shared_ptr<Buffer>& partial,\n-                                 const std::shared_ptr<Buffer>& completion,\n-                                 const std::shared_ptr<Buffer>& block,\n-                                 int64_t block_index, bool is_final) {\n-    ARROW_ASSIGN_OR_RAISE(auto result,\n-                          Parse(partial, completion, block, block_index, is_final));\n-    RETURN_NOT_OK(ProcessData(result.parser, block_index));\n-    return result.parsed_bytes;\n-  }\n-\n-  // Trigger conversion of parsed block data\n-  Status ProcessData(const std::shared_ptr<BlockParser>& parser, int64_t block_index) {\n-    for (auto& decoder : column_decoders_) {\n-      decoder->Insert(block_index, parser);\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<std::shared_ptr<RecordBatch>> DecodeNextBatch() {\n-    DCHECK(!column_decoders_.empty());\n-    ArrayVector arrays;\n-    arrays.reserve(column_decoders_.size());\n-    Status st;\n-    for (auto& decoder : column_decoders_) {\n-      auto maybe_array = decoder->NextChunk();\n-      if (!maybe_array.ok()) {\n-        // If there's an error, still fetch results from other decoders to\n-        // keep them in sync.\n-        st &= maybe_array.status();\n-      } else {\n-        arrays.push_back(*std::move(maybe_array));\n-      }\n-    }\n-    RETURN_NOT_OK(st);\n-    DCHECK_EQ(arrays.size(), column_decoders_.size());\n-    const bool is_null = (arrays[0] == nullptr);\n-#ifndef NDEBUG\n-    for (const auto& array : arrays) {\n-      DCHECK_EQ(array == nullptr, is_null);\n-    }\n-#endif\n-    if (is_null) {\n-      eof_ = true;\n-      return nullptr;\n-    }\n-\n-    if (schema_ == nullptr) {\n-      FieldVector fields(arrays.size());\n-      for (size_t i = 0; i < arrays.size(); ++i) {\n-        fields[i] = field(conversion_schema_.columns[i].name, arrays[i]->type());\n-      }\n-      schema_ = arrow::schema(std::move(fields));\n-    }\n-    const auto n_rows = arrays[0]->length();\n-    return RecordBatch::Make(schema_, n_rows, std::move(arrays));\n-  }\n-\n-  // Column decoders (in ConversionSchema order)\n-  std::vector<std::shared_ptr<ColumnDecoder>> column_decoders_;\n-  std::shared_ptr<Schema> schema_;\n-  std::shared_ptr<RecordBatch> pending_batch_;\n-  AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator_;\n-  Executor* cpu_executor_;\n-  bool eof_ = false;\n-};\n-\n-/////////////////////////////////////////////////////////////////////////\n-// Serial StreamingReader implementation\n-\n-class SerialStreamingReader : public BaseStreamingReader,\n-                              public std::enable_shared_from_this<SerialStreamingReader> {\n- public:\n-  using BaseStreamingReader::BaseStreamingReader;\n-\n-  Future<std::shared_ptr<csv::StreamingReader>> Init() override {\n+  Future<> Init(Executor* cpu_executor) {\n     ARROW_ASSIGN_OR_RAISE(auto istream_it,\n                           io::MakeInputStreamIterator(input_, read_options_.block_size));\n \n     // TODO Consider exposing readahead as a read option (ARROW-12090)\n     ARROW_ASSIGN_OR_RAISE(auto bg_it, MakeBackgroundGenerator(std::move(istream_it),\n                                                               io_context_.executor()));\n \n-    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor_);\n+    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor);\n \n-    buffer_generator_ = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n-    task_group_ = internal::TaskGroup::MakeSerial(io_context_.stop_token());\n+    auto buffer_generator = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n \n+    int max_readahead = cpu_executor->GetCapacity();\n     auto self = shared_from_this();\n-    // Read schema from first batch\n-    return ReadNextAsync(true).Then(\n-        [self](const std::shared_ptr<RecordBatch>& first_batch)\n-            -> Result<std::shared_ptr<csv::StreamingReader>> {\n-          self->pending_batch_ = first_batch;\n-          DCHECK_NE(self->schema_, nullptr);\n-          return self;\n-        });\n-  }\n \n-  Result<std::shared_ptr<RecordBatch>> DecodeBatchAndUpdateSchema() {\n-    auto maybe_batch = DecodeNextBatch();\n-    if (schema_ == nullptr && maybe_batch.ok()) {\n-      schema_ = (*maybe_batch)->schema();\n-    }\n-    return maybe_batch;\n+    return buffer_generator().Then([self, buffer_generator, max_readahead](\n+                                       const std::shared_ptr<Buffer>& first_buffer) {\n+      return self->InitAfterFirstBuffer(first_buffer, buffer_generator, max_readahead);\n+    });\n   }\n \n-  Future<std::shared_ptr<RecordBatch>> DoReadNext(\n-      std::shared_ptr<SerialStreamingReader> self) {\n-    auto batch = std::move(pending_batch_);\n-    if (batch != nullptr) {\n-      return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-    }\n+  std::shared_ptr<Schema> schema() const override { return schema_; }\n \n-    if (!source_eof_) {\n-      return block_generator_()\n-          .Then([self](const CSVBlock& maybe_block) -> Status {\n-            if (!IsIterationEnd(maybe_block)) {\n-              self->bytes_parsed_ += maybe_block.bytes_skipped;\n-              self->last_block_index_ = maybe_block.block_index;\n-              auto maybe_parsed = self->ParseAndInsert(\n-                  maybe_block.partial, maybe_block.completion, maybe_block.buffer,\n-                  maybe_block.block_index, maybe_block.is_final);\n-              if (!maybe_parsed.ok()) {\n-                // Parse error => bail out\n-                self->eof_ = true;\n-                return maybe_parsed.status();\n-              }\n-              self->bytes_parsed_ += *maybe_parsed;\n-              RETURN_NOT_OK(maybe_block.consume_bytes(*maybe_parsed));\n-            } else {\n-              self->source_eof_ = true;\n-              for (auto& decoder : self->column_decoders_) {\n-                decoder->SetEOF(self->last_block_index_ + 1);\n-              }\n-            }\n-            return Status::OK();\n-          })\n-          .Then([self]() -> Result<std::shared_ptr<RecordBatch>> {\n-            return self->DecodeBatchAndUpdateSchema();\n-          });\n-    }\n-    return Future<std::shared_ptr<RecordBatch>>::MakeFinished(\n-        DecodeBatchAndUpdateSchema());\n-  }\n+  int64_t bytes_read() const override { return bytes_decoded_.load(); }\n \n-  Future<std::shared_ptr<RecordBatch>> ReadNextSkippingEmpty(\n-      std::shared_ptr<SerialStreamingReader> self, bool internal_read) {\n-    return DoReadNext(self).Then(\n-        [self, internal_read](const std::shared_ptr<RecordBatch>& batch) {\n-          if (batch != nullptr && batch->num_rows() == 0) {\n-            return self->ReadNextSkippingEmpty(self, internal_read);\n-          }\n-          if (!internal_read) {\n-            self->bytes_decoded_ += self->bytes_parsed_;\n-            self->bytes_parsed_ = 0;\n-          }\n-          return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-        });\n+  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n+    auto next_fut = ReadNextAsync();\n+    auto next_result = next_fut.result();\n+    return std::move(next_result).Value(batch);\n   }\n \n   Future<std::shared_ptr<RecordBatch>> ReadNextAsync() override {\n-    return ReadNextAsync(false);\n-  };\n-\n-  int64_t bytes_read() const override { return bytes_decoded_; }\n+    return record_batch_gen_();\n+  }\n \n  protected:\n-  Future<> SetupReader(std::shared_ptr<SerialStreamingReader> self) {\n-    return buffer_generator_().Then([self](const std::shared_ptr<Buffer>& first_buffer) {\n-      if (first_buffer == nullptr) {\n-        return Status::Invalid(\"Empty CSV file\");\n-      }\n-      auto own_first_buffer = first_buffer;\n-      auto start = own_first_buffer->data();\n-      RETURN_NOT_OK(self->ProcessHeader(own_first_buffer, &own_first_buffer));\n-      self->bytes_decoded_ = own_first_buffer->data() - start;\n-      RETURN_NOT_OK(self->MakeColumnDecoders());\n-\n-      self->block_generator_ = SerialBlockReader::MakeAsyncIterator(\n-          std::move(self->buffer_generator_), MakeChunker(self->parse_options_),\n-          std::move(own_first_buffer), self->read_options_.skip_rows_after_names);\n-      return Status::OK();\n-    });\n+  Future<> InitAfterFirstBuffer(const std::shared_ptr<Buffer>& first_buffer,\n+                                AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator,\n+                                int max_readahead) {\n+    if (first_buffer == nullptr) {\n+      return Status::Invalid(\"Empty CSV file\");\n+    }\n+\n+    std::shared_ptr<Buffer> after_header;\n+    ARROW_ASSIGN_OR_RAISE(auto header_bytes_consumed,\n+                          ProcessHeader(first_buffer, &after_header));\n+    bytes_decoded_.fetch_add(header_bytes_consumed);\n+    auto parser_op =\n+        BlockParsingOperator(io_context_, parse_options_, num_csv_cols_, count_rows_);\n+    ARROW_ASSIGN_OR_RAISE(auto decoder_op, BlockDecodingOperator::Make(\n+                                               io_context_, convert_options_,\n+                                               conversion_schema_, &bytes_decoded_));\n+    auto block_gen = SerialBlockReader::MakeAsyncIterator(\n+        std::move(buffer_generator), MakeChunker(parse_options_), std::move(after_header),\n+        read_options_.skip_rows_after_names);\n+    auto parsed_block_gen =\n+        MakeMappedGenerator<ParsedBlock>(std::move(block_gen), std::move(parser_op));\n+    auto rb_gen = MakeMappedGenerator<std::shared_ptr<RecordBatch>>(\n+        std::move(parsed_block_gen), decoder_op);\n\nReview comment:\n       Could use `std::move(decoder_op)`\n\n##########\nFile path: python/pyarrow/tests/test_csv.py\n##########\n@@ -1507,8 +1488,8 @@ def test_stress_block_sizes(self):\n class TestSerialStreamingCSVRead(BaseTestStreamingCSVRead, unittest.TestCase):\n \n     def open_csv(self, *args, **kwargs):\n-        read_options = kwargs.setdefault('read_options', ReadOptions())\n-        read_options.use_threads = False\n+        # read_options = kwargs.setdefault('read_options', ReadOptions())\n+        # read_options.use_threads = False\n\nReview comment:\n       Is this still to test SerialStreamingCSV? Should there be two classes so that all test get run for serial and non serial?\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -546,6 +719,8 @@ class ReaderMixin {\n     return ParseResult{std::move(parser), static_cast<int64_t>(parsed_size)};\n   }\n \n+  friend class HeaderParsingOperator;\n\nReview comment:\n       What is this class?\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -347,6 +348,175 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+namespace csv {\n+\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n+    auto parser = std::make_shared<BlockParser>(\n+        io_context_.pool(), parse_options_, num_csv_cols_, num_rows_seen_, max_num_rows);\n+\n+    std::shared_ptr<Buffer> straddling;\n+    std::vector<util::string_view> views;\n+    if (block.partial->size() != 0 || block.completion->size() != 0) {\n+      if (block.partial->size() == 0) {\n+        straddling = block.completion;\n+      } else if (block.completion->size() == 0) {\n+        straddling = block.partial;\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            straddling,\n+            ConcatenateBuffers({block.partial, block.completion}, io_context_.pool()));\n+      }\n+      views = {util::string_view(*straddling), util::string_view(*block.buffer)};\n+    } else {\n+      views = {util::string_view(*block.buffer)};\n+    }\n+    uint32_t parsed_size;\n+    if (block.is_final) {\n+      RETURN_NOT_OK(parser->ParseFinal(views, &parsed_size));\n+    } else {\n+      RETURN_NOT_OK(parser->Parse(views, &parsed_size));\n+    }\n+    if (count_rows_) {\n+      num_rows_seen_ += parser->num_rows();\n+    }\n+    RETURN_NOT_OK(block.consume_bytes(parsed_size));\n+    return ParsedBlock{std::move(parser), block.block_index,\n+                       static_cast<int64_t>(parsed_size) + block.bytes_skipped};\n+  }\n+\n+ private:\n+  io::IOContext io_context_;\n+  ParseOptions parse_options_;\n+  int num_csv_cols_;\n+  bool count_rows_;\n+  int num_rows_seen_ = 0;\n+};\n+\n+class BlockDecodingOperator {\n+ public:\n+  Future<std::shared_ptr<RecordBatch>> operator()(const ParsedBlock& block) {\n+    DCHECK(!state_->column_decoders.empty());\n+    std::vector<Future<std::shared_ptr<Array>>> decoded_array_futs;\n+    for (auto& decoder : state_->column_decoders) {\n+      decoded_array_futs.push_back(decoder->Decode(block.parser));\n+    }\n+    auto bytes_parsed_or_skipped = block.bytes_parsed_or_skipped;\n+    auto decoded_arrays_fut = All(decoded_array_futs);\n+    auto state = state_;\n+    return decoded_arrays_fut.Then(\n+        [state, bytes_parsed_or_skipped](\n+            const std::vector<Result<std::shared_ptr<Array>>>& maybe_decoded_arrays)\n+            -> Result<std::shared_ptr<RecordBatch>> {\n+          state->bytes_decoded_->fetch_add(bytes_parsed_or_skipped);\n+          ARROW_ASSIGN_OR_RAISE(auto decoded_arrays,\n+                                internal::UnwrapOrRaise(maybe_decoded_arrays));\n+          return state->DecodedArraysToBatch(decoded_arrays);\n+        });\n+  }\n+\n+  static Result<BlockDecodingOperator> Make(io::IOContext io_context,\n+                                            ConvertOptions convert_options,\n+                                            ConversionSchema conversion_schema,\n+                                            std::atomic<int64_t>* bytes_decoded) {\n+    BlockDecodingOperator op(std::move(io_context), std::move(convert_options),\n+                             std::move(conversion_schema), bytes_decoded);\n+    RETURN_NOT_OK(op.state_->MakeColumnDecoders());\n+    return op;\n+  }\n+\n+ private:\n+  BlockDecodingOperator(io::IOContext io_context, ConvertOptions convert_options,\n+                        ConversionSchema conversion_schema,\n+                        std::atomic<int64_t>* bytes_decoded)\n+      : state_(std::make_shared<State>(std::move(io_context), std::move(convert_options),\n+                                       std::move(conversion_schema), bytes_decoded)) {}\n+\n+  struct State {\n+    State(io::IOContext io_context, ConvertOptions convert_options,\n+          ConversionSchema conversion_schema, std::atomic<int64_t>* bytes_decoded)\n+        : io_context(std::move(io_context)),\n+          convert_options(std::move(convert_options)),\n+          conversion_schema(std::move(conversion_schema)),\n+          bytes_decoded_(bytes_decoded) {}\n+\n+    Result<std::shared_ptr<RecordBatch>> DecodedArraysToBatch(\n+        std::vector<std::shared_ptr<Array>>& arrays) {\n+      if (schema == nullptr) {\n+        FieldVector fields(arrays.size());\n+        for (size_t i = 0; i < arrays.size(); ++i) {\n+          fields[i] = field(conversion_schema.columns[i].name, arrays[i]->type());\n+        }\n+        schema = arrow::schema(std::move(fields));\n+      }\n+      const auto n_rows = arrays[0]->length();\n+      return RecordBatch::Make(schema, n_rows, std::move(arrays));\n+    }\n+\n+    // Make column decoders from conversion schema\n+    Status MakeColumnDecoders() {\n+      for (const auto& column : conversion_schema.columns) {\n+        std::shared_ptr<ColumnDecoder> decoder;\n+        if (column.is_missing) {\n+          ARROW_ASSIGN_OR_RAISE(decoder,\n+                                ColumnDecoder::MakeNull(io_context.pool(), column.type));\n+        } else if (column.type != nullptr) {\n+          ARROW_ASSIGN_OR_RAISE(\n+              decoder, ColumnDecoder::Make(io_context.pool(), column.type, column.index,\n+                                           convert_options));\n+        } else {\n+          ARROW_ASSIGN_OR_RAISE(\n+              decoder,\n+              ColumnDecoder::Make(io_context.pool(), column.index, convert_options));\n+        }\n+        column_decoders.push_back(std::move(decoder));\n+      }\n+      return Status::OK();\n+    }\n+\n+    io::IOContext io_context;\n\nReview comment:\n       This is only used by MakeColumnDecoders and probably could be passed in as an argument to just that method.\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -347,6 +348,175 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+namespace csv {\n+\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n+    auto parser = std::make_shared<BlockParser>(\n+        io_context_.pool(), parse_options_, num_csv_cols_, num_rows_seen_, max_num_rows);\n+\n+    std::shared_ptr<Buffer> straddling;\n+    std::vector<util::string_view> views;\n+    if (block.partial->size() != 0 || block.completion->size() != 0) {\n+      if (block.partial->size() == 0) {\n+        straddling = block.completion;\n+      } else if (block.completion->size() == 0) {\n+        straddling = block.partial;\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            straddling,\n+            ConcatenateBuffers({block.partial, block.completion}, io_context_.pool()));\n+      }\n+      views = {util::string_view(*straddling), util::string_view(*block.buffer)};\n+    } else {\n+      views = {util::string_view(*block.buffer)};\n+    }\n+    uint32_t parsed_size;\n+    if (block.is_final) {\n+      RETURN_NOT_OK(parser->ParseFinal(views, &parsed_size));\n+    } else {\n+      RETURN_NOT_OK(parser->Parse(views, &parsed_size));\n+    }\n+    if (count_rows_) {\n+      num_rows_seen_ += parser->num_rows();\n+    }\n+    RETURN_NOT_OK(block.consume_bytes(parsed_size));\n+    return ParsedBlock{std::move(parser), block.block_index,\n+                       static_cast<int64_t>(parsed_size) + block.bytes_skipped};\n+  }\n+\n+ private:\n+  io::IOContext io_context_;\n+  ParseOptions parse_options_;\n+  int num_csv_cols_;\n+  bool count_rows_;\n+  int num_rows_seen_ = 0;\n+};\n+\n+class BlockDecodingOperator {\n+ public:\n+  Future<std::shared_ptr<RecordBatch>> operator()(const ParsedBlock& block) {\n+    DCHECK(!state_->column_decoders.empty());\n+    std::vector<Future<std::shared_ptr<Array>>> decoded_array_futs;\n+    for (auto& decoder : state_->column_decoders) {\n+      decoded_array_futs.push_back(decoder->Decode(block.parser));\n+    }\n+    auto bytes_parsed_or_skipped = block.bytes_parsed_or_skipped;\n+    auto decoded_arrays_fut = All(decoded_array_futs);\n+    auto state = state_;\n+    return decoded_arrays_fut.Then(\n+        [state, bytes_parsed_or_skipped](\n+            const std::vector<Result<std::shared_ptr<Array>>>& maybe_decoded_arrays)\n+            -> Result<std::shared_ptr<RecordBatch>> {\n+          state->bytes_decoded_->fetch_add(bytes_parsed_or_skipped);\n+          ARROW_ASSIGN_OR_RAISE(auto decoded_arrays,\n+                                internal::UnwrapOrRaise(maybe_decoded_arrays));\n+          return state->DecodedArraysToBatch(decoded_arrays);\n+        });\n+  }\n+\n+  static Result<BlockDecodingOperator> Make(io::IOContext io_context,\n+                                            ConvertOptions convert_options,\n+                                            ConversionSchema conversion_schema,\n+                                            std::atomic<int64_t>* bytes_decoded) {\n+    BlockDecodingOperator op(std::move(io_context), std::move(convert_options),\n+                             std::move(conversion_schema), bytes_decoded);\n+    RETURN_NOT_OK(op.state_->MakeColumnDecoders());\n+    return op;\n+  }\n+\n+ private:\n+  BlockDecodingOperator(io::IOContext io_context, ConvertOptions convert_options,\n+                        ConversionSchema conversion_schema,\n+                        std::atomic<int64_t>* bytes_decoded)\n+      : state_(std::make_shared<State>(std::move(io_context), std::move(convert_options),\n+                                       std::move(conversion_schema), bytes_decoded)) {}\n+\n+  struct State {\n+    State(io::IOContext io_context, ConvertOptions convert_options,\n+          ConversionSchema conversion_schema, std::atomic<int64_t>* bytes_decoded)\n+        : io_context(std::move(io_context)),\n+          convert_options(std::move(convert_options)),\n+          conversion_schema(std::move(conversion_schema)),\n+          bytes_decoded_(bytes_decoded) {}\n+\n+    Result<std::shared_ptr<RecordBatch>> DecodedArraysToBatch(\n+        std::vector<std::shared_ptr<Array>>& arrays) {\n+      if (schema == nullptr) {\n+        FieldVector fields(arrays.size());\n+        for (size_t i = 0; i < arrays.size(); ++i) {\n+          fields[i] = field(conversion_schema.columns[i].name, arrays[i]->type());\n+        }\n+        schema = arrow::schema(std::move(fields));\n+      }\n+      const auto n_rows = arrays[0]->length();\n+      return RecordBatch::Make(schema, n_rows, std::move(arrays));\n+    }\n+\n+    // Make column decoders from conversion schema\n+    Status MakeColumnDecoders() {\n+      for (const auto& column : conversion_schema.columns) {\n+        std::shared_ptr<ColumnDecoder> decoder;\n+        if (column.is_missing) {\n+          ARROW_ASSIGN_OR_RAISE(decoder,\n+                                ColumnDecoder::MakeNull(io_context.pool(), column.type));\n+        } else if (column.type != nullptr) {\n+          ARROW_ASSIGN_OR_RAISE(\n+              decoder, ColumnDecoder::Make(io_context.pool(), column.type, column.index,\n+                                           convert_options));\n+        } else {\n+          ARROW_ASSIGN_OR_RAISE(\n+              decoder,\n+              ColumnDecoder::Make(io_context.pool(), column.index, convert_options));\n+        }\n+        column_decoders.push_back(std::move(decoder));\n+      }\n+      return Status::OK();\n+    }\n+\n+    io::IOContext io_context;\n+    ConvertOptions convert_options;\n+    ConversionSchema conversion_schema;\n+    std::vector<std::shared_ptr<ColumnDecoder>> column_decoders;\n+    std::shared_ptr<Schema> schema;\n+    std::atomic<int64_t>* bytes_decoded_;\n\nReview comment:\n       Should the `_` suffix be removed here like the other struct members?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-02T02:51:35.106+0000",
                    "updated": "2021-07-02T02:51:35.106+0000",
                    "started": "2021-07-02T02:51:35.106+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "617957",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620307",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#discussion_r665828607\n\n\n\n##########\nFile path: cpp/src/arrow/csv/reader_test.cc\n##########\n@@ -227,37 +289,47 @@ TEST(StreamingReaderTest, BytesRead) {\n \n     auto read_options = ReadOptions::Defaults();\n     read_options.block_size = 20;\n+    read_options.use_threads = false;\n     ASSERT_OK_AND_ASSIGN(\n         auto streaming_reader,\n         StreamingReader::Make(io::default_io_context(), input, read_options,\n                               ParseOptions::Defaults(), ConvertOptions::Defaults()));\n     std::shared_ptr<RecordBatch> batch;\n-    int64_t bytes = 6;  // Size of header\n+    int64_t bytes = 18;  // Size of header and first batch\n     do {\n       ASSERT_EQ(bytes, streaming_reader->bytes_read());\n       ASSERT_OK(streaming_reader->ReadNext(&batch));\n       bytes += 12;  // Add size of each row\n-    } while (batch);\n+    } while (bytes <= 42);\n+    ASSERT_EQ(42, streaming_reader->bytes_read());\n+    // Should be able to read past the end without bumping bytes_read\n+    ASSERT_OK(streaming_reader->ReadNext(&batch));\n     ASSERT_EQ(42, streaming_reader->bytes_read());\n+    ASSERT_EQ(batch.get(), nullptr);\n   }\n \n   // Interaction of skip_rows and bytes_read()\n   {\n     auto input = std::make_shared<io::BufferReader>(table_buffer);\n \n     auto read_options = ReadOptions::Defaults();\n-    read_options.skip_rows = 2;\n+    read_options.skip_rows = 1;\n+    read_options.block_size = 32;\n     ASSERT_OK_AND_ASSIGN(\n         auto streaming_reader,\n         StreamingReader::Make(io::default_io_context(), input, read_options,\n                               ParseOptions::Defaults(), ConvertOptions::Defaults()));\n     std::shared_ptr<RecordBatch> batch;\n-    // first two rows and third row as header\n+    // Skip the actual header (6 bytes) and then treat first row as header (12 bytes)\n+    // and then streaming reader reads in first batch (12 bytes)\n     ASSERT_EQ(30, streaming_reader->bytes_read());\n     ASSERT_OK(streaming_reader->ReadNext(&batch));\n     ASSERT_NE(batch.get(), nullptr);\n     ASSERT_EQ(42, streaming_reader->bytes_read());\n     ASSERT_OK(streaming_reader->ReadNext(&batch));\n+    ASSERT_NE(batch.get(), nullptr);\n+    ASSERT_EQ(42, streaming_reader->bytes_read());\n+    ASSERT_OK(streaming_reader->ReadNext(&batch));\n     ASSERT_EQ(batch.get(), nullptr);\n\nReview comment:\n       I moved the increment of `decoded_bytes_` to be after the readahead.  So now...\r\n    * bytes_decoded_ will not be incremented until the reader asks for the batch\r\n    * The header bytes and skip before header are still marked read after Make (I think this is fair as they have been \"consumed\" by this point)\r\n    * Bytes skipped after the header are marked consumed after the first batch is delivered\r\n   \r\n   I think this is close enough to what you are after.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T02:39:53.750+0000",
                    "updated": "2021-07-08T02:39:53.750+0000",
                    "started": "2021-07-08T02:39:53.750+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620307",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620308",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#discussion_r665828703\n\n\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -640,264 +815,112 @@ class BaseTableReader : public ReaderMixin, public csv::TableReader {\n /////////////////////////////////////////////////////////////////////////\n // Base class for streaming readers\n \n-class BaseStreamingReader : public ReaderMixin, public csv::StreamingReader {\n+class StreamingReaderImpl : public ReaderMixin,\n+                            public csv::StreamingReader,\n+                            public std::enable_shared_from_this<StreamingReaderImpl> {\n  public:\n-  BaseStreamingReader(io::IOContext io_context, Executor* cpu_executor,\n-                      std::shared_ptr<io::InputStream> input,\n+  StreamingReaderImpl(io::IOContext io_context, std::shared_ptr<io::InputStream> input,\n                       const ReadOptions& read_options, const ParseOptions& parse_options,\n                       const ConvertOptions& convert_options, bool count_rows)\n       : ReaderMixin(io_context, std::move(input), read_options, parse_options,\n                     convert_options, count_rows),\n-        cpu_executor_(cpu_executor) {}\n-\n-  virtual Future<std::shared_ptr<csv::StreamingReader>> Init() = 0;\n-\n-  std::shared_ptr<Schema> schema() const override { return schema_; }\n-\n-  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n-    auto next_fut = ReadNextAsync();\n-    auto next_result = next_fut.result();\n-    return std::move(next_result).Value(batch);\n-  }\n+        bytes_decoded_(0) {}\n \n- protected:\n-  // Make column decoders from conversion schema\n-  Status MakeColumnDecoders() {\n-    for (const auto& column : conversion_schema_.columns) {\n-      std::shared_ptr<ColumnDecoder> decoder;\n-      if (column.is_missing) {\n-        ARROW_ASSIGN_OR_RAISE(decoder, ColumnDecoder::MakeNull(io_context_.pool(),\n-                                                               column.type, task_group_));\n-      } else if (column.type != nullptr) {\n-        ARROW_ASSIGN_OR_RAISE(\n-            decoder, ColumnDecoder::Make(io_context_.pool(), column.type, column.index,\n-                                         convert_options_, task_group_));\n-      } else {\n-        ARROW_ASSIGN_OR_RAISE(decoder,\n-                              ColumnDecoder::Make(io_context_.pool(), column.index,\n-                                                  convert_options_, task_group_));\n-      }\n-      column_decoders_.push_back(std::move(decoder));\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<int64_t> ParseAndInsert(const std::shared_ptr<Buffer>& partial,\n-                                 const std::shared_ptr<Buffer>& completion,\n-                                 const std::shared_ptr<Buffer>& block,\n-                                 int64_t block_index, bool is_final) {\n-    ARROW_ASSIGN_OR_RAISE(auto result,\n-                          Parse(partial, completion, block, block_index, is_final));\n-    RETURN_NOT_OK(ProcessData(result.parser, block_index));\n-    return result.parsed_bytes;\n-  }\n-\n-  // Trigger conversion of parsed block data\n-  Status ProcessData(const std::shared_ptr<BlockParser>& parser, int64_t block_index) {\n-    for (auto& decoder : column_decoders_) {\n-      decoder->Insert(block_index, parser);\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<std::shared_ptr<RecordBatch>> DecodeNextBatch() {\n-    DCHECK(!column_decoders_.empty());\n-    ArrayVector arrays;\n-    arrays.reserve(column_decoders_.size());\n-    Status st;\n-    for (auto& decoder : column_decoders_) {\n-      auto maybe_array = decoder->NextChunk();\n-      if (!maybe_array.ok()) {\n-        // If there's an error, still fetch results from other decoders to\n-        // keep them in sync.\n-        st &= maybe_array.status();\n-      } else {\n-        arrays.push_back(*std::move(maybe_array));\n-      }\n-    }\n-    RETURN_NOT_OK(st);\n-    DCHECK_EQ(arrays.size(), column_decoders_.size());\n-    const bool is_null = (arrays[0] == nullptr);\n-#ifndef NDEBUG\n-    for (const auto& array : arrays) {\n-      DCHECK_EQ(array == nullptr, is_null);\n-    }\n-#endif\n-    if (is_null) {\n-      eof_ = true;\n-      return nullptr;\n-    }\n-\n-    if (schema_ == nullptr) {\n-      FieldVector fields(arrays.size());\n-      for (size_t i = 0; i < arrays.size(); ++i) {\n-        fields[i] = field(conversion_schema_.columns[i].name, arrays[i]->type());\n-      }\n-      schema_ = arrow::schema(std::move(fields));\n-    }\n-    const auto n_rows = arrays[0]->length();\n-    return RecordBatch::Make(schema_, n_rows, std::move(arrays));\n-  }\n-\n-  // Column decoders (in ConversionSchema order)\n-  std::vector<std::shared_ptr<ColumnDecoder>> column_decoders_;\n-  std::shared_ptr<Schema> schema_;\n-  std::shared_ptr<RecordBatch> pending_batch_;\n-  AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator_;\n-  Executor* cpu_executor_;\n-  bool eof_ = false;\n-};\n-\n-/////////////////////////////////////////////////////////////////////////\n-// Serial StreamingReader implementation\n-\n-class SerialStreamingReader : public BaseStreamingReader,\n-                              public std::enable_shared_from_this<SerialStreamingReader> {\n- public:\n-  using BaseStreamingReader::BaseStreamingReader;\n-\n-  Future<std::shared_ptr<csv::StreamingReader>> Init() override {\n+  Future<> Init(Executor* cpu_executor) {\n     ARROW_ASSIGN_OR_RAISE(auto istream_it,\n                           io::MakeInputStreamIterator(input_, read_options_.block_size));\n \n     // TODO Consider exposing readahead as a read option (ARROW-12090)\n     ARROW_ASSIGN_OR_RAISE(auto bg_it, MakeBackgroundGenerator(std::move(istream_it),\n                                                               io_context_.executor()));\n \n-    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor_);\n+    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor);\n \n-    buffer_generator_ = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n-    task_group_ = internal::TaskGroup::MakeSerial(io_context_.stop_token());\n+    auto buffer_generator = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n \n+    int max_readahead = cpu_executor->GetCapacity();\n     auto self = shared_from_this();\n-    // Read schema from first batch\n-    return ReadNextAsync(true).Then(\n-        [self](const std::shared_ptr<RecordBatch>& first_batch)\n-            -> Result<std::shared_ptr<csv::StreamingReader>> {\n-          self->pending_batch_ = first_batch;\n-          DCHECK_NE(self->schema_, nullptr);\n-          return self;\n-        });\n-  }\n \n-  Result<std::shared_ptr<RecordBatch>> DecodeBatchAndUpdateSchema() {\n-    auto maybe_batch = DecodeNextBatch();\n-    if (schema_ == nullptr && maybe_batch.ok()) {\n-      schema_ = (*maybe_batch)->schema();\n-    }\n-    return maybe_batch;\n+    return buffer_generator().Then([self, buffer_generator, max_readahead](\n+                                       const std::shared_ptr<Buffer>& first_buffer) {\n+      return self->InitAfterFirstBuffer(first_buffer, buffer_generator, max_readahead);\n+    });\n   }\n \n-  Future<std::shared_ptr<RecordBatch>> DoReadNext(\n-      std::shared_ptr<SerialStreamingReader> self) {\n-    auto batch = std::move(pending_batch_);\n-    if (batch != nullptr) {\n-      return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-    }\n+  std::shared_ptr<Schema> schema() const override { return schema_; }\n \n-    if (!source_eof_) {\n-      return block_generator_()\n-          .Then([self](const CSVBlock& maybe_block) -> Status {\n-            if (!IsIterationEnd(maybe_block)) {\n-              self->bytes_parsed_ += maybe_block.bytes_skipped;\n-              self->last_block_index_ = maybe_block.block_index;\n-              auto maybe_parsed = self->ParseAndInsert(\n-                  maybe_block.partial, maybe_block.completion, maybe_block.buffer,\n-                  maybe_block.block_index, maybe_block.is_final);\n-              if (!maybe_parsed.ok()) {\n-                // Parse error => bail out\n-                self->eof_ = true;\n-                return maybe_parsed.status();\n-              }\n-              self->bytes_parsed_ += *maybe_parsed;\n-              RETURN_NOT_OK(maybe_block.consume_bytes(*maybe_parsed));\n-            } else {\n-              self->source_eof_ = true;\n-              for (auto& decoder : self->column_decoders_) {\n-                decoder->SetEOF(self->last_block_index_ + 1);\n-              }\n-            }\n-            return Status::OK();\n-          })\n-          .Then([self]() -> Result<std::shared_ptr<RecordBatch>> {\n-            return self->DecodeBatchAndUpdateSchema();\n-          });\n-    }\n-    return Future<std::shared_ptr<RecordBatch>>::MakeFinished(\n-        DecodeBatchAndUpdateSchema());\n-  }\n+  int64_t bytes_read() const override { return bytes_decoded_.load(); }\n \n-  Future<std::shared_ptr<RecordBatch>> ReadNextSkippingEmpty(\n-      std::shared_ptr<SerialStreamingReader> self, bool internal_read) {\n-    return DoReadNext(self).Then(\n-        [self, internal_read](const std::shared_ptr<RecordBatch>& batch) {\n-          if (batch != nullptr && batch->num_rows() == 0) {\n-            return self->ReadNextSkippingEmpty(self, internal_read);\n-          }\n-          if (!internal_read) {\n-            self->bytes_decoded_ += self->bytes_parsed_;\n-            self->bytes_parsed_ = 0;\n-          }\n-          return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-        });\n+  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n+    auto next_fut = ReadNextAsync();\n+    auto next_result = next_fut.result();\n+    return std::move(next_result).Value(batch);\n   }\n \n   Future<std::shared_ptr<RecordBatch>> ReadNextAsync() override {\n-    return ReadNextAsync(false);\n-  };\n-\n-  int64_t bytes_read() const override { return bytes_decoded_; }\n+    return record_batch_gen_();\n+  }\n \n  protected:\n-  Future<> SetupReader(std::shared_ptr<SerialStreamingReader> self) {\n-    return buffer_generator_().Then([self](const std::shared_ptr<Buffer>& first_buffer) {\n-      if (first_buffer == nullptr) {\n-        return Status::Invalid(\"Empty CSV file\");\n-      }\n-      auto own_first_buffer = first_buffer;\n-      auto start = own_first_buffer->data();\n-      RETURN_NOT_OK(self->ProcessHeader(own_first_buffer, &own_first_buffer));\n-      self->bytes_decoded_ = own_first_buffer->data() - start;\n-      RETURN_NOT_OK(self->MakeColumnDecoders());\n-\n-      self->block_generator_ = SerialBlockReader::MakeAsyncIterator(\n-          std::move(self->buffer_generator_), MakeChunker(self->parse_options_),\n-          std::move(own_first_buffer), self->read_options_.skip_rows_after_names);\n-      return Status::OK();\n-    });\n+  Future<> InitAfterFirstBuffer(const std::shared_ptr<Buffer>& first_buffer,\n+                                AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator,\n+                                int max_readahead) {\n+    if (first_buffer == nullptr) {\n+      return Status::Invalid(\"Empty CSV file\");\n+    }\n+\n+    std::shared_ptr<Buffer> after_header;\n+    ARROW_ASSIGN_OR_RAISE(auto header_bytes_consumed,\n+                          ProcessHeader(first_buffer, &after_header));\n+    bytes_decoded_.fetch_add(header_bytes_consumed);\n+    auto parser_op =\n+        BlockParsingOperator(io_context_, parse_options_, num_csv_cols_, count_rows_);\n+    ARROW_ASSIGN_OR_RAISE(auto decoder_op, BlockDecodingOperator::Make(\n+                                               io_context_, convert_options_,\n+                                               conversion_schema_, &bytes_decoded_));\n+    auto block_gen = SerialBlockReader::MakeAsyncIterator(\n+        std::move(buffer_generator), MakeChunker(parse_options_), std::move(after_header),\n+        read_options_.skip_rows_after_names);\n+    auto parsed_block_gen =\n+        MakeMappedGenerator<ParsedBlock>(std::move(block_gen), std::move(parser_op));\n+    auto rb_gen = MakeMappedGenerator<std::shared_ptr<RecordBatch>>(\n+        std::move(parsed_block_gen), decoder_op);\n+    auto self = shared_from_this();\n+    return rb_gen().Then(\n+        [self, rb_gen, max_readahead](const std::shared_ptr<RecordBatch>& first_batch) {\n+          return self->InitAfterFirstBatch(first_batch, std::move(rb_gen), max_readahead);\n+        });\n   }\n \n-  Future<std::shared_ptr<RecordBatch>> ReadNextAsync(bool internal_read) {\n-    if (eof_) {\n-      return Future<std::shared_ptr<RecordBatch>>::MakeFinished(nullptr);\n-    }\n-    if (io_context_.stop_token().IsStopRequested()) {\n-      eof_ = true;\n-      return io_context_.stop_token().Poll();\n+  Status InitAfterFirstBatch(const std::shared_ptr<RecordBatch>& first_batch,\n+                             AsyncGenerator<std::shared_ptr<RecordBatch>> batch_gen,\n+                             int max_readahead) {\n+    schema_ = first_batch->schema();\n+\n+    AsyncGenerator<std::shared_ptr<RecordBatch>> readahead_gen;\n+    if (read_options_.use_threads) {\n+      readahead_gen = MakeReadaheadGenerator(std::move(batch_gen), max_readahead);\n+    } else {\n+      readahead_gen = std::move(batch_gen);\n     }\n-    auto self = shared_from_this();\n-    if (!block_generator_) {\n-      return SetupReader(self).Then(\n-          [self, internal_read]() -> Future<std::shared_ptr<RecordBatch>> {\n-            return self->ReadNextSkippingEmpty(self, internal_read);\n-          },\n-          [self](const Status& err) -> Result<std::shared_ptr<RecordBatch>> {\n-            self->eof_ = true;\n-            return err;\n-          });\n+\n+    AsyncGenerator<std::shared_ptr<RecordBatch>> restarted_gen;\n+    // Streaming reader should not emit empty record batches\n+    if (first_batch->num_rows() > 0) {\n+      restarted_gen = MakeGeneratorStartsWith({first_batch}, std::move(readahead_gen));\n     } else {\n-      return self->ReadNextSkippingEmpty(self, internal_read);\n+      restarted_gen = std::move(readahead_gen);\n     }\n+    record_batch_gen_ =\n+        MakeCancellable(std::move(restarted_gen), io_context_.stop_token());\n+    return Status::OK();\n   }\n \n-  bool source_eof_ = false;\n-  int64_t last_block_index_ = 0;\n-  AsyncGenerator<CSVBlock> block_generator_;\n-  // bytes of data parsed but not yet decoded\n-  int64_t bytes_parsed_ = 0;\n+  std::shared_ptr<Schema> schema_;\n+  AsyncGenerator<std::shared_ptr<RecordBatch>> record_batch_gen_;\n   // bytes which have been decoded for caller\n-  int64_t bytes_decoded_ = 0;\n-};\n+  std::atomic<int64_t> bytes_decoded_;\n+};  // namespace\n\nReview comment:\n       Oops.  Removed.\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -640,264 +815,112 @@ class BaseTableReader : public ReaderMixin, public csv::TableReader {\n /////////////////////////////////////////////////////////////////////////\n // Base class for streaming readers\n \n-class BaseStreamingReader : public ReaderMixin, public csv::StreamingReader {\n+class StreamingReaderImpl : public ReaderMixin,\n+                            public csv::StreamingReader,\n+                            public std::enable_shared_from_this<StreamingReaderImpl> {\n  public:\n-  BaseStreamingReader(io::IOContext io_context, Executor* cpu_executor,\n-                      std::shared_ptr<io::InputStream> input,\n+  StreamingReaderImpl(io::IOContext io_context, std::shared_ptr<io::InputStream> input,\n                       const ReadOptions& read_options, const ParseOptions& parse_options,\n                       const ConvertOptions& convert_options, bool count_rows)\n       : ReaderMixin(io_context, std::move(input), read_options, parse_options,\n                     convert_options, count_rows),\n-        cpu_executor_(cpu_executor) {}\n-\n-  virtual Future<std::shared_ptr<csv::StreamingReader>> Init() = 0;\n-\n-  std::shared_ptr<Schema> schema() const override { return schema_; }\n-\n-  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n-    auto next_fut = ReadNextAsync();\n-    auto next_result = next_fut.result();\n-    return std::move(next_result).Value(batch);\n-  }\n+        bytes_decoded_(0) {}\n \n- protected:\n-  // Make column decoders from conversion schema\n-  Status MakeColumnDecoders() {\n-    for (const auto& column : conversion_schema_.columns) {\n-      std::shared_ptr<ColumnDecoder> decoder;\n-      if (column.is_missing) {\n-        ARROW_ASSIGN_OR_RAISE(decoder, ColumnDecoder::MakeNull(io_context_.pool(),\n-                                                               column.type, task_group_));\n-      } else if (column.type != nullptr) {\n-        ARROW_ASSIGN_OR_RAISE(\n-            decoder, ColumnDecoder::Make(io_context_.pool(), column.type, column.index,\n-                                         convert_options_, task_group_));\n-      } else {\n-        ARROW_ASSIGN_OR_RAISE(decoder,\n-                              ColumnDecoder::Make(io_context_.pool(), column.index,\n-                                                  convert_options_, task_group_));\n-      }\n-      column_decoders_.push_back(std::move(decoder));\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<int64_t> ParseAndInsert(const std::shared_ptr<Buffer>& partial,\n-                                 const std::shared_ptr<Buffer>& completion,\n-                                 const std::shared_ptr<Buffer>& block,\n-                                 int64_t block_index, bool is_final) {\n-    ARROW_ASSIGN_OR_RAISE(auto result,\n-                          Parse(partial, completion, block, block_index, is_final));\n-    RETURN_NOT_OK(ProcessData(result.parser, block_index));\n-    return result.parsed_bytes;\n-  }\n-\n-  // Trigger conversion of parsed block data\n-  Status ProcessData(const std::shared_ptr<BlockParser>& parser, int64_t block_index) {\n-    for (auto& decoder : column_decoders_) {\n-      decoder->Insert(block_index, parser);\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<std::shared_ptr<RecordBatch>> DecodeNextBatch() {\n-    DCHECK(!column_decoders_.empty());\n-    ArrayVector arrays;\n-    arrays.reserve(column_decoders_.size());\n-    Status st;\n-    for (auto& decoder : column_decoders_) {\n-      auto maybe_array = decoder->NextChunk();\n-      if (!maybe_array.ok()) {\n-        // If there's an error, still fetch results from other decoders to\n-        // keep them in sync.\n-        st &= maybe_array.status();\n-      } else {\n-        arrays.push_back(*std::move(maybe_array));\n-      }\n-    }\n-    RETURN_NOT_OK(st);\n-    DCHECK_EQ(arrays.size(), column_decoders_.size());\n-    const bool is_null = (arrays[0] == nullptr);\n-#ifndef NDEBUG\n-    for (const auto& array : arrays) {\n-      DCHECK_EQ(array == nullptr, is_null);\n-    }\n-#endif\n-    if (is_null) {\n-      eof_ = true;\n-      return nullptr;\n-    }\n-\n-    if (schema_ == nullptr) {\n-      FieldVector fields(arrays.size());\n-      for (size_t i = 0; i < arrays.size(); ++i) {\n-        fields[i] = field(conversion_schema_.columns[i].name, arrays[i]->type());\n-      }\n-      schema_ = arrow::schema(std::move(fields));\n-    }\n-    const auto n_rows = arrays[0]->length();\n-    return RecordBatch::Make(schema_, n_rows, std::move(arrays));\n-  }\n-\n-  // Column decoders (in ConversionSchema order)\n-  std::vector<std::shared_ptr<ColumnDecoder>> column_decoders_;\n-  std::shared_ptr<Schema> schema_;\n-  std::shared_ptr<RecordBatch> pending_batch_;\n-  AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator_;\n-  Executor* cpu_executor_;\n-  bool eof_ = false;\n-};\n-\n-/////////////////////////////////////////////////////////////////////////\n-// Serial StreamingReader implementation\n-\n-class SerialStreamingReader : public BaseStreamingReader,\n-                              public std::enable_shared_from_this<SerialStreamingReader> {\n- public:\n-  using BaseStreamingReader::BaseStreamingReader;\n-\n-  Future<std::shared_ptr<csv::StreamingReader>> Init() override {\n+  Future<> Init(Executor* cpu_executor) {\n     ARROW_ASSIGN_OR_RAISE(auto istream_it,\n                           io::MakeInputStreamIterator(input_, read_options_.block_size));\n \n     // TODO Consider exposing readahead as a read option (ARROW-12090)\n     ARROW_ASSIGN_OR_RAISE(auto bg_it, MakeBackgroundGenerator(std::move(istream_it),\n                                                               io_context_.executor()));\n \n-    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor_);\n+    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor);\n \n-    buffer_generator_ = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n-    task_group_ = internal::TaskGroup::MakeSerial(io_context_.stop_token());\n+    auto buffer_generator = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n \n+    int max_readahead = cpu_executor->GetCapacity();\n     auto self = shared_from_this();\n-    // Read schema from first batch\n-    return ReadNextAsync(true).Then(\n-        [self](const std::shared_ptr<RecordBatch>& first_batch)\n-            -> Result<std::shared_ptr<csv::StreamingReader>> {\n-          self->pending_batch_ = first_batch;\n-          DCHECK_NE(self->schema_, nullptr);\n-          return self;\n-        });\n-  }\n \n-  Result<std::shared_ptr<RecordBatch>> DecodeBatchAndUpdateSchema() {\n-    auto maybe_batch = DecodeNextBatch();\n-    if (schema_ == nullptr && maybe_batch.ok()) {\n-      schema_ = (*maybe_batch)->schema();\n-    }\n-    return maybe_batch;\n+    return buffer_generator().Then([self, buffer_generator, max_readahead](\n+                                       const std::shared_ptr<Buffer>& first_buffer) {\n+      return self->InitAfterFirstBuffer(first_buffer, buffer_generator, max_readahead);\n+    });\n   }\n \n-  Future<std::shared_ptr<RecordBatch>> DoReadNext(\n-      std::shared_ptr<SerialStreamingReader> self) {\n-    auto batch = std::move(pending_batch_);\n-    if (batch != nullptr) {\n-      return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-    }\n+  std::shared_ptr<Schema> schema() const override { return schema_; }\n \n-    if (!source_eof_) {\n-      return block_generator_()\n-          .Then([self](const CSVBlock& maybe_block) -> Status {\n-            if (!IsIterationEnd(maybe_block)) {\n-              self->bytes_parsed_ += maybe_block.bytes_skipped;\n-              self->last_block_index_ = maybe_block.block_index;\n-              auto maybe_parsed = self->ParseAndInsert(\n-                  maybe_block.partial, maybe_block.completion, maybe_block.buffer,\n-                  maybe_block.block_index, maybe_block.is_final);\n-              if (!maybe_parsed.ok()) {\n-                // Parse error => bail out\n-                self->eof_ = true;\n-                return maybe_parsed.status();\n-              }\n-              self->bytes_parsed_ += *maybe_parsed;\n-              RETURN_NOT_OK(maybe_block.consume_bytes(*maybe_parsed));\n-            } else {\n-              self->source_eof_ = true;\n-              for (auto& decoder : self->column_decoders_) {\n-                decoder->SetEOF(self->last_block_index_ + 1);\n-              }\n-            }\n-            return Status::OK();\n-          })\n-          .Then([self]() -> Result<std::shared_ptr<RecordBatch>> {\n-            return self->DecodeBatchAndUpdateSchema();\n-          });\n-    }\n-    return Future<std::shared_ptr<RecordBatch>>::MakeFinished(\n-        DecodeBatchAndUpdateSchema());\n-  }\n+  int64_t bytes_read() const override { return bytes_decoded_.load(); }\n \n-  Future<std::shared_ptr<RecordBatch>> ReadNextSkippingEmpty(\n-      std::shared_ptr<SerialStreamingReader> self, bool internal_read) {\n-    return DoReadNext(self).Then(\n-        [self, internal_read](const std::shared_ptr<RecordBatch>& batch) {\n-          if (batch != nullptr && batch->num_rows() == 0) {\n-            return self->ReadNextSkippingEmpty(self, internal_read);\n-          }\n-          if (!internal_read) {\n-            self->bytes_decoded_ += self->bytes_parsed_;\n-            self->bytes_parsed_ = 0;\n-          }\n-          return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-        });\n+  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n+    auto next_fut = ReadNextAsync();\n+    auto next_result = next_fut.result();\n+    return std::move(next_result).Value(batch);\n   }\n \n   Future<std::shared_ptr<RecordBatch>> ReadNextAsync() override {\n-    return ReadNextAsync(false);\n-  };\n-\n-  int64_t bytes_read() const override { return bytes_decoded_; }\n+    return record_batch_gen_();\n+  }\n \n  protected:\n-  Future<> SetupReader(std::shared_ptr<SerialStreamingReader> self) {\n-    return buffer_generator_().Then([self](const std::shared_ptr<Buffer>& first_buffer) {\n-      if (first_buffer == nullptr) {\n-        return Status::Invalid(\"Empty CSV file\");\n-      }\n-      auto own_first_buffer = first_buffer;\n-      auto start = own_first_buffer->data();\n-      RETURN_NOT_OK(self->ProcessHeader(own_first_buffer, &own_first_buffer));\n-      self->bytes_decoded_ = own_first_buffer->data() - start;\n-      RETURN_NOT_OK(self->MakeColumnDecoders());\n-\n-      self->block_generator_ = SerialBlockReader::MakeAsyncIterator(\n-          std::move(self->buffer_generator_), MakeChunker(self->parse_options_),\n-          std::move(own_first_buffer), self->read_options_.skip_rows_after_names);\n-      return Status::OK();\n-    });\n+  Future<> InitAfterFirstBuffer(const std::shared_ptr<Buffer>& first_buffer,\n+                                AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator,\n+                                int max_readahead) {\n+    if (first_buffer == nullptr) {\n+      return Status::Invalid(\"Empty CSV file\");\n+    }\n+\n+    std::shared_ptr<Buffer> after_header;\n+    ARROW_ASSIGN_OR_RAISE(auto header_bytes_consumed,\n+                          ProcessHeader(first_buffer, &after_header));\n+    bytes_decoded_.fetch_add(header_bytes_consumed);\n+    auto parser_op =\n+        BlockParsingOperator(io_context_, parse_options_, num_csv_cols_, count_rows_);\n+    ARROW_ASSIGN_OR_RAISE(auto decoder_op, BlockDecodingOperator::Make(\n+                                               io_context_, convert_options_,\n+                                               conversion_schema_, &bytes_decoded_));\n+    auto block_gen = SerialBlockReader::MakeAsyncIterator(\n+        std::move(buffer_generator), MakeChunker(parse_options_), std::move(after_header),\n+        read_options_.skip_rows_after_names);\n+    auto parsed_block_gen =\n+        MakeMappedGenerator<ParsedBlock>(std::move(block_gen), std::move(parser_op));\n+    auto rb_gen = MakeMappedGenerator<std::shared_ptr<RecordBatch>>(\n+        std::move(parsed_block_gen), decoder_op);\n\nReview comment:\n       Thanks, added.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T02:40:19.832+0000",
                    "updated": "2021-07-08T02:40:19.832+0000",
                    "started": "2021-07-08T02:40:19.832+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620308",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620309",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#discussion_r665828836\n\n\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -347,6 +348,175 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+namespace csv {\n+\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n+    auto parser = std::make_shared<BlockParser>(\n+        io_context_.pool(), parse_options_, num_csv_cols_, num_rows_seen_, max_num_rows);\n+\n+    std::shared_ptr<Buffer> straddling;\n+    std::vector<util::string_view> views;\n+    if (block.partial->size() != 0 || block.completion->size() != 0) {\n+      if (block.partial->size() == 0) {\n+        straddling = block.completion;\n+      } else if (block.completion->size() == 0) {\n+        straddling = block.partial;\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            straddling,\n+            ConcatenateBuffers({block.partial, block.completion}, io_context_.pool()));\n+      }\n+      views = {util::string_view(*straddling), util::string_view(*block.buffer)};\n+    } else {\n+      views = {util::string_view(*block.buffer)};\n+    }\n+    uint32_t parsed_size;\n+    if (block.is_final) {\n+      RETURN_NOT_OK(parser->ParseFinal(views, &parsed_size));\n+    } else {\n+      RETURN_NOT_OK(parser->Parse(views, &parsed_size));\n+    }\n+    if (count_rows_) {\n+      num_rows_seen_ += parser->num_rows();\n+    }\n+    RETURN_NOT_OK(block.consume_bytes(parsed_size));\n+    return ParsedBlock{std::move(parser), block.block_index,\n+                       static_cast<int64_t>(parsed_size) + block.bytes_skipped};\n+  }\n+\n+ private:\n+  io::IOContext io_context_;\n+  ParseOptions parse_options_;\n+  int num_csv_cols_;\n+  bool count_rows_;\n+  int num_rows_seen_ = 0;\n+};\n+\n+class BlockDecodingOperator {\n+ public:\n+  Future<std::shared_ptr<RecordBatch>> operator()(const ParsedBlock& block) {\n+    DCHECK(!state_->column_decoders.empty());\n+    std::vector<Future<std::shared_ptr<Array>>> decoded_array_futs;\n+    for (auto& decoder : state_->column_decoders) {\n+      decoded_array_futs.push_back(decoder->Decode(block.parser));\n+    }\n+    auto bytes_parsed_or_skipped = block.bytes_parsed_or_skipped;\n+    auto decoded_arrays_fut = All(decoded_array_futs);\n+    auto state = state_;\n+    return decoded_arrays_fut.Then(\n+        [state, bytes_parsed_or_skipped](\n+            const std::vector<Result<std::shared_ptr<Array>>>& maybe_decoded_arrays)\n+            -> Result<std::shared_ptr<RecordBatch>> {\n+          state->bytes_decoded_->fetch_add(bytes_parsed_or_skipped);\n+          ARROW_ASSIGN_OR_RAISE(auto decoded_arrays,\n+                                internal::UnwrapOrRaise(maybe_decoded_arrays));\n+          return state->DecodedArraysToBatch(decoded_arrays);\n+        });\n+  }\n+\n+  static Result<BlockDecodingOperator> Make(io::IOContext io_context,\n+                                            ConvertOptions convert_options,\n+                                            ConversionSchema conversion_schema,\n+                                            std::atomic<int64_t>* bytes_decoded) {\n+    BlockDecodingOperator op(std::move(io_context), std::move(convert_options),\n+                             std::move(conversion_schema), bytes_decoded);\n+    RETURN_NOT_OK(op.state_->MakeColumnDecoders());\n+    return op;\n+  }\n+\n+ private:\n+  BlockDecodingOperator(io::IOContext io_context, ConvertOptions convert_options,\n+                        ConversionSchema conversion_schema,\n+                        std::atomic<int64_t>* bytes_decoded)\n+      : state_(std::make_shared<State>(std::move(io_context), std::move(convert_options),\n+                                       std::move(conversion_schema), bytes_decoded)) {}\n+\n+  struct State {\n+    State(io::IOContext io_context, ConvertOptions convert_options,\n+          ConversionSchema conversion_schema, std::atomic<int64_t>* bytes_decoded)\n+        : io_context(std::move(io_context)),\n+          convert_options(std::move(convert_options)),\n+          conversion_schema(std::move(conversion_schema)),\n+          bytes_decoded_(bytes_decoded) {}\n+\n+    Result<std::shared_ptr<RecordBatch>> DecodedArraysToBatch(\n+        std::vector<std::shared_ptr<Array>>& arrays) {\n+      if (schema == nullptr) {\n+        FieldVector fields(arrays.size());\n+        for (size_t i = 0; i < arrays.size(); ++i) {\n+          fields[i] = field(conversion_schema.columns[i].name, arrays[i]->type());\n+        }\n+        schema = arrow::schema(std::move(fields));\n+      }\n+      const auto n_rows = arrays[0]->length();\n+      return RecordBatch::Make(schema, n_rows, std::move(arrays));\n+    }\n+\n+    // Make column decoders from conversion schema\n+    Status MakeColumnDecoders() {\n+      for (const auto& column : conversion_schema.columns) {\n+        std::shared_ptr<ColumnDecoder> decoder;\n+        if (column.is_missing) {\n+          ARROW_ASSIGN_OR_RAISE(decoder,\n+                                ColumnDecoder::MakeNull(io_context.pool(), column.type));\n+        } else if (column.type != nullptr) {\n+          ARROW_ASSIGN_OR_RAISE(\n+              decoder, ColumnDecoder::Make(io_context.pool(), column.type, column.index,\n+                                           convert_options));\n+        } else {\n+          ARROW_ASSIGN_OR_RAISE(\n+              decoder,\n+              ColumnDecoder::Make(io_context.pool(), column.index, convert_options));\n+        }\n+        column_decoders.push_back(std::move(decoder));\n+      }\n+      return Status::OK();\n+    }\n+\n+    io::IOContext io_context;\n+    ConvertOptions convert_options;\n+    ConversionSchema conversion_schema;\n+    std::vector<std::shared_ptr<ColumnDecoder>> column_decoders;\n+    std::shared_ptr<Schema> schema;\n+    std::atomic<int64_t>* bytes_decoded_;\n\nReview comment:\n       I ended up getting rid of this member but yes, it should have been removed.\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -347,6 +348,175 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+namespace csv {\n+\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n+    auto parser = std::make_shared<BlockParser>(\n+        io_context_.pool(), parse_options_, num_csv_cols_, num_rows_seen_, max_num_rows);\n+\n+    std::shared_ptr<Buffer> straddling;\n+    std::vector<util::string_view> views;\n+    if (block.partial->size() != 0 || block.completion->size() != 0) {\n+      if (block.partial->size() == 0) {\n+        straddling = block.completion;\n+      } else if (block.completion->size() == 0) {\n+        straddling = block.partial;\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            straddling,\n+            ConcatenateBuffers({block.partial, block.completion}, io_context_.pool()));\n+      }\n+      views = {util::string_view(*straddling), util::string_view(*block.buffer)};\n+    } else {\n+      views = {util::string_view(*block.buffer)};\n+    }\n+    uint32_t parsed_size;\n+    if (block.is_final) {\n+      RETURN_NOT_OK(parser->ParseFinal(views, &parsed_size));\n+    } else {\n+      RETURN_NOT_OK(parser->Parse(views, &parsed_size));\n+    }\n+    if (count_rows_) {\n+      num_rows_seen_ += parser->num_rows();\n+    }\n+    RETURN_NOT_OK(block.consume_bytes(parsed_size));\n+    return ParsedBlock{std::move(parser), block.block_index,\n+                       static_cast<int64_t>(parsed_size) + block.bytes_skipped};\n+  }\n+\n+ private:\n+  io::IOContext io_context_;\n+  ParseOptions parse_options_;\n+  int num_csv_cols_;\n+  bool count_rows_;\n+  int num_rows_seen_ = 0;\n+};\n+\n+class BlockDecodingOperator {\n+ public:\n+  Future<std::shared_ptr<RecordBatch>> operator()(const ParsedBlock& block) {\n+    DCHECK(!state_->column_decoders.empty());\n+    std::vector<Future<std::shared_ptr<Array>>> decoded_array_futs;\n+    for (auto& decoder : state_->column_decoders) {\n+      decoded_array_futs.push_back(decoder->Decode(block.parser));\n+    }\n+    auto bytes_parsed_or_skipped = block.bytes_parsed_or_skipped;\n+    auto decoded_arrays_fut = All(decoded_array_futs);\n+    auto state = state_;\n+    return decoded_arrays_fut.Then(\n+        [state, bytes_parsed_or_skipped](\n+            const std::vector<Result<std::shared_ptr<Array>>>& maybe_decoded_arrays)\n+            -> Result<std::shared_ptr<RecordBatch>> {\n+          state->bytes_decoded_->fetch_add(bytes_parsed_or_skipped);\n+          ARROW_ASSIGN_OR_RAISE(auto decoded_arrays,\n+                                internal::UnwrapOrRaise(maybe_decoded_arrays));\n+          return state->DecodedArraysToBatch(decoded_arrays);\n+        });\n+  }\n+\n+  static Result<BlockDecodingOperator> Make(io::IOContext io_context,\n+                                            ConvertOptions convert_options,\n+                                            ConversionSchema conversion_schema,\n+                                            std::atomic<int64_t>* bytes_decoded) {\n+    BlockDecodingOperator op(std::move(io_context), std::move(convert_options),\n+                             std::move(conversion_schema), bytes_decoded);\n+    RETURN_NOT_OK(op.state_->MakeColumnDecoders());\n+    return op;\n+  }\n+\n+ private:\n+  BlockDecodingOperator(io::IOContext io_context, ConvertOptions convert_options,\n+                        ConversionSchema conversion_schema,\n+                        std::atomic<int64_t>* bytes_decoded)\n+      : state_(std::make_shared<State>(std::move(io_context), std::move(convert_options),\n+                                       std::move(conversion_schema), bytes_decoded)) {}\n+\n+  struct State {\n+    State(io::IOContext io_context, ConvertOptions convert_options,\n+          ConversionSchema conversion_schema, std::atomic<int64_t>* bytes_decoded)\n+        : io_context(std::move(io_context)),\n+          convert_options(std::move(convert_options)),\n+          conversion_schema(std::move(conversion_schema)),\n+          bytes_decoded_(bytes_decoded) {}\n+\n+    Result<std::shared_ptr<RecordBatch>> DecodedArraysToBatch(\n+        std::vector<std::shared_ptr<Array>>& arrays) {\n+      if (schema == nullptr) {\n+        FieldVector fields(arrays.size());\n+        for (size_t i = 0; i < arrays.size(); ++i) {\n+          fields[i] = field(conversion_schema.columns[i].name, arrays[i]->type());\n+        }\n+        schema = arrow::schema(std::move(fields));\n+      }\n+      const auto n_rows = arrays[0]->length();\n+      return RecordBatch::Make(schema, n_rows, std::move(arrays));\n+    }\n+\n+    // Make column decoders from conversion schema\n+    Status MakeColumnDecoders() {\n+      for (const auto& column : conversion_schema.columns) {\n+        std::shared_ptr<ColumnDecoder> decoder;\n+        if (column.is_missing) {\n+          ARROW_ASSIGN_OR_RAISE(decoder,\n+                                ColumnDecoder::MakeNull(io_context.pool(), column.type));\n+        } else if (column.type != nullptr) {\n+          ARROW_ASSIGN_OR_RAISE(\n+              decoder, ColumnDecoder::Make(io_context.pool(), column.type, column.index,\n+                                           convert_options));\n+        } else {\n+          ARROW_ASSIGN_OR_RAISE(\n+              decoder,\n+              ColumnDecoder::Make(io_context.pool(), column.index, convert_options));\n+        }\n+        column_decoders.push_back(std::move(decoder));\n+      }\n+      return Status::OK();\n+    }\n+\n+    io::IOContext io_context;\n\nReview comment:\n       Good observation, I've done this.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T02:40:44.199+0000",
                    "updated": "2021-07-08T02:40:44.199+0000",
                    "started": "2021-07-08T02:40:44.198+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620309",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620310",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#discussion_r665828969\n\n\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -546,6 +719,8 @@ class ReaderMixin {\n     return ParseResult{std::move(parser), static_cast<int64_t>(parsed_size)};\n   }\n \n+  friend class HeaderParsingOperator;\n\nReview comment:\n       Leftover from a previous implementation.  Removed.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T02:40:57.466+0000",
                    "updated": "2021-07-08T02:40:57.466+0000",
                    "started": "2021-07-08T02:40:57.466+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620310",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620311",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#discussion_r665829229\n\n\n\n##########\nFile path: python/pyarrow/tests/test_csv.py\n##########\n@@ -1507,8 +1488,8 @@ def test_stress_block_sizes(self):\n class TestSerialStreamingCSVRead(BaseTestStreamingCSVRead, unittest.TestCase):\n \n     def open_csv(self, *args, **kwargs):\n-        read_options = kwargs.setdefault('read_options', ReadOptions())\n-        read_options.use_threads = False\n+        # read_options = kwargs.setdefault('read_options', ReadOptions())\n+        # read_options.use_threads = False\n\nReview comment:\n       Ah, good point (and clumsy of me to leave those comments in there).  I've changed up the test so now there is no base class and every test is parameterized on use_threads=True/False.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T02:41:40.757+0000",
                    "updated": "2021-07-08T02:41:40.757+0000",
                    "started": "2021-07-08T02:41:40.757+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620311",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620318",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-876084196\n\n\n   Thanks for the feedback @n3world.  I think I was able to update `bytes_read` to align with your use case a little better.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T03:04:01.547+0000",
                    "updated": "2021-07-08T03:04:01.547+0000",
                    "started": "2021-07-08T03:04:01.547+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620318",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620319",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-876084717\n\n\n   @ursabot please benchmark\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T03:05:31.289+0000",
                    "updated": "2021-07-08T03:05:31.289+0000",
                    "started": "2021-07-08T03:05:31.288+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620319",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620320",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot commented on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-876085503\n\n\n   Benchmark runs are scheduled for baseline = cf6a7ff65f4e2920641d116a3ba1f578b2bd8a9e and contender = cd899de2debcd7ccb0c8d1e3f7840a3cebf77742. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Scheduled] [ec2-t3-xlarge-us-east-2 (mimalloc)](https://conbench.ursa.dev/compare/runs/96acb784c87842b2bfd28e17a9a90432...306db50e487d4f3b96c24478a545ae90/)\n   [Scheduled] [ursa-i9-9960x (mimalloc)](https://conbench.ursa.dev/compare/runs/5744b64d14d448198229be1dbb5265e7...92dae5529b9744579798c8f74c668e6f/)\n   [Scheduled] [ursa-thinkcentre-m75q (mimalloc)](https://conbench.ursa.dev/compare/runs/cd0a0e80ad2c4de0b60cda38c58b64a4...1f2427e345fe40779a5ba8f8978af8bc/)\n   Supported benchmarks:\n   ursa-i9-9960x: langs = Python, R\n   ursa-thinkcentre-m75q: langs = C++, Java\n   ec2-t3-xlarge-us-east-2: cloud = True\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T03:06:25.222+0000",
                    "updated": "2021-07-08T03:06:25.222+0000",
                    "started": "2021-07-08T03:06:25.222+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620320",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620323",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot edited a comment on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-876085503\n\n\n   Benchmark runs are scheduled for baseline = cf6a7ff65f4e2920641d116a3ba1f578b2bd8a9e and contender = cd899de2debcd7ccb0c8d1e3f7840a3cebf77742. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ec2-t3-xlarge-us-east-2 (mimalloc)](https://conbench.ursa.dev/compare/runs/96acb784c87842b2bfd28e17a9a90432...306db50e487d4f3b96c24478a545ae90/)\n   [Scheduled] [ursa-i9-9960x (mimalloc)](https://conbench.ursa.dev/compare/runs/5744b64d14d448198229be1dbb5265e7...92dae5529b9744579798c8f74c668e6f/)\n   [Scheduled] [ursa-thinkcentre-m75q (mimalloc)](https://conbench.ursa.dev/compare/runs/cd0a0e80ad2c4de0b60cda38c58b64a4...1f2427e345fe40779a5ba8f8978af8bc/)\n   Supported benchmarks:\n   ursa-i9-9960x: langs = Python, R\n   ursa-thinkcentre-m75q: langs = C++, Java\n   ec2-t3-xlarge-us-east-2: cloud = True\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T03:18:15.777+0000",
                    "updated": "2021-07-08T03:18:15.777+0000",
                    "started": "2021-07-08T03:18:15.777+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620323",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620340",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot edited a comment on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-876085503\n\n\n   Benchmark runs are scheduled for baseline = cf6a7ff65f4e2920641d116a3ba1f578b2bd8a9e and contender = cd899de2debcd7ccb0c8d1e3f7840a3cebf77742. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ec2-t3-xlarge-us-east-2 (mimalloc)](https://conbench.ursa.dev/compare/runs/96acb784c87842b2bfd28e17a9a90432...306db50e487d4f3b96c24478a545ae90/)\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ursa-i9-9960x (mimalloc)](https://conbench.ursa.dev/compare/runs/5744b64d14d448198229be1dbb5265e7...92dae5529b9744579798c8f74c668e6f/)\n   [Scheduled] [ursa-thinkcentre-m75q (mimalloc)](https://conbench.ursa.dev/compare/runs/cd0a0e80ad2c4de0b60cda38c58b64a4...1f2427e345fe40779a5ba8f8978af8bc/)\n   Supported benchmarks:\n   ursa-i9-9960x: langs = Python, R\n   ursa-thinkcentre-m75q: langs = C++, Java\n   ec2-t3-xlarge-us-east-2: cloud = True\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T04:39:20.449+0000",
                    "updated": "2021-07-08T04:39:20.449+0000",
                    "started": "2021-07-08T04:39:20.449+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620340",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/620342",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot edited a comment on pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#issuecomment-876085503\n\n\n   Benchmark runs are scheduled for baseline = cf6a7ff65f4e2920641d116a3ba1f578b2bd8a9e and contender = cd899de2debcd7ccb0c8d1e3f7840a3cebf77742. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ec2-t3-xlarge-us-east-2 (mimalloc)](https://conbench.ursa.dev/compare/runs/96acb784c87842b2bfd28e17a9a90432...306db50e487d4f3b96c24478a545ae90/)\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ursa-i9-9960x (mimalloc)](https://conbench.ursa.dev/compare/runs/5744b64d14d448198229be1dbb5265e7...92dae5529b9744579798c8f74c668e6f/)\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ursa-thinkcentre-m75q (mimalloc)](https://conbench.ursa.dev/compare/runs/cd0a0e80ad2c4de0b60cda38c58b64a4...1f2427e345fe40779a5ba8f8978af8bc/)\n   Supported benchmarks:\n   ursa-i9-9960x: langs = Python, R\n   ursa-thinkcentre-m75q: langs = C++, Java\n   ec2-t3-xlarge-us-east-2: cloud = True\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-08T04:50:29.604+0000",
                    "updated": "2021-07-08T04:50:29.604+0000",
                    "started": "2021-07-08T04:50:29.604+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "620342",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/623297",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#discussion_r669094247\n\n\n\n##########\nFile path: cpp/src/arrow/csv/column_decoder.cc\n##########\n@@ -142,49 +64,38 @@ class ConcreteColumnDecoder : public ColumnDecoder {\n \n   MemoryPool* pool_;\n   int32_t col_index_;\n-\n-  std::vector<Future<std::shared_ptr<Array>>> chunks_;\n-  int64_t num_chunks_;\n-  int64_t next_chunk_;\n-\n-  std::mutex mutex_;\n+  internal::Executor* executor_;\n };\n \n //////////////////////////////////////////////////////////////////////////\n // Null column decoder implementation (for a column not in the CSV file)\n \n class NullColumnDecoder : public ConcreteColumnDecoder {\n  public:\n-  explicit NullColumnDecoder(const std::shared_ptr<DataType>& type, MemoryPool* pool,\n-                             const std::shared_ptr<internal::TaskGroup>& task_group)\n-      : ConcreteColumnDecoder(pool, task_group), type_(type) {}\n+  explicit NullColumnDecoder(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n+      : ConcreteColumnDecoder(pool), type_(type) {}\n \n-  void Insert(int64_t block_index, const std::shared_ptr<BlockParser>& parser) override;\n+  Future<std::shared_ptr<Array>> Decode(\n+      const std::shared_ptr<BlockParser>& parser) override;\n \n  protected:\n   std::shared_ptr<DataType> type() const override { return type_; }\n \n   std::shared_ptr<DataType> type_;\n };\n \n-void NullColumnDecoder::Insert(int64_t block_index,\n-                               const std::shared_ptr<BlockParser>& parser) {\n-  PrepareChunk(block_index);\n-\n+Future<std::shared_ptr<Array>> NullColumnDecoder::Decode(\n+    const std::shared_ptr<BlockParser>& parser) {\n   // Spawn a task that will build an array of nulls with the right DataType\n   const int32_t num_rows = parser->num_rows();\n   DCHECK_GE(num_rows, 0);\n \n-  task_group_->Append([=]() -> Status {\n-    std::unique_ptr<ArrayBuilder> builder;\n-    RETURN_NOT_OK(MakeBuilder(pool_, type_, &builder));\n-    std::shared_ptr<Array> array;\n-    RETURN_NOT_OK(builder->AppendNulls(num_rows));\n-    RETURN_NOT_OK(builder->Finish(&array));\n-\n-    SetChunk(block_index, array);\n-    return Status::OK();\n-  });\n+  std::unique_ptr<ArrayBuilder> builder;\n+  RETURN_NOT_OK(MakeBuilder(pool_, type_, &builder));\n+  std::shared_ptr<Array> array;\n+  RETURN_NOT_OK(builder->AppendNulls(num_rows));\n+  RETURN_NOT_OK(builder->Finish(&array));\n+  return Future<std::shared_ptr<Array>>::MakeFinished(std::move(array));\n\nReview comment:\n       ```suggestion\r\n     DCHECK_GE(parser->num_rows(), 0);\r\n     return MakeArrayOfNull(type_, parser->num_rows(), pool_);\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/column_decoder.cc\n##########\n@@ -283,84 +188,62 @@ Result<std::shared_ptr<Array>> InferringColumnDecoder::RunInference(\n     // (no one else should be updating converter_ concurrently)\n     auto maybe_array = converter_->Convert(*parser, col_index_);\n \n-    std::unique_lock<std::mutex> lock(mutex_);\n     if (maybe_array.ok() || !infer_status_.can_loosen_type()) {\n       // Conversion succeeded, or failed definitively\n+      DCHECK(!type_frozen_);\n+      type_frozen_ = true;\n       return maybe_array;\n     }\n     // Conversion failed temporarily, try another type\n     infer_status_.LoosenType(maybe_array.status());\n-    RETURN_NOT_OK(UpdateType());\n+    auto update_status = UpdateType();\n+    if (!update_status.ok()) {\n+      return update_status;\n+    }\n   }\n }\n \n-void InferringColumnDecoder::Insert(int64_t block_index,\n-                                    const std::shared_ptr<BlockParser>& parser) {\n-  PrepareChunk(block_index);\n-\n+Future<std::shared_ptr<Array>> InferringColumnDecoder::Decode(\n+    const std::shared_ptr<BlockParser>& parser) {\n+  bool already_taken = first_inferrer_.fetch_or(1);\n   // First block: run inference\n-  if (block_index == 0) {\n-    task_group_->Append([=]() -> Status {\n-      auto maybe_array = RunInference(parser);\n-\n-      std::unique_lock<std::mutex> lock(mutex_);\n-      DCHECK(!type_frozen_);\n-      type_frozen_ = true;\n-      SetChunkUnlocked(block_index, std::move(maybe_array));\n-      return Status::OK();\n-    });\n-    return;\n+  if (!already_taken) {\n+    auto maybe_array = RunInference(parser);\n+    first_inference_run_.MarkFinished();\n+    return Future<std::shared_ptr<Array>>::MakeFinished(maybe_array);\n\nReview comment:\n       ```suggestion\r\n       return Future<std::shared_ptr<Array>>::MakeFinished(std::move(maybe_array));\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -349,6 +350,182 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+struct DecodedBlock {\n+  std::shared_ptr<RecordBatch> record_batch;\n+  // Represents the number of input bytes represented by this batch\n+  // This will include bytes skipped when skipping rows after the header\n+  int64_t bytes_processed;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+template <>\n+struct IterationTraits<csv::DecodedBlock> {\n+  static csv::DecodedBlock End() { return csv::DecodedBlock{nullptr, -1}; }\n+  static bool IsEnd(const csv::DecodedBlock& val) { return val.bytes_processed < 0; }\n+};\n+\n+namespace csv {\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n+    auto parser = std::make_shared<BlockParser>(\n+        io_context_.pool(), parse_options_, num_csv_cols_, num_rows_seen_, max_num_rows);\n+\n+    std::shared_ptr<Buffer> straddling;\n+    std::vector<util::string_view> views;\n+    if (block.partial->size() != 0 || block.completion->size() != 0) {\n+      if (block.partial->size() == 0) {\n+        straddling = block.completion;\n+      } else if (block.completion->size() == 0) {\n+        straddling = block.partial;\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            straddling,\n+            ConcatenateBuffers({block.partial, block.completion}, io_context_.pool()));\n+      }\n+      views = {util::string_view(*straddling), util::string_view(*block.buffer)};\n+    } else {\n+      views = {util::string_view(*block.buffer)};\n+    }\n+    uint32_t parsed_size;\n+    if (block.is_final) {\n+      RETURN_NOT_OK(parser->ParseFinal(views, &parsed_size));\n+    } else {\n+      RETURN_NOT_OK(parser->Parse(views, &parsed_size));\n+    }\n+    if (count_rows_) {\n+      num_rows_seen_ += parser->num_rows();\n+    }\n+    RETURN_NOT_OK(block.consume_bytes(parsed_size));\n+    return ParsedBlock{std::move(parser), block.block_index,\n+                       static_cast<int64_t>(parsed_size) + block.bytes_skipped};\n+  }\n+\n+ private:\n+  io::IOContext io_context_;\n+  ParseOptions parse_options_;\n+  int num_csv_cols_;\n+  bool count_rows_;\n+  int num_rows_seen_ = 0;\n+};\n+\n+class BlockDecodingOperator {\n+ public:\n+  Future<DecodedBlock> operator()(const ParsedBlock& block) {\n+    DCHECK(!state_->column_decoders.empty());\n+    std::vector<Future<std::shared_ptr<Array>>> decoded_array_futs;\n+    for (auto& decoder : state_->column_decoders) {\n+      decoded_array_futs.push_back(decoder->Decode(block.parser));\n+    }\n+    auto bytes_parsed_or_skipped = block.bytes_parsed_or_skipped;\n+    auto decoded_arrays_fut = All(decoded_array_futs);\n+    auto state = state_;\n+    return decoded_arrays_fut.Then(\n+        [state, bytes_parsed_or_skipped](\n+            const std::vector<Result<std::shared_ptr<Array>>>& maybe_decoded_arrays)\n+            -> Result<DecodedBlock> {\n+          ARROW_ASSIGN_OR_RAISE(auto decoded_arrays,\n+                                internal::UnwrapOrRaise(maybe_decoded_arrays));\n+\n+          ARROW_ASSIGN_OR_RAISE(auto batch, state->DecodedArraysToBatch(decoded_arrays));\n\nReview comment:\n       ```suggestion\r\n             ARROW_ASSIGN_OR_RAISE(auto batch,\r\n                                   state->DecodedArraysToBatch(std::move(decoded_arrays)));\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -1139,8 +1177,9 @@ class CSVRowCounter : public ReaderMixin,\n   }\n \n   Future<int64_t> DoCount(const std::shared_ptr<CSVRowCounter>& self) {\n-    // We must return a value instead of Status/Future<> to work with MakeMappedGenerator,\n-    // and we must use a type with a valid end value to work with IterationEnd.\n+    // We must return a value instead of Status/Future<> to work with\n+    // MakeMappedGenerator, and we must use a type with a valid end value to work with\n\nReview comment:\n       ```suggestion\r\n       // count_cb must return a value instead of Status/Future<> to work with\r\n       // MakeMappedGenerator, and it must use a type with a valid end value to work with\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -642,263 +822,119 @@ class BaseTableReader : public ReaderMixin, public csv::TableReader {\n /////////////////////////////////////////////////////////////////////////\n // Base class for streaming readers\n \n-class BaseStreamingReader : public ReaderMixin, public csv::StreamingReader {\n+class StreamingReaderImpl : public ReaderMixin,\n+                            public csv::StreamingReader,\n+                            public std::enable_shared_from_this<StreamingReaderImpl> {\n  public:\n-  BaseStreamingReader(io::IOContext io_context, Executor* cpu_executor,\n-                      std::shared_ptr<io::InputStream> input,\n+  StreamingReaderImpl(io::IOContext io_context, std::shared_ptr<io::InputStream> input,\n                       const ReadOptions& read_options, const ParseOptions& parse_options,\n                       const ConvertOptions& convert_options, bool count_rows)\n       : ReaderMixin(io_context, std::move(input), read_options, parse_options,\n                     convert_options, count_rows),\n-        cpu_executor_(cpu_executor) {}\n-\n-  virtual Future<std::shared_ptr<csv::StreamingReader>> Init() = 0;\n-\n-  std::shared_ptr<Schema> schema() const override { return schema_; }\n-\n-  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n-    auto next_fut = ReadNextAsync();\n-    auto next_result = next_fut.result();\n-    return std::move(next_result).Value(batch);\n-  }\n-\n- protected:\n-  // Make column decoders from conversion schema\n-  Status MakeColumnDecoders() {\n-    for (const auto& column : conversion_schema_.columns) {\n-      std::shared_ptr<ColumnDecoder> decoder;\n-      if (column.is_missing) {\n-        ARROW_ASSIGN_OR_RAISE(decoder, ColumnDecoder::MakeNull(io_context_.pool(),\n-                                                               column.type, task_group_));\n-      } else if (column.type != nullptr) {\n-        ARROW_ASSIGN_OR_RAISE(\n-            decoder, ColumnDecoder::Make(io_context_.pool(), column.type, column.index,\n-                                         convert_options_, task_group_));\n-      } else {\n-        ARROW_ASSIGN_OR_RAISE(decoder,\n-                              ColumnDecoder::Make(io_context_.pool(), column.index,\n-                                                  convert_options_, task_group_));\n-      }\n-      column_decoders_.push_back(std::move(decoder));\n-    }\n-    return Status::OK();\n-  }\n+        bytes_decoded_(std::make_shared<std::atomic<int64_t>>(0)) {}\n \n-  Result<int64_t> ParseAndInsert(const std::shared_ptr<Buffer>& partial,\n-                                 const std::shared_ptr<Buffer>& completion,\n-                                 const std::shared_ptr<Buffer>& block,\n-                                 int64_t block_index, bool is_final) {\n-    ARROW_ASSIGN_OR_RAISE(auto result,\n-                          Parse(partial, completion, block, block_index, is_final));\n-    RETURN_NOT_OK(ProcessData(result.parser, block_index));\n-    return result.parsed_bytes;\n-  }\n-\n-  // Trigger conversion of parsed block data\n-  Status ProcessData(const std::shared_ptr<BlockParser>& parser, int64_t block_index) {\n-    for (auto& decoder : column_decoders_) {\n-      decoder->Insert(block_index, parser);\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<std::shared_ptr<RecordBatch>> DecodeNextBatch() {\n-    DCHECK(!column_decoders_.empty());\n-    ArrayVector arrays;\n-    arrays.reserve(column_decoders_.size());\n-    Status st;\n-    for (auto& decoder : column_decoders_) {\n-      auto maybe_array = decoder->NextChunk();\n-      if (!maybe_array.ok()) {\n-        // If there's an error, still fetch results from other decoders to\n-        // keep them in sync.\n-        st &= maybe_array.status();\n-      } else {\n-        arrays.push_back(*std::move(maybe_array));\n-      }\n-    }\n-    RETURN_NOT_OK(st);\n-    DCHECK_EQ(arrays.size(), column_decoders_.size());\n-    const bool is_null = (arrays[0] == nullptr);\n-#ifndef NDEBUG\n-    for (const auto& array : arrays) {\n-      DCHECK_EQ(array == nullptr, is_null);\n-    }\n-#endif\n-    if (is_null) {\n-      eof_ = true;\n-      return nullptr;\n-    }\n-\n-    if (schema_ == nullptr) {\n-      FieldVector fields(arrays.size());\n-      for (size_t i = 0; i < arrays.size(); ++i) {\n-        fields[i] = field(conversion_schema_.columns[i].name, arrays[i]->type());\n-      }\n-      schema_ = arrow::schema(std::move(fields));\n-    }\n-    const auto n_rows = arrays[0]->length();\n-    return RecordBatch::Make(schema_, n_rows, std::move(arrays));\n-  }\n-\n-  // Column decoders (in ConversionSchema order)\n-  std::vector<std::shared_ptr<ColumnDecoder>> column_decoders_;\n-  std::shared_ptr<Schema> schema_;\n-  std::shared_ptr<RecordBatch> pending_batch_;\n-  AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator_;\n-  Executor* cpu_executor_;\n-  bool eof_ = false;\n-};\n-\n-/////////////////////////////////////////////////////////////////////////\n-// Serial StreamingReader implementation\n-\n-class SerialStreamingReader : public BaseStreamingReader,\n-                              public std::enable_shared_from_this<SerialStreamingReader> {\n- public:\n-  using BaseStreamingReader::BaseStreamingReader;\n-\n-  Future<std::shared_ptr<csv::StreamingReader>> Init() override {\n+  Future<> Init(Executor* cpu_executor) {\n     ARROW_ASSIGN_OR_RAISE(auto istream_it,\n                           io::MakeInputStreamIterator(input_, read_options_.block_size));\n \n     // TODO Consider exposing readahead as a read option (ARROW-12090)\n     ARROW_ASSIGN_OR_RAISE(auto bg_it, MakeBackgroundGenerator(std::move(istream_it),\n                                                               io_context_.executor()));\n \n-    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor_);\n+    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor);\n \n-    buffer_generator_ = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n-    task_group_ = internal::TaskGroup::MakeSerial(io_context_.stop_token());\n+    auto buffer_generator = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n \n+    int max_readahead = cpu_executor->GetCapacity();\n     auto self = shared_from_this();\n-    // Read schema from first batch\n-    return ReadNextAsync(true).Then(\n-        [self](const std::shared_ptr<RecordBatch>& first_batch)\n-            -> Result<std::shared_ptr<csv::StreamingReader>> {\n-          self->pending_batch_ = first_batch;\n-          DCHECK_NE(self->schema_, nullptr);\n-          return self;\n-        });\n-  }\n \n-  Result<std::shared_ptr<RecordBatch>> DecodeBatchAndUpdateSchema() {\n-    auto maybe_batch = DecodeNextBatch();\n-    if (schema_ == nullptr && maybe_batch.ok()) {\n-      schema_ = (*maybe_batch)->schema();\n-    }\n-    return maybe_batch;\n+    return buffer_generator().Then([self, buffer_generator, max_readahead](\n+                                       const std::shared_ptr<Buffer>& first_buffer) {\n+      return self->InitAfterFirstBuffer(first_buffer, buffer_generator, max_readahead);\n+    });\n   }\n \n-  Future<std::shared_ptr<RecordBatch>> DoReadNext(\n-      std::shared_ptr<SerialStreamingReader> self) {\n-    auto batch = std::move(pending_batch_);\n-    if (batch != nullptr) {\n-      return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-    }\n+  std::shared_ptr<Schema> schema() const override { return schema_; }\n \n-    if (!source_eof_) {\n-      return block_generator_()\n-          .Then([self](const CSVBlock& maybe_block) -> Status {\n-            if (!IsIterationEnd(maybe_block)) {\n-              self->bytes_parsed_ += maybe_block.bytes_skipped;\n-              self->last_block_index_ = maybe_block.block_index;\n-              auto maybe_parsed = self->ParseAndInsert(\n-                  maybe_block.partial, maybe_block.completion, maybe_block.buffer,\n-                  maybe_block.block_index, maybe_block.is_final);\n-              if (!maybe_parsed.ok()) {\n-                // Parse error => bail out\n-                self->eof_ = true;\n-                return maybe_parsed.status();\n-              }\n-              self->bytes_parsed_ += *maybe_parsed;\n-              RETURN_NOT_OK(maybe_block.consume_bytes(*maybe_parsed));\n-            } else {\n-              self->source_eof_ = true;\n-              for (auto& decoder : self->column_decoders_) {\n-                decoder->SetEOF(self->last_block_index_ + 1);\n-              }\n-            }\n-            return Status::OK();\n-          })\n-          .Then([self]() -> Result<std::shared_ptr<RecordBatch>> {\n-            return self->DecodeBatchAndUpdateSchema();\n-          });\n-    }\n-    return Future<std::shared_ptr<RecordBatch>>::MakeFinished(\n-        DecodeBatchAndUpdateSchema());\n-  }\n+  int64_t bytes_read() const override { return bytes_decoded_->load(); }\n \n-  Future<std::shared_ptr<RecordBatch>> ReadNextSkippingEmpty(\n-      std::shared_ptr<SerialStreamingReader> self, bool internal_read) {\n-    return DoReadNext(self).Then(\n-        [self, internal_read](const std::shared_ptr<RecordBatch>& batch) {\n-          if (batch != nullptr && batch->num_rows() == 0) {\n-            return self->ReadNextSkippingEmpty(self, internal_read);\n-          }\n-          if (!internal_read) {\n-            self->bytes_decoded_ += self->bytes_parsed_;\n-            self->bytes_parsed_ = 0;\n-          }\n-          return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-        });\n+  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n+    auto next_fut = ReadNextAsync();\n+    auto next_result = next_fut.result();\n+    return std::move(next_result).Value(batch);\n   }\n \n   Future<std::shared_ptr<RecordBatch>> ReadNextAsync() override {\n-    return ReadNextAsync(false);\n-  };\n-\n-  int64_t bytes_read() const override { return bytes_decoded_; }\n+    return record_batch_gen_();\n+  }\n \n  protected:\n-  Future<> SetupReader(std::shared_ptr<SerialStreamingReader> self) {\n-    return buffer_generator_().Then([self](const std::shared_ptr<Buffer>& first_buffer) {\n-      if (first_buffer == nullptr) {\n-        return Status::Invalid(\"Empty CSV file\");\n-      }\n-      auto own_first_buffer = first_buffer;\n-      auto start = own_first_buffer->data();\n-      RETURN_NOT_OK(self->ProcessHeader(own_first_buffer, &own_first_buffer));\n-      self->bytes_decoded_ = own_first_buffer->data() - start;\n-      RETURN_NOT_OK(self->MakeColumnDecoders());\n-\n-      self->block_generator_ = SerialBlockReader::MakeAsyncIterator(\n-          std::move(self->buffer_generator_), MakeChunker(self->parse_options_),\n-          std::move(own_first_buffer), self->read_options_.skip_rows_after_names);\n-      return Status::OK();\n+  Future<> InitAfterFirstBuffer(const std::shared_ptr<Buffer>& first_buffer,\n+                                AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator,\n+                                int max_readahead) {\n+    if (first_buffer == nullptr) {\n+      return Status::Invalid(\"Empty CSV file\");\n+    }\n+\n+    std::shared_ptr<Buffer> after_header;\n+    ARROW_ASSIGN_OR_RAISE(auto header_bytes_consumed,\n+                          ProcessHeader(first_buffer, &after_header));\n+    bytes_decoded_->fetch_add(header_bytes_consumed);\n\nReview comment:\n       Nit: could you space these out a little?\n\n##########\nFile path: cpp/src/arrow/csv/column_decoder.cc\n##########\n@@ -283,84 +188,62 @@ Result<std::shared_ptr<Array>> InferringColumnDecoder::RunInference(\n     // (no one else should be updating converter_ concurrently)\n     auto maybe_array = converter_->Convert(*parser, col_index_);\n \n-    std::unique_lock<std::mutex> lock(mutex_);\n     if (maybe_array.ok() || !infer_status_.can_loosen_type()) {\n       // Conversion succeeded, or failed definitively\n+      DCHECK(!type_frozen_);\n+      type_frozen_ = true;\n       return maybe_array;\n     }\n     // Conversion failed temporarily, try another type\n     infer_status_.LoosenType(maybe_array.status());\n-    RETURN_NOT_OK(UpdateType());\n+    auto update_status = UpdateType();\n+    if (!update_status.ok()) {\n+      return update_status;\n+    }\n   }\n }\n \n-void InferringColumnDecoder::Insert(int64_t block_index,\n-                                    const std::shared_ptr<BlockParser>& parser) {\n-  PrepareChunk(block_index);\n-\n+Future<std::shared_ptr<Array>> InferringColumnDecoder::Decode(\n+    const std::shared_ptr<BlockParser>& parser) {\n+  bool already_taken = first_inferrer_.fetch_or(1);\n   // First block: run inference\n-  if (block_index == 0) {\n-    task_group_->Append([=]() -> Status {\n-      auto maybe_array = RunInference(parser);\n-\n-      std::unique_lock<std::mutex> lock(mutex_);\n-      DCHECK(!type_frozen_);\n-      type_frozen_ = true;\n-      SetChunkUnlocked(block_index, std::move(maybe_array));\n-      return Status::OK();\n-    });\n-    return;\n+  if (!already_taken) {\n+    auto maybe_array = RunInference(parser);\n+    first_inference_run_.MarkFinished();\n+    return Future<std::shared_ptr<Array>>::MakeFinished(maybe_array);\n   }\n \n   // Non-first block: wait for inference to finish on first block now,\n   // without blocking a TaskGroup thread.\n-  {\n-    std::unique_lock<std::mutex> lock(mutex_);\n-    PrepareChunkUnlocked(0);\n-    WaitForChunkUnlocked(0);\n-    if (!chunks_[0].status().ok()) {\n-      // Failed converting first chunk: bail out by marking EOF,\n-      // because we can't decide a type for the other chunks.\n-      SetChunkUnlocked(block_index, std::shared_ptr<Array>());\n-    }\n+  return first_inference_run_.Then([this, parser] {\n     DCHECK(type_frozen_);\n-  }\n-\n-  // Then use the inferred type to convert this block.\n-  task_group_->Append([=]() -> Status {\n     auto maybe_array = converter_->Convert(*parser, col_index_);\n-\n-    SetChunk(block_index, std::move(maybe_array));\n-    return Status::OK();\n+    return maybe_array;\n\nReview comment:\n       ```suggestion\r\n       return converter_->Convert(*parser, col_index_);\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -349,6 +350,182 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+struct DecodedBlock {\n+  std::shared_ptr<RecordBatch> record_batch;\n+  // Represents the number of input bytes represented by this batch\n+  // This will include bytes skipped when skipping rows after the header\n+  int64_t bytes_processed;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+template <>\n+struct IterationTraits<csv::DecodedBlock> {\n+  static csv::DecodedBlock End() { return csv::DecodedBlock{nullptr, -1}; }\n+  static bool IsEnd(const csv::DecodedBlock& val) { return val.bytes_processed < 0; }\n+};\n+\n+namespace csv {\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n+    auto parser = std::make_shared<BlockParser>(\n+        io_context_.pool(), parse_options_, num_csv_cols_, num_rows_seen_, max_num_rows);\n+\n+    std::shared_ptr<Buffer> straddling;\n+    std::vector<util::string_view> views;\n+    if (block.partial->size() != 0 || block.completion->size() != 0) {\n+      if (block.partial->size() == 0) {\n+        straddling = block.completion;\n+      } else if (block.completion->size() == 0) {\n+        straddling = block.partial;\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            straddling,\n+            ConcatenateBuffers({block.partial, block.completion}, io_context_.pool()));\n+      }\n+      views = {util::string_view(*straddling), util::string_view(*block.buffer)};\n+    } else {\n+      views = {util::string_view(*block.buffer)};\n+    }\n+    uint32_t parsed_size;\n+    if (block.is_final) {\n+      RETURN_NOT_OK(parser->ParseFinal(views, &parsed_size));\n+    } else {\n+      RETURN_NOT_OK(parser->Parse(views, &parsed_size));\n+    }\n+    if (count_rows_) {\n+      num_rows_seen_ += parser->num_rows();\n+    }\n+    RETURN_NOT_OK(block.consume_bytes(parsed_size));\n+    return ParsedBlock{std::move(parser), block.block_index,\n+                       static_cast<int64_t>(parsed_size) + block.bytes_skipped};\n+  }\n+\n+ private:\n+  io::IOContext io_context_;\n+  ParseOptions parse_options_;\n+  int num_csv_cols_;\n+  bool count_rows_;\n+  int num_rows_seen_ = 0;\n+};\n+\n+class BlockDecodingOperator {\n+ public:\n+  Future<DecodedBlock> operator()(const ParsedBlock& block) {\n+    DCHECK(!state_->column_decoders.empty());\n+    std::vector<Future<std::shared_ptr<Array>>> decoded_array_futs;\n+    for (auto& decoder : state_->column_decoders) {\n+      decoded_array_futs.push_back(decoder->Decode(block.parser));\n+    }\n+    auto bytes_parsed_or_skipped = block.bytes_parsed_or_skipped;\n+    auto decoded_arrays_fut = All(decoded_array_futs);\n+    auto state = state_;\n+    return decoded_arrays_fut.Then(\n+        [state, bytes_parsed_or_skipped](\n+            const std::vector<Result<std::shared_ptr<Array>>>& maybe_decoded_arrays)\n+            -> Result<DecodedBlock> {\n+          ARROW_ASSIGN_OR_RAISE(auto decoded_arrays,\n+                                internal::UnwrapOrRaise(maybe_decoded_arrays));\n+\n+          ARROW_ASSIGN_OR_RAISE(auto batch, state->DecodedArraysToBatch(decoded_arrays));\n+          return DecodedBlock{std::move(batch), bytes_parsed_or_skipped};\n+        });\n+  }\n+\n+  static Result<BlockDecodingOperator> Make(io::IOContext io_context,\n+                                            ConvertOptions convert_options,\n+                                            ConversionSchema conversion_schema) {\n+    BlockDecodingOperator op(std::move(io_context), std::move(convert_options),\n+                             std::move(conversion_schema));\n+    RETURN_NOT_OK(op.state_->MakeColumnDecoders(io_context));\n+    return op;\n+  }\n+\n+ private:\n+  BlockDecodingOperator(io::IOContext io_context, ConvertOptions convert_options,\n+                        ConversionSchema conversion_schema)\n+      : state_(std::make_shared<State>(std::move(io_context), std::move(convert_options),\n+                                       std::move(conversion_schema))) {}\n+\n+  struct State {\n+    State(io::IOContext io_context, ConvertOptions convert_options,\n+          ConversionSchema conversion_schema)\n+        : convert_options(std::move(convert_options)),\n+          conversion_schema(std::move(conversion_schema)) {}\n+\n+    Result<std::shared_ptr<RecordBatch>> DecodedArraysToBatch(\n+        std::vector<std::shared_ptr<Array>>& arrays) {\n\nReview comment:\n       ```suggestion\r\n           std::vector<std::shared_ptr<Array>> arrays) {\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -349,6 +350,182 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+struct DecodedBlock {\n+  std::shared_ptr<RecordBatch> record_batch;\n+  // Represents the number of input bytes represented by this batch\n+  // This will include bytes skipped when skipping rows after the header\n+  int64_t bytes_processed;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+template <>\n+struct IterationTraits<csv::DecodedBlock> {\n+  static csv::DecodedBlock End() { return csv::DecodedBlock{nullptr, -1}; }\n+  static bool IsEnd(const csv::DecodedBlock& val) { return val.bytes_processed < 0; }\n+};\n+\n+namespace csv {\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n\nReview comment:\n       ```suggestion\r\n   // A function object that takes in a buffer of CSV data and returns a parsed batch of CSV\r\n   // data (CSVBlock -> ParsedBlock) for use with MakeMappedGenerator.\r\n   // The parsed batch contains a list of offsets for each of the columns so that columns\r\n   // can be individually scanned\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -349,6 +350,182 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+struct DecodedBlock {\n+  std::shared_ptr<RecordBatch> record_batch;\n+  // Represents the number of input bytes represented by this batch\n+  // This will include bytes skipped when skipping rows after the header\n+  int64_t bytes_processed;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+template <>\n+struct IterationTraits<csv::DecodedBlock> {\n+  static csv::DecodedBlock End() { return csv::DecodedBlock{nullptr, -1}; }\n+  static bool IsEnd(const csv::DecodedBlock& val) { return val.bytes_processed < 0; }\n+};\n+\n+namespace csv {\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n+    auto parser = std::make_shared<BlockParser>(\n+        io_context_.pool(), parse_options_, num_csv_cols_, num_rows_seen_, max_num_rows);\n+\n+    std::shared_ptr<Buffer> straddling;\n+    std::vector<util::string_view> views;\n+    if (block.partial->size() != 0 || block.completion->size() != 0) {\n+      if (block.partial->size() == 0) {\n+        straddling = block.completion;\n+      } else if (block.completion->size() == 0) {\n+        straddling = block.partial;\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            straddling,\n+            ConcatenateBuffers({block.partial, block.completion}, io_context_.pool()));\n+      }\n+      views = {util::string_view(*straddling), util::string_view(*block.buffer)};\n+    } else {\n+      views = {util::string_view(*block.buffer)};\n+    }\n+    uint32_t parsed_size;\n+    if (block.is_final) {\n+      RETURN_NOT_OK(parser->ParseFinal(views, &parsed_size));\n+    } else {\n+      RETURN_NOT_OK(parser->Parse(views, &parsed_size));\n+    }\n+    if (count_rows_) {\n+      num_rows_seen_ += parser->num_rows();\n+    }\n+    RETURN_NOT_OK(block.consume_bytes(parsed_size));\n+    return ParsedBlock{std::move(parser), block.block_index,\n+                       static_cast<int64_t>(parsed_size) + block.bytes_skipped};\n+  }\n+\n+ private:\n+  io::IOContext io_context_;\n+  ParseOptions parse_options_;\n+  int num_csv_cols_;\n+  bool count_rows_;\n+  int num_rows_seen_ = 0;\n+};\n+\n+class BlockDecodingOperator {\n\nReview comment:\n       ```suggestion\r\n   // A function object that takes in parsed batch of CSV data and decodes it to an arrow\r\n   // record batch (ParsedBlock -> DecodedBlock) for use with MakeMappedGenerator.\r\n   class BlockDecodingOperator {\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -349,6 +350,182 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+struct DecodedBlock {\n+  std::shared_ptr<RecordBatch> record_batch;\n+  // Represents the number of input bytes represented by this batch\n+  // This will include bytes skipped when skipping rows after the header\n+  int64_t bytes_processed;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+template <>\n+struct IterationTraits<csv::DecodedBlock> {\n+  static csv::DecodedBlock End() { return csv::DecodedBlock{nullptr, -1}; }\n+  static bool IsEnd(const csv::DecodedBlock& val) { return val.bytes_processed < 0; }\n+};\n+\n+namespace csv {\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n\nReview comment:\n       ```suggestion\r\n       constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\r\n   ```\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -349,6 +350,182 @@ class ThreadedBlockReader : public BlockReader {\n   }\n };\n \n+struct ParsedBlock {\n+  std::shared_ptr<BlockParser> parser;\n+  int64_t block_index;\n+  int64_t bytes_parsed_or_skipped;\n+};\n+\n+struct DecodedBlock {\n+  std::shared_ptr<RecordBatch> record_batch;\n+  // Represents the number of input bytes represented by this batch\n+  // This will include bytes skipped when skipping rows after the header\n+  int64_t bytes_processed;\n+};\n+\n+}  // namespace\n+\n+}  // namespace csv\n+\n+template <>\n+struct IterationTraits<csv::ParsedBlock> {\n+  static csv::ParsedBlock End() { return csv::ParsedBlock{nullptr, -1, -1}; }\n+  static bool IsEnd(const csv::ParsedBlock& val) { return val.block_index < 0; }\n+};\n+\n+template <>\n+struct IterationTraits<csv::DecodedBlock> {\n+  static csv::DecodedBlock End() { return csv::DecodedBlock{nullptr, -1}; }\n+  static bool IsEnd(const csv::DecodedBlock& val) { return val.bytes_processed < 0; }\n+};\n+\n+namespace csv {\n+namespace {\n+\n+// A functor that takes in a buffer of CSV data and returns a parsed batch of CSV data.\n+// The parsed batch contains a list of offsets for each of the columns so that columns\n+// can be individually scanned\n+//\n+// This operator is not re-entrant\n+class BlockParsingOperator {\n+ public:\n+  BlockParsingOperator(io::IOContext io_context, ParseOptions parse_options,\n+                       int num_csv_cols, bool count_rows)\n+      : io_context_(io_context),\n+        parse_options_(parse_options),\n+        num_csv_cols_(num_csv_cols),\n+        count_rows_(count_rows) {}\n+\n+  Result<ParsedBlock> operator()(const CSVBlock& block) {\n+    static constexpr int32_t max_num_rows = std::numeric_limits<int32_t>::max();\n+    auto parser = std::make_shared<BlockParser>(\n+        io_context_.pool(), parse_options_, num_csv_cols_, num_rows_seen_, max_num_rows);\n+\n+    std::shared_ptr<Buffer> straddling;\n+    std::vector<util::string_view> views;\n+    if (block.partial->size() != 0 || block.completion->size() != 0) {\n+      if (block.partial->size() == 0) {\n+        straddling = block.completion;\n+      } else if (block.completion->size() == 0) {\n+        straddling = block.partial;\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            straddling,\n+            ConcatenateBuffers({block.partial, block.completion}, io_context_.pool()));\n+      }\n+      views = {util::string_view(*straddling), util::string_view(*block.buffer)};\n+    } else {\n+      views = {util::string_view(*block.buffer)};\n+    }\n+    uint32_t parsed_size;\n+    if (block.is_final) {\n+      RETURN_NOT_OK(parser->ParseFinal(views, &parsed_size));\n+    } else {\n+      RETURN_NOT_OK(parser->Parse(views, &parsed_size));\n+    }\n+    if (count_rows_) {\n+      num_rows_seen_ += parser->num_rows();\n+    }\n+    RETURN_NOT_OK(block.consume_bytes(parsed_size));\n+    return ParsedBlock{std::move(parser), block.block_index,\n+                       static_cast<int64_t>(parsed_size) + block.bytes_skipped};\n+  }\n+\n+ private:\n+  io::IOContext io_context_;\n+  ParseOptions parse_options_;\n+  int num_csv_cols_;\n+  bool count_rows_;\n+  int num_rows_seen_ = 0;\n+};\n+\n+class BlockDecodingOperator {\n+ public:\n+  Future<DecodedBlock> operator()(const ParsedBlock& block) {\n+    DCHECK(!state_->column_decoders.empty());\n+    std::vector<Future<std::shared_ptr<Array>>> decoded_array_futs;\n+    for (auto& decoder : state_->column_decoders) {\n+      decoded_array_futs.push_back(decoder->Decode(block.parser));\n+    }\n+    auto bytes_parsed_or_skipped = block.bytes_parsed_or_skipped;\n+    auto decoded_arrays_fut = All(decoded_array_futs);\n\nReview comment:\n       ```suggestion\r\n       auto decoded_arrays_fut = All(std::move(decoded_array_futs));\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-15T20:57:47.403+0000",
                    "updated": "2021-07-15T20:57:47.403+0000",
                    "started": "2021-07-15T20:57:47.403+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "623297",
                    "issueId": "13362740"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/worklog/623417",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10568:\nURL: https://github.com/apache/arrow/pull/10568#discussion_r670966620\n\n\n\n##########\nFile path: cpp/src/arrow/csv/reader.cc\n##########\n@@ -642,263 +822,119 @@ class BaseTableReader : public ReaderMixin, public csv::TableReader {\n /////////////////////////////////////////////////////////////////////////\n // Base class for streaming readers\n \n-class BaseStreamingReader : public ReaderMixin, public csv::StreamingReader {\n+class StreamingReaderImpl : public ReaderMixin,\n+                            public csv::StreamingReader,\n+                            public std::enable_shared_from_this<StreamingReaderImpl> {\n  public:\n-  BaseStreamingReader(io::IOContext io_context, Executor* cpu_executor,\n-                      std::shared_ptr<io::InputStream> input,\n+  StreamingReaderImpl(io::IOContext io_context, std::shared_ptr<io::InputStream> input,\n                       const ReadOptions& read_options, const ParseOptions& parse_options,\n                       const ConvertOptions& convert_options, bool count_rows)\n       : ReaderMixin(io_context, std::move(input), read_options, parse_options,\n                     convert_options, count_rows),\n-        cpu_executor_(cpu_executor) {}\n-\n-  virtual Future<std::shared_ptr<csv::StreamingReader>> Init() = 0;\n-\n-  std::shared_ptr<Schema> schema() const override { return schema_; }\n-\n-  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n-    auto next_fut = ReadNextAsync();\n-    auto next_result = next_fut.result();\n-    return std::move(next_result).Value(batch);\n-  }\n-\n- protected:\n-  // Make column decoders from conversion schema\n-  Status MakeColumnDecoders() {\n-    for (const auto& column : conversion_schema_.columns) {\n-      std::shared_ptr<ColumnDecoder> decoder;\n-      if (column.is_missing) {\n-        ARROW_ASSIGN_OR_RAISE(decoder, ColumnDecoder::MakeNull(io_context_.pool(),\n-                                                               column.type, task_group_));\n-      } else if (column.type != nullptr) {\n-        ARROW_ASSIGN_OR_RAISE(\n-            decoder, ColumnDecoder::Make(io_context_.pool(), column.type, column.index,\n-                                         convert_options_, task_group_));\n-      } else {\n-        ARROW_ASSIGN_OR_RAISE(decoder,\n-                              ColumnDecoder::Make(io_context_.pool(), column.index,\n-                                                  convert_options_, task_group_));\n-      }\n-      column_decoders_.push_back(std::move(decoder));\n-    }\n-    return Status::OK();\n-  }\n+        bytes_decoded_(std::make_shared<std::atomic<int64_t>>(0)) {}\n \n-  Result<int64_t> ParseAndInsert(const std::shared_ptr<Buffer>& partial,\n-                                 const std::shared_ptr<Buffer>& completion,\n-                                 const std::shared_ptr<Buffer>& block,\n-                                 int64_t block_index, bool is_final) {\n-    ARROW_ASSIGN_OR_RAISE(auto result,\n-                          Parse(partial, completion, block, block_index, is_final));\n-    RETURN_NOT_OK(ProcessData(result.parser, block_index));\n-    return result.parsed_bytes;\n-  }\n-\n-  // Trigger conversion of parsed block data\n-  Status ProcessData(const std::shared_ptr<BlockParser>& parser, int64_t block_index) {\n-    for (auto& decoder : column_decoders_) {\n-      decoder->Insert(block_index, parser);\n-    }\n-    return Status::OK();\n-  }\n-\n-  Result<std::shared_ptr<RecordBatch>> DecodeNextBatch() {\n-    DCHECK(!column_decoders_.empty());\n-    ArrayVector arrays;\n-    arrays.reserve(column_decoders_.size());\n-    Status st;\n-    for (auto& decoder : column_decoders_) {\n-      auto maybe_array = decoder->NextChunk();\n-      if (!maybe_array.ok()) {\n-        // If there's an error, still fetch results from other decoders to\n-        // keep them in sync.\n-        st &= maybe_array.status();\n-      } else {\n-        arrays.push_back(*std::move(maybe_array));\n-      }\n-    }\n-    RETURN_NOT_OK(st);\n-    DCHECK_EQ(arrays.size(), column_decoders_.size());\n-    const bool is_null = (arrays[0] == nullptr);\n-#ifndef NDEBUG\n-    for (const auto& array : arrays) {\n-      DCHECK_EQ(array == nullptr, is_null);\n-    }\n-#endif\n-    if (is_null) {\n-      eof_ = true;\n-      return nullptr;\n-    }\n-\n-    if (schema_ == nullptr) {\n-      FieldVector fields(arrays.size());\n-      for (size_t i = 0; i < arrays.size(); ++i) {\n-        fields[i] = field(conversion_schema_.columns[i].name, arrays[i]->type());\n-      }\n-      schema_ = arrow::schema(std::move(fields));\n-    }\n-    const auto n_rows = arrays[0]->length();\n-    return RecordBatch::Make(schema_, n_rows, std::move(arrays));\n-  }\n-\n-  // Column decoders (in ConversionSchema order)\n-  std::vector<std::shared_ptr<ColumnDecoder>> column_decoders_;\n-  std::shared_ptr<Schema> schema_;\n-  std::shared_ptr<RecordBatch> pending_batch_;\n-  AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator_;\n-  Executor* cpu_executor_;\n-  bool eof_ = false;\n-};\n-\n-/////////////////////////////////////////////////////////////////////////\n-// Serial StreamingReader implementation\n-\n-class SerialStreamingReader : public BaseStreamingReader,\n-                              public std::enable_shared_from_this<SerialStreamingReader> {\n- public:\n-  using BaseStreamingReader::BaseStreamingReader;\n-\n-  Future<std::shared_ptr<csv::StreamingReader>> Init() override {\n+  Future<> Init(Executor* cpu_executor) {\n     ARROW_ASSIGN_OR_RAISE(auto istream_it,\n                           io::MakeInputStreamIterator(input_, read_options_.block_size));\n \n     // TODO Consider exposing readahead as a read option (ARROW-12090)\n     ARROW_ASSIGN_OR_RAISE(auto bg_it, MakeBackgroundGenerator(std::move(istream_it),\n                                                               io_context_.executor()));\n \n-    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor_);\n+    auto transferred_it = MakeTransferredGenerator(bg_it, cpu_executor);\n \n-    buffer_generator_ = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n-    task_group_ = internal::TaskGroup::MakeSerial(io_context_.stop_token());\n+    auto buffer_generator = CSVBufferIterator::MakeAsync(std::move(transferred_it));\n \n+    int max_readahead = cpu_executor->GetCapacity();\n     auto self = shared_from_this();\n-    // Read schema from first batch\n-    return ReadNextAsync(true).Then(\n-        [self](const std::shared_ptr<RecordBatch>& first_batch)\n-            -> Result<std::shared_ptr<csv::StreamingReader>> {\n-          self->pending_batch_ = first_batch;\n-          DCHECK_NE(self->schema_, nullptr);\n-          return self;\n-        });\n-  }\n \n-  Result<std::shared_ptr<RecordBatch>> DecodeBatchAndUpdateSchema() {\n-    auto maybe_batch = DecodeNextBatch();\n-    if (schema_ == nullptr && maybe_batch.ok()) {\n-      schema_ = (*maybe_batch)->schema();\n-    }\n-    return maybe_batch;\n+    return buffer_generator().Then([self, buffer_generator, max_readahead](\n+                                       const std::shared_ptr<Buffer>& first_buffer) {\n+      return self->InitAfterFirstBuffer(first_buffer, buffer_generator, max_readahead);\n+    });\n   }\n \n-  Future<std::shared_ptr<RecordBatch>> DoReadNext(\n-      std::shared_ptr<SerialStreamingReader> self) {\n-    auto batch = std::move(pending_batch_);\n-    if (batch != nullptr) {\n-      return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-    }\n+  std::shared_ptr<Schema> schema() const override { return schema_; }\n \n-    if (!source_eof_) {\n-      return block_generator_()\n-          .Then([self](const CSVBlock& maybe_block) -> Status {\n-            if (!IsIterationEnd(maybe_block)) {\n-              self->bytes_parsed_ += maybe_block.bytes_skipped;\n-              self->last_block_index_ = maybe_block.block_index;\n-              auto maybe_parsed = self->ParseAndInsert(\n-                  maybe_block.partial, maybe_block.completion, maybe_block.buffer,\n-                  maybe_block.block_index, maybe_block.is_final);\n-              if (!maybe_parsed.ok()) {\n-                // Parse error => bail out\n-                self->eof_ = true;\n-                return maybe_parsed.status();\n-              }\n-              self->bytes_parsed_ += *maybe_parsed;\n-              RETURN_NOT_OK(maybe_block.consume_bytes(*maybe_parsed));\n-            } else {\n-              self->source_eof_ = true;\n-              for (auto& decoder : self->column_decoders_) {\n-                decoder->SetEOF(self->last_block_index_ + 1);\n-              }\n-            }\n-            return Status::OK();\n-          })\n-          .Then([self]() -> Result<std::shared_ptr<RecordBatch>> {\n-            return self->DecodeBatchAndUpdateSchema();\n-          });\n-    }\n-    return Future<std::shared_ptr<RecordBatch>>::MakeFinished(\n-        DecodeBatchAndUpdateSchema());\n-  }\n+  int64_t bytes_read() const override { return bytes_decoded_->load(); }\n \n-  Future<std::shared_ptr<RecordBatch>> ReadNextSkippingEmpty(\n-      std::shared_ptr<SerialStreamingReader> self, bool internal_read) {\n-    return DoReadNext(self).Then(\n-        [self, internal_read](const std::shared_ptr<RecordBatch>& batch) {\n-          if (batch != nullptr && batch->num_rows() == 0) {\n-            return self->ReadNextSkippingEmpty(self, internal_read);\n-          }\n-          if (!internal_read) {\n-            self->bytes_decoded_ += self->bytes_parsed_;\n-            self->bytes_parsed_ = 0;\n-          }\n-          return Future<std::shared_ptr<RecordBatch>>::MakeFinished(batch);\n-        });\n+  Status ReadNext(std::shared_ptr<RecordBatch>* batch) override {\n+    auto next_fut = ReadNextAsync();\n+    auto next_result = next_fut.result();\n+    return std::move(next_result).Value(batch);\n   }\n \n   Future<std::shared_ptr<RecordBatch>> ReadNextAsync() override {\n-    return ReadNextAsync(false);\n-  };\n-\n-  int64_t bytes_read() const override { return bytes_decoded_; }\n+    return record_batch_gen_();\n+  }\n \n  protected:\n-  Future<> SetupReader(std::shared_ptr<SerialStreamingReader> self) {\n-    return buffer_generator_().Then([self](const std::shared_ptr<Buffer>& first_buffer) {\n-      if (first_buffer == nullptr) {\n-        return Status::Invalid(\"Empty CSV file\");\n-      }\n-      auto own_first_buffer = first_buffer;\n-      auto start = own_first_buffer->data();\n-      RETURN_NOT_OK(self->ProcessHeader(own_first_buffer, &own_first_buffer));\n-      self->bytes_decoded_ = own_first_buffer->data() - start;\n-      RETURN_NOT_OK(self->MakeColumnDecoders());\n-\n-      self->block_generator_ = SerialBlockReader::MakeAsyncIterator(\n-          std::move(self->buffer_generator_), MakeChunker(self->parse_options_),\n-          std::move(own_first_buffer), self->read_options_.skip_rows_after_names);\n-      return Status::OK();\n+  Future<> InitAfterFirstBuffer(const std::shared_ptr<Buffer>& first_buffer,\n+                                AsyncGenerator<std::shared_ptr<Buffer>> buffer_generator,\n+                                int max_readahead) {\n+    if (first_buffer == nullptr) {\n+      return Status::Invalid(\"Empty CSV file\");\n+    }\n+\n+    std::shared_ptr<Buffer> after_header;\n+    ARROW_ASSIGN_OR_RAISE(auto header_bytes_consumed,\n+                          ProcessHeader(first_buffer, &after_header));\n+    bytes_decoded_->fetch_add(header_bytes_consumed);\n\nReview comment:\n       Done.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-16T05:00:46.640+0000",
                    "updated": "2021-07-16T05:00:46.640+0000",
                    "started": "2021-07-16T05:00:46.640+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "623417",
                    "issueId": "13362740"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 12600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@7e7f5caf[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@50f218d9[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1b8bf420[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@5b7f0358[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@22b2f551[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@17d8afdf[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@533663f0[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@70f0cf0d[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@455db43f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@17a94d1c[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@776085fa[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@19eb4bc[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 12600,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Jul 16 21:52:23 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-07-16T21:52:23.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11889/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-03-06T01:29:22.000+0000",
        "updated": "2021-07-16T21:52:30.000+0000",
        "timeoriginalestimate": null,
        "description": "Currently the streaming CSV reader does not allow for much parallelism.\u00a0 It doesn't allow for reading more than one segment at once (useful in S3) and it doesn't allow for column fan-out for parsing & converting.\r\n\r\nIt seems both of these options would speed up CSV reading in some scenarios although it's possible this is mostly mitigated in cases where there are many more files than cores (as per-file parallelism will occupy all the cores anyways).",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "3.5h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 12600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Add parallelism to streaming CSV reader",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/comment/17308885",
                    "id": "17308885",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "I'll add that this probably means making {{ColumnDecoder}} async (perhaps turning it into a generator).\r\n\r\nIt would be nice if the solution could also tackle ARROW-11853 at the same time, since both issues will require significant reworking of the {{ColumnDecoder}} internals anyway.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2021-03-25T18:23:30.821+0000",
                    "updated": "2021-03-25T18:23:30.821+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/comment/17364009",
                    "id": "17364009",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "It'll probably be at least another day (probably more, I'll target the end of the week) before this is PR-ready but some notes:\r\n * The approach I'm taking is to create a functor for parsing (CSVBlock -> ParsedBlock) and another for decoding (ParsedBlock -> Array) and then hook\u00a0 the whole thing up as an iterator/generator.\r\n * Since it was already in place I'll be keeping the per-column parallelism but I'm also adding parallel readahead (for conversion/decoding) on the batches themselves so I don't expect per-column parallelism is strictly neccesary for performance.\r\n * There doesn't seem to be much use case for eagerly blocking (i.e. ThreadedBlockReader).\u00a0 It seems pretty unlikely we will need a multi-threaded parser.\u00a0 So for the moment I expect I can reuse SerialBlockReader and just ensure it is not pulled async-reentrantly\r\n * The column builders & decoders are getting even more similar, I suspect I could probably combine the two into a single set of types with a boolean \"try_reconvert\" flag or something.\u00a0 For example, the decoders already had an array of chunks although I can't see any reason they needed more than a single chunk.\r\n * In order to address this and ARROW-11853 each ParsedBlock will create its own ThreadedTaskGroup.\u00a0 The future for that parsed block will be completed when all columns have been decoded and any \"recode\" tasks that were launched by that parsed block have finished.\u00a0 Finish will be called on each future so a failure (or a cancellation) should get caught pretty quickly.\u00a0 The stored futures might still hang around for coordination but they won't be waited on so we shouldn't deadlock there.\r\n * The table reader and the streaming readers are starting to become more and more similar as well.\u00a0 It may end up that they can be combined as well where the table readers set \"try_reconvert\" to true and have some kind of emplace_into_table step at the end (although this might be a bit tricky with ordering & reconversion).\r\n *",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2021-06-16T01:59:24.982+0000",
                    "updated": "2021-06-16T01:59:24.982+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13362740/comment/17382364",
                    "id": "17382364",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Issue resolved by pull request 10568\n[https://github.com/apache/arrow/pull/10568]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2021-07-16T21:52:23.310+0000",
                    "updated": "2021-07-16T21:52:23.310+0000"
                }
            ],
            "maxResults": 3,
            "total": 3,
            "startAt": 0
        },
        "customfield_12311820": "0|z0od5k:",
        "customfield_12314139": null
    }
}