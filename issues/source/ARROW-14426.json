{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13407824",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824",
    "key": "ARROW-14426",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350591",
                "id": "12350591",
                "description": "",
                "name": "7.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-02-03"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jonkeane",
            "name": "jonkeane",
            "key": "jonkeane",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34057",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34057",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34057",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34057"
            },
            "displayName": "Jonathan Keane",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jonkeane",
            "name": "jonkeane",
            "key": "jonkeane",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34057",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34057",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34057",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34057"
            },
            "displayName": "Jonathan Keane",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 15600,
            "total": 15600,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 15600,
            "total": 15600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-14426/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 26,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/670470",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace opened a new pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556\n\n\n   This adds `min_rows_per_group` and `max_rows_per_group` to `arrow::dataset::FileSystemDatasetWriteOptions` which give the user control over the size of row groups that are created by the dataset writer.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T05:26:12.783+0000",
                    "updated": "2021-10-27T05:26:12.783+0000",
                    "started": "2021-10-27T05:26:12.783+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670470",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/670471",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-952550067\n\n\n   https://issues.apache.org/jira/browse/ARROW-14426\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T05:26:37.679+0000",
                    "updated": "2021-10-27T05:26:37.679+0000",
                    "started": "2021-10-27T05:26:37.679+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670471",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/670813",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-953044514\n\n\n   (I haven't read the PR yet, so apologies, but) I thought the Parquet writer at least had a configurable row group size? (This would help the IPC writer though!)\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T15:29:11.188+0000",
                    "updated": "2021-10-27T15:29:11.188+0000",
                    "started": "2021-10-27T15:29:11.188+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670813",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/671006",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-953269749\n\n\n   Yes, the parquet writer has a configurable max row group size but it does not have a configurable min row group size.  The latter is helpful in particular for dataset writing because each incoming batch is split into N smaller partition batches.  If we then turn around and write those batches immediately we can often end up with a bunch of small row groups which is undesirable.  Also, the behavior of the max row group size is not quite what I'd want.  For example, if the max row group size is 1 million rows and I send a bunch of batches with 1.1 million rows then I'll end up with a bunch of row groups with 1 million rows and a bunch of row groups with 100k rows.\r\n   \r\n   We could push all these features down into the writers themselves I suppose.  It might be better from a separation of concerns point of view.  Although it would make it a little harder to enforce `max_rows_staged` unless we also added \"force write\" operation to the writers.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T20:07:49.893+0000",
                    "updated": "2021-10-27T20:07:49.893+0000",
                    "started": "2021-10-27T20:07:49.893+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "671006",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/671010",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-953281265\n\n\n   Ah ok, makes sense. I don't think we need to push things down per se since this is a common concern. (I'll try to get through this PR when I can)\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T20:23:53.523+0000",
                    "updated": "2021-10-27T20:23:53.523+0000",
                    "started": "2021-10-27T20:23:53.522+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "671010",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/671011",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-953284114\n\n\n   It appears I still need to format it, I'll move to draft until I get the CI passing.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T20:27:54.243+0000",
                    "updated": "2021-10-27T20:27:54.243+0000",
                    "started": "2021-10-27T20:27:54.243+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "671011",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/671051",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#discussion_r737844990\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer_test.cc\n##########\n@@ -145,8 +160,10 @@ class DatasetWriterTestFixture : public testing::Test {\n     for (const auto& expected_file : expected_files) {\n       util::optional<MockFileInfo> written_file = FindFile(expected_file.filename);\n       AssertFileCreated(written_file, expected_file.filename);\n+      int num_batches;\n\nReview comment:\n       nit: initialize to 0 to avoid warning on some compilers\n\n##########\nFile path: cpp/src/arrow/table.h\n##########\n@@ -208,6 +208,15 @@ class ARROW_EXPORT Table {\n   Result<std::shared_ptr<Table>> CombineChunks(\n       MemoryPool* pool = default_memory_pool()) const;\n \n+  /// \\brief Make a new record batch by combining the chunks this table has.\n+  ///\n+  /// All the underlying chunks in the ChunkedArray of each column are\n+  /// concatenated into a single chunk.\n+  ///\n+  /// \\param[in] pool The pool for buffer allocations\n+  Result<std::shared_ptr<RecordBatch>> CombineChunksToBatch(\n\nReview comment:\n       nit: maybe this should be named Concatenate for consistency?\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer_test.cc\n##########\n@@ -210,7 +229,67 @@ TEST_F(DatasetWriterTestFixture, MaxRowsManyWrites) {\n   ASSERT_FINISHES_OK(dataset_writer->WriteRecordBatch(MakeBatch(3), \"\"));\n   ASSERT_FINISHES_OK(dataset_writer->WriteRecordBatch(MakeBatch(3), \"\"));\n   ASSERT_FINISHES_OK(dataset_writer->Finish());\n-  AssertCreatedData({{\"testdir/chunk-0.arrow\", 0, 10}, {\"testdir/chunk-1.arrow\", 10, 8}});\n+  AssertCreatedData(\n+      {{\"testdir/chunk-0.arrow\", 0, 10, 4}, {\"testdir/chunk-1.arrow\", 10, 8, 3}});\n\nReview comment:\n       Without looking at the implementation yet, it's a little surprising that you'd get so many batches when the max rows per group is 10.\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer.cc\n##########\n@@ -83,128 +84,163 @@ class Throttle {\n   std::mutex mutex_;\n };\n \n+struct DatasetWriterState {\n+  DatasetWriterState(uint64_t rows_in_flight, uint64_t max_open_files,\n+                     uint64_t max_rows_staged)\n+      : rows_in_flight_throttle(rows_in_flight),\n+        open_files_throttle(max_open_files),\n+        staged_rows_count(0),\n+        max_rows_staged(max_rows_staged) {}\n+\n+  bool StagingFull() const { return staged_rows_count.load() >= max_rows_staged; }\n+\n+  // Throttle for how many rows the dataset writer will allow to be in process memory\n+  // When this is exceeded the dataset writer will pause / apply backpressure\n+  Throttle rows_in_flight_throttle;\n+  // Control for how many files the dataset writer will open.  When this is exceeded\n+  // the dataset writer will pause and it will also close the largest open file.\n+  Throttle open_files_throttle;\n+  // Control for how many rows the dataset writer will allow to be staged.  A row is\n+  // staged if it is waiting for more rows to reach minimum_batch_size.  If this is\n+  // exceeded then the largest staged batch is unstaged (no backpressure is applied)\n+  std::atomic<uint64_t> staged_rows_count;\n+  // If too many rows get staged we will end up with poor performance and, if more rows\n+  // are staged than max_rows_queued we will end up with deadlock.  To avoid this, once\n+  // we have too many staged rows we just ignore min_rows_per_group\n+  const uint64_t max_rows_staged;\n+  // Mutex to guard access to the file visitors in the writer options\n+  std::mutex visitors_mutex;\n+};\n+\n class DatasetWriterFileQueue : public util::AsyncDestroyable {\n  public:\n   explicit DatasetWriterFileQueue(const Future<std::shared_ptr<FileWriter>>& writer_fut,\n                                   const FileSystemDatasetWriteOptions& options,\n-                                  std::mutex* visitors_mutex)\n-      : options_(options), visitors_mutex_(visitors_mutex) {\n-    running_task_ = Future<>::Make();\n-    writer_fut.AddCallback(\n-        [this](const Result<std::shared_ptr<FileWriter>>& maybe_writer) {\n-          if (maybe_writer.ok()) {\n-            writer_ = *maybe_writer;\n-            Flush();\n-          } else {\n-            Abort(maybe_writer.status());\n-          }\n-        });\n+                                  DatasetWriterState* writer_state)\n+      : options_(options), writer_state_(writer_state) {\n+    // If this AddTask call fails (e.g. we're given an already failing future) then we\n+    // will get the error later when we try and write to it.\n\nReview comment:\n       This is because the next AddTask call will fail?\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer.cc\n##########\n@@ -83,128 +84,163 @@ class Throttle {\n   std::mutex mutex_;\n };\n \n+struct DatasetWriterState {\n+  DatasetWriterState(uint64_t rows_in_flight, uint64_t max_open_files,\n+                     uint64_t max_rows_staged)\n+      : rows_in_flight_throttle(rows_in_flight),\n+        open_files_throttle(max_open_files),\n+        staged_rows_count(0),\n+        max_rows_staged(max_rows_staged) {}\n+\n+  bool StagingFull() const { return staged_rows_count.load() >= max_rows_staged; }\n+\n+  // Throttle for how many rows the dataset writer will allow to be in process memory\n+  // When this is exceeded the dataset writer will pause / apply backpressure\n+  Throttle rows_in_flight_throttle;\n+  // Control for how many files the dataset writer will open.  When this is exceeded\n+  // the dataset writer will pause and it will also close the largest open file.\n+  Throttle open_files_throttle;\n+  // Control for how many rows the dataset writer will allow to be staged.  A row is\n+  // staged if it is waiting for more rows to reach minimum_batch_size.  If this is\n+  // exceeded then the largest staged batch is unstaged (no backpressure is applied)\n+  std::atomic<uint64_t> staged_rows_count;\n+  // If too many rows get staged we will end up with poor performance and, if more rows\n+  // are staged than max_rows_queued we will end up with deadlock.  To avoid this, once\n+  // we have too many staged rows we just ignore min_rows_per_group\n+  const uint64_t max_rows_staged;\n+  // Mutex to guard access to the file visitors in the writer options\n+  std::mutex visitors_mutex;\n+};\n+\n class DatasetWriterFileQueue : public util::AsyncDestroyable {\n  public:\n   explicit DatasetWriterFileQueue(const Future<std::shared_ptr<FileWriter>>& writer_fut,\n                                   const FileSystemDatasetWriteOptions& options,\n-                                  std::mutex* visitors_mutex)\n-      : options_(options), visitors_mutex_(visitors_mutex) {\n-    running_task_ = Future<>::Make();\n-    writer_fut.AddCallback(\n-        [this](const Result<std::shared_ptr<FileWriter>>& maybe_writer) {\n-          if (maybe_writer.ok()) {\n-            writer_ = *maybe_writer;\n-            Flush();\n-          } else {\n-            Abort(maybe_writer.status());\n-          }\n-        });\n+                                  DatasetWriterState* writer_state)\n+      : options_(options), writer_state_(writer_state) {\n+    // If this AddTask call fails (e.g. we're given an already failing future) then we\n+    // will get the error later when we try and write to it.\n+    ARROW_UNUSED(file_tasks_.AddTask([this, writer_fut] {\n+      return writer_fut.Then(\n+          [this](const std::shared_ptr<FileWriter>& writer) { writer_ = writer; });\n+    }));\n   }\n \n-  Future<uint64_t> Push(std::shared_ptr<RecordBatch> batch) {\n-    std::unique_lock<std::mutex> lk(mutex);\n-    write_queue_.push_back(std::move(batch));\n-    Future<uint64_t> write_future = Future<uint64_t>::Make();\n-    write_futures_.push_back(write_future);\n-    if (!running_task_.is_valid()) {\n-      running_task_ = Future<>::Make();\n-      FlushUnlocked(std::move(lk));\n+  Result<std::shared_ptr<RecordBatch>> PopStagedBatch() {\n+    std::vector<std::shared_ptr<RecordBatch>> batches_to_write;\n+    uint64_t num_rows = 0;\n+    while (!staged_batches_.empty()) {\n+      std::shared_ptr<RecordBatch> next = std::move(staged_batches_.front());\n+      staged_batches_.pop_front();\n+      if (num_rows + next->num_rows() <= options_.max_rows_per_group) {\n+        num_rows += next->num_rows();\n+        batches_to_write.push_back(std::move(next));\n+        if (num_rows == options_.max_rows_per_group) {\n+          break;\n+        }\n+      } else {\n+        uint64_t remaining = options_.max_rows_per_group - num_rows;\n+        std::shared_ptr<RecordBatch> next_partial =\n+            next->Slice(0, static_cast<int64_t>(remaining));\n+        batches_to_write.push_back(std::move(next_partial));\n+        std::shared_ptr<RecordBatch> next_remainder =\n+            next->Slice(static_cast<int64_t>(remaining));\n+        staged_batches_.push_front(std::move(next_remainder));\n+        break;\n+      }\n     }\n-    return write_future;\n+    DCHECK_GT(batches_to_write.size(), 0);\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Table> table,\n+                          Table::FromRecordBatches(batches_to_write));\n+    return table->CombineChunksToBatch();\n+  }\n+\n+  Status ScheduleBatch(std::shared_ptr<RecordBatch> batch) {\n+    struct WriteTask {\n+      Future<> operator()() { return self->WriteNext(std::move(batch)); }\n+      DatasetWriterFileQueue* self;\n+      std::shared_ptr<RecordBatch> batch;\n+    };\n+    return file_tasks_.AddTask(WriteTask{this, std::move(batch)});\n+  }\n+\n+  Result<int64_t> PopAndDeliverStagedBatch() {\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<RecordBatch> next_batch, PopStagedBatch());\n+    int64_t rows_popped = next_batch->num_rows();\n+    rows_currently_staged_ -= next_batch->num_rows();\n+    ARROW_RETURN_NOT_OK(ScheduleBatch(std::move(next_batch)));\n+    return rows_popped;\n+  }\n+\n+  // Stage batches, popping and delivering batches if enough data has arrived\n+  Status Push(std::shared_ptr<RecordBatch> batch) {\n+    uint64_t delta_staged = batch->num_rows();\n+    rows_currently_staged_ += delta_staged;\n+    staged_batches_.push_back(std::move(batch));\n+    while (!staged_batches_.empty() &&\n+           (writer_state_->StagingFull() ||\n+            rows_currently_staged_ >= options_.min_rows_per_group)) {\n+      ARROW_ASSIGN_OR_RAISE(int64_t rows_popped, PopAndDeliverStagedBatch());\n+      delta_staged -= rows_popped;\n+    }\n+    // Note, delta_staged may be negative if we were able to deliver some data\n+    writer_state_->staged_rows_count += delta_staged;\n+    return Status::OK();\n   }\n \n   Future<> DoDestroy() override {\n-    std::lock_guard<std::mutex> lg(mutex);\n-    if (!running_task_.is_valid()) {\n-      RETURN_NOT_OK(DoFinish());\n-      return Future<>::MakeFinished();\n+    if (!aborted_) {\n+      writer_state_->staged_rows_count -= rows_currently_staged_;\n+      while (!staged_batches_.empty()) {\n+        RETURN_NOT_OK(PopAndDeliverStagedBatch());\n+      }\n     }\n-    return running_task_.Then([this] { return DoFinish(); });\n+    return file_tasks_.End().Then([this] { return DoFinish(); });\n   }\n \n  private:\n-  Future<uint64_t> WriteNext() {\n+  Future<> WriteNext(std::shared_ptr<RecordBatch> next) {\n+    struct WriteTask {\n+      Status operator()() {\n+        int64_t rows_to_release = batch->num_rows();\n+        Status status = self->writer_->Write(batch);\n+        self->writer_state_->rows_in_flight_throttle.Release(rows_to_release);\n+        return status;\n+      }\n+      DatasetWriterFileQueue* self;\n+      std::shared_ptr<RecordBatch> batch;\n+    };\n     // May want to prototype / measure someday pushing the async write down further\n     return DeferNotOk(\n-        io::default_io_context().executor()->Submit([this]() -> Result<uint64_t> {\n-          DCHECK(running_task_.is_valid());\n-          std::unique_lock<std::mutex> lk(mutex);\n-          const std::shared_ptr<RecordBatch>& to_write = write_queue_.front();\n-          Future<uint64_t> on_complete = write_futures_.front();\n-          uint64_t rows_to_write = to_write->num_rows();\n-          lk.unlock();\n-          Status status = writer_->Write(to_write);\n-          lk.lock();\n-          write_queue_.pop_front();\n-          write_futures_.pop_front();\n-          lk.unlock();\n-          if (!status.ok()) {\n-            on_complete.MarkFinished(status);\n-          } else {\n-            on_complete.MarkFinished(rows_to_write);\n-          }\n-          return rows_to_write;\n-        }));\n+        io::default_io_context().executor()->Submit(WriteTask{this, std::move(next)}));\n   }\n \n   Status DoFinish() {\n     {\n-      std::lock_guard<std::mutex> lg(*visitors_mutex_);\n+      std::lock_guard<std::mutex> lg(writer_state_->visitors_mutex);\n       RETURN_NOT_OK(options_.writer_pre_finish(writer_.get()));\n     }\n     RETURN_NOT_OK(writer_->Finish());\n     {\n-      std::lock_guard<std::mutex> lg(*visitors_mutex_);\n+      std::lock_guard<std::mutex> lg(writer_state_->visitors_mutex);\n       return options_.writer_post_finish(writer_.get());\n     }\n   }\n \n   void Abort(Status err) {\n-    std::vector<Future<uint64_t>> futures_to_abort;\n-    Future<> old_running_task = running_task_;\n-    {\n-      std::lock_guard<std::mutex> lg(mutex);\n-      write_queue_.clear();\n-      futures_to_abort =\n-          std::vector<Future<uint64_t>>(write_futures_.begin(), write_futures_.end());\n-      write_futures_.clear();\n-      running_task_ = Future<>();\n-    }\n-    for (auto& fut : futures_to_abort) {\n-      fut.MarkFinished(err);\n-    }\n-    old_running_task.MarkFinished(std::move(err));\n-  }\n-\n-  void Flush() {\n-    std::unique_lock<std::mutex> lk(mutex);\n-    FlushUnlocked(std::move(lk));\n-  }\n-\n-  void FlushUnlocked(std::unique_lock<std::mutex> lk) {\n-    if (write_queue_.empty()) {\n-      Future<> old_running_task = running_task_;\n-      running_task_ = Future<>();\n-      lk.unlock();\n-      old_running_task.MarkFinished();\n-      return;\n-    }\n-    WriteNext().AddCallback([this](const Result<uint64_t>& res) {\n-      if (res.ok()) {\n-        Flush();\n-      } else {\n-        Abort(res.status());\n-      }\n-    });\n+    aborted_ = true;\n\nReview comment:\n       If I'm not mistaken, this method is no longer used.\n\n##########\nFile path: cpp/src/arrow/util/async_util.h\n##########\n@@ -176,6 +176,13 @@ class ARROW_EXPORT SerializedAsyncTaskGroup {\n   /// The returned future that will finish when all tasks have been consumed.\n   Future<> End();\n \n+  /// Abort a task group\n+  ///\n+  /// Tasks that have not been started will be discarded\n+  ///\n+  /// The returned future will finish when all running tasks have finished.\n+  Future<> Abort(Status err);\n\nReview comment:\n       nit: perhaps add a dedicated test for the new method?\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer_test.cc\n##########\n@@ -210,7 +229,67 @@ TEST_F(DatasetWriterTestFixture, MaxRowsManyWrites) {\n   ASSERT_FINISHES_OK(dataset_writer->WriteRecordBatch(MakeBatch(3), \"\"));\n   ASSERT_FINISHES_OK(dataset_writer->WriteRecordBatch(MakeBatch(3), \"\"));\n   ASSERT_FINISHES_OK(dataset_writer->Finish());\n-  AssertCreatedData({{\"testdir/chunk-0.arrow\", 0, 10}, {\"testdir/chunk-1.arrow\", 10, 8}});\n+  AssertCreatedData(\n+      {{\"testdir/chunk-0.arrow\", 0, 10, 4}, {\"testdir/chunk-1.arrow\", 10, 8, 3}});\n\nReview comment:\n       Ah, without a min rows per group, we're not queueing rows so the limit is being evaluated per batch - that makes sense. (This is just me talking to myself and not necessarily something to act on.)\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T21:28:13.251+0000",
                    "updated": "2021-10-27T21:28:13.251+0000",
                    "started": "2021-10-27T21:28:13.251+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "671051",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/671058",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#discussion_r737863205\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer.cc\n##########\n@@ -83,128 +84,163 @@ class Throttle {\n   std::mutex mutex_;\n };\n \n+struct DatasetWriterState {\n+  DatasetWriterState(uint64_t rows_in_flight, uint64_t max_open_files,\n+                     uint64_t max_rows_staged)\n+      : rows_in_flight_throttle(rows_in_flight),\n+        open_files_throttle(max_open_files),\n+        staged_rows_count(0),\n+        max_rows_staged(max_rows_staged) {}\n+\n+  bool StagingFull() const { return staged_rows_count.load() >= max_rows_staged; }\n+\n+  // Throttle for how many rows the dataset writer will allow to be in process memory\n+  // When this is exceeded the dataset writer will pause / apply backpressure\n+  Throttle rows_in_flight_throttle;\n+  // Control for how many files the dataset writer will open.  When this is exceeded\n+  // the dataset writer will pause and it will also close the largest open file.\n+  Throttle open_files_throttle;\n+  // Control for how many rows the dataset writer will allow to be staged.  A row is\n+  // staged if it is waiting for more rows to reach minimum_batch_size.  If this is\n+  // exceeded then the largest staged batch is unstaged (no backpressure is applied)\n+  std::atomic<uint64_t> staged_rows_count;\n+  // If too many rows get staged we will end up with poor performance and, if more rows\n+  // are staged than max_rows_queued we will end up with deadlock.  To avoid this, once\n+  // we have too many staged rows we just ignore min_rows_per_group\n+  const uint64_t max_rows_staged;\n+  // Mutex to guard access to the file visitors in the writer options\n+  std::mutex visitors_mutex;\n+};\n+\n class DatasetWriterFileQueue : public util::AsyncDestroyable {\n  public:\n   explicit DatasetWriterFileQueue(const Future<std::shared_ptr<FileWriter>>& writer_fut,\n                                   const FileSystemDatasetWriteOptions& options,\n-                                  std::mutex* visitors_mutex)\n-      : options_(options), visitors_mutex_(visitors_mutex) {\n-    running_task_ = Future<>::Make();\n-    writer_fut.AddCallback(\n-        [this](const Result<std::shared_ptr<FileWriter>>& maybe_writer) {\n-          if (maybe_writer.ok()) {\n-            writer_ = *maybe_writer;\n-            Flush();\n-          } else {\n-            Abort(maybe_writer.status());\n-          }\n-        });\n+                                  DatasetWriterState* writer_state)\n+      : options_(options), writer_state_(writer_state) {\n+    // If this AddTask call fails (e.g. we're given an already failing future) then we\n+    // will get the error later when we try and write to it.\n\nReview comment:\n       Yes.  Or, if for whatever reason we have no more `AddTask` calls then it will fail when we close it.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T21:33:47.114+0000",
                    "updated": "2021-10-27T21:33:47.114+0000",
                    "started": "2021-10-27T21:33:47.114+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "671058",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/673799",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jonkeane commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-958132986\n\n\n   Is there anything else that needs to be done on this?\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T20:15:04.144+0000",
                    "updated": "2021-11-02T20:15:04.144+0000",
                    "started": "2021-11-02T20:15:04.143+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673799",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/673928",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jonkeane commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-958132986\n\n\n   Is there anything else that needs to be done on this?\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T21:21:35.245+0000",
                    "updated": "2021-11-02T21:21:35.245+0000",
                    "started": "2021-11-02T21:21:35.245+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673928",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/674292",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#discussion_r741556010\n\n\n\n##########\nFile path: cpp/src/arrow/util/async_util.h\n##########\n@@ -176,6 +176,13 @@ class ARROW_EXPORT SerializedAsyncTaskGroup {\n   /// The returned future that will finish when all tasks have been consumed.\n   Future<> End();\n \n+  /// Abort a task group\n+  ///\n+  /// Tasks that have not been started will be discarded\n+  ///\n+  /// The returned future will finish when all running tasks have finished.\n+  Future<> Abort(Status err);\n\nReview comment:\n       Added.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-03T00:51:58.667+0000",
                    "updated": "2021-11-03T00:51:58.667+0000",
                    "started": "2021-11-03T00:51:58.666+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "674292",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/674293",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#discussion_r741556138\n\n\n\n##########\nFile path: cpp/src/arrow/util/async_util.h\n##########\n@@ -176,6 +176,13 @@ class ARROW_EXPORT SerializedAsyncTaskGroup {\n   /// The returned future that will finish when all tasks have been consumed.\n   Future<> End();\n \n+  /// Abort a task group\n+  ///\n+  /// Tasks that have not been started will be discarded\n+  ///\n+  /// The returned future will finish when all running tasks have finished.\n+  Future<> Abort(Status err);\n\nReview comment:\n       Although now that I took abort out of the dataset writer this isn't used.  But, it will come back in later so I'd like to just leave it.\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer_test.cc\n##########\n@@ -145,8 +160,10 @@ class DatasetWriterTestFixture : public testing::Test {\n     for (const auto& expected_file : expected_files) {\n       util::optional<MockFileInfo> written_file = FindFile(expected_file.filename);\n       AssertFileCreated(written_file, expected_file.filename);\n+      int num_batches;\n\nReview comment:\n       Done\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-03T00:52:31.943+0000",
                    "updated": "2021-11-03T00:52:31.943+0000",
                    "started": "2021-11-03T00:52:31.943+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "674293",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/674294",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#discussion_r741556380\n\n\n\n##########\nFile path: cpp/src/arrow/table.h\n##########\n@@ -208,6 +208,15 @@ class ARROW_EXPORT Table {\n   Result<std::shared_ptr<Table>> CombineChunks(\n       MemoryPool* pool = default_memory_pool()) const;\n \n+  /// \\brief Make a new record batch by combining the chunks this table has.\n+  ///\n+  /// All the underlying chunks in the ChunkedArray of each column are\n+  /// concatenated into a single chunk.\n+  ///\n+  /// \\param[in] pool The pool for buffer allocations\n+  Result<std::shared_ptr<RecordBatch>> CombineChunksToBatch(\n\nReview comment:\n       There is already a ConcatenateTables which combines multiple tables into a single tables.  Also, there is already a CombineChunks which concatenates the underlying arrays but leaves it as a table. Since the behavior is more similar to the latter I went with that naming scheme.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-03T00:53:25.269+0000",
                    "updated": "2021-11-03T00:53:25.269+0000",
                    "started": "2021-11-03T00:53:25.269+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "674294",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/674295",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#discussion_r741557476\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer.cc\n##########\n@@ -83,128 +84,163 @@ class Throttle {\n   std::mutex mutex_;\n };\n \n+struct DatasetWriterState {\n+  DatasetWriterState(uint64_t rows_in_flight, uint64_t max_open_files,\n+                     uint64_t max_rows_staged)\n+      : rows_in_flight_throttle(rows_in_flight),\n+        open_files_throttle(max_open_files),\n+        staged_rows_count(0),\n+        max_rows_staged(max_rows_staged) {}\n+\n+  bool StagingFull() const { return staged_rows_count.load() >= max_rows_staged; }\n+\n+  // Throttle for how many rows the dataset writer will allow to be in process memory\n+  // When this is exceeded the dataset writer will pause / apply backpressure\n+  Throttle rows_in_flight_throttle;\n+  // Control for how many files the dataset writer will open.  When this is exceeded\n+  // the dataset writer will pause and it will also close the largest open file.\n+  Throttle open_files_throttle;\n+  // Control for how many rows the dataset writer will allow to be staged.  A row is\n+  // staged if it is waiting for more rows to reach minimum_batch_size.  If this is\n+  // exceeded then the largest staged batch is unstaged (no backpressure is applied)\n+  std::atomic<uint64_t> staged_rows_count;\n+  // If too many rows get staged we will end up with poor performance and, if more rows\n+  // are staged than max_rows_queued we will end up with deadlock.  To avoid this, once\n+  // we have too many staged rows we just ignore min_rows_per_group\n+  const uint64_t max_rows_staged;\n+  // Mutex to guard access to the file visitors in the writer options\n+  std::mutex visitors_mutex;\n+};\n+\n class DatasetWriterFileQueue : public util::AsyncDestroyable {\n  public:\n   explicit DatasetWriterFileQueue(const Future<std::shared_ptr<FileWriter>>& writer_fut,\n                                   const FileSystemDatasetWriteOptions& options,\n-                                  std::mutex* visitors_mutex)\n-      : options_(options), visitors_mutex_(visitors_mutex) {\n-    running_task_ = Future<>::Make();\n-    writer_fut.AddCallback(\n-        [this](const Result<std::shared_ptr<FileWriter>>& maybe_writer) {\n-          if (maybe_writer.ok()) {\n-            writer_ = *maybe_writer;\n-            Flush();\n-          } else {\n-            Abort(maybe_writer.status());\n-          }\n-        });\n+                                  DatasetWriterState* writer_state)\n+      : options_(options), writer_state_(writer_state) {\n+    // If this AddTask call fails (e.g. we're given an already failing future) then we\n+    // will get the error later when we try and write to it.\n+    ARROW_UNUSED(file_tasks_.AddTask([this, writer_fut] {\n+      return writer_fut.Then(\n+          [this](const std::shared_ptr<FileWriter>& writer) { writer_ = writer; });\n+    }));\n   }\n \n-  Future<uint64_t> Push(std::shared_ptr<RecordBatch> batch) {\n-    std::unique_lock<std::mutex> lk(mutex);\n-    write_queue_.push_back(std::move(batch));\n-    Future<uint64_t> write_future = Future<uint64_t>::Make();\n-    write_futures_.push_back(write_future);\n-    if (!running_task_.is_valid()) {\n-      running_task_ = Future<>::Make();\n-      FlushUnlocked(std::move(lk));\n+  Result<std::shared_ptr<RecordBatch>> PopStagedBatch() {\n+    std::vector<std::shared_ptr<RecordBatch>> batches_to_write;\n+    uint64_t num_rows = 0;\n+    while (!staged_batches_.empty()) {\n+      std::shared_ptr<RecordBatch> next = std::move(staged_batches_.front());\n+      staged_batches_.pop_front();\n+      if (num_rows + next->num_rows() <= options_.max_rows_per_group) {\n+        num_rows += next->num_rows();\n+        batches_to_write.push_back(std::move(next));\n+        if (num_rows == options_.max_rows_per_group) {\n+          break;\n+        }\n+      } else {\n+        uint64_t remaining = options_.max_rows_per_group - num_rows;\n+        std::shared_ptr<RecordBatch> next_partial =\n+            next->Slice(0, static_cast<int64_t>(remaining));\n+        batches_to_write.push_back(std::move(next_partial));\n+        std::shared_ptr<RecordBatch> next_remainder =\n+            next->Slice(static_cast<int64_t>(remaining));\n+        staged_batches_.push_front(std::move(next_remainder));\n+        break;\n+      }\n     }\n-    return write_future;\n+    DCHECK_GT(batches_to_write.size(), 0);\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Table> table,\n+                          Table::FromRecordBatches(batches_to_write));\n+    return table->CombineChunksToBatch();\n+  }\n+\n+  Status ScheduleBatch(std::shared_ptr<RecordBatch> batch) {\n+    struct WriteTask {\n+      Future<> operator()() { return self->WriteNext(std::move(batch)); }\n+      DatasetWriterFileQueue* self;\n+      std::shared_ptr<RecordBatch> batch;\n+    };\n+    return file_tasks_.AddTask(WriteTask{this, std::move(batch)});\n+  }\n+\n+  Result<int64_t> PopAndDeliverStagedBatch() {\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<RecordBatch> next_batch, PopStagedBatch());\n+    int64_t rows_popped = next_batch->num_rows();\n+    rows_currently_staged_ -= next_batch->num_rows();\n+    ARROW_RETURN_NOT_OK(ScheduleBatch(std::move(next_batch)));\n+    return rows_popped;\n+  }\n+\n+  // Stage batches, popping and delivering batches if enough data has arrived\n+  Status Push(std::shared_ptr<RecordBatch> batch) {\n+    uint64_t delta_staged = batch->num_rows();\n+    rows_currently_staged_ += delta_staged;\n+    staged_batches_.push_back(std::move(batch));\n+    while (!staged_batches_.empty() &&\n+           (writer_state_->StagingFull() ||\n+            rows_currently_staged_ >= options_.min_rows_per_group)) {\n+      ARROW_ASSIGN_OR_RAISE(int64_t rows_popped, PopAndDeliverStagedBatch());\n+      delta_staged -= rows_popped;\n+    }\n+    // Note, delta_staged may be negative if we were able to deliver some data\n+    writer_state_->staged_rows_count += delta_staged;\n+    return Status::OK();\n   }\n \n   Future<> DoDestroy() override {\n-    std::lock_guard<std::mutex> lg(mutex);\n-    if (!running_task_.is_valid()) {\n-      RETURN_NOT_OK(DoFinish());\n-      return Future<>::MakeFinished();\n+    if (!aborted_) {\n+      writer_state_->staged_rows_count -= rows_currently_staged_;\n+      while (!staged_batches_.empty()) {\n+        RETURN_NOT_OK(PopAndDeliverStagedBatch());\n+      }\n     }\n-    return running_task_.Then([this] { return DoFinish(); });\n+    return file_tasks_.End().Then([this] { return DoFinish(); });\n   }\n \n  private:\n-  Future<uint64_t> WriteNext() {\n+  Future<> WriteNext(std::shared_ptr<RecordBatch> next) {\n+    struct WriteTask {\n+      Status operator()() {\n+        int64_t rows_to_release = batch->num_rows();\n+        Status status = self->writer_->Write(batch);\n+        self->writer_state_->rows_in_flight_throttle.Release(rows_to_release);\n+        return status;\n+      }\n+      DatasetWriterFileQueue* self;\n+      std::shared_ptr<RecordBatch> batch;\n+    };\n     // May want to prototype / measure someday pushing the async write down further\n     return DeferNotOk(\n-        io::default_io_context().executor()->Submit([this]() -> Result<uint64_t> {\n-          DCHECK(running_task_.is_valid());\n-          std::unique_lock<std::mutex> lk(mutex);\n-          const std::shared_ptr<RecordBatch>& to_write = write_queue_.front();\n-          Future<uint64_t> on_complete = write_futures_.front();\n-          uint64_t rows_to_write = to_write->num_rows();\n-          lk.unlock();\n-          Status status = writer_->Write(to_write);\n-          lk.lock();\n-          write_queue_.pop_front();\n-          write_futures_.pop_front();\n-          lk.unlock();\n-          if (!status.ok()) {\n-            on_complete.MarkFinished(status);\n-          } else {\n-            on_complete.MarkFinished(rows_to_write);\n-          }\n-          return rows_to_write;\n-        }));\n+        io::default_io_context().executor()->Submit(WriteTask{this, std::move(next)}));\n   }\n \n   Status DoFinish() {\n     {\n-      std::lock_guard<std::mutex> lg(*visitors_mutex_);\n+      std::lock_guard<std::mutex> lg(writer_state_->visitors_mutex);\n       RETURN_NOT_OK(options_.writer_pre_finish(writer_.get()));\n     }\n     RETURN_NOT_OK(writer_->Finish());\n     {\n-      std::lock_guard<std::mutex> lg(*visitors_mutex_);\n+      std::lock_guard<std::mutex> lg(writer_state_->visitors_mutex);\n       return options_.writer_post_finish(writer_.get());\n     }\n   }\n \n   void Abort(Status err) {\n-    std::vector<Future<uint64_t>> futures_to_abort;\n-    Future<> old_running_task = running_task_;\n-    {\n-      std::lock_guard<std::mutex> lg(mutex);\n-      write_queue_.clear();\n-      futures_to_abort =\n-          std::vector<Future<uint64_t>>(write_futures_.begin(), write_futures_.end());\n-      write_futures_.clear();\n-      running_task_ = Future<>();\n-    }\n-    for (auto& fut : futures_to_abort) {\n-      fut.MarkFinished(err);\n-    }\n-    old_running_task.MarkFinished(std::move(err));\n-  }\n-\n-  void Flush() {\n-    std::unique_lock<std::mutex> lk(mutex);\n-    FlushUnlocked(std::move(lk));\n-  }\n-\n-  void FlushUnlocked(std::unique_lock<std::mutex> lk) {\n-    if (write_queue_.empty()) {\n-      Future<> old_running_task = running_task_;\n-      running_task_ = Future<>();\n-      lk.unlock();\n-      old_running_task.MarkFinished();\n-      return;\n-    }\n-    WriteNext().AddCallback([this](const Result<uint64_t>& res) {\n-      if (res.ok()) {\n-        Flush();\n-      } else {\n-        Abort(res.status());\n-      }\n-    });\n+    aborted_ = true;\n\nReview comment:\n       Hmm, true, but that isn't necessarily a good thing.  I suppose the various task groups should probably auto-abort once they receive a failed status.  This would match the behavior of the synchronous task groups.  At the moment this behavior is probably ok (if we fail a write call we will just keep trying to write to the file) but it would be cleaner to bail out sooner.  I've added ARROW-14565 to address in a follow-up as it has implications elsewhere.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-03T00:57:10.213+0000",
                    "updated": "2021-11-03T00:57:10.213+0000",
                    "started": "2021-11-03T00:57:10.213+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "674295",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/674518",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-958873513\n\n\n   Some of these failures seem like legitimate deadlocks unfortunately.  I'll try and reproduce.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-03T10:39:41.919+0000",
                    "updated": "2021-11-03T10:39:41.919+0000",
                    "started": "2021-11-03T10:39:41.918+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "674518",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/675160",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#discussion_r741556010\n\n\n\n##########\nFile path: cpp/src/arrow/util/async_util.h\n##########\n@@ -176,6 +176,13 @@ class ARROW_EXPORT SerializedAsyncTaskGroup {\n   /// The returned future that will finish when all tasks have been consumed.\n   Future<> End();\n \n+  /// Abort a task group\n+  ///\n+  /// Tasks that have not been started will be discarded\n+  ///\n+  /// The returned future will finish when all running tasks have finished.\n+  Future<> Abort(Status err);\n\nReview comment:\n       Added.\n\n##########\nFile path: cpp/src/arrow/util/async_util.h\n##########\n@@ -176,6 +176,13 @@ class ARROW_EXPORT SerializedAsyncTaskGroup {\n   /// The returned future that will finish when all tasks have been consumed.\n   Future<> End();\n \n+  /// Abort a task group\n+  ///\n+  /// Tasks that have not been started will be discarded\n+  ///\n+  /// The returned future will finish when all running tasks have finished.\n+  Future<> Abort(Status err);\n\nReview comment:\n       Although now that I took abort out of the dataset writer this isn't used.  But, it will come back in later so I'd like to just leave it.\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer_test.cc\n##########\n@@ -145,8 +160,10 @@ class DatasetWriterTestFixture : public testing::Test {\n     for (const auto& expected_file : expected_files) {\n       util::optional<MockFileInfo> written_file = FindFile(expected_file.filename);\n       AssertFileCreated(written_file, expected_file.filename);\n+      int num_batches;\n\nReview comment:\n       Done\n\n##########\nFile path: cpp/src/arrow/table.h\n##########\n@@ -208,6 +208,15 @@ class ARROW_EXPORT Table {\n   Result<std::shared_ptr<Table>> CombineChunks(\n       MemoryPool* pool = default_memory_pool()) const;\n \n+  /// \\brief Make a new record batch by combining the chunks this table has.\n+  ///\n+  /// All the underlying chunks in the ChunkedArray of each column are\n+  /// concatenated into a single chunk.\n+  ///\n+  /// \\param[in] pool The pool for buffer allocations\n+  Result<std::shared_ptr<RecordBatch>> CombineChunksToBatch(\n\nReview comment:\n       There is already a ConcatenateTables which combines multiple tables into a single tables.  Also, there is already a CombineChunks which concatenates the underlying arrays but leaves it as a table. Since the behavior is more similar to the latter I went with that naming scheme.\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer.cc\n##########\n@@ -83,128 +84,163 @@ class Throttle {\n   std::mutex mutex_;\n };\n \n+struct DatasetWriterState {\n+  DatasetWriterState(uint64_t rows_in_flight, uint64_t max_open_files,\n+                     uint64_t max_rows_staged)\n+      : rows_in_flight_throttle(rows_in_flight),\n+        open_files_throttle(max_open_files),\n+        staged_rows_count(0),\n+        max_rows_staged(max_rows_staged) {}\n+\n+  bool StagingFull() const { return staged_rows_count.load() >= max_rows_staged; }\n+\n+  // Throttle for how many rows the dataset writer will allow to be in process memory\n+  // When this is exceeded the dataset writer will pause / apply backpressure\n+  Throttle rows_in_flight_throttle;\n+  // Control for how many files the dataset writer will open.  When this is exceeded\n+  // the dataset writer will pause and it will also close the largest open file.\n+  Throttle open_files_throttle;\n+  // Control for how many rows the dataset writer will allow to be staged.  A row is\n+  // staged if it is waiting for more rows to reach minimum_batch_size.  If this is\n+  // exceeded then the largest staged batch is unstaged (no backpressure is applied)\n+  std::atomic<uint64_t> staged_rows_count;\n+  // If too many rows get staged we will end up with poor performance and, if more rows\n+  // are staged than max_rows_queued we will end up with deadlock.  To avoid this, once\n+  // we have too many staged rows we just ignore min_rows_per_group\n+  const uint64_t max_rows_staged;\n+  // Mutex to guard access to the file visitors in the writer options\n+  std::mutex visitors_mutex;\n+};\n+\n class DatasetWriterFileQueue : public util::AsyncDestroyable {\n  public:\n   explicit DatasetWriterFileQueue(const Future<std::shared_ptr<FileWriter>>& writer_fut,\n                                   const FileSystemDatasetWriteOptions& options,\n-                                  std::mutex* visitors_mutex)\n-      : options_(options), visitors_mutex_(visitors_mutex) {\n-    running_task_ = Future<>::Make();\n-    writer_fut.AddCallback(\n-        [this](const Result<std::shared_ptr<FileWriter>>& maybe_writer) {\n-          if (maybe_writer.ok()) {\n-            writer_ = *maybe_writer;\n-            Flush();\n-          } else {\n-            Abort(maybe_writer.status());\n-          }\n-        });\n+                                  DatasetWriterState* writer_state)\n+      : options_(options), writer_state_(writer_state) {\n+    // If this AddTask call fails (e.g. we're given an already failing future) then we\n+    // will get the error later when we try and write to it.\n+    ARROW_UNUSED(file_tasks_.AddTask([this, writer_fut] {\n+      return writer_fut.Then(\n+          [this](const std::shared_ptr<FileWriter>& writer) { writer_ = writer; });\n+    }));\n   }\n \n-  Future<uint64_t> Push(std::shared_ptr<RecordBatch> batch) {\n-    std::unique_lock<std::mutex> lk(mutex);\n-    write_queue_.push_back(std::move(batch));\n-    Future<uint64_t> write_future = Future<uint64_t>::Make();\n-    write_futures_.push_back(write_future);\n-    if (!running_task_.is_valid()) {\n-      running_task_ = Future<>::Make();\n-      FlushUnlocked(std::move(lk));\n+  Result<std::shared_ptr<RecordBatch>> PopStagedBatch() {\n+    std::vector<std::shared_ptr<RecordBatch>> batches_to_write;\n+    uint64_t num_rows = 0;\n+    while (!staged_batches_.empty()) {\n+      std::shared_ptr<RecordBatch> next = std::move(staged_batches_.front());\n+      staged_batches_.pop_front();\n+      if (num_rows + next->num_rows() <= options_.max_rows_per_group) {\n+        num_rows += next->num_rows();\n+        batches_to_write.push_back(std::move(next));\n+        if (num_rows == options_.max_rows_per_group) {\n+          break;\n+        }\n+      } else {\n+        uint64_t remaining = options_.max_rows_per_group - num_rows;\n+        std::shared_ptr<RecordBatch> next_partial =\n+            next->Slice(0, static_cast<int64_t>(remaining));\n+        batches_to_write.push_back(std::move(next_partial));\n+        std::shared_ptr<RecordBatch> next_remainder =\n+            next->Slice(static_cast<int64_t>(remaining));\n+        staged_batches_.push_front(std::move(next_remainder));\n+        break;\n+      }\n     }\n-    return write_future;\n+    DCHECK_GT(batches_to_write.size(), 0);\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Table> table,\n+                          Table::FromRecordBatches(batches_to_write));\n+    return table->CombineChunksToBatch();\n+  }\n+\n+  Status ScheduleBatch(std::shared_ptr<RecordBatch> batch) {\n+    struct WriteTask {\n+      Future<> operator()() { return self->WriteNext(std::move(batch)); }\n+      DatasetWriterFileQueue* self;\n+      std::shared_ptr<RecordBatch> batch;\n+    };\n+    return file_tasks_.AddTask(WriteTask{this, std::move(batch)});\n+  }\n+\n+  Result<int64_t> PopAndDeliverStagedBatch() {\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<RecordBatch> next_batch, PopStagedBatch());\n+    int64_t rows_popped = next_batch->num_rows();\n+    rows_currently_staged_ -= next_batch->num_rows();\n+    ARROW_RETURN_NOT_OK(ScheduleBatch(std::move(next_batch)));\n+    return rows_popped;\n+  }\n+\n+  // Stage batches, popping and delivering batches if enough data has arrived\n+  Status Push(std::shared_ptr<RecordBatch> batch) {\n+    uint64_t delta_staged = batch->num_rows();\n+    rows_currently_staged_ += delta_staged;\n+    staged_batches_.push_back(std::move(batch));\n+    while (!staged_batches_.empty() &&\n+           (writer_state_->StagingFull() ||\n+            rows_currently_staged_ >= options_.min_rows_per_group)) {\n+      ARROW_ASSIGN_OR_RAISE(int64_t rows_popped, PopAndDeliverStagedBatch());\n+      delta_staged -= rows_popped;\n+    }\n+    // Note, delta_staged may be negative if we were able to deliver some data\n+    writer_state_->staged_rows_count += delta_staged;\n+    return Status::OK();\n   }\n \n   Future<> DoDestroy() override {\n-    std::lock_guard<std::mutex> lg(mutex);\n-    if (!running_task_.is_valid()) {\n-      RETURN_NOT_OK(DoFinish());\n-      return Future<>::MakeFinished();\n+    if (!aborted_) {\n+      writer_state_->staged_rows_count -= rows_currently_staged_;\n+      while (!staged_batches_.empty()) {\n+        RETURN_NOT_OK(PopAndDeliverStagedBatch());\n+      }\n     }\n-    return running_task_.Then([this] { return DoFinish(); });\n+    return file_tasks_.End().Then([this] { return DoFinish(); });\n   }\n \n  private:\n-  Future<uint64_t> WriteNext() {\n+  Future<> WriteNext(std::shared_ptr<RecordBatch> next) {\n+    struct WriteTask {\n+      Status operator()() {\n+        int64_t rows_to_release = batch->num_rows();\n+        Status status = self->writer_->Write(batch);\n+        self->writer_state_->rows_in_flight_throttle.Release(rows_to_release);\n+        return status;\n+      }\n+      DatasetWriterFileQueue* self;\n+      std::shared_ptr<RecordBatch> batch;\n+    };\n     // May want to prototype / measure someday pushing the async write down further\n     return DeferNotOk(\n-        io::default_io_context().executor()->Submit([this]() -> Result<uint64_t> {\n-          DCHECK(running_task_.is_valid());\n-          std::unique_lock<std::mutex> lk(mutex);\n-          const std::shared_ptr<RecordBatch>& to_write = write_queue_.front();\n-          Future<uint64_t> on_complete = write_futures_.front();\n-          uint64_t rows_to_write = to_write->num_rows();\n-          lk.unlock();\n-          Status status = writer_->Write(to_write);\n-          lk.lock();\n-          write_queue_.pop_front();\n-          write_futures_.pop_front();\n-          lk.unlock();\n-          if (!status.ok()) {\n-            on_complete.MarkFinished(status);\n-          } else {\n-            on_complete.MarkFinished(rows_to_write);\n-          }\n-          return rows_to_write;\n-        }));\n+        io::default_io_context().executor()->Submit(WriteTask{this, std::move(next)}));\n   }\n \n   Status DoFinish() {\n     {\n-      std::lock_guard<std::mutex> lg(*visitors_mutex_);\n+      std::lock_guard<std::mutex> lg(writer_state_->visitors_mutex);\n       RETURN_NOT_OK(options_.writer_pre_finish(writer_.get()));\n     }\n     RETURN_NOT_OK(writer_->Finish());\n     {\n-      std::lock_guard<std::mutex> lg(*visitors_mutex_);\n+      std::lock_guard<std::mutex> lg(writer_state_->visitors_mutex);\n       return options_.writer_post_finish(writer_.get());\n     }\n   }\n \n   void Abort(Status err) {\n-    std::vector<Future<uint64_t>> futures_to_abort;\n-    Future<> old_running_task = running_task_;\n-    {\n-      std::lock_guard<std::mutex> lg(mutex);\n-      write_queue_.clear();\n-      futures_to_abort =\n-          std::vector<Future<uint64_t>>(write_futures_.begin(), write_futures_.end());\n-      write_futures_.clear();\n-      running_task_ = Future<>();\n-    }\n-    for (auto& fut : futures_to_abort) {\n-      fut.MarkFinished(err);\n-    }\n-    old_running_task.MarkFinished(std::move(err));\n-  }\n-\n-  void Flush() {\n-    std::unique_lock<std::mutex> lk(mutex);\n-    FlushUnlocked(std::move(lk));\n-  }\n-\n-  void FlushUnlocked(std::unique_lock<std::mutex> lk) {\n-    if (write_queue_.empty()) {\n-      Future<> old_running_task = running_task_;\n-      running_task_ = Future<>();\n-      lk.unlock();\n-      old_running_task.MarkFinished();\n-      return;\n-    }\n-    WriteNext().AddCallback([this](const Result<uint64_t>& res) {\n-      if (res.ok()) {\n-        Flush();\n-      } else {\n-        Abort(res.status());\n-      }\n-    });\n+    aborted_ = true;\n\nReview comment:\n       Hmm, true, but that isn't necessarily a good thing.  I suppose the various task groups should probably auto-abort once they receive a failed status.  This would match the behavior of the synchronous task groups.  At the moment this behavior is probably ok (if we fail a write call we will just keep trying to write to the file) but it would be cleaner to bail out sooner.  I've added ARROW-14565 to address in a follow-up as it has implications elsewhere.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-04T00:18:41.434+0000",
                    "updated": "2021-11-04T00:18:41.434+0000",
                    "started": "2021-11-04T00:18:41.433+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "675160",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/675206",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-958873513\n\n\n   Some of these failures seem like legitimate deadlocks unfortunately.  I'll try and reproduce.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-04T00:25:30.752+0000",
                    "updated": "2021-11-04T00:25:30.752+0000",
                    "started": "2021-11-04T00:25:30.752+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "675206",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/675667",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#discussion_r741556010\n\n\n\n##########\nFile path: cpp/src/arrow/util/async_util.h\n##########\n@@ -176,6 +176,13 @@ class ARROW_EXPORT SerializedAsyncTaskGroup {\n   /// The returned future that will finish when all tasks have been consumed.\n   Future<> End();\n \n+  /// Abort a task group\n+  ///\n+  /// Tasks that have not been started will be discarded\n+  ///\n+  /// The returned future will finish when all running tasks have finished.\n+  Future<> Abort(Status err);\n\nReview comment:\n       Added.\n\n##########\nFile path: cpp/src/arrow/util/async_util.h\n##########\n@@ -176,6 +176,13 @@ class ARROW_EXPORT SerializedAsyncTaskGroup {\n   /// The returned future that will finish when all tasks have been consumed.\n   Future<> End();\n \n+  /// Abort a task group\n+  ///\n+  /// Tasks that have not been started will be discarded\n+  ///\n+  /// The returned future will finish when all running tasks have finished.\n+  Future<> Abort(Status err);\n\nReview comment:\n       Although now that I took abort out of the dataset writer this isn't used.  But, it will come back in later so I'd like to just leave it.\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer_test.cc\n##########\n@@ -145,8 +160,10 @@ class DatasetWriterTestFixture : public testing::Test {\n     for (const auto& expected_file : expected_files) {\n       util::optional<MockFileInfo> written_file = FindFile(expected_file.filename);\n       AssertFileCreated(written_file, expected_file.filename);\n+      int num_batches;\n\nReview comment:\n       Done\n\n##########\nFile path: cpp/src/arrow/table.h\n##########\n@@ -208,6 +208,15 @@ class ARROW_EXPORT Table {\n   Result<std::shared_ptr<Table>> CombineChunks(\n       MemoryPool* pool = default_memory_pool()) const;\n \n+  /// \\brief Make a new record batch by combining the chunks this table has.\n+  ///\n+  /// All the underlying chunks in the ChunkedArray of each column are\n+  /// concatenated into a single chunk.\n+  ///\n+  /// \\param[in] pool The pool for buffer allocations\n+  Result<std::shared_ptr<RecordBatch>> CombineChunksToBatch(\n\nReview comment:\n       There is already a ConcatenateTables which combines multiple tables into a single tables.  Also, there is already a CombineChunks which concatenates the underlying arrays but leaves it as a table. Since the behavior is more similar to the latter I went with that naming scheme.\n\n##########\nFile path: cpp/src/arrow/dataset/dataset_writer.cc\n##########\n@@ -83,128 +84,163 @@ class Throttle {\n   std::mutex mutex_;\n };\n \n+struct DatasetWriterState {\n+  DatasetWriterState(uint64_t rows_in_flight, uint64_t max_open_files,\n+                     uint64_t max_rows_staged)\n+      : rows_in_flight_throttle(rows_in_flight),\n+        open_files_throttle(max_open_files),\n+        staged_rows_count(0),\n+        max_rows_staged(max_rows_staged) {}\n+\n+  bool StagingFull() const { return staged_rows_count.load() >= max_rows_staged; }\n+\n+  // Throttle for how many rows the dataset writer will allow to be in process memory\n+  // When this is exceeded the dataset writer will pause / apply backpressure\n+  Throttle rows_in_flight_throttle;\n+  // Control for how many files the dataset writer will open.  When this is exceeded\n+  // the dataset writer will pause and it will also close the largest open file.\n+  Throttle open_files_throttle;\n+  // Control for how many rows the dataset writer will allow to be staged.  A row is\n+  // staged if it is waiting for more rows to reach minimum_batch_size.  If this is\n+  // exceeded then the largest staged batch is unstaged (no backpressure is applied)\n+  std::atomic<uint64_t> staged_rows_count;\n+  // If too many rows get staged we will end up with poor performance and, if more rows\n+  // are staged than max_rows_queued we will end up with deadlock.  To avoid this, once\n+  // we have too many staged rows we just ignore min_rows_per_group\n+  const uint64_t max_rows_staged;\n+  // Mutex to guard access to the file visitors in the writer options\n+  std::mutex visitors_mutex;\n+};\n+\n class DatasetWriterFileQueue : public util::AsyncDestroyable {\n  public:\n   explicit DatasetWriterFileQueue(const Future<std::shared_ptr<FileWriter>>& writer_fut,\n                                   const FileSystemDatasetWriteOptions& options,\n-                                  std::mutex* visitors_mutex)\n-      : options_(options), visitors_mutex_(visitors_mutex) {\n-    running_task_ = Future<>::Make();\n-    writer_fut.AddCallback(\n-        [this](const Result<std::shared_ptr<FileWriter>>& maybe_writer) {\n-          if (maybe_writer.ok()) {\n-            writer_ = *maybe_writer;\n-            Flush();\n-          } else {\n-            Abort(maybe_writer.status());\n-          }\n-        });\n+                                  DatasetWriterState* writer_state)\n+      : options_(options), writer_state_(writer_state) {\n+    // If this AddTask call fails (e.g. we're given an already failing future) then we\n+    // will get the error later when we try and write to it.\n+    ARROW_UNUSED(file_tasks_.AddTask([this, writer_fut] {\n+      return writer_fut.Then(\n+          [this](const std::shared_ptr<FileWriter>& writer) { writer_ = writer; });\n+    }));\n   }\n \n-  Future<uint64_t> Push(std::shared_ptr<RecordBatch> batch) {\n-    std::unique_lock<std::mutex> lk(mutex);\n-    write_queue_.push_back(std::move(batch));\n-    Future<uint64_t> write_future = Future<uint64_t>::Make();\n-    write_futures_.push_back(write_future);\n-    if (!running_task_.is_valid()) {\n-      running_task_ = Future<>::Make();\n-      FlushUnlocked(std::move(lk));\n+  Result<std::shared_ptr<RecordBatch>> PopStagedBatch() {\n+    std::vector<std::shared_ptr<RecordBatch>> batches_to_write;\n+    uint64_t num_rows = 0;\n+    while (!staged_batches_.empty()) {\n+      std::shared_ptr<RecordBatch> next = std::move(staged_batches_.front());\n+      staged_batches_.pop_front();\n+      if (num_rows + next->num_rows() <= options_.max_rows_per_group) {\n+        num_rows += next->num_rows();\n+        batches_to_write.push_back(std::move(next));\n+        if (num_rows == options_.max_rows_per_group) {\n+          break;\n+        }\n+      } else {\n+        uint64_t remaining = options_.max_rows_per_group - num_rows;\n+        std::shared_ptr<RecordBatch> next_partial =\n+            next->Slice(0, static_cast<int64_t>(remaining));\n+        batches_to_write.push_back(std::move(next_partial));\n+        std::shared_ptr<RecordBatch> next_remainder =\n+            next->Slice(static_cast<int64_t>(remaining));\n+        staged_batches_.push_front(std::move(next_remainder));\n+        break;\n+      }\n     }\n-    return write_future;\n+    DCHECK_GT(batches_to_write.size(), 0);\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Table> table,\n+                          Table::FromRecordBatches(batches_to_write));\n+    return table->CombineChunksToBatch();\n+  }\n+\n+  Status ScheduleBatch(std::shared_ptr<RecordBatch> batch) {\n+    struct WriteTask {\n+      Future<> operator()() { return self->WriteNext(std::move(batch)); }\n+      DatasetWriterFileQueue* self;\n+      std::shared_ptr<RecordBatch> batch;\n+    };\n+    return file_tasks_.AddTask(WriteTask{this, std::move(batch)});\n+  }\n+\n+  Result<int64_t> PopAndDeliverStagedBatch() {\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<RecordBatch> next_batch, PopStagedBatch());\n+    int64_t rows_popped = next_batch->num_rows();\n+    rows_currently_staged_ -= next_batch->num_rows();\n+    ARROW_RETURN_NOT_OK(ScheduleBatch(std::move(next_batch)));\n+    return rows_popped;\n+  }\n+\n+  // Stage batches, popping and delivering batches if enough data has arrived\n+  Status Push(std::shared_ptr<RecordBatch> batch) {\n+    uint64_t delta_staged = batch->num_rows();\n+    rows_currently_staged_ += delta_staged;\n+    staged_batches_.push_back(std::move(batch));\n+    while (!staged_batches_.empty() &&\n+           (writer_state_->StagingFull() ||\n+            rows_currently_staged_ >= options_.min_rows_per_group)) {\n+      ARROW_ASSIGN_OR_RAISE(int64_t rows_popped, PopAndDeliverStagedBatch());\n+      delta_staged -= rows_popped;\n+    }\n+    // Note, delta_staged may be negative if we were able to deliver some data\n+    writer_state_->staged_rows_count += delta_staged;\n+    return Status::OK();\n   }\n \n   Future<> DoDestroy() override {\n-    std::lock_guard<std::mutex> lg(mutex);\n-    if (!running_task_.is_valid()) {\n-      RETURN_NOT_OK(DoFinish());\n-      return Future<>::MakeFinished();\n+    if (!aborted_) {\n+      writer_state_->staged_rows_count -= rows_currently_staged_;\n+      while (!staged_batches_.empty()) {\n+        RETURN_NOT_OK(PopAndDeliverStagedBatch());\n+      }\n     }\n-    return running_task_.Then([this] { return DoFinish(); });\n+    return file_tasks_.End().Then([this] { return DoFinish(); });\n   }\n \n  private:\n-  Future<uint64_t> WriteNext() {\n+  Future<> WriteNext(std::shared_ptr<RecordBatch> next) {\n+    struct WriteTask {\n+      Status operator()() {\n+        int64_t rows_to_release = batch->num_rows();\n+        Status status = self->writer_->Write(batch);\n+        self->writer_state_->rows_in_flight_throttle.Release(rows_to_release);\n+        return status;\n+      }\n+      DatasetWriterFileQueue* self;\n+      std::shared_ptr<RecordBatch> batch;\n+    };\n     // May want to prototype / measure someday pushing the async write down further\n     return DeferNotOk(\n-        io::default_io_context().executor()->Submit([this]() -> Result<uint64_t> {\n-          DCHECK(running_task_.is_valid());\n-          std::unique_lock<std::mutex> lk(mutex);\n-          const std::shared_ptr<RecordBatch>& to_write = write_queue_.front();\n-          Future<uint64_t> on_complete = write_futures_.front();\n-          uint64_t rows_to_write = to_write->num_rows();\n-          lk.unlock();\n-          Status status = writer_->Write(to_write);\n-          lk.lock();\n-          write_queue_.pop_front();\n-          write_futures_.pop_front();\n-          lk.unlock();\n-          if (!status.ok()) {\n-            on_complete.MarkFinished(status);\n-          } else {\n-            on_complete.MarkFinished(rows_to_write);\n-          }\n-          return rows_to_write;\n-        }));\n+        io::default_io_context().executor()->Submit(WriteTask{this, std::move(next)}));\n   }\n \n   Status DoFinish() {\n     {\n-      std::lock_guard<std::mutex> lg(*visitors_mutex_);\n+      std::lock_guard<std::mutex> lg(writer_state_->visitors_mutex);\n       RETURN_NOT_OK(options_.writer_pre_finish(writer_.get()));\n     }\n     RETURN_NOT_OK(writer_->Finish());\n     {\n-      std::lock_guard<std::mutex> lg(*visitors_mutex_);\n+      std::lock_guard<std::mutex> lg(writer_state_->visitors_mutex);\n       return options_.writer_post_finish(writer_.get());\n     }\n   }\n \n   void Abort(Status err) {\n-    std::vector<Future<uint64_t>> futures_to_abort;\n-    Future<> old_running_task = running_task_;\n-    {\n-      std::lock_guard<std::mutex> lg(mutex);\n-      write_queue_.clear();\n-      futures_to_abort =\n-          std::vector<Future<uint64_t>>(write_futures_.begin(), write_futures_.end());\n-      write_futures_.clear();\n-      running_task_ = Future<>();\n-    }\n-    for (auto& fut : futures_to_abort) {\n-      fut.MarkFinished(err);\n-    }\n-    old_running_task.MarkFinished(std::move(err));\n-  }\n-\n-  void Flush() {\n-    std::unique_lock<std::mutex> lk(mutex);\n-    FlushUnlocked(std::move(lk));\n-  }\n-\n-  void FlushUnlocked(std::unique_lock<std::mutex> lk) {\n-    if (write_queue_.empty()) {\n-      Future<> old_running_task = running_task_;\n-      running_task_ = Future<>();\n-      lk.unlock();\n-      old_running_task.MarkFinished();\n-      return;\n-    }\n-    WriteNext().AddCallback([this](const Result<uint64_t>& res) {\n-      if (res.ok()) {\n-        Flush();\n-      } else {\n-        Abort(res.status());\n-      }\n-    });\n+    aborted_ = true;\n\nReview comment:\n       Hmm, true, but that isn't necessarily a good thing.  I suppose the various task groups should probably auto-abort once they receive a failed status.  This would match the behavior of the synchronous task groups.  At the moment this behavior is probably ok (if we fail a write call we will just keep trying to write to the file) but it would be cleaner to bail out sooner.  I've added ARROW-14565 to address in a follow-up as it has implications elsewhere.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-04T01:15:11.781+0000",
                    "updated": "2021-11-04T01:15:11.781+0000",
                    "started": "2021-11-04T01:15:11.781+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "675667",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/675712",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-958873513\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-04T01:21:02.627+0000",
                    "updated": "2021-11-04T01:21:02.627+0000",
                    "started": "2021-11-04T01:21:02.627+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "675712",
                    "issueId": "13407824"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/worklog/693040",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #11556:\nURL: https://github.com/apache/arrow/pull/11556#issuecomment-989620707\n\n\n   Rebased.  Found and fixed a deadlock / race condition in the serialized async task group.  The only build error now appears to be an unrelated plasma test.\r\n   \r\n   @lidavidm I assume you're still good with this getting merged in?  I don't know if you wanted to take a glance at the commit I added or not.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-12-09T08:25:42.141+0000",
                    "updated": "2021-12-09T08:25:42.141+0000",
                    "started": "2021-12-09T08:25:42.140+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "693040",
                    "issueId": "13407824"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "id": "1",
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "name": "Bug",
            "subtask": false,
            "avatarId": 21133
        },
        "timespent": 15600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@33ee82e4[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4cc6ebb7[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@43332d44[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@38d3fdc6[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@339524e0[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@5c43e57f[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@640aabe[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@215a4a7d[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3bf2c892[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@7859aa8e[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7bcfea61[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@8b55ec7[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 15600,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Dec 09 13:04:57 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-12-09T13:04:57.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-14426/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-10-21T19:21:07.000+0000",
        "updated": "2021-12-14T10:41:09.000+0000",
        "timeoriginalestimate": null,
        "description": "Right now we right whatever chunks we get, but if those chunks are exceptionally small, we should bundle them up and write out a configurable minimum row group size",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "4h 20m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 15600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Add a minimum_row_group_size to dataset writing",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/comment/17432764",
                    "id": "17432764",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "So things can get a little tricky in certain situations.  For example, if min_row_groups_size is 1M and the max_rows_queued is 64M and you just so happen to have 900k rows per file and are creating 100 files then you would end up in deadlock because it wouldn't write anything and it would hit the max_rows_queued limit.\r\n\r\nEven if you were only creating 50 files it would still be non-ideal because none of the writes would happen until the entire dataset had accumulated in memory.\r\n\r\nTo work around this I think I will create a soft limit (defaulting to 8M rows because I like nice round powers of two) of batchable rows.  Once there are more than 8M batchable rows I will start evicting batches, even though they are smaller than min_row_group_size.\r\n\r\nI'm fairly certain this will go unnoticed in 99% of scenarios until some point in the future when I've forgotten all of this and I'm debugging why a small batch got created.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2021-10-22T01:48:45.503+0000",
                    "updated": "2021-10-22T01:48:45.503+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13407824/comment/17456429",
                    "id": "17456429",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 11556\n[https://github.com/apache/arrow/pull/11556]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2021-12-09T13:04:57.022+0000",
                    "updated": "2021-12-09T13:04:57.022+0000"
                }
            ],
            "maxResults": 2,
            "total": 2,
            "startAt": 0
        },
        "customfield_12311820": "0|z0w28g:",
        "customfield_12314139": null
    }
}