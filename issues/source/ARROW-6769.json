{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13260123",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123",
    "key": "ARROW-6769",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12340948",
                "id": "12340948",
                "description": "",
                "name": "0.16.0",
                "archived": false,
                "released": true,
                "releaseDate": "2020-02-07"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "dataset",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12571164",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12571164",
                "type": {
                    "id": "10032",
                    "name": "Blocker",
                    "inward": "is blocked by",
                    "outward": "blocks",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"
                },
                "inwardIssue": {
                    "id": "13260122",
                    "key": "ARROW-6768",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260122",
                    "fields": {
                        "summary": "[C++][Dataset] Implement dataset::Scan to Table helper function",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=fsaintjacques",
            "name": "fsaintjacques",
            "key": "fsaintjacques",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=fsaintjacques&avatarId=37276",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=fsaintjacques&avatarId=37276",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=fsaintjacques&avatarId=37276",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=fsaintjacques&avatarId=37276"
            },
            "displayName": "Francois Saint-Jacques",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=fsaintjacques",
            "name": "fsaintjacques",
            "key": "fsaintjacques",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=fsaintjacques&avatarId=37276",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=fsaintjacques&avatarId=37276",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=fsaintjacques&avatarId=37276",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=fsaintjacques&avatarId=37276"
            },
            "displayName": "Francois Saint-Jacques",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=fsaintjacques",
            "name": "fsaintjacques",
            "key": "fsaintjacques",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=fsaintjacques&avatarId=37276",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=fsaintjacques&avatarId=37276",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=fsaintjacques&avatarId=37276",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=fsaintjacques&avatarId=37276"
            },
            "displayName": "Francois Saint-Jacques",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 21000,
            "total": 21000,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 21000,
            "total": 21000,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-6769/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 53,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329283",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #5675: ARROW-6769: [Dataset][C++] End to End test\nURL: https://github.com/apache/arrow/pull/5675\n \n \n   The following adds a full integration test (with in-memory MockFilesystem) of the dataset module. It also:\r\n   \r\n   - ScannerBuilder pushes the filter and columns selection to the constructed Scanner.\r\n   - Ensure DataSourceDiscovery make use of PartitionScheme\r\n   \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T17:29:17.855+0000",
                    "updated": "2019-10-16T17:29:17.855+0000",
                    "started": "2019-10-16T17:29:17.854+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329283",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329285",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on issue #5675: ARROW-6769: [Dataset][C++] End to End test\nURL: https://github.com/apache/arrow/pull/5675#issuecomment-542809363\n \n \n   https://issues.apache.org/jira/browse/ARROW-6769\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T17:32:07.011+0000",
                    "updated": "2019-10-16T17:32:07.011+0000",
                    "started": "2019-10-16T17:32:07.011+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329285",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329368",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335661525\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/dataset_test.cc\n ##########\n @@ -230,5 +233,170 @@ TEST(TestProjector, NonTrivial) {\n   AssertBatchesEqual(*expected_batch, *reconciled_batch);\n }\n \n+class TestEndToEnd : public TestDataset {\n+  void SetUp() {\n+    schema_ = schema({\n+        field(\"country\", utf8()),\n+        field(\"region\", utf8()),\n+        field(\"model\", utf8()),\n+        field(\"year\", int32()),\n+        field(\"sales\", int32()),\n+    });\n+\n+    using PathAndContent = std::vector<std::pair<std::string, std::string>>;\n+    auto files = PathAndContent{{\"/2018/01/US.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"3\", \"sales\": 742},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"S\", \"sales\": 304},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"X\", \"sales\": 136},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"Y\", \"sales\": 27}\n+      ])\"},\n+                                {\"/2018/01/CA.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"3\", \"sales\": 512},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"S\", \"sales\": 978},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"X\", \"sales\": 1},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"Y\", \"sales\": 69}\n+      ])\"},\n+                                {\"/2019/01/US.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 273},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 13},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 54},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 21}\n+      ])\"},\n+                                {\"/2019/01/CA.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 152},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 10},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 42},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 37}\n+      ])\"}};\n+\n+    auto fs = std::make_shared<fs::internal::MockFileSystem>(fs::kNoTime);\n+    for (const auto& f : files) {\n+      ARROW_EXPECT_OK(fs->CreateFile(f.first, f.second, /* recursive */ true));\n+    }\n+\n+    fs_ = fs;\n+  }\n+\n+ protected:\n+  std::shared_ptr<fs::FileSystem> fs_;\n+  std::shared_ptr<Schema> schema_;\n+};\n+\n+TEST_F(TestEndToEnd, EndToEndSingleSource) {\n+  // The dataset API is divided in 3 parts:\n+  //  - Creation\n+  //  - Querying\n+  //  - Consuming\n+\n+  // Creation.\n+  //\n+  // A Dataset is the union of one or more DataSources with the same schema.\n+  // Example of DataSource, FileSystemDataSource, OdbcDataSource,\n+  // FlightDataSource.\n+\n+  // A DataSource is composed of DataFragments. Each DataFragment can yield\n+  // multiple RecordBatch. DataSource can be created manually or \"discovered\"\n+  // via the DataSourceDiscovery interface.\n+  //\n+  // Each DataSourceDiscovery will have custom options.\n+  std::shared_ptr<DataSourceDiscovery> discovery;\n+\n+  // The user must specify which FileFormat is used to create FileFragment.\n+  // This option is specific to FileSystemBasedDataSource (and the builder).\n+  //\n+  // Note the JSONRecordBatchFileFormat requires a schema before creating the Discovery\n+  // class which is used to discover the schema. This chicken-and-egg problem\n+  // is only required for JSONRecordBatchFileFormat because it doesn't do\n+  // schema inspection.\n+  auto format = std::make_shared<JSONRecordBatchFileFormat>(schema_);\n+  // A selector is used to filter (or crawl) the files/directories of a\n+  // filesystem. If the options in Selector are not enough, the\n+  // FileSystemDataSourceDiscovery class also supports an explicit list of\n+  // fs::FileStats instead of the selector.\n+  fs::Selector s;\n+  s.base_dir = \"/\";\n+  s.recursive = true;\n+  ASSERT_OK(FileSystemDataSourceDiscovery::Make(fs_.get(), s, format, &discovery));\n+\n+  // DataFragments might have compatible but slightly different schemas, e.g.\n+  // schema evolved by adding/renaming columns. In this case, the schema is\n+  // passed to the dataset constructor.\n+  std::shared_ptr<Schema> inspected_schema;\n+  ASSERT_OK(discovery->Inspect(&inspected_schema));\n+  EXPECT_EQ(*schema_, *inspected_schema);\n+\n+  // Partitions expressions can be discovered for DataSource and DataFragments.\n+  // This metadata is then used in conjuction with the query filter to apply\n+  // the pushdown predicate optimization.\n+  auto partition_schema =\n+      schema({field(\"year\", int32()), field(\"month\", int32()), field(\"country\", utf8())});\n+  // The SchemaPartitionScheme is a simple scheme where the path is split with\n+  // the directory separator character and the components are typed and named\n+  // with the equivalent index in the schema, e.g.\n+  // (with the previous defined schema):\n+  //\n+  // - \"/2019\" -> {\"year\": 2019}\n+  // - \"/2019/01 -> {\"year\": 2019, \"month\": 1}\n+  // - \"/2019/01/CA -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  // - \"/2019/01/CA/a_file.json -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  auto partition_scheme = std::make_shared<SchemaPartitionScheme>(partition_schema);\n+  ASSERT_OK(discovery->SetPartitionScheme(partition_scheme));\n+\n+  // Build the DataSource where partitions are attached to fragments (files).\n+  std::shared_ptr<DataSource> datasource;\n+  ASSERT_OK(discovery->Finish(&datasource));\n+\n+  // Create the Dataset from our single DataSource.\n+  std::shared_ptr<Dataset> dataset =\n+      std::make_shared<Dataset>(DataSourceVector{datasource}, inspected_schema);\n+\n+  // Querying.\n+  //\n+  // The Scan operator materialize data from io into memory. Avoiding data\n+  // transfer is a critical optimization done by analytical engine. Thus, a\n+  // Scan can take multiple options, notably a subset of columns and a filter\n+  // expression.\n+  std::unique_ptr<ScannerBuilder> scanner_builder;\n+  ASSERT_OK(dataset->NewScan(&scanner_builder));\n+\n+  // An optional subset of columns can be provided. This will trickle to\n+  // DataFragment drivers. The net effect is that only columns of interest will\n+  // be materialized if the DataFragment supports it. This is the major benefit\n+  // of using a colum-wise format versus a row-wise format.\n \n Review comment:\n   ```suggestion\r\n     // of using a column-major format versus a row-major format.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:05.885+0000",
                    "updated": "2019-10-16T20:34:05.885+0000",
                    "started": "2019-10-16T20:34:05.885+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329368",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329369",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335687724\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.h\n ##########\n @@ -150,34 +153,55 @@ class ARROW_DS_EXPORT SimpleScanner : public Scanner {\n   std::shared_ptr<ScanContext> context_;\n };\n \n+/// \\brief ScannerBuilder is a factory class to construct a Scanner. It is used\n+/// to pass information, notably a potential filter expression and a subset of\n+/// columns to materialize.\n class ARROW_DS_EXPORT ScannerBuilder {\n  public:\n   ScannerBuilder(std::shared_ptr<Dataset> dataset,\n                  std::shared_ptr<ScanContext> scan_context);\n \n-  /// \\brief Set\n-  ScannerBuilder* Project(const std::vector<std::string>& columns);\n-\n-  ScannerBuilder* Filter(std::shared_ptr<Expression> filter);\n-  ScannerBuilder* Filter(const Expression& filter);\n-\n-  ScannerBuilder* FilterEvaluator(std::shared_ptr<ExpressionEvaluator> evaluator);\n+  /// \\brief Set the subset of columns to materialize.\n+  ///\n+  /// This columns be passed down to DataSources and corresponding DataFragments.\n+  /// The goal is to avoid copying/deserializing columns that will be\n+  /// ignored/dropped further down the compute chain.\n+  ///\n+  /// \\param[in] columns list of columns to project. Order and duplicates will\n+  ///            be preserved.\n+  ///\n+  /// \\return Failure if any column names does not exists in the dataset's\n+  ///         Schema.\n+  Status Project(const std::vector<std::string>& columns);\n \n-  ScannerBuilder* SetGlobalFileOptions(std::shared_ptr<FileScanOptions> options);\n+  /// \\brief Set the filter expression to return only rows matching the filter.\n+  ///\n+  /// The predicate will be passed down to DataSources and corresponding\n+  /// DataFragments to exploit pushdown predicate if possible either due to\n \n Review comment:\n   ```suggestion\r\n     /// DataFragments to exploit predicate pushdown if possible using\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.131+0000",
                    "updated": "2019-10-16T20:34:06.131+0000",
                    "started": "2019-10-16T20:34:06.130+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329369",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329370",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335662222\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/dataset_test.cc\n ##########\n @@ -230,5 +233,170 @@ TEST(TestProjector, NonTrivial) {\n   AssertBatchesEqual(*expected_batch, *reconciled_batch);\n }\n \n+class TestEndToEnd : public TestDataset {\n+  void SetUp() {\n+    schema_ = schema({\n+        field(\"country\", utf8()),\n+        field(\"region\", utf8()),\n+        field(\"model\", utf8()),\n+        field(\"year\", int32()),\n+        field(\"sales\", int32()),\n+    });\n+\n+    using PathAndContent = std::vector<std::pair<std::string, std::string>>;\n+    auto files = PathAndContent{{\"/2018/01/US.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"3\", \"sales\": 742},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"S\", \"sales\": 304},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"X\", \"sales\": 136},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"Y\", \"sales\": 27}\n+      ])\"},\n+                                {\"/2018/01/CA.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"3\", \"sales\": 512},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"S\", \"sales\": 978},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"X\", \"sales\": 1},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"Y\", \"sales\": 69}\n+      ])\"},\n+                                {\"/2019/01/US.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 273},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 13},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 54},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 21}\n+      ])\"},\n+                                {\"/2019/01/CA.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 152},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 10},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 42},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 37}\n+      ])\"}};\n+\n+    auto fs = std::make_shared<fs::internal::MockFileSystem>(fs::kNoTime);\n+    for (const auto& f : files) {\n+      ARROW_EXPECT_OK(fs->CreateFile(f.first, f.second, /* recursive */ true));\n+    }\n+\n+    fs_ = fs;\n+  }\n+\n+ protected:\n+  std::shared_ptr<fs::FileSystem> fs_;\n+  std::shared_ptr<Schema> schema_;\n+};\n+\n+TEST_F(TestEndToEnd, EndToEndSingleSource) {\n+  // The dataset API is divided in 3 parts:\n+  //  - Creation\n+  //  - Querying\n+  //  - Consuming\n+\n+  // Creation.\n+  //\n+  // A Dataset is the union of one or more DataSources with the same schema.\n+  // Example of DataSource, FileSystemDataSource, OdbcDataSource,\n+  // FlightDataSource.\n+\n+  // A DataSource is composed of DataFragments. Each DataFragment can yield\n+  // multiple RecordBatch. DataSource can be created manually or \"discovered\"\n+  // via the DataSourceDiscovery interface.\n+  //\n+  // Each DataSourceDiscovery will have custom options.\n+  std::shared_ptr<DataSourceDiscovery> discovery;\n+\n+  // The user must specify which FileFormat is used to create FileFragment.\n+  // This option is specific to FileSystemBasedDataSource (and the builder).\n+  //\n+  // Note the JSONRecordBatchFileFormat requires a schema before creating the Discovery\n+  // class which is used to discover the schema. This chicken-and-egg problem\n+  // is only required for JSONRecordBatchFileFormat because it doesn't do\n+  // schema inspection.\n+  auto format = std::make_shared<JSONRecordBatchFileFormat>(schema_);\n+  // A selector is used to filter (or crawl) the files/directories of a\n+  // filesystem. If the options in Selector are not enough, the\n+  // FileSystemDataSourceDiscovery class also supports an explicit list of\n+  // fs::FileStats instead of the selector.\n+  fs::Selector s;\n+  s.base_dir = \"/\";\n+  s.recursive = true;\n+  ASSERT_OK(FileSystemDataSourceDiscovery::Make(fs_.get(), s, format, &discovery));\n+\n+  // DataFragments might have compatible but slightly different schemas, e.g.\n+  // schema evolved by adding/renaming columns. In this case, the schema is\n+  // passed to the dataset constructor.\n+  std::shared_ptr<Schema> inspected_schema;\n+  ASSERT_OK(discovery->Inspect(&inspected_schema));\n+  EXPECT_EQ(*schema_, *inspected_schema);\n+\n+  // Partitions expressions can be discovered for DataSource and DataFragments.\n+  // This metadata is then used in conjuction with the query filter to apply\n+  // the pushdown predicate optimization.\n+  auto partition_schema =\n+      schema({field(\"year\", int32()), field(\"month\", int32()), field(\"country\", utf8())});\n+  // The SchemaPartitionScheme is a simple scheme where the path is split with\n+  // the directory separator character and the components are typed and named\n+  // with the equivalent index in the schema, e.g.\n+  // (with the previous defined schema):\n+  //\n+  // - \"/2019\" -> {\"year\": 2019}\n+  // - \"/2019/01 -> {\"year\": 2019, \"month\": 1}\n+  // - \"/2019/01/CA -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  // - \"/2019/01/CA/a_file.json -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  auto partition_scheme = std::make_shared<SchemaPartitionScheme>(partition_schema);\n+  ASSERT_OK(discovery->SetPartitionScheme(partition_scheme));\n+\n+  // Build the DataSource where partitions are attached to fragments (files).\n+  std::shared_ptr<DataSource> datasource;\n+  ASSERT_OK(discovery->Finish(&datasource));\n+\n+  // Create the Dataset from our single DataSource.\n+  std::shared_ptr<Dataset> dataset =\n+      std::make_shared<Dataset>(DataSourceVector{datasource}, inspected_schema);\n+\n+  // Querying.\n+  //\n+  // The Scan operator materialize data from io into memory. Avoiding data\n+  // transfer is a critical optimization done by analytical engine. Thus, a\n+  // Scan can take multiple options, notably a subset of columns and a filter\n+  // expression.\n+  std::unique_ptr<ScannerBuilder> scanner_builder;\n+  ASSERT_OK(dataset->NewScan(&scanner_builder));\n+\n+  // An optional subset of columns can be provided. This will trickle to\n+  // DataFragment drivers. The net effect is that only columns of interest will\n+  // be materialized if the DataFragment supports it. This is the major benefit\n+  // of using a colum-wise format versus a row-wise format.\n+  //\n+  // This API decouples the DataSource/DataFragment implementation and column\n+  // projection from the query part.\n+  //\n+  // For example, a ParquetFileDataFragment can only read the necessary bytes\n \n Review comment:\n   ```suggestion\r\n     // For example, a ParquetFileDataFragment may read the necessary byte ranges exclusively,\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.202+0000",
                    "updated": "2019-10-16T20:34:06.202+0000",
                    "started": "2019-10-16T20:34:06.201+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329370",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329371",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335661750\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/dataset_test.cc\n ##########\n @@ -230,5 +233,170 @@ TEST(TestProjector, NonTrivial) {\n   AssertBatchesEqual(*expected_batch, *reconciled_batch);\n }\n \n+class TestEndToEnd : public TestDataset {\n+  void SetUp() {\n+    schema_ = schema({\n+        field(\"country\", utf8()),\n+        field(\"region\", utf8()),\n+        field(\"model\", utf8()),\n+        field(\"year\", int32()),\n+        field(\"sales\", int32()),\n+    });\n+\n+    using PathAndContent = std::vector<std::pair<std::string, std::string>>;\n+    auto files = PathAndContent{{\"/2018/01/US.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"3\", \"sales\": 742},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"S\", \"sales\": 304},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"X\", \"sales\": 136},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"Y\", \"sales\": 27}\n+      ])\"},\n+                                {\"/2018/01/CA.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"3\", \"sales\": 512},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"S\", \"sales\": 978},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"X\", \"sales\": 1},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"Y\", \"sales\": 69}\n+      ])\"},\n+                                {\"/2019/01/US.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 273},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 13},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 54},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 21}\n+      ])\"},\n+                                {\"/2019/01/CA.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 152},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 10},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 42},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 37}\n+      ])\"}};\n+\n+    auto fs = std::make_shared<fs::internal::MockFileSystem>(fs::kNoTime);\n+    for (const auto& f : files) {\n+      ARROW_EXPECT_OK(fs->CreateFile(f.first, f.second, /* recursive */ true));\n+    }\n+\n+    fs_ = fs;\n+  }\n+\n+ protected:\n+  std::shared_ptr<fs::FileSystem> fs_;\n+  std::shared_ptr<Schema> schema_;\n+};\n+\n+TEST_F(TestEndToEnd, EndToEndSingleSource) {\n+  // The dataset API is divided in 3 parts:\n+  //  - Creation\n+  //  - Querying\n+  //  - Consuming\n+\n+  // Creation.\n+  //\n+  // A Dataset is the union of one or more DataSources with the same schema.\n+  // Example of DataSource, FileSystemDataSource, OdbcDataSource,\n+  // FlightDataSource.\n+\n+  // A DataSource is composed of DataFragments. Each DataFragment can yield\n+  // multiple RecordBatch. DataSource can be created manually or \"discovered\"\n+  // via the DataSourceDiscovery interface.\n+  //\n+  // Each DataSourceDiscovery will have custom options.\n+  std::shared_ptr<DataSourceDiscovery> discovery;\n+\n+  // The user must specify which FileFormat is used to create FileFragment.\n+  // This option is specific to FileSystemBasedDataSource (and the builder).\n+  //\n+  // Note the JSONRecordBatchFileFormat requires a schema before creating the Discovery\n+  // class which is used to discover the schema. This chicken-and-egg problem\n+  // is only required for JSONRecordBatchFileFormat because it doesn't do\n+  // schema inspection.\n+  auto format = std::make_shared<JSONRecordBatchFileFormat>(schema_);\n+  // A selector is used to filter (or crawl) the files/directories of a\n+  // filesystem. If the options in Selector are not enough, the\n+  // FileSystemDataSourceDiscovery class also supports an explicit list of\n+  // fs::FileStats instead of the selector.\n+  fs::Selector s;\n+  s.base_dir = \"/\";\n+  s.recursive = true;\n+  ASSERT_OK(FileSystemDataSourceDiscovery::Make(fs_.get(), s, format, &discovery));\n+\n+  // DataFragments might have compatible but slightly different schemas, e.g.\n+  // schema evolved by adding/renaming columns. In this case, the schema is\n+  // passed to the dataset constructor.\n+  std::shared_ptr<Schema> inspected_schema;\n+  ASSERT_OK(discovery->Inspect(&inspected_schema));\n+  EXPECT_EQ(*schema_, *inspected_schema);\n+\n+  // Partitions expressions can be discovered for DataSource and DataFragments.\n+  // This metadata is then used in conjuction with the query filter to apply\n+  // the pushdown predicate optimization.\n+  auto partition_schema =\n+      schema({field(\"year\", int32()), field(\"month\", int32()), field(\"country\", utf8())});\n+  // The SchemaPartitionScheme is a simple scheme where the path is split with\n+  // the directory separator character and the components are typed and named\n+  // with the equivalent index in the schema, e.g.\n+  // (with the previous defined schema):\n+  //\n+  // - \"/2019\" -> {\"year\": 2019}\n+  // - \"/2019/01 -> {\"year\": 2019, \"month\": 1}\n+  // - \"/2019/01/CA -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  // - \"/2019/01/CA/a_file.json -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  auto partition_scheme = std::make_shared<SchemaPartitionScheme>(partition_schema);\n+  ASSERT_OK(discovery->SetPartitionScheme(partition_scheme));\n+\n+  // Build the DataSource where partitions are attached to fragments (files).\n+  std::shared_ptr<DataSource> datasource;\n+  ASSERT_OK(discovery->Finish(&datasource));\n+\n+  // Create the Dataset from our single DataSource.\n+  std::shared_ptr<Dataset> dataset =\n+      std::make_shared<Dataset>(DataSourceVector{datasource}, inspected_schema);\n+\n+  // Querying.\n+  //\n+  // The Scan operator materialize data from io into memory. Avoiding data\n \n Review comment:\n   ```suggestion\r\n     // The Scan operator materializes data from io into memory. Avoiding data\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.454+0000",
                    "updated": "2019-10-16T20:34:06.454+0000",
                    "started": "2019-10-16T20:34:06.454+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329371",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329372",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335674459\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/partition.cc\n ##########\n @@ -90,5 +88,17 @@ Result<std::shared_ptr<Expression>> HivePartitionScheme::Parse(\n   return ConvertPartitionKeys(GetUnconvertedKeys(path), *schema_);\n }\n \n+Status ApplyPartitionScheme(PartitionScheme& scheme, std::vector<fs::FileStats> files,\n+                            PathPartitions* out) {\n+  for (const auto& file : files) {\n+    const auto& path = file.path();\n+    std::shared_ptr<Expression> partition;\n+    RETURN_NOT_OK(scheme.Parse(path, &partition));\n+    (*out)[path] = partition;\n \n Review comment:\n   ```suggestion\r\n       out->emplace(path, std::move(partition));\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.463+0000",
                    "updated": "2019-10-16T20:34:06.463+0000",
                    "started": "2019-10-16T20:34:06.463+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329372",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329373",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335663080\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/dataset_test.cc\n ##########\n @@ -230,5 +233,170 @@ TEST(TestProjector, NonTrivial) {\n   AssertBatchesEqual(*expected_batch, *reconciled_batch);\n }\n \n+class TestEndToEnd : public TestDataset {\n+  void SetUp() {\n+    schema_ = schema({\n+        field(\"country\", utf8()),\n+        field(\"region\", utf8()),\n+        field(\"model\", utf8()),\n+        field(\"year\", int32()),\n+        field(\"sales\", int32()),\n+    });\n+\n+    using PathAndContent = std::vector<std::pair<std::string, std::string>>;\n+    auto files = PathAndContent{{\"/2018/01/US.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"3\", \"sales\": 742},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"S\", \"sales\": 304},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"X\", \"sales\": 136},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"Y\", \"sales\": 27}\n+      ])\"},\n+                                {\"/2018/01/CA.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"3\", \"sales\": 512},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"S\", \"sales\": 978},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"X\", \"sales\": 1},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"Y\", \"sales\": 69}\n+      ])\"},\n+                                {\"/2019/01/US.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 273},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 13},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 54},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 21}\n+      ])\"},\n+                                {\"/2019/01/CA.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 152},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 10},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 42},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 37}\n+      ])\"}};\n+\n+    auto fs = std::make_shared<fs::internal::MockFileSystem>(fs::kNoTime);\n+    for (const auto& f : files) {\n+      ARROW_EXPECT_OK(fs->CreateFile(f.first, f.second, /* recursive */ true));\n+    }\n+\n+    fs_ = fs;\n+  }\n+\n+ protected:\n+  std::shared_ptr<fs::FileSystem> fs_;\n+  std::shared_ptr<Schema> schema_;\n+};\n+\n+TEST_F(TestEndToEnd, EndToEndSingleSource) {\n+  // The dataset API is divided in 3 parts:\n+  //  - Creation\n+  //  - Querying\n+  //  - Consuming\n+\n+  // Creation.\n+  //\n+  // A Dataset is the union of one or more DataSources with the same schema.\n+  // Example of DataSource, FileSystemDataSource, OdbcDataSource,\n+  // FlightDataSource.\n+\n+  // A DataSource is composed of DataFragments. Each DataFragment can yield\n+  // multiple RecordBatch. DataSource can be created manually or \"discovered\"\n+  // via the DataSourceDiscovery interface.\n+  //\n+  // Each DataSourceDiscovery will have custom options.\n+  std::shared_ptr<DataSourceDiscovery> discovery;\n+\n+  // The user must specify which FileFormat is used to create FileFragment.\n+  // This option is specific to FileSystemBasedDataSource (and the builder).\n+  //\n+  // Note the JSONRecordBatchFileFormat requires a schema before creating the Discovery\n+  // class which is used to discover the schema. This chicken-and-egg problem\n+  // is only required for JSONRecordBatchFileFormat because it doesn't do\n+  // schema inspection.\n+  auto format = std::make_shared<JSONRecordBatchFileFormat>(schema_);\n+  // A selector is used to filter (or crawl) the files/directories of a\n+  // filesystem. If the options in Selector are not enough, the\n+  // FileSystemDataSourceDiscovery class also supports an explicit list of\n+  // fs::FileStats instead of the selector.\n+  fs::Selector s;\n+  s.base_dir = \"/\";\n+  s.recursive = true;\n+  ASSERT_OK(FileSystemDataSourceDiscovery::Make(fs_.get(), s, format, &discovery));\n+\n+  // DataFragments might have compatible but slightly different schemas, e.g.\n+  // schema evolved by adding/renaming columns. In this case, the schema is\n+  // passed to the dataset constructor.\n+  std::shared_ptr<Schema> inspected_schema;\n+  ASSERT_OK(discovery->Inspect(&inspected_schema));\n+  EXPECT_EQ(*schema_, *inspected_schema);\n+\n+  // Partitions expressions can be discovered for DataSource and DataFragments.\n+  // This metadata is then used in conjuction with the query filter to apply\n+  // the pushdown predicate optimization.\n+  auto partition_schema =\n+      schema({field(\"year\", int32()), field(\"month\", int32()), field(\"country\", utf8())});\n+  // The SchemaPartitionScheme is a simple scheme where the path is split with\n+  // the directory separator character and the components are typed and named\n+  // with the equivalent index in the schema, e.g.\n+  // (with the previous defined schema):\n+  //\n+  // - \"/2019\" -> {\"year\": 2019}\n+  // - \"/2019/01 -> {\"year\": 2019, \"month\": 1}\n+  // - \"/2019/01/CA -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  // - \"/2019/01/CA/a_file.json -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  auto partition_scheme = std::make_shared<SchemaPartitionScheme>(partition_schema);\n+  ASSERT_OK(discovery->SetPartitionScheme(partition_scheme));\n+\n+  // Build the DataSource where partitions are attached to fragments (files).\n+  std::shared_ptr<DataSource> datasource;\n+  ASSERT_OK(discovery->Finish(&datasource));\n+\n+  // Create the Dataset from our single DataSource.\n+  std::shared_ptr<Dataset> dataset =\n+      std::make_shared<Dataset>(DataSourceVector{datasource}, inspected_schema);\n+\n+  // Querying.\n+  //\n+  // The Scan operator materialize data from io into memory. Avoiding data\n+  // transfer is a critical optimization done by analytical engine. Thus, a\n+  // Scan can take multiple options, notably a subset of columns and a filter\n+  // expression.\n+  std::unique_ptr<ScannerBuilder> scanner_builder;\n+  ASSERT_OK(dataset->NewScan(&scanner_builder));\n+\n+  // An optional subset of columns can be provided. This will trickle to\n+  // DataFragment drivers. The net effect is that only columns of interest will\n+  // be materialized if the DataFragment supports it. This is the major benefit\n+  // of using a colum-wise format versus a row-wise format.\n+  //\n+  // This API decouples the DataSource/DataFragment implementation and column\n+  // projection from the query part.\n+  //\n+  // For example, a ParquetFileDataFragment can only read the necessary bytes\n+  // ranges, or an OdbcDataFragment could craft the query with proper SELECT\n \n Review comment:\n   ```suggestion\r\n     // ranges, or an OdbcDataFragment could convert the projection to a SELECT\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.628+0000",
                    "updated": "2019-10-16T20:34:06.628+0000",
                    "started": "2019-10-16T20:34:06.627+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329373",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329374",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335664384\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/dataset_test.cc\n ##########\n @@ -230,5 +233,170 @@ TEST(TestProjector, NonTrivial) {\n   AssertBatchesEqual(*expected_batch, *reconciled_batch);\n }\n \n+class TestEndToEnd : public TestDataset {\n+  void SetUp() {\n+    schema_ = schema({\n+        field(\"country\", utf8()),\n+        field(\"region\", utf8()),\n+        field(\"model\", utf8()),\n+        field(\"year\", int32()),\n+        field(\"sales\", int32()),\n+    });\n+\n+    using PathAndContent = std::vector<std::pair<std::string, std::string>>;\n+    auto files = PathAndContent{{\"/2018/01/US.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"3\", \"sales\": 742},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"S\", \"sales\": 304},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"X\", \"sales\": 136},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"Y\", \"sales\": 27}\n+      ])\"},\n+                                {\"/2018/01/CA.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"3\", \"sales\": 512},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"S\", \"sales\": 978},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"X\", \"sales\": 1},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"Y\", \"sales\": 69}\n+      ])\"},\n+                                {\"/2019/01/US.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 273},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 13},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 54},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 21}\n+      ])\"},\n+                                {\"/2019/01/CA.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 152},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 10},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 42},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 37}\n+      ])\"}};\n+\n+    auto fs = std::make_shared<fs::internal::MockFileSystem>(fs::kNoTime);\n+    for (const auto& f : files) {\n+      ARROW_EXPECT_OK(fs->CreateFile(f.first, f.second, /* recursive */ true));\n+    }\n+\n+    fs_ = fs;\n+  }\n+\n+ protected:\n+  std::shared_ptr<fs::FileSystem> fs_;\n+  std::shared_ptr<Schema> schema_;\n+};\n+\n+TEST_F(TestEndToEnd, EndToEndSingleSource) {\n+  // The dataset API is divided in 3 parts:\n+  //  - Creation\n+  //  - Querying\n+  //  - Consuming\n+\n+  // Creation.\n+  //\n+  // A Dataset is the union of one or more DataSources with the same schema.\n+  // Example of DataSource, FileSystemDataSource, OdbcDataSource,\n+  // FlightDataSource.\n+\n+  // A DataSource is composed of DataFragments. Each DataFragment can yield\n+  // multiple RecordBatch. DataSource can be created manually or \"discovered\"\n+  // via the DataSourceDiscovery interface.\n+  //\n+  // Each DataSourceDiscovery will have custom options.\n+  std::shared_ptr<DataSourceDiscovery> discovery;\n+\n+  // The user must specify which FileFormat is used to create FileFragment.\n+  // This option is specific to FileSystemBasedDataSource (and the builder).\n+  //\n+  // Note the JSONRecordBatchFileFormat requires a schema before creating the Discovery\n+  // class which is used to discover the schema. This chicken-and-egg problem\n+  // is only required for JSONRecordBatchFileFormat because it doesn't do\n+  // schema inspection.\n+  auto format = std::make_shared<JSONRecordBatchFileFormat>(schema_);\n+  // A selector is used to filter (or crawl) the files/directories of a\n+  // filesystem. If the options in Selector are not enough, the\n+  // FileSystemDataSourceDiscovery class also supports an explicit list of\n+  // fs::FileStats instead of the selector.\n+  fs::Selector s;\n+  s.base_dir = \"/\";\n+  s.recursive = true;\n+  ASSERT_OK(FileSystemDataSourceDiscovery::Make(fs_.get(), s, format, &discovery));\n+\n+  // DataFragments might have compatible but slightly different schemas, e.g.\n+  // schema evolved by adding/renaming columns. In this case, the schema is\n+  // passed to the dataset constructor.\n+  std::shared_ptr<Schema> inspected_schema;\n+  ASSERT_OK(discovery->Inspect(&inspected_schema));\n+  EXPECT_EQ(*schema_, *inspected_schema);\n+\n+  // Partitions expressions can be discovered for DataSource and DataFragments.\n+  // This metadata is then used in conjuction with the query filter to apply\n+  // the pushdown predicate optimization.\n+  auto partition_schema =\n+      schema({field(\"year\", int32()), field(\"month\", int32()), field(\"country\", utf8())});\n+  // The SchemaPartitionScheme is a simple scheme where the path is split with\n+  // the directory separator character and the components are typed and named\n+  // with the equivalent index in the schema, e.g.\n+  // (with the previous defined schema):\n+  //\n+  // - \"/2019\" -> {\"year\": 2019}\n+  // - \"/2019/01 -> {\"year\": 2019, \"month\": 1}\n+  // - \"/2019/01/CA -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  // - \"/2019/01/CA/a_file.json -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  auto partition_scheme = std::make_shared<SchemaPartitionScheme>(partition_schema);\n+  ASSERT_OK(discovery->SetPartitionScheme(partition_scheme));\n+\n+  // Build the DataSource where partitions are attached to fragments (files).\n+  std::shared_ptr<DataSource> datasource;\n+  ASSERT_OK(discovery->Finish(&datasource));\n+\n+  // Create the Dataset from our single DataSource.\n+  std::shared_ptr<Dataset> dataset =\n+      std::make_shared<Dataset>(DataSourceVector{datasource}, inspected_schema);\n+\n+  // Querying.\n+  //\n+  // The Scan operator materialize data from io into memory. Avoiding data\n+  // transfer is a critical optimization done by analytical engine. Thus, a\n+  // Scan can take multiple options, notably a subset of columns and a filter\n+  // expression.\n+  std::unique_ptr<ScannerBuilder> scanner_builder;\n+  ASSERT_OK(dataset->NewScan(&scanner_builder));\n+\n+  // An optional subset of columns can be provided. This will trickle to\n+  // DataFragment drivers. The net effect is that only columns of interest will\n+  // be materialized if the DataFragment supports it. This is the major benefit\n+  // of using a colum-wise format versus a row-wise format.\n+  //\n+  // This API decouples the DataSource/DataFragment implementation and column\n+  // projection from the query part.\n+  //\n+  // For example, a ParquetFileDataFragment can only read the necessary bytes\n+  // ranges, or an OdbcDataFragment could craft the query with proper SELECT\n+  // statement. The CsvFileDataFragment wouldn't benefit from this as much, but\n+  // could still skip converting un-needed columns.\n+  ASSERT_OK(scanner_builder->Project({\"sales\", \"model\"}));\n+\n+  // An optional filter expression may also be specified. The filter expression\n+  // is matched on input rows and only matching rows are returned.\n+  // Predicate pushdown optimization is applied on partitions if possible.\n \n Review comment:\n   ```suggestion\r\n     // Predicate pushdown optimizations are applied using partition information if available.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.710+0000",
                    "updated": "2019-10-16T20:34:06.710+0000",
                    "started": "2019-10-16T20:34:06.709+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329374",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329375",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335664103\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/dataset_test.cc\n ##########\n @@ -230,5 +233,170 @@ TEST(TestProjector, NonTrivial) {\n   AssertBatchesEqual(*expected_batch, *reconciled_batch);\n }\n \n+class TestEndToEnd : public TestDataset {\n+  void SetUp() {\n+    schema_ = schema({\n+        field(\"country\", utf8()),\n+        field(\"region\", utf8()),\n+        field(\"model\", utf8()),\n+        field(\"year\", int32()),\n+        field(\"sales\", int32()),\n+    });\n+\n+    using PathAndContent = std::vector<std::pair<std::string, std::string>>;\n+    auto files = PathAndContent{{\"/2018/01/US.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"3\", \"sales\": 742},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"S\", \"sales\": 304},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"X\", \"sales\": 136},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"Y\", \"sales\": 27}\n+      ])\"},\n+                                {\"/2018/01/CA.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"3\", \"sales\": 512},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"S\", \"sales\": 978},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"X\", \"sales\": 1},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"Y\", \"sales\": 69}\n+      ])\"},\n+                                {\"/2019/01/US.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 273},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 13},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 54},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 21}\n+      ])\"},\n+                                {\"/2019/01/CA.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 152},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 10},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 42},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 37}\n+      ])\"}};\n+\n+    auto fs = std::make_shared<fs::internal::MockFileSystem>(fs::kNoTime);\n+    for (const auto& f : files) {\n+      ARROW_EXPECT_OK(fs->CreateFile(f.first, f.second, /* recursive */ true));\n+    }\n+\n+    fs_ = fs;\n+  }\n+\n+ protected:\n+  std::shared_ptr<fs::FileSystem> fs_;\n+  std::shared_ptr<Schema> schema_;\n+};\n+\n+TEST_F(TestEndToEnd, EndToEndSingleSource) {\n+  // The dataset API is divided in 3 parts:\n+  //  - Creation\n+  //  - Querying\n+  //  - Consuming\n+\n+  // Creation.\n+  //\n+  // A Dataset is the union of one or more DataSources with the same schema.\n+  // Example of DataSource, FileSystemDataSource, OdbcDataSource,\n+  // FlightDataSource.\n+\n+  // A DataSource is composed of DataFragments. Each DataFragment can yield\n+  // multiple RecordBatch. DataSource can be created manually or \"discovered\"\n+  // via the DataSourceDiscovery interface.\n+  //\n+  // Each DataSourceDiscovery will have custom options.\n+  std::shared_ptr<DataSourceDiscovery> discovery;\n+\n+  // The user must specify which FileFormat is used to create FileFragment.\n+  // This option is specific to FileSystemBasedDataSource (and the builder).\n+  //\n+  // Note the JSONRecordBatchFileFormat requires a schema before creating the Discovery\n+  // class which is used to discover the schema. This chicken-and-egg problem\n+  // is only required for JSONRecordBatchFileFormat because it doesn't do\n+  // schema inspection.\n+  auto format = std::make_shared<JSONRecordBatchFileFormat>(schema_);\n+  // A selector is used to filter (or crawl) the files/directories of a\n+  // filesystem. If the options in Selector are not enough, the\n+  // FileSystemDataSourceDiscovery class also supports an explicit list of\n+  // fs::FileStats instead of the selector.\n+  fs::Selector s;\n+  s.base_dir = \"/\";\n+  s.recursive = true;\n+  ASSERT_OK(FileSystemDataSourceDiscovery::Make(fs_.get(), s, format, &discovery));\n+\n+  // DataFragments might have compatible but slightly different schemas, e.g.\n+  // schema evolved by adding/renaming columns. In this case, the schema is\n+  // passed to the dataset constructor.\n+  std::shared_ptr<Schema> inspected_schema;\n+  ASSERT_OK(discovery->Inspect(&inspected_schema));\n+  EXPECT_EQ(*schema_, *inspected_schema);\n+\n+  // Partitions expressions can be discovered for DataSource and DataFragments.\n+  // This metadata is then used in conjuction with the query filter to apply\n+  // the pushdown predicate optimization.\n+  auto partition_schema =\n+      schema({field(\"year\", int32()), field(\"month\", int32()), field(\"country\", utf8())});\n+  // The SchemaPartitionScheme is a simple scheme where the path is split with\n+  // the directory separator character and the components are typed and named\n+  // with the equivalent index in the schema, e.g.\n+  // (with the previous defined schema):\n+  //\n+  // - \"/2019\" -> {\"year\": 2019}\n+  // - \"/2019/01 -> {\"year\": 2019, \"month\": 1}\n+  // - \"/2019/01/CA -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  // - \"/2019/01/CA/a_file.json -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  auto partition_scheme = std::make_shared<SchemaPartitionScheme>(partition_schema);\n+  ASSERT_OK(discovery->SetPartitionScheme(partition_scheme));\n+\n+  // Build the DataSource where partitions are attached to fragments (files).\n+  std::shared_ptr<DataSource> datasource;\n+  ASSERT_OK(discovery->Finish(&datasource));\n+\n+  // Create the Dataset from our single DataSource.\n+  std::shared_ptr<Dataset> dataset =\n+      std::make_shared<Dataset>(DataSourceVector{datasource}, inspected_schema);\n+\n+  // Querying.\n+  //\n+  // The Scan operator materialize data from io into memory. Avoiding data\n+  // transfer is a critical optimization done by analytical engine. Thus, a\n+  // Scan can take multiple options, notably a subset of columns and a filter\n+  // expression.\n+  std::unique_ptr<ScannerBuilder> scanner_builder;\n+  ASSERT_OK(dataset->NewScan(&scanner_builder));\n+\n+  // An optional subset of columns can be provided. This will trickle to\n+  // DataFragment drivers. The net effect is that only columns of interest will\n+  // be materialized if the DataFragment supports it. This is the major benefit\n+  // of using a colum-wise format versus a row-wise format.\n+  //\n+  // This API decouples the DataSource/DataFragment implementation and column\n+  // projection from the query part.\n+  //\n+  // For example, a ParquetFileDataFragment can only read the necessary bytes\n+  // ranges, or an OdbcDataFragment could craft the query with proper SELECT\n+  // statement. The CsvFileDataFragment wouldn't benefit from this as much, but\n+  // could still skip converting un-needed columns.\n+  ASSERT_OK(scanner_builder->Project({\"sales\", \"model\"}));\n+\n+  // An optional filter expression may also be specified. The filter expression\n+  // is matched on input rows and only matching rows are returned.\n \n Review comment:\n   ```suggestion\r\n     // is evaluated against input rows. Only rows for which the filter evauates to true are yielded.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.732+0000",
                    "updated": "2019-10-16T20:34:06.732+0000",
                    "started": "2019-10-16T20:34:06.732+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329375",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329376",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335688104\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.h\n ##########\n @@ -150,34 +153,55 @@ class ARROW_DS_EXPORT SimpleScanner : public Scanner {\n   std::shared_ptr<ScanContext> context_;\n };\n \n+/// \\brief ScannerBuilder is a factory class to construct a Scanner. It is used\n+/// to pass information, notably a potential filter expression and a subset of\n+/// columns to materialize.\n class ARROW_DS_EXPORT ScannerBuilder {\n  public:\n   ScannerBuilder(std::shared_ptr<Dataset> dataset,\n                  std::shared_ptr<ScanContext> scan_context);\n \n-  /// \\brief Set\n-  ScannerBuilder* Project(const std::vector<std::string>& columns);\n-\n-  ScannerBuilder* Filter(std::shared_ptr<Expression> filter);\n-  ScannerBuilder* Filter(const Expression& filter);\n-\n-  ScannerBuilder* FilterEvaluator(std::shared_ptr<ExpressionEvaluator> evaluator);\n+  /// \\brief Set the subset of columns to materialize.\n+  ///\n+  /// This columns be passed down to DataSources and corresponding DataFragments.\n+  /// The goal is to avoid copying/deserializing columns that will be\n+  /// ignored/dropped further down the compute chain.\n+  ///\n+  /// \\param[in] columns list of columns to project. Order and duplicates will\n+  ///            be preserved.\n+  ///\n+  /// \\return Failure if any column names does not exists in the dataset's\n+  ///         Schema.\n+  Status Project(const std::vector<std::string>& columns);\n \n-  ScannerBuilder* SetGlobalFileOptions(std::shared_ptr<FileScanOptions> options);\n+  /// \\brief Set the filter expression to return only rows matching the filter.\n+  ///\n+  /// The predicate will be passed down to DataSources and corresponding\n+  /// DataFragments to exploit pushdown predicate if possible either due to\n+  /// partitions information, or DataFragment internal knowledge, e.g. Parquet\n \n Review comment:\n   ```suggestion\r\n     /// partition information or DataFragment internal metadata, e.g. Parquet statistics.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.794+0000",
                    "updated": "2019-10-16T20:34:06.794+0000",
                    "started": "2019-10-16T20:34:06.794+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329376",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329377",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335688864\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.h\n ##########\n @@ -150,34 +153,55 @@ class ARROW_DS_EXPORT SimpleScanner : public Scanner {\n   std::shared_ptr<ScanContext> context_;\n };\n \n+/// \\brief ScannerBuilder is a factory class to construct a Scanner. It is used\n+/// to pass information, notably a potential filter expression and a subset of\n+/// columns to materialize.\n class ARROW_DS_EXPORT ScannerBuilder {\n  public:\n   ScannerBuilder(std::shared_ptr<Dataset> dataset,\n                  std::shared_ptr<ScanContext> scan_context);\n \n-  /// \\brief Set\n-  ScannerBuilder* Project(const std::vector<std::string>& columns);\n-\n-  ScannerBuilder* Filter(std::shared_ptr<Expression> filter);\n-  ScannerBuilder* Filter(const Expression& filter);\n-\n-  ScannerBuilder* FilterEvaluator(std::shared_ptr<ExpressionEvaluator> evaluator);\n+  /// \\brief Set the subset of columns to materialize.\n+  ///\n+  /// This columns be passed down to DataSources and corresponding DataFragments.\n+  /// The goal is to avoid copying/deserializing columns that will be\n \n Review comment:\n   ```suggestion\r\n     /// The goal is to avoid loading/copying/deserializing columns that will\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.797+0000",
                    "updated": "2019-10-16T20:34:06.797+0000",
                    "started": "2019-10-16T20:34:06.797+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329377",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329378",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335688713\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.h\n ##########\n @@ -150,34 +153,55 @@ class ARROW_DS_EXPORT SimpleScanner : public Scanner {\n   std::shared_ptr<ScanContext> context_;\n };\n \n+/// \\brief ScannerBuilder is a factory class to construct a Scanner. It is used\n+/// to pass information, notably a potential filter expression and a subset of\n+/// columns to materialize.\n class ARROW_DS_EXPORT ScannerBuilder {\n  public:\n   ScannerBuilder(std::shared_ptr<Dataset> dataset,\n                  std::shared_ptr<ScanContext> scan_context);\n \n-  /// \\brief Set\n-  ScannerBuilder* Project(const std::vector<std::string>& columns);\n-\n-  ScannerBuilder* Filter(std::shared_ptr<Expression> filter);\n-  ScannerBuilder* Filter(const Expression& filter);\n-\n-  ScannerBuilder* FilterEvaluator(std::shared_ptr<ExpressionEvaluator> evaluator);\n+  /// \\brief Set the subset of columns to materialize.\n+  ///\n+  /// This columns be passed down to DataSources and corresponding DataFragments.\n \n Review comment:\n   ```suggestion\r\n     /// This subset wil be passed down to DataSources and corresponding DataFragments.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.966+0000",
                    "updated": "2019-10-16T20:34:06.966+0000",
                    "started": "2019-10-16T20:34:06.965+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329378",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329379",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335674636\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/partition.cc\n ##########\n @@ -90,5 +88,17 @@ Result<std::shared_ptr<Expression>> HivePartitionScheme::Parse(\n   return ConvertPartitionKeys(GetUnconvertedKeys(path), *schema_);\n }\n \n+Status ApplyPartitionScheme(PartitionScheme& scheme, std::vector<fs::FileStats> files,\n+                            PathPartitions* out) {\n \n Review comment:\n   ```suggestion\r\n                               PathPartitions* out) {\r\n     out->clear();\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:06.969+0000",
                    "updated": "2019-10-16T20:34:06.969+0000",
                    "started": "2019-10-16T20:34:06.968+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329379",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329380",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335694730\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.cc\n ##########\n @@ -66,53 +119,96 @@ ScanTaskIterator SimpleScanner::Scan() {\n   // Second, transforms Iterator<DataFragment> into a unified\n   // Iterator<ScanTask>. The first Iterator::Next invocation is going to do\n   // all the work of unwinding the chained iterators.\n-  return GetScanTaskIterator(std::move(fragments_it), context_);\n+  auto scan_task_it = GetScanTaskIterator(std::move(fragments_it), context_);\n+  // Third, apply the filter evaluator to incoming RecordBatches.\n+  auto filter_it =\n+      FilterScanTask(std::move(scan_task_it), options_->evaluator, options_->filter);\n+  // Finally, apply the final projection to scan_task_it if necessary. The\n+  // schema returned to the user might not be the same as given by the\n+  // DataFragment, e.g. due to columns required by the filter but not in the\n+  // final schema.\n+  return ProjectScanTask(std::move(filter_it), options_->projector);\n+}\n+\n+Status ScanTaskIteratorFromRecordBatch(std::vector<std::shared_ptr<RecordBatch>> batches,\n+                                       ScanTaskIterator* out) {\n+  std::unique_ptr<ScanTask> scan_task = internal::make_unique<SimpleScanTask>(batches);\n+\n+  std::vector<std::unique_ptr<ScanTask>> tasks;\n+  tasks.emplace_back(std::move(scan_task));\n+\n+  *out = MakeVectorIterator(std::move(tasks));\n+  return Status::OK();\n }\n \n ScannerBuilder::ScannerBuilder(std::shared_ptr<Dataset> dataset,\n                                std::shared_ptr<ScanContext> scan_context)\n     : dataset_(std::move(dataset)),\n       scan_options_(ScanOptions::Defaults()),\n-      scan_context_(std::move(scan_context)) {}\n+      scan_context_(std::move(scan_context)),\n+      pool_(default_memory_pool()) {}\n+\n+Status EnsureColumnsInSchema(const std::shared_ptr<Schema>& schema,\n+                             const std::vector<std::string>& columns) {\n+  for (const auto& column : columns) {\n+    if (schema->GetFieldByName(column) == nullptr) {\n+      return Status::Invalid(\"Requested column \", column,\n+                             \" not found in dataset's schema.\");\n+    }\n+  }\n \n-ScannerBuilder* ScannerBuilder::Project(const std::vector<std::string>& columns) {\n-  return this;\n+  return Status::OK();\n }\n \n-ScannerBuilder* ScannerBuilder::Filter(std::shared_ptr<Expression> filter) {\n-  scan_options_->filter = std::move(filter);\n-  return this;\n+Status ScannerBuilder::Project(const std::vector<std::string>& columns) {\n+  RETURN_NOT_OK(EnsureColumnsInSchema(schema(), columns));\n+  project_columns_ = columns;\n+  return Status::OK();\n }\n \n-ScannerBuilder* ScannerBuilder::Filter(const Expression& filter) {\n-  return Filter(filter.Copy());\n+Status ScannerBuilder::Filter(std::shared_ptr<Expression> filter) {\n+  if (filter != nullptr) {\n+    RETURN_NOT_OK(EnsureColumnsInSchema(schema(), FieldsInExpression(*filter)));\n+  }\n+  scan_options_->filter = std::move(filter);\n+  return Status::OK();\n }\n \n-ScannerBuilder* ScannerBuilder::FilterEvaluator(\n-    std::shared_ptr<ExpressionEvaluator> evaluator) {\n-  scan_options_->evaluator = std::move(evaluator);\n-  return this;\n+Status ScannerBuilder::SetMemoryPool(MemoryPool* pool) {\n+  pool_ = pool;\n+  return Status::OK();\n }\n \n-ScannerBuilder* ScannerBuilder::SetGlobalFileOptions(\n-    std::shared_ptr<FileScanOptions> options) {\n-  return this;\n-}\n+Status ScannerBuilder::Filter(const Expression& filter) { return Filter(filter.Copy()); }\n \n-ScannerBuilder* ScannerBuilder::IncludePartitionKeys(bool include) {\n-  scan_options_->include_partition_keys = include;\n-  return this;\n+std::shared_ptr<Schema> SchemaFromColumnNames(\n+    const std::shared_ptr<Schema>& input, const std::vector<std::string>& column_names) {\n+  std::vector<std::shared_ptr<Field>> columns;\n+  for (const auto& name : column_names) {\n+    columns.push_back(input->GetFieldByName(name));\n+  }\n+\n+  return std::make_shared<Schema>(columns);\n }\n \n Status ScannerBuilder::Finish(std::unique_ptr<Scanner>* out) const {\n+  if (!project_columns_.empty()) {\n+    scan_options_->projector = std::make_shared<RecordBatchProjector>(\n+        scan_context_->pool, SchemaFromColumnNames(schema(), project_columns_));\n+  }\n+\n+  if (scan_options_->filter != nullptr) {\n \n Review comment:\n   dead null check\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:07.107+0000",
                    "updated": "2019-10-16T20:34:07.107+0000",
                    "started": "2019-10-16T20:34:07.107+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329380",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329381",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335664864\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/dataset_test.cc\n ##########\n @@ -230,5 +233,170 @@ TEST(TestProjector, NonTrivial) {\n   AssertBatchesEqual(*expected_batch, *reconciled_batch);\n }\n \n+class TestEndToEnd : public TestDataset {\n+  void SetUp() {\n+    schema_ = schema({\n+        field(\"country\", utf8()),\n+        field(\"region\", utf8()),\n+        field(\"model\", utf8()),\n+        field(\"year\", int32()),\n+        field(\"sales\", int32()),\n+    });\n+\n+    using PathAndContent = std::vector<std::pair<std::string, std::string>>;\n+    auto files = PathAndContent{{\"/2018/01/US.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"3\", \"sales\": 742},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"S\", \"sales\": 304},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"X\", \"sales\": 136},\n+        {\"country\": \"US\", \"region\": \"NY\", \"year\": 2018, \"model\": \"Y\", \"sales\": 27}\n+      ])\"},\n+                                {\"/2018/01/CA.json\", R\"([\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"3\", \"sales\": 512},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"S\", \"sales\": 978},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"X\", \"sales\": 1},\n+        {\"country\": \"US\", \"region\": \"CA\", \"year\": 2018, \"model\": \"Y\", \"sales\": 69}\n+      ])\"},\n+                                {\"/2019/01/US.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 273},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 13},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 54},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 21}\n+      ])\"},\n+                                {\"/2019/01/CA.json\", R\"([\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"3\", \"sales\": 152},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"S\", \"sales\": 10},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"X\", \"sales\": 42},\n+        {\"country\": \"CA\", \"region\": \"QC\", \"year\": 2019, \"model\": \"Y\", \"sales\": 37}\n+      ])\"}};\n+\n+    auto fs = std::make_shared<fs::internal::MockFileSystem>(fs::kNoTime);\n+    for (const auto& f : files) {\n+      ARROW_EXPECT_OK(fs->CreateFile(f.first, f.second, /* recursive */ true));\n+    }\n+\n+    fs_ = fs;\n+  }\n+\n+ protected:\n+  std::shared_ptr<fs::FileSystem> fs_;\n+  std::shared_ptr<Schema> schema_;\n+};\n+\n+TEST_F(TestEndToEnd, EndToEndSingleSource) {\n+  // The dataset API is divided in 3 parts:\n+  //  - Creation\n+  //  - Querying\n+  //  - Consuming\n+\n+  // Creation.\n+  //\n+  // A Dataset is the union of one or more DataSources with the same schema.\n+  // Example of DataSource, FileSystemDataSource, OdbcDataSource,\n+  // FlightDataSource.\n+\n+  // A DataSource is composed of DataFragments. Each DataFragment can yield\n+  // multiple RecordBatch. DataSource can be created manually or \"discovered\"\n+  // via the DataSourceDiscovery interface.\n+  //\n+  // Each DataSourceDiscovery will have custom options.\n+  std::shared_ptr<DataSourceDiscovery> discovery;\n+\n+  // The user must specify which FileFormat is used to create FileFragment.\n+  // This option is specific to FileSystemBasedDataSource (and the builder).\n+  //\n+  // Note the JSONRecordBatchFileFormat requires a schema before creating the Discovery\n+  // class which is used to discover the schema. This chicken-and-egg problem\n+  // is only required for JSONRecordBatchFileFormat because it doesn't do\n+  // schema inspection.\n+  auto format = std::make_shared<JSONRecordBatchFileFormat>(schema_);\n+  // A selector is used to filter (or crawl) the files/directories of a\n+  // filesystem. If the options in Selector are not enough, the\n+  // FileSystemDataSourceDiscovery class also supports an explicit list of\n+  // fs::FileStats instead of the selector.\n+  fs::Selector s;\n+  s.base_dir = \"/\";\n+  s.recursive = true;\n+  ASSERT_OK(FileSystemDataSourceDiscovery::Make(fs_.get(), s, format, &discovery));\n+\n+  // DataFragments might have compatible but slightly different schemas, e.g.\n+  // schema evolved by adding/renaming columns. In this case, the schema is\n+  // passed to the dataset constructor.\n+  std::shared_ptr<Schema> inspected_schema;\n+  ASSERT_OK(discovery->Inspect(&inspected_schema));\n+  EXPECT_EQ(*schema_, *inspected_schema);\n+\n+  // Partitions expressions can be discovered for DataSource and DataFragments.\n+  // This metadata is then used in conjuction with the query filter to apply\n+  // the pushdown predicate optimization.\n+  auto partition_schema =\n+      schema({field(\"year\", int32()), field(\"month\", int32()), field(\"country\", utf8())});\n+  // The SchemaPartitionScheme is a simple scheme where the path is split with\n+  // the directory separator character and the components are typed and named\n+  // with the equivalent index in the schema, e.g.\n+  // (with the previous defined schema):\n+  //\n+  // - \"/2019\" -> {\"year\": 2019}\n+  // - \"/2019/01 -> {\"year\": 2019, \"month\": 1}\n+  // - \"/2019/01/CA -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  // - \"/2019/01/CA/a_file.json -> {\"year\": 2019, \"month\": 1, \"country\": \"CA\"}\n+  auto partition_scheme = std::make_shared<SchemaPartitionScheme>(partition_schema);\n+  ASSERT_OK(discovery->SetPartitionScheme(partition_scheme));\n+\n+  // Build the DataSource where partitions are attached to fragments (files).\n+  std::shared_ptr<DataSource> datasource;\n+  ASSERT_OK(discovery->Finish(&datasource));\n+\n+  // Create the Dataset from our single DataSource.\n+  std::shared_ptr<Dataset> dataset =\n+      std::make_shared<Dataset>(DataSourceVector{datasource}, inspected_schema);\n+\n+  // Querying.\n+  //\n+  // The Scan operator materialize data from io into memory. Avoiding data\n+  // transfer is a critical optimization done by analytical engine. Thus, a\n+  // Scan can take multiple options, notably a subset of columns and a filter\n+  // expression.\n+  std::unique_ptr<ScannerBuilder> scanner_builder;\n+  ASSERT_OK(dataset->NewScan(&scanner_builder));\n+\n+  // An optional subset of columns can be provided. This will trickle to\n+  // DataFragment drivers. The net effect is that only columns of interest will\n+  // be materialized if the DataFragment supports it. This is the major benefit\n+  // of using a colum-wise format versus a row-wise format.\n+  //\n+  // This API decouples the DataSource/DataFragment implementation and column\n+  // projection from the query part.\n+  //\n+  // For example, a ParquetFileDataFragment can only read the necessary bytes\n+  // ranges, or an OdbcDataFragment could craft the query with proper SELECT\n+  // statement. The CsvFileDataFragment wouldn't benefit from this as much, but\n+  // could still skip converting un-needed columns.\n+  ASSERT_OK(scanner_builder->Project({\"sales\", \"model\"}));\n+\n+  // An optional filter expression may also be specified. The filter expression\n+  // is matched on input rows and only matching rows are returned.\n+  // Predicate pushdown optimization is applied on partitions if possible.\n+  //\n+  // This API decouples predicate pushdown from the DataSource implementation\n+  // and partition discovery.\n+  //\n+  // The following filter tests both predicate pushdown and post filtering\n+  // without partition information.\n \n Review comment:\n   (Because year is a partition field whereas sales is not)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:07.111+0000",
                    "updated": "2019-10-16T20:34:07.111+0000",
                    "started": "2019-10-16T20:34:07.110+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329381",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329382",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335683716\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.cc\n ##########\n @@ -58,6 +55,62 @@ static ScanTaskIterator GetScanTaskIterator(DataFragmentIterator fragments,\n   return MakeFlattenIterator(std::move(maybe_scantask_it));\n }\n \n+template <typename Fn>\n \n Review comment:\n   I think `MapScanTask` is an unclear level of lambdas and mappings. Please refactor to a `FilteredAndProjectedScanTask` which holds a pointer to the evaluator, expression, and projector and constructs the filtered and projected record batch iterator in `Scan()`.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:07.241+0000",
                    "updated": "2019-10-16T20:34:07.241+0000",
                    "started": "2019-10-16T20:34:07.239+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329382",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329383",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335690939\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.h\n ##########\n @@ -46,16 +49,13 @@ class ARROW_DS_EXPORT ScanOptions {\n \n   // Filter\n   std::shared_ptr<Expression> filter;\n-\n   // Evaluator for Filter\n   std::shared_ptr<ExpressionEvaluator> evaluator;\n \n-  // Schema to which record batches will be projected\n+  // Schema to which record batches will be reconciled\n   std::shared_ptr<Schema> schema;\n-\n-  std::vector<std::shared_ptr<FileScanOptions>> options;\n-\n-  bool include_partition_keys = true;\n+  // Projecting the final RecordBatch to the requested schema.\n \n Review comment:\n   ```suggestion\r\n     // Projector for reconciling the final RecordBatch to the requested schema.\r\n   ```\r\n   \r\n   With this added, is `ScanOptions::schema` redundant? Will it ever be unequal to projector->schema()?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:07.245+0000",
                    "updated": "2019-10-16T20:34:07.245+0000",
                    "started": "2019-10-16T20:34:07.244+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329383",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329384",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335687556\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.h\n ##########\n @@ -150,34 +153,55 @@ class ARROW_DS_EXPORT SimpleScanner : public Scanner {\n   std::shared_ptr<ScanContext> context_;\n };\n \n+/// \\brief ScannerBuilder is a factory class to construct a Scanner. It is used\n+/// to pass information, notably a potential filter expression and a subset of\n+/// columns to materialize.\n class ARROW_DS_EXPORT ScannerBuilder {\n  public:\n   ScannerBuilder(std::shared_ptr<Dataset> dataset,\n                  std::shared_ptr<ScanContext> scan_context);\n \n-  /// \\brief Set\n-  ScannerBuilder* Project(const std::vector<std::string>& columns);\n-\n-  ScannerBuilder* Filter(std::shared_ptr<Expression> filter);\n-  ScannerBuilder* Filter(const Expression& filter);\n-\n-  ScannerBuilder* FilterEvaluator(std::shared_ptr<ExpressionEvaluator> evaluator);\n+  /// \\brief Set the subset of columns to materialize.\n+  ///\n+  /// This columns be passed down to DataSources and corresponding DataFragments.\n+  /// The goal is to avoid copying/deserializing columns that will be\n+  /// ignored/dropped further down the compute chain.\n+  ///\n+  /// \\param[in] columns list of columns to project. Order and duplicates will\n+  ///            be preserved.\n+  ///\n+  /// \\return Failure if any column names does not exists in the dataset's\n+  ///         Schema.\n+  Status Project(const std::vector<std::string>& columns);\n \n-  ScannerBuilder* SetGlobalFileOptions(std::shared_ptr<FileScanOptions> options);\n+  /// \\brief Set the filter expression to return only rows matching the filter.\n+  ///\n+  /// The predicate will be passed down to DataSources and corresponding\n+  /// DataFragments to exploit pushdown predicate if possible either due to\n+  /// partitions information, or DataFragment internal knowledge, e.g. Parquet\n+  /// statistics.\n+  ///\n+  /// \\param[in] filter expression to filter rows with.\n+  ///\n+  /// \\return Failure if any referenced columns does not exists in the dataset's\n \n Review comment:\n   ```suggestion\r\n     /// \\return Failure if any referenced columns does not exist in the dataset's\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:07.252+0000",
                    "updated": "2019-10-16T20:34:07.252+0000",
                    "started": "2019-10-16T20:34:07.251+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329384",
                    "issueId": "13260123"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/worklog/329385",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #5675: ARROW-6769: [Dataset][C++] End to end test\nURL: https://github.com/apache/arrow/pull/5675#discussion_r335692269\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/scanner.cc\n ##########\n @@ -66,53 +119,96 @@ ScanTaskIterator SimpleScanner::Scan() {\n   // Second, transforms Iterator<DataFragment> into a unified\n   // Iterator<ScanTask>. The first Iterator::Next invocation is going to do\n   // all the work of unwinding the chained iterators.\n-  return GetScanTaskIterator(std::move(fragments_it), context_);\n+  auto scan_task_it = GetScanTaskIterator(std::move(fragments_it), context_);\n+  // Third, apply the filter evaluator to incoming RecordBatches.\n+  auto filter_it =\n+      FilterScanTask(std::move(scan_task_it), options_->evaluator, options_->filter);\n+  // Finally, apply the final projection to scan_task_it if necessary. The\n+  // schema returned to the user might not be the same as given by the\n+  // DataFragment, e.g. due to columns required by the filter but not in the\n+  // final schema.\n+  return ProjectScanTask(std::move(filter_it), options_->projector);\n+}\n+\n+Status ScanTaskIteratorFromRecordBatch(std::vector<std::shared_ptr<RecordBatch>> batches,\n+                                       ScanTaskIterator* out) {\n+  std::unique_ptr<ScanTask> scan_task = internal::make_unique<SimpleScanTask>(batches);\n+\n+  std::vector<std::unique_ptr<ScanTask>> tasks;\n+  tasks.emplace_back(std::move(scan_task));\n+\n+  *out = MakeVectorIterator(std::move(tasks));\n+  return Status::OK();\n }\n \n ScannerBuilder::ScannerBuilder(std::shared_ptr<Dataset> dataset,\n                                std::shared_ptr<ScanContext> scan_context)\n     : dataset_(std::move(dataset)),\n       scan_options_(ScanOptions::Defaults()),\n-      scan_context_(std::move(scan_context)) {}\n+      scan_context_(std::move(scan_context)),\n+      pool_(default_memory_pool()) {}\n+\n+Status EnsureColumnsInSchema(const std::shared_ptr<Schema>& schema,\n+                             const std::vector<std::string>& columns) {\n+  for (const auto& column : columns) {\n+    if (schema->GetFieldByName(column) == nullptr) {\n+      return Status::Invalid(\"Requested column \", column,\n+                             \" not found in dataset's schema.\");\n+    }\n+  }\n \n-ScannerBuilder* ScannerBuilder::Project(const std::vector<std::string>& columns) {\n-  return this;\n+  return Status::OK();\n }\n \n-ScannerBuilder* ScannerBuilder::Filter(std::shared_ptr<Expression> filter) {\n-  scan_options_->filter = std::move(filter);\n-  return this;\n+Status ScannerBuilder::Project(const std::vector<std::string>& columns) {\n+  RETURN_NOT_OK(EnsureColumnsInSchema(schema(), columns));\n+  project_columns_ = columns;\n+  return Status::OK();\n }\n \n-ScannerBuilder* ScannerBuilder::Filter(const Expression& filter) {\n-  return Filter(filter.Copy());\n+Status ScannerBuilder::Filter(std::shared_ptr<Expression> filter) {\n+  if (filter != nullptr) {\n \n Review comment:\n   Dead null check\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-16T20:34:07.473+0000",
                    "updated": "2019-10-16T20:34:07.473+0000",
                    "started": "2019-10-16T20:34:07.473+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "329385",
                    "issueId": "13260123"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 21000,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@24abbd85[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3383d0f1[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@285f1537[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@5f65c95[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3815669c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@67437d10[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@37e52a62[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@40caff[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@13727ded[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@6fa5396e[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@710da8c2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@779c932e[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 21000,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Oct 22 12:43:37 UTC 2019",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2019-10-22T12:43:36.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-6769/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2019-10-02T14:44:32.000+0000",
        "updated": "2019-10-22T12:43:37.000+0000",
        "timeoriginalestimate": null,
        "description": "1. Create a DataSource from a known\u00a0directory and a PartitionScheme. \r\n2. Create a Dataset from the previous DataSource. \r\n3. Request a ScannerBuilder from previous Dataset. \r\n4. Add filter expression to ScannerBuilder (and other options). \r\n5. Finalize into a Scan operation. \r\n6. Materialize into an arrow::Table.\r\n\r\n\u00a0",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "5h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 21000
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Dataset] End to End dataset integration test case",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13260123/comment/16957013",
                    "id": "16957013",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bkietz",
                        "name": "bkietz",
                        "key": "bkietz",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bkietz&avatarId=37277",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bkietz&avatarId=37277",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bkietz&avatarId=37277",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bkietz&avatarId=37277"
                        },
                        "displayName": "Ben Kietzman",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 5675\n[https://github.com/apache/arrow/pull/5675]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bkietz",
                        "name": "bkietz",
                        "key": "bkietz",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bkietz&avatarId=37277",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bkietz&avatarId=37277",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bkietz&avatarId=37277",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bkietz&avatarId=37277"
                        },
                        "displayName": "Ben Kietzman",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-10-22T12:43:37.035+0000",
                    "updated": "2019-10-22T12:43:37.035+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z078dk:",
        "customfield_12314139": null
    }
}