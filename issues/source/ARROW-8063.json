{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13290889",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889",
    "key": "ARROW-8063",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12346687",
                "id": "12346687",
                "description": "",
                "name": "0.17.0",
                "archived": false,
                "released": true,
                "releaseDate": "2020-04-20"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "documentation",
            "parquet",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12582809",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12582809",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13290648",
                    "key": "ARROW-8047",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290648",
                    "fields": {
                        "summary": "[Python][Documentation] Document migration from ParquetDataset to pyarrow.datasets",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                            "description": "The issue is open and ready for the assignee to start work on it.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                            "name": "Open",
                            "id": "1",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                                "id": 2,
                                "key": "new",
                                "colorName": "blue-gray",
                                "name": "To Do"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
                            "name": "Critical",
                            "id": "2"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
            "name": "jorisvandenbossche",
            "key": "jorisvandenbossche",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Joris Van den Bossche",
            "active": true,
            "timeZone": "Europe/Brussels"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332532",
                "id": "12332532",
                "name": "Documentation"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
            "name": "jorisvandenbossche",
            "key": "jorisvandenbossche",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Joris Van den Bossche",
            "active": true,
            "timeZone": "Europe/Brussels"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
            "name": "jorisvandenbossche",
            "key": "jorisvandenbossche",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Joris Van den Bossche",
            "active": true,
            "timeZone": "Europe/Brussels"
        },
        "aggregateprogress": {
            "progress": 20400,
            "total": 20400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 20400,
            "total": 20400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-8063/votes",
            "votes": 1,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 47,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/412977",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779\n \n \n   \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-31T09:28:20.369+0000",
                    "updated": "2020-03-31T09:28:20.369+0000",
                    "started": "2020-03-31T09:28:20.369+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "412977",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/412978",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on issue #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#issuecomment-606509277\n \n \n   https://issues.apache.org/jira/browse/ARROW-8063\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-31T09:31:34.088+0000",
                    "updated": "2020-03-31T09:31:34.088+0000",
                    "started": "2020-03-31T09:31:34.088+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "412978",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/417313",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs commented on issue #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#issuecomment-610095190\n \n \n   Don't forget to trigger `test-ubuntu-18.04-docs` crossbow task before merging.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-06T23:51:22.977+0000",
                    "updated": "2020-04-06T23:51:22.977+0000",
                    "started": "2020-04-06T23:51:22.977+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "417313",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419660",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r405063940\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,280 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n \n Review comment:\n   Should we be listing features we don't yet support?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:08.731+0000",
                    "updated": "2020-04-09T19:39:08.731+0000",
                    "started": "2020-04-09T19:39:08.730+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419660",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419662",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r405063312\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,280 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n \n Review comment:\n   This is a comma separated list of features followed by a bullet pointed list of features. Please concatenate them\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:08.734+0000",
                    "updated": "2020-04-09T19:39:08.734+0000",
                    "started": "2020-04-09T19:39:08.734+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419662",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419661",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406379589\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n \n Review comment:\n   ```suggestion\r\n   reading Parquet datasets: ``pyarrow.dataset``'s goal is similar but not specific\r\n   to the Parquet format and not tied to Python: the same datasets API is exposed\r\n   in the R bindings or Arrow. In addition ``pyarrow.dataset`` boasts improved\r\n   perfomance and new features (e.g. filtering within files rather than only on\r\n   partition keys).\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:08.734+0000",
                    "updated": "2020-04-09T19:39:08.734+0000",
                    "started": "2020-04-09T19:39:08.734+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419661",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419663",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406381161\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n \n Review comment:\n   ```suggestion\r\n   .. TODO Full blown example with NYC taxi data to show off, afterwards explain all parts:\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:09.009+0000",
                    "updated": "2020-04-09T19:39:09.009+0000",
                    "started": "2020-04-09T19:39:09.009+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419663",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419664",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406382781\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n \n Review comment:\n   ```suggestion\r\n   For example, let's create a small dataset consisting\r\n   of a directory with two parquet files:\r\n   \r\n   .. ipython:: python\r\n   \r\n       import pyarrow as pa\r\n       import pyarrow.dataset as ds\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:09.306+0000",
                    "updated": "2020-04-09T19:39:09.306+0000",
                    "started": "2020-04-09T19:39:09.306+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419664",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419665",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406384397\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n \n Review comment:\n   ```suggestion\r\n   In addition to a base directory path, :func:`dataset` accepts\r\n   a path to a single file or a list of file paths.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:09.760+0000",
                    "updated": "2020-04-09T19:39:09.760+0000",
                    "started": "2020-04-09T19:39:09.759+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419665",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419666",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406384863\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n \n Review comment:\n   ```suggestion\r\n   Creating a :class:`Dataset` object loads nothing into memory, it only\r\n   crawls the directory to find all the files:\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:09.845+0000",
                    "updated": "2020-04-09T19:39:09.845+0000",
                    "started": "2020-04-09T19:39:09.845+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419666",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419667",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406386484\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n \n Review comment:\n   ```suggestion\r\n   Using the :meth:`Dataset.to_table` method we can read the dataset (or a portion of it) into a\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:09.848+0000",
                    "updated": "2020-04-09T19:39:09.848+0000",
                    "started": "2020-04-09T19:39:09.848+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419667",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419668",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406388197\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n+pyarrow Table (note this will read everything at once, which can require a lot\n+of memory, see below on filtering / iterative loading):\n+\n+\n+.. ipython:: python\n+\n+    dataset.to_table()\n+    # converting to pandas to see the contents of the scanned table\n+    dataset.to_table().to_pandas()\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files as dataset source, but the Dataset API\n+aims to provide a consistent interface for multiple file formats and sources.\n \n Review comment:\n   ```suggestion\r\n   The above examples use Parquet files as dataset source but the Dataset API\r\n   provides a consistent interface across multiple file formats and sources.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.014+0000",
                    "updated": "2020-04-09T19:39:10.014+0000",
                    "started": "2020-04-09T19:39:10.014+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419668",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419669",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406385066\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n \n Review comment:\n   ```suggestion\r\n   ... and infers the dataset's schema (by default from the first file):\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.017+0000",
                    "updated": "2020-04-09T19:39:10.017+0000",
                    "started": "2020-04-09T19:39:10.017+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419669",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419670",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406402918\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n+pyarrow Table (note this will read everything at once, which can require a lot\n+of memory, see below on filtering / iterative loading):\n+\n+\n+.. ipython:: python\n+\n+    dataset.to_table()\n+    # converting to pandas to see the contents of the scanned table\n+    dataset.to_table().to_pandas()\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files as dataset source, but the Dataset API\n+aims to provide a consistent interface for multiple file formats and sources.\n+Currently, Parquet and Feather / Arrow IPC file format are supported; more\n+formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. ipython:: python\n+\n+    import pyarrow.feather as feather\n+\n+    feather.write_feather(table, base / \"data.feather\")\n+\n+then we can read the Feather file using the same functions, but with specifying\n+``format=\"ipc\"``:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"data.feather\", format=\"ipc\")\n+    dataset.to_table().to_pandas().head()\n+\n+The format name as a string, like:\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=\"parquet\")\n+\n+is a short-hand for\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=ds.ParquetFileFormat())\n+\n+Those file format objects can take keywords to customize reading (see for\n+example :class:`ParquetFileFormat`).\n \n Review comment:\n   ```suggestion\r\n   Customizing file formats\r\n   ~~~~~~~~~~~~~~~~~~~~~~~~\r\n   \r\n   class:`FileFormat`s can be customized using keywords. For example::\r\n   \r\n       format = ds.ParquetFileFormat(read_options={'dictionary_columns': ['a']})\r\n       ds.dataset(..., format=format)\r\n   \r\n   Will configure column ``a`` to be dictionary encoded on scan.\r\n   The format name as a string, like::\r\n   \r\n       ds.dataset(..., format=\"parquet\")\r\n   \r\n   is short hand for a default constructed class:`ParquetFileFormat`.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.256+0000",
                    "updated": "2020-04-09T19:39:10.256+0000",
                    "started": "2020-04-09T19:39:10.256+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419670",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419671",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406418322\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n+pyarrow Table (note this will read everything at once, which can require a lot\n+of memory, see below on filtering / iterative loading):\n+\n+\n+.. ipython:: python\n+\n+    dataset.to_table()\n+    # converting to pandas to see the contents of the scanned table\n+    dataset.to_table().to_pandas()\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files as dataset source, but the Dataset API\n+aims to provide a consistent interface for multiple file formats and sources.\n+Currently, Parquet and Feather / Arrow IPC file format are supported; more\n+formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. ipython:: python\n+\n+    import pyarrow.feather as feather\n+\n+    feather.write_feather(table, base / \"data.feather\")\n+\n+then we can read the Feather file using the same functions, but with specifying\n+``format=\"ipc\"``:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"data.feather\", format=\"ipc\")\n+    dataset.to_table().to_pandas().head()\n+\n+The format name as a string, like:\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=\"parquet\")\n+\n+is a short-hand for\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=ds.ParquetFileFormat())\n+\n+Those file format objects can take keywords to customize reading (see for\n+example :class:`ParquetFileFormat`).\n+\n+\n+Filtering data\n+--------------\n+\n+To avoid reading all data when only needing a subset, the ``columns`` and\n+``filter`` keywords can be used.\n+\n+The ``columns`` keyword can be used to only read a selection of the columns,\n+and accepts a list of column names:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset.to_table(columns=['a', 'b']).to_pandas()\n+\n+With the ``filter`` keyword, rows which do not match the filter predicate will\n+not be included in the returned table. The keyword expects a boolean expression\n \n Review comment:\n   ```suggestion\r\n   not be included in the returned table. The keyword expects a class:`Expression`\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.259+0000",
                    "updated": "2020-04-09T19:39:10.259+0000",
                    "started": "2020-04-09T19:39:10.259+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419671",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419672",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406422611\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n+pyarrow Table (note this will read everything at once, which can require a lot\n+of memory, see below on filtering / iterative loading):\n+\n+\n+.. ipython:: python\n+\n+    dataset.to_table()\n+    # converting to pandas to see the contents of the scanned table\n+    dataset.to_table().to_pandas()\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files as dataset source, but the Dataset API\n+aims to provide a consistent interface for multiple file formats and sources.\n+Currently, Parquet and Feather / Arrow IPC file format are supported; more\n+formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. ipython:: python\n+\n+    import pyarrow.feather as feather\n+\n+    feather.write_feather(table, base / \"data.feather\")\n+\n+then we can read the Feather file using the same functions, but with specifying\n+``format=\"ipc\"``:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"data.feather\", format=\"ipc\")\n+    dataset.to_table().to_pandas().head()\n+\n+The format name as a string, like:\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=\"parquet\")\n+\n+is a short-hand for\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=ds.ParquetFileFormat())\n+\n+Those file format objects can take keywords to customize reading (see for\n+example :class:`ParquetFileFormat`).\n+\n+\n+Filtering data\n+--------------\n+\n+To avoid reading all data when only needing a subset, the ``columns`` and\n+``filter`` keywords can be used.\n+\n+The ``columns`` keyword can be used to only read a selection of the columns,\n+and accepts a list of column names:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset.to_table(columns=['a', 'b']).to_pandas()\n+\n+With the ``filter`` keyword, rows which do not match the filter predicate will\n+not be included in the returned table. The keyword expects a boolean expression\n+involving one of the columns, and those expressions can be created using the\n+:func:`field` helper function:\n+\n+.. ipython:: python\n+\n+    dataset.to_table(filter=ds.field('a') >= 7).to_pandas()\n+    dataset.to_table(filter=ds.field('c') == 2).to_pandas()\n+\n+Any column can be referenced using the :func:`field` function (which creates a\n+:class:`FieldExpression`), and many different expressions can be created,\n+including the standard boolean comparison operators (equal, larger/less than,\n+etc), but also set membership testing:\n+\n+.. ipython:: python\n+\n+    ds.field('a') != 3\n+    ds.field('a').isin([1, 2, 3])\n+\n+\n+Reading partitioned data\n+------------------------\n+\n+A dataset consisting of a flat directory with files was already shown above.\n \n Review comment:\n   ```suggestion\r\n   Above, a dataset consisting of a flat directory with files was shown.\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.272+0000",
                    "updated": "2020-04-09T19:39:10.272+0000",
                    "started": "2020-04-09T19:39:10.272+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419672",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419673",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406424411\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n+pyarrow Table (note this will read everything at once, which can require a lot\n+of memory, see below on filtering / iterative loading):\n+\n+\n+.. ipython:: python\n+\n+    dataset.to_table()\n+    # converting to pandas to see the contents of the scanned table\n+    dataset.to_table().to_pandas()\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files as dataset source, but the Dataset API\n+aims to provide a consistent interface for multiple file formats and sources.\n+Currently, Parquet and Feather / Arrow IPC file format are supported; more\n+formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. ipython:: python\n+\n+    import pyarrow.feather as feather\n+\n+    feather.write_feather(table, base / \"data.feather\")\n+\n+then we can read the Feather file using the same functions, but with specifying\n+``format=\"ipc\"``:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"data.feather\", format=\"ipc\")\n+    dataset.to_table().to_pandas().head()\n+\n+The format name as a string, like:\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=\"parquet\")\n+\n+is a short-hand for\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=ds.ParquetFileFormat())\n+\n+Those file format objects can take keywords to customize reading (see for\n+example :class:`ParquetFileFormat`).\n+\n+\n+Filtering data\n+--------------\n+\n+To avoid reading all data when only needing a subset, the ``columns`` and\n+``filter`` keywords can be used.\n+\n+The ``columns`` keyword can be used to only read a selection of the columns,\n+and accepts a list of column names:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset.to_table(columns=['a', 'b']).to_pandas()\n+\n+With the ``filter`` keyword, rows which do not match the filter predicate will\n+not be included in the returned table. The keyword expects a boolean expression\n+involving one of the columns, and those expressions can be created using the\n+:func:`field` helper function:\n+\n+.. ipython:: python\n+\n+    dataset.to_table(filter=ds.field('a') >= 7).to_pandas()\n+    dataset.to_table(filter=ds.field('c') == 2).to_pandas()\n+\n+Any column can be referenced using the :func:`field` function (which creates a\n+:class:`FieldExpression`), and many different expressions can be created,\n+including the standard boolean comparison operators (equal, larger/less than,\n+etc), but also set membership testing:\n+\n+.. ipython:: python\n+\n+    ds.field('a') != 3\n+    ds.field('a').isin([1, 2, 3])\n+\n+\n+Reading partitioned data\n+------------------------\n+\n+A dataset consisting of a flat directory with files was already shown above.\n+But the dataset can also contain nested directories defining a partitioned\n+dataset, where the sub-directory names hold information about which subset\n+of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. The:func:`~pyarrow.parquet.write_to_dataset`\n+function can write such hive-like partitioned datasets.\n+\n+.. ipython:: python\n+\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5,\n+                      'part': ['a'] * 5 + ['b'] * 5})\n+    pq.write_to_dataset(table, str(base / \"parquet_dataset_partitioned\"),\n+                        partition_cols=['part'])\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset with :func:`dataset`, we now specify that the dataset\n+uses a hive-like partitioning scheme with the `partitioning` keyword:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(str(base / \"parquet_dataset_partitioned2\"), format=\"parquet\",\n+                         partitioning=\"hive\")\n+    dataset.files\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. ipython:: python\n+\n+    dataset.to_table().to_pandas().head(3)\n+\n+We can now filter on the partition keys, which avoids loading certain files\n+at all if they do not match the predicate:\n \n Review comment:\n   ```suggestion\r\n   We can now filter on the partition keys, which avoids loading files\r\n   altogether if they do not match the predicate:\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.275+0000",
                    "updated": "2020-04-09T19:39:10.275+0000",
                    "started": "2020-04-09T19:39:10.274+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419673",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419674",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406387276\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n+pyarrow Table (note this will read everything at once, which can require a lot\n \n Review comment:\n   ```suggestion\r\n   pyarrow Table (note that depending on the size of your dataset this can require a lot\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.341+0000",
                    "updated": "2020-04-09T19:39:10.341+0000",
                    "started": "2020-04-09T19:39:10.341+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419674",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419675",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406432239\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n+pyarrow Table (note this will read everything at once, which can require a lot\n+of memory, see below on filtering / iterative loading):\n+\n+\n+.. ipython:: python\n+\n+    dataset.to_table()\n+    # converting to pandas to see the contents of the scanned table\n+    dataset.to_table().to_pandas()\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files as dataset source, but the Dataset API\n+aims to provide a consistent interface for multiple file formats and sources.\n+Currently, Parquet and Feather / Arrow IPC file format are supported; more\n+formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. ipython:: python\n+\n+    import pyarrow.feather as feather\n+\n+    feather.write_feather(table, base / \"data.feather\")\n+\n+then we can read the Feather file using the same functions, but with specifying\n+``format=\"ipc\"``:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"data.feather\", format=\"ipc\")\n+    dataset.to_table().to_pandas().head()\n+\n+The format name as a string, like:\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=\"parquet\")\n+\n+is a short-hand for\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=ds.ParquetFileFormat())\n+\n+Those file format objects can take keywords to customize reading (see for\n+example :class:`ParquetFileFormat`).\n+\n+\n+Filtering data\n+--------------\n+\n+To avoid reading all data when only needing a subset, the ``columns`` and\n+``filter`` keywords can be used.\n+\n+The ``columns`` keyword can be used to only read a selection of the columns,\n+and accepts a list of column names:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset.to_table(columns=['a', 'b']).to_pandas()\n+\n+With the ``filter`` keyword, rows which do not match the filter predicate will\n+not be included in the returned table. The keyword expects a boolean expression\n+involving one of the columns, and those expressions can be created using the\n+:func:`field` helper function:\n+\n+.. ipython:: python\n+\n+    dataset.to_table(filter=ds.field('a') >= 7).to_pandas()\n+    dataset.to_table(filter=ds.field('c') == 2).to_pandas()\n+\n+Any column can be referenced using the :func:`field` function (which creates a\n+:class:`FieldExpression`), and many different expressions can be created,\n+including the standard boolean comparison operators (equal, larger/less than,\n+etc), but also set membership testing:\n+\n+.. ipython:: python\n+\n+    ds.field('a') != 3\n+    ds.field('a').isin([1, 2, 3])\n+\n+\n+Reading partitioned data\n+------------------------\n+\n+A dataset consisting of a flat directory with files was already shown above.\n+But the dataset can also contain nested directories defining a partitioned\n+dataset, where the sub-directory names hold information about which subset\n+of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. The:func:`~pyarrow.parquet.write_to_dataset`\n+function can write such hive-like partitioned datasets.\n+\n+.. ipython:: python\n+\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5,\n+                      'part': ['a'] * 5 + ['b'] * 5})\n+    pq.write_to_dataset(table, str(base / \"parquet_dataset_partitioned\"),\n+                        partition_cols=['part'])\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset with :func:`dataset`, we now specify that the dataset\n+uses a hive-like partitioning scheme with the `partitioning` keyword:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(str(base / \"parquet_dataset_partitioned2\"), format=\"parquet\",\n+                         partitioning=\"hive\")\n+    dataset.files\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. ipython:: python\n+\n+    dataset.to_table().to_pandas().head(3)\n+\n+We can now filter on the partition keys, which avoids loading certain files\n+at all if they do not match the predicate:\n+\n+.. ipython:: python\n+\n+    dataset.to_table(filter=ds.field(\"part\") == \"b\").to_pandas()\n+\n+\n+Different partitioning schemes\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above example uses a hive-like directory scheme, such as \"/year=2009/month=11/day=15\".\n+We specified this passing the ``partitioning=\"hive\"`` keyword. In this case,\n+the types of the partition keys are inferred from the file paths.\n+\n+It is also possible to explicitly define the schema of the partition keys\n+using the :func:`partitioning` function. For example:\n+\n+.. code-block::\n+\n+    part = ds.partitioning(\n+        pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8()), (\"day\", pa.int32())]),\n+        flavor=\"hive\"\n+    )\n+    dataset = ds.dataset(..., partitioning=part)\n+\n+In addition to the hive-like directory scheme, also a \"directory partitioning\"\n+scheme is supported, where the segments in the file path are the values of\n+the partition keys without including the name. The equivalent (year, month, day)\n+example would be \"/2019/11/15/\".\n+\n+Since the names are not included in the file paths, those need to be specified:\n+\n+.. code-block::\n+\n+    part = ds.partitioning(field_names=[\"year\", \"month\", \"day\"])\n+\n+Also here, a full schema can be provided to not let the types be inferred\n+from the file paths.\n+\n+\n+Reading from cloud storage\n+--------------------------\n+\n+- example with s3 filesystem / hdfs filesystem\n+\n+\n+\n+Manual specification of the Dataset\n+-----------------------------------\n+\n+The :func:`dataset` function allows to easily create a Dataset from a directory,\n+crawling all subdirectories for files and partitioning information. However,\n+when the dataset files and partitions are already known (for example, when this\n+information is stored in metadata), it is also possible to create a Dataset\n+explicitly using the lower level API without any automatic discovery or\n+inference.\n+\n+For the example here, we are going to use a dataset where the file names contain\n+additional partitioning information:\n+\n+.. ipython:: python\n+\n+    # creating a dummy dataset: directory with two files\n+    table = pa.table({'col1': range(3), 'col2': np.random.randn(3)})\n+    pq.write_table(table, \"parquet_dataset_manual/data_file1.parquet\")\n+    pq.write_table(table, \"parquet_dataset_manual/data_file2.parquet\")\n+\n+To create a Dataset from a list of files, we need to specify the schema, format,\n+filesystem, and paths manually:\n+\n+.. ipython:: python\n+\n+    import pyarrow.fs\n+\n+    schema = pa.schema([(\"file\", pa.int64()), (\"col1\", pa.int64()), (\"col2\", pa.float64())])\n+\n+    dataset = ds.FileSystemDataset(\n+        schema, None, ds.ParquetFileFormat(), pa.fs.LocalFileSystem(),\n+        [\"parquet_dataset_manual/data_file1.parquet\", \"parquet_dataset_manual/data_file2.parquet\"],\n+        [ds.field('file') == 1, ds.field('file') == 2])\n+\n+We also specified the \"partition expressions\" for our files, so this information\n+is included when reading the data and can be used for filtering:\n \n Review comment:\n   ```suggestion\r\n   Since we specified the \"partition expressions\" for our files, this information\r\n   is materialized as columns when reading the data and can be used for filtering:\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.388+0000",
                    "updated": "2020-04-09T19:39:10.388+0000",
                    "started": "2020-04-09T19:39:10.388+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419675",
                    "issueId": "13290889"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/worklog/419676",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on pull request #6779: ARROW-8063: [Python][Dataset] Start user guide for pyarrow.dataset\nURL: https://github.com/apache/arrow/pull/6779#discussion_r406411361\n \n \n\n ##########\n File path: docs/source/python/dataset.rst\n ##########\n @@ -0,0 +1,354 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. currentmodule:: pyarrow.dataset\n+\n+.. _dataset:\n+\n+Tabular Datasets\n+================\n+\n+.. warning::\n+\n+    The ``pyarrow.dataset`` module is experimental (specifically the classes),\n+    and a stable API is not yet guaranteed.\n+\n+The ``pyarrow.dataset`` module provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file, datasets:\n+\n+* A unified interface for different sources: supporting different sources\n+  (files, database connection, ..), different file systems (local, cloud) and\n+  different file formats (Parquet, CSV, JSON, Feather, ..)\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with pedicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading or fine-grained managing of tasks.\n+\n+\n+For those familiar with the existing :class:`pyarrow.parquet.ParquetDataset` for\n+reading Parquet datasets: the goal of ``pyarrow.dataset`` is to provide similar\n+functionality, but not specific to the Parquet format, not tied to Python (for\n+example the R bindings of Arrow also have an interface for the Dateset API) and\n+with improved functionality (e.g. filtering on the file level and not only\n+partition keys).\n+\n+\n+\n+Reading Datasets\n+----------------\n+\n+\n+Full blown example with NYC taxi data to show off, afterwards explain all parts:\n+\n+.. ipython:: python\n+\n+    import pyarrow as pa\n+    import pyarrow.dataset as ds\n+\n+...\n+\n+\n+For the next examples, we are first going to create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. ipython:: python\n+\n+    import tempfile\n+    import pathlib\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    (base / \"parquet_dataset\").mkdir(exist_ok=True)\n+\n+    # creating an Arrow Table\n+    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [1, 2] * 5})\n+\n+    # writing it into two parquet files\n+    import pyarrow.parquet as pq\n+    pq.write_table(table.slice(0, 5), base / \"parquet_dataset/data1.parquet\")\n+    pq.write_table(table.slice(5, 10), base / \"parquet_dataset/data2.parquet\")\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`Dataset` object can be created with the :func:`dataset` function. We\n+can pass it the path to the directory containing the data files:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"parquet_dataset\", format=\"parquet\")\n+    dataset\n+\n+Alternatively, it also accepts a path to a single file, or a list of file\n+paths.\n+\n+Creating this :class:`Dataset` object did yet materialize the full dataset, but\n+it crawled the directory to find all the files:\n+\n+.. ipython:: python\n+\n+    dataset.files\n+\n+and it did infer the schema (by default from the first file):\n+\n+.. ipython:: python\n+\n+    print(dataset.schema.to_string(show_field_metadata=False))\n+\n+With the :meth:`Dataset.to_table` method, we can read the dataset into a\n+pyarrow Table (note this will read everything at once, which can require a lot\n+of memory, see below on filtering / iterative loading):\n+\n+\n+.. ipython:: python\n+\n+    dataset.to_table()\n+    # converting to pandas to see the contents of the scanned table\n+    dataset.to_table().to_pandas()\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files as dataset source, but the Dataset API\n+aims to provide a consistent interface for multiple file formats and sources.\n+Currently, Parquet and Feather / Arrow IPC file format are supported; more\n+formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. ipython:: python\n+\n+    import pyarrow.feather as feather\n+\n+    feather.write_feather(table, base / \"data.feather\")\n+\n+then we can read the Feather file using the same functions, but with specifying\n+``format=\"ipc\"``:\n+\n+.. ipython:: python\n+\n+    dataset = ds.dataset(base / \"data.feather\", format=\"ipc\")\n+    dataset.to_table().to_pandas().head()\n+\n+The format name as a string, like:\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=\"parquet\")\n+\n+is a short-hand for\n+\n+.. code-block::\n+\n+    ds.dataset(..., format=ds.ParquetFileFormat())\n+\n+Those file format objects can take keywords to customize reading (see for\n+example :class:`ParquetFileFormat`).\n+\n+\n+Filtering data\n+--------------\n+\n+To avoid reading all data when only needing a subset, the ``columns`` and\n+``filter`` keywords can be used.\n+\n+The ``columns`` keyword can be used to only read a selection of the columns,\n+and accepts a list of column names:\n \n Review comment:\n   ```suggestion\r\n   The ``columns`` keyword can be used to only read the named columns:\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-04-09T19:39:10.451+0000",
                    "updated": "2020-04-09T19:39:10.451+0000",
                    "started": "2020-04-09T19:39:10.450+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "419676",
                    "issueId": "13290889"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 20400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@7adbccb6[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@321462a4[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5b3bba1d[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@4aabe29c[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@244b2538[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@5027d6ed[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@56ca65a6[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@4360d2e1[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3eff5d50[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@500164dc[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@55e3cc2f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@1051fba8[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 20400,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Apr 14 23:01:13 UTC 2020",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2020-04-14T23:01:13.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-8063/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2020-03-10T18:24:25.000+0000",
        "updated": "2020-04-14T23:01:14.000+0000",
        "timeoriginalestimate": null,
        "description": "Currently, we only have API docs (https://arrow.apache.org/docs/python/api/dataset.html), but we also need prose docs explaining what the dataset module does with examples.\r\n\r\nThis can also include guidelines on how to use this instead of the ParquetDataset API (depending on how we end up doing ARROW-8039), this aspect is also covered by ARROW-8047",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "5h 40m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 20400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python] Add user guide documentation for Datasets API",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13290889/comment/17083675",
                    "id": "17083675",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "body": "Issue resolved by pull request 6779\n[https://github.com/apache/arrow/pull/6779]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "created": "2020-04-14T23:01:13.960+0000",
                    "updated": "2020-04-14T23:01:13.960+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0ce0o:",
        "customfield_12314139": null
    }
}