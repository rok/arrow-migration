{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13404218",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218",
    "key": "ARROW-14182",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12351550",
                "id": "12351550",
                "name": "9.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-08-03"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available",
            "query-engine"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350323",
                "id": "12350323",
                "description": "",
                "name": "6.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-10-26"
            }
        ],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12625254",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12625254",
                "type": {
                    "id": "12310460",
                    "name": "Child-Issue",
                    "inward": "is a child of",
                    "outward": "is a parent of",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310460"
                },
                "outwardIssue": {
                    "id": "13408535",
                    "key": "ARROW-14479",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13408535",
                    "fields": {
                        "summary": "[C++][Compute] Hash Join microbenchmarks",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12623884",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12623884",
                "type": {
                    "id": "12310460",
                    "name": "Child-Issue",
                    "inward": "is a child of",
                    "outward": "is a parent of",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310460"
                },
                "inwardIssue": {
                    "id": "13376404",
                    "key": "ARROW-12633",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376404",
                    "fields": {
                        "summary": "[C++] Query engine umbrella issue",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                            "description": "The issue is open and ready for the assignee to start work on it.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                            "name": "Open",
                            "id": "1",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                                "id": 2,
                                "key": "new",
                                "colorName": "blue-gray",
                                "name": "To Do"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [
            {
                "id": "13439066",
                "key": "ARROW-16166",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066",
                "fields": {
                    "summary": "[C++] Create lightweight utilities for manipulating primitive arrays",
                    "status": {
                        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                        "name": "Resolved",
                        "id": "5",
                        "statusCategory": {
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                            "id": 3,
                            "key": "done",
                            "colorName": "green",
                            "name": "Done"
                        }
                    },
                    "priority": {
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                        "name": "Major",
                        "id": "3"
                    },
                    "issuetype": {
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                        "id": "7",
                        "description": "The sub-task of the issue",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                        "name": "Sub-task",
                        "subtask": true,
                        "avatarId": 21146
                    }
                }
            },
            {
                "id": "13445283",
                "key": "ARROW-16590",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/13445283",
                "fields": {
                    "summary": "[C++] Consolidate files dealing with row-major storage, add some helper methods",
                    "status": {
                        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                        "name": "Resolved",
                        "id": "5",
                        "statusCategory": {
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                            "id": 3,
                            "key": "done",
                            "colorName": "green",
                            "name": "Done"
                        }
                    },
                    "priority": {
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                        "name": "Major",
                        "id": "3"
                    },
                    "issuetype": {
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                        "id": "7",
                        "description": "The sub-task of the issue",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                        "name": "Sub-task",
                        "subtask": true,
                        "avatarId": 21146
                    }
                }
            },
            {
                "id": "13446459",
                "key": "ARROW-16637",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/13446459",
                "fields": {
                    "summary": "[C++] Add row-based utilities for encoding a batch and merging row tables",
                    "status": {
                        "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                        "description": "The issue is open and ready for the assignee to start work on it.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                        "name": "Open",
                        "id": "1",
                        "statusCategory": {
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                            "id": 2,
                            "key": "new",
                            "colorName": "blue-gray",
                            "name": "To Do"
                        }
                    },
                    "priority": {
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                        "name": "Major",
                        "id": "3"
                    },
                    "issuetype": {
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                        "id": "7",
                        "description": "The sub-task of the issue",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                        "name": "Sub-task",
                        "subtask": true,
                        "avatarId": 21146
                    }
                }
            }
        ],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 61200,
            "total": 61200,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 24600,
            "total": 24600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-14182/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 41,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/719936",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa opened a new pull request #12326:\nURL: https://github.com/apache/arrow/pull/12326\n\n\n   Faster implementation of hash join.\r\n   \r\n   Work in progress.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-03T08:31:54.887+0000",
                    "updated": "2022-02-03T08:31:54.887+0000",
                    "started": "2022-02-03T08:31:54.887+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "719936",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/719937",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #12326:\nURL: https://github.com/apache/arrow/pull/12326#issuecomment-1028724724\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-03T08:32:17.493+0000",
                    "updated": "2022-02-03T08:32:17.493+0000",
                    "started": "2022-02-03T08:32:17.493+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "719937",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/736236",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #12326:\nURL: https://github.com/apache/arrow/pull/12326#issuecomment-1058385123\n\n\n   @save-buffer PTAL\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-03T19:00:08.127+0000",
                    "updated": "2022-03-03T19:00:08.127+0000",
                    "started": "2022-03-03T19:00:08.127+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "736236",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/742720",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "save-buffer commented on a change in pull request #12326:\nURL: https://github.com/apache/arrow/pull/12326#discussion_r827301614\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join.cc\n##########\n@@ -98,7 +99,8 @@ class HashJoinBasicImpl : public HashJoinImpl {\n     ctx_ = ctx;\n     join_type_ = join_type;\n     num_threads_ = num_threads;\n-    schema_mgr_ = schema_mgr;\n+    schema_[0] = proj_map_left;\n+    schema_[1] = proj_map_right;\n\nReview comment:\n       Why did we get rid of schema manager? \n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join.h\n##########\n@@ -98,12 +102,13 @@ class ARROW_EXPORT HashJoinSchema {\n \n class HashJoinImpl {\n  public:\n-  using OutputBatchCallback = std::function<void(ExecBatch)>;\n+  using OutputBatchCallback = std::function<void(int64_t, ExecBatch)>;\n\nReview comment:\n       What's this new int64 parameter for? Doesn't it just get ignored later? \n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -504,8 +532,26 @@ class HashJoinNode : public ExecNode {\n     // Generate output schema\n     std::shared_ptr<Schema> output_schema = schema_mgr->MakeOutputSchema(\n         join_options.output_suffix_for_left, join_options.output_suffix_for_right);\n+\n     // Create hash join implementation object\n-    ARROW_ASSIGN_OR_RAISE(std::unique_ptr<HashJoinImpl> impl, HashJoinImpl::MakeBasic());\n+    // SwissJoin does not support:\n+    // a) 64-bit string offsets\n+    // b) residual predicates\n+    // c) dictionaries\n+    //\n\nReview comment:\n       How much work would it be to implement this for big endian? \r\n   Are there any plans for big strings and dictionaries? I know residual predicates are planned to come up shortly\n\n##########\nFile path: cpp/src/arrow/compute/exec/key_compare.h\n##########\n@@ -123,7 +127,7 @@ class KeyCompare {\n       KeyEncoder::KeyEncoderContext* ctx, const KeyEncoder::KeyColumnArray& col,\n       const KeyEncoder::KeyRowArray& rows, uint8_t* match_bytevector);\n \n-  static void CompareVarBinaryColumnToRow_avx2(\n+  static uint32_t CompareVarBinaryColumnToRow_avx2(\n\nReview comment:\n       Can you add a comment about what this returns?\n\n##########\nFile path: cpp/src/arrow/compute/exec/key_hash.h\n##########\n@@ -32,75 +32,111 @@ namespace compute {\n // Implementations are based on xxh3 32-bit algorithm description from:\n // https://github.com/Cyan4973/xxHash/blob/dev/doc/xxhash_spec.md\n //\n-class Hashing {\n+class Hashing32 {\n\nReview comment:\n       Overall good, I'll take it on faith that it works, but function names should be switched to PascalCase.\n\n##########\nFile path: cpp/src/arrow/compute/exec/key_map.h\n##########\n@@ -28,29 +28,44 @@ namespace arrow {\n namespace compute {\n \n class SwissTable {\n+  friend class SwissTableMerge;\n+\n  public:\n   SwissTable() = default;\n   ~SwissTable() { cleanup(); }\n \n   using EqualImpl =\n       std::function<void(int num_keys, const uint16_t* selection /* may be null */,\n                          const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n-                         uint16_t* out_selection_mismatch)>;\n-  using AppendImpl = std::function<Status(int num_keys, const uint16_t* selection)>;\n+                         uint16_t* out_selection_mismatch, void* callback_ctx)>;\n+  using AppendImpl =\n+      std::function<Status(int num_keys, const uint16_t* selection, void* callback_ctx)>;\n\nReview comment:\n       What is `callback_ctx`?\n\n##########\nFile path: cpp/src/arrow/compute/exec/partition_util.h\n##########\n@@ -0,0 +1,131 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <atomic>\n+#include <cassert>\n+#include <cstdint>\n+#include <functional>\n+#include <random>\n+#include \"arrow/buffer.h\"\n+#include \"arrow/compute/exec/util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class PartitionSort {\n+ public:\n+  // Bucket sort rows on partition ids.\n+  // Include in the output exclusive cummulative sum of bucket sizes.\n+  // This corresponds to ranges in the sorted array containing all row ids for\n+  // each of the partitions.\n+  //\n+  template <class INPUT_PRTN_ID_FN, class OUTPUT_POS_FN>\n+  static void Eval(int num_rows, int num_prtns, uint16_t* prtn_ranges,\n+                   INPUT_PRTN_ID_FN prtn_id_impl, OUTPUT_POS_FN output_pos_impl) {\n+    ARROW_DCHECK(num_rows > 0 && num_rows <= (1 << 15));\n+    ARROW_DCHECK(num_prtns >= 1 && num_prtns <= (1 << 15));\n+\n+    memset(prtn_ranges, 0, (num_prtns + 1) * sizeof(uint16_t));\n+\n+    for (int i = 0; i < num_rows; ++i) {\n+      int prtn_id = static_cast<int>(prtn_id_impl(i));\n+      ++prtn_ranges[prtn_id + 1];\n+    }\n+\n+    uint16_t sum = 0;\n+    for (int i = 0; i < num_prtns; ++i) {\n+      uint16_t sum_next = sum + prtn_ranges[i + 1];\n+      prtn_ranges[i + 1] = sum;\n+      sum = sum_next;\n+    }\n+\n+    for (int i = 0; i < num_rows; ++i) {\n+      int prtn_id = static_cast<int>(prtn_id_impl(i));\n+      int pos = prtn_ranges[prtn_id + 1]++;\n+      output_pos_impl(i, pos);\n+    }\n+  }\n+};\n+\n+class PartitionLocks {\n+ public:\n+  PartitionLocks();\n+  ~PartitionLocks();\n+  void Init(int num_prtns);\n+  void CleanUp();\n+  bool AcquirePartitionLock(int num_prtns, const int* prtns_to_try, bool limit_retries,\n+                            int max_retries, int* locked_prtn_id,\n+                            int* locked_prtn_id_pos);\n+  void ReleasePartitionLock(int prtn_id);\n+\n+  template <typename IS_PRTN_EMPTY_FN, typename PROCESS_PRTN_FN>\n+  Status ForEachPartition(int* temp_unprocessed_prtns, IS_PRTN_EMPTY_FN is_prtn_empty_fn,\n+                          PROCESS_PRTN_FN process_prtn_fn) {\n+    int num_unprocessed_partitions = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      bool is_prtn_empty = is_prtn_empty_fn(i);\n+      if (!is_prtn_empty) {\n+        temp_unprocessed_prtns[num_unprocessed_partitions++] = i;\n+      }\n+    }\n+    while (num_unprocessed_partitions > 0) {\n+      int locked_prtn_id;\n+      int locked_prtn_id_pos;\n+      AcquirePartitionLock(num_unprocessed_partitions, temp_unprocessed_prtns,\n+                           /*limit_retries=*/false, /*max_retries=*/-1, &locked_prtn_id,\n+                           &locked_prtn_id_pos);\n+      {\n+        class AutoReleaseLock {\n+         public:\n+          AutoReleaseLock(PartitionLocks* locks, int prtn_id)\n+              : locks(locks), prtn_id(prtn_id) {}\n+          ~AutoReleaseLock() { locks->ReleasePartitionLock(prtn_id); }\n+          PartitionLocks* locks;\n+          int prtn_id;\n+        } auto_release_lock(this, locked_prtn_id);\n+        ARROW_RETURN_NOT_OK(process_prtn_fn(locked_prtn_id));\n+      }\n+      if (locked_prtn_id_pos < num_unprocessed_partitions - 1) {\n+        temp_unprocessed_prtns[locked_prtn_id_pos] =\n+            temp_unprocessed_prtns[num_unprocessed_partitions - 1];\n+      }\n+      --num_unprocessed_partitions;\n+    }\n+    return Status::OK();\n+  }\n+\n+ private:\n+  std::atomic<bool>* lock_ptr(int prtn_id);\n+  int random_int(int num_values);\n+\n+  struct PartitionLock {\n+    static constexpr int kCacheLineBytes = 64;\n+    std::atomic<bool> lock;\n+    uint8_t padding[kCacheLineBytes];\n+  };\n+  int num_prtns_;\n+  PartitionLock* locks_;\n+\n+  std::seed_seq rand_seed_;\n+  std::mt19937 rand_engine_;\n\nReview comment:\n       I think the `pcg` random number generators are supposed to be better and faster (`pcg32_fast`) - could switch to that one. Also is this random engine thread-safe? \n\n##########\nFile path: cpp/src/arrow/compute/exec/partition_util.h\n##########\n@@ -0,0 +1,131 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <atomic>\n+#include <cassert>\n+#include <cstdint>\n+#include <functional>\n+#include <random>\n+#include \"arrow/buffer.h\"\n+#include \"arrow/compute/exec/util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class PartitionSort {\n+ public:\n+  // Bucket sort rows on partition ids.\n+  // Include in the output exclusive cummulative sum of bucket sizes.\n+  // This corresponds to ranges in the sorted array containing all row ids for\n+  // each of the partitions.\n+  //\n+  template <class INPUT_PRTN_ID_FN, class OUTPUT_POS_FN>\n+  static void Eval(int num_rows, int num_prtns, uint16_t* prtn_ranges,\n+                   INPUT_PRTN_ID_FN prtn_id_impl, OUTPUT_POS_FN output_pos_impl) {\n+    ARROW_DCHECK(num_rows > 0 && num_rows <= (1 << 15));\n+    ARROW_DCHECK(num_prtns >= 1 && num_prtns <= (1 << 15));\n+\n+    memset(prtn_ranges, 0, (num_prtns + 1) * sizeof(uint16_t));\n+\n+    for (int i = 0; i < num_rows; ++i) {\n+      int prtn_id = static_cast<int>(prtn_id_impl(i));\n+      ++prtn_ranges[prtn_id + 1];\n+    }\n+\n+    uint16_t sum = 0;\n+    for (int i = 0; i < num_prtns; ++i) {\n+      uint16_t sum_next = sum + prtn_ranges[i + 1];\n+      prtn_ranges[i + 1] = sum;\n+      sum = sum_next;\n+    }\n+\n+    for (int i = 0; i < num_rows; ++i) {\n+      int prtn_id = static_cast<int>(prtn_id_impl(i));\n+      int pos = prtn_ranges[prtn_id + 1]++;\n+      output_pos_impl(i, pos);\n+    }\n+  }\n+};\n+\n+class PartitionLocks {\n+ public:\n+  PartitionLocks();\n+  ~PartitionLocks();\n+  void Init(int num_prtns);\n+  void CleanUp();\n+  bool AcquirePartitionLock(int num_prtns, const int* prtns_to_try, bool limit_retries,\n+                            int max_retries, int* locked_prtn_id,\n+                            int* locked_prtn_id_pos);\n+  void ReleasePartitionLock(int prtn_id);\n+\n+  template <typename IS_PRTN_EMPTY_FN, typename PROCESS_PRTN_FN>\n+  Status ForEachPartition(int* temp_unprocessed_prtns, IS_PRTN_EMPTY_FN is_prtn_empty_fn,\n+                          PROCESS_PRTN_FN process_prtn_fn) {\n+    int num_unprocessed_partitions = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      bool is_prtn_empty = is_prtn_empty_fn(i);\n+      if (!is_prtn_empty) {\n+        temp_unprocessed_prtns[num_unprocessed_partitions++] = i;\n+      }\n+    }\n+    while (num_unprocessed_partitions > 0) {\n+      int locked_prtn_id;\n+      int locked_prtn_id_pos;\n+      AcquirePartitionLock(num_unprocessed_partitions, temp_unprocessed_prtns,\n+                           /*limit_retries=*/false, /*max_retries=*/-1, &locked_prtn_id,\n+                           &locked_prtn_id_pos);\n+      {\n+        class AutoReleaseLock {\n+         public:\n+          AutoReleaseLock(PartitionLocks* locks, int prtn_id)\n+              : locks(locks), prtn_id(prtn_id) {}\n+          ~AutoReleaseLock() { locks->ReleasePartitionLock(prtn_id); }\n+          PartitionLocks* locks;\n+          int prtn_id;\n+        } auto_release_lock(this, locked_prtn_id);\n+        ARROW_RETURN_NOT_OK(process_prtn_fn(locked_prtn_id));\n+      }\n+      if (locked_prtn_id_pos < num_unprocessed_partitions - 1) {\n+        temp_unprocessed_prtns[locked_prtn_id_pos] =\n+            temp_unprocessed_prtns[num_unprocessed_partitions - 1];\n+      }\n+      --num_unprocessed_partitions;\n+    }\n+    return Status::OK();\n+  }\n+\n+ private:\n+  std::atomic<bool>* lock_ptr(int prtn_id);\n\nReview comment:\n       nit: snake_case\n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.h\n##########\n@@ -0,0 +1,876 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class ResizableArrayData {\n+ public:\n+  ResizableArrayData()\n+      : log_num_rows_min_(0),\n+        pool_(NULLPTR),\n+        num_rows_(0),\n+        num_rows_allocated_(0),\n+        var_len_buf_size_(0) {}\n+  ~ResizableArrayData() { Clear(true); }\n+  void Init(const std::shared_ptr<DataType>& data_type, MemoryPool* pool,\n+            int log_num_rows_min);\n+  void Clear(bool release_buffers);\n+  Status ResizeFixedLengthBuffers(int num_rows_new);\n+  Status ResizeVaryingLengthBuffer();\n+  int num_rows() const { return num_rows_; }\n+  KeyEncoder::KeyColumnArray column_array() const;\n+  KeyEncoder::KeyColumnMetadata column_metadata() const {\n+    return ColumnMetadataFromDataType(data_type_);\n+  }\n+  std::shared_ptr<ArrayData> array_data() const;\n+  uint8_t* mutable_data(int i) {\n+    return i == 0   ? non_null_buf_->mutable_data()\n+           : i == 1 ? fixed_len_buf_->mutable_data()\n+                    : var_len_buf_->mutable_data();\n+  }\n+\n+ private:\n+  static constexpr int64_t kNumPaddingBytes = 64;\n+  int log_num_rows_min_;\n+  std::shared_ptr<DataType> data_type_;\n+  MemoryPool* pool_;\n+  int num_rows_;\n+  int num_rows_allocated_;\n+  int var_len_buf_size_;\n+  std::shared_ptr<ResizableBuffer> non_null_buf_;\n+  std::shared_ptr<ResizableBuffer> fixed_len_buf_;\n+  std::shared_ptr<ResizableBuffer> var_len_buf_;\n+};\n+\n+class ExecBatchBuilder {\n+ public:\n+  static Status AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                               ResizableArrayData& target, int num_rows_to_append,\n+                               const uint16_t* row_ids, MemoryPool* pool);\n+\n+  static Status AppendNulls(const std::shared_ptr<DataType>& type,\n+                            ResizableArrayData& target, int num_rows_to_append,\n+                            MemoryPool* pool);\n+\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int num_cols,\n+                        const int* col_ids = NULLPTR);\n+\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int* num_appended, int num_cols,\n+                        const int* col_ids = NULLPTR);\n+\n+  Status AppendNulls(MemoryPool* pool,\n+                     const std::vector<std::shared_ptr<DataType>>& types,\n+                     int num_rows_to_append);\n+\n+  Status AppendNulls(MemoryPool* pool,\n+                     const std::vector<std::shared_ptr<DataType>>& types,\n+                     int num_rows_to_append, int* num_appended);\n+\n+  // Should only be called if num_rows() returns non-zero.\n+  //\n+  ExecBatch Flush();\n+\n+  int num_rows() const { return values_.empty() ? 0 : values_[0].num_rows(); }\n+\n+  static int num_rows_max() { return 1 << kLogNumRows; }\n+\n+ private:\n+  static constexpr int kLogNumRows = 15;\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  // The sequence of row_ids provided must be non-decreasing.\n+  //\n+  static int NumRowsToSkip(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                           const uint16_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                    const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  template <bool OUTPUT_BYTE_ALIGNED>\n+  static void CollectBitsImp(const uint8_t* input_bits, int64_t input_bits_offset,\n+                             uint8_t* output_bits, int64_t output_bits_offset,\n+                             int num_rows, const uint16_t* row_ids);\n+  static void CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                          uint8_t* output_bits, int64_t output_bits_offset, int num_rows,\n+                          const uint16_t* row_ids);\n+\n+  std::vector<ResizableArrayData> values_;\n+};\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const KeyEncoder::KeyRowMetadata& row_metadata,\n+                               int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                           int num_rows, const uint32_t* row_ids,\n+                           int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const KeyEncoder::KeyRowArray& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n\nReview comment:\n       Evaluating lambda per row seems like it's really expensive... does compiler do some fancy inlining or something? \n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.h\n##########\n@@ -0,0 +1,876 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class ResizableArrayData {\n+ public:\n+  ResizableArrayData()\n+      : log_num_rows_min_(0),\n+        pool_(NULLPTR),\n+        num_rows_(0),\n+        num_rows_allocated_(0),\n+        var_len_buf_size_(0) {}\n+  ~ResizableArrayData() { Clear(true); }\n+  void Init(const std::shared_ptr<DataType>& data_type, MemoryPool* pool,\n+            int log_num_rows_min);\n+  void Clear(bool release_buffers);\n+  Status ResizeFixedLengthBuffers(int num_rows_new);\n+  Status ResizeVaryingLengthBuffer();\n+  int num_rows() const { return num_rows_; }\n+  KeyEncoder::KeyColumnArray column_array() const;\n+  KeyEncoder::KeyColumnMetadata column_metadata() const {\n+    return ColumnMetadataFromDataType(data_type_);\n+  }\n+  std::shared_ptr<ArrayData> array_data() const;\n+  uint8_t* mutable_data(int i) {\n+    return i == 0   ? non_null_buf_->mutable_data()\n+           : i == 1 ? fixed_len_buf_->mutable_data()\n+                    : var_len_buf_->mutable_data();\n+  }\n+\n+ private:\n+  static constexpr int64_t kNumPaddingBytes = 64;\n+  int log_num_rows_min_;\n+  std::shared_ptr<DataType> data_type_;\n+  MemoryPool* pool_;\n+  int num_rows_;\n+  int num_rows_allocated_;\n+  int var_len_buf_size_;\n+  std::shared_ptr<ResizableBuffer> non_null_buf_;\n+  std::shared_ptr<ResizableBuffer> fixed_len_buf_;\n+  std::shared_ptr<ResizableBuffer> var_len_buf_;\n+};\n+\n+class ExecBatchBuilder {\n+ public:\n+  static Status AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                               ResizableArrayData& target, int num_rows_to_append,\n+                               const uint16_t* row_ids, MemoryPool* pool);\n+\n+  static Status AppendNulls(const std::shared_ptr<DataType>& type,\n+                            ResizableArrayData& target, int num_rows_to_append,\n+                            MemoryPool* pool);\n+\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int num_cols,\n+                        const int* col_ids = NULLPTR);\n+\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int* num_appended, int num_cols,\n+                        const int* col_ids = NULLPTR);\n+\n+  Status AppendNulls(MemoryPool* pool,\n+                     const std::vector<std::shared_ptr<DataType>>& types,\n+                     int num_rows_to_append);\n+\n+  Status AppendNulls(MemoryPool* pool,\n+                     const std::vector<std::shared_ptr<DataType>>& types,\n+                     int num_rows_to_append, int* num_appended);\n+\n+  // Should only be called if num_rows() returns non-zero.\n+  //\n+  ExecBatch Flush();\n+\n+  int num_rows() const { return values_.empty() ? 0 : values_[0].num_rows(); }\n+\n+  static int num_rows_max() { return 1 << kLogNumRows; }\n+\n+ private:\n+  static constexpr int kLogNumRows = 15;\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  // The sequence of row_ids provided must be non-decreasing.\n+  //\n+  static int NumRowsToSkip(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                           const uint16_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                    const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  template <bool OUTPUT_BYTE_ALIGNED>\n+  static void CollectBitsImp(const uint8_t* input_bits, int64_t input_bits_offset,\n+                             uint8_t* output_bits, int64_t output_bits_offset,\n+                             int num_rows, const uint16_t* row_ids);\n+  static void CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                          uint8_t* output_bits, int64_t output_bits_offset, int num_rows,\n+                          const uint16_t* row_ids);\n+\n+  std::vector<ResizableArrayData> values_;\n+};\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const KeyEncoder::KeyRowMetadata& row_metadata,\n+                               int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                           int num_rows, const uint32_t* row_ids,\n+                           int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const KeyEncoder::KeyRowArray& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const KeyEncoder::KeyRowArray& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const KeyEncoder::KeyRowArray& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                             int num_rows, const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const KeyEncoder::KeyRowMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(\n+      MemoryPool* pool, const ExecBatch& batch, int begin_row_id, int end_row_id,\n+      int num_row_ids, const uint16_t* row_ids,\n+      std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n\nReview comment:\n       Is this low priority? Should we make a JIRA and get rid of comment?\n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.cc\n##########\n@@ -0,0 +1,3295 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_compare.h\"\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+void ResizableArrayData::Init(const std::shared_ptr<DataType>& data_type,\n+                              MemoryPool* pool, int log_num_rows_min) {\n+#ifndef NDEBUG\n+  if (num_rows_allocated_ > 0) {\n+    ARROW_DCHECK(data_type_ != NULLPTR);\n+    KeyEncoder::KeyColumnMetadata metadata_before =\n+        ColumnMetadataFromDataType(data_type_);\n+    KeyEncoder::KeyColumnMetadata metadata_after = ColumnMetadataFromDataType(data_type);\n+    ARROW_DCHECK(metadata_before.is_fixed_length == metadata_after.is_fixed_length &&\n+                 metadata_before.fixed_length == metadata_after.fixed_length);\n+  }\n+#endif\n+  Clear(/*release_buffers=*/false);\n+  log_num_rows_min_ = log_num_rows_min;\n+  data_type_ = data_type;\n+  pool_ = pool;\n+}\n+\n+void ResizableArrayData::Clear(bool release_buffers) {\n+  num_rows_ = 0;\n+  if (release_buffers) {\n+    non_null_buf_.reset();\n+    fixed_len_buf_.reset();\n+    var_len_buf_.reset();\n+    num_rows_allocated_ = 0;\n+    var_len_buf_size_ = 0;\n+  }\n+}\n+\n+Status ResizableArrayData::ResizeFixedLengthBuffers(int num_rows_new) {\n+  ARROW_DCHECK(num_rows_new >= 0);\n+  if (num_rows_new <= num_rows_allocated_) {\n+    num_rows_ = num_rows_new;\n+    return Status::OK();\n+  }\n+\n+  int num_rows_allocated_new = 1 << log_num_rows_min_;\n+  while (num_rows_allocated_new < num_rows_new) {\n+    num_rows_allocated_new *= 2;\n+  }\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (fixed_len_buf_ == NULLPTR) {\n+    ARROW_DCHECK(non_null_buf_ == NULLPTR && var_len_buf_ == NULLPTR);\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        non_null_buf_,\n+        AllocateResizableBuffer(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes, pool_));\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes,\n+                pool_));\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes,\n+                pool_));\n+      }\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(\n+          fixed_len_buf_,\n+          AllocateResizableBuffer(\n+              (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes, pool_));\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(var_len_buf_, AllocateResizableBuffer(\n+                                            sizeof(uint64_t) + kNumPaddingBytes, pool_));\n+\n+    var_len_buf_size_ = sizeof(uint64_t);\n+  } else {\n+    ARROW_DCHECK(non_null_buf_ != NULLPTR && var_len_buf_ != NULLPTR);\n+\n+    RETURN_NOT_OK(non_null_buf_->Resize(bit_util::BytesForBits(num_rows_allocated_new) +\n+                                        kNumPaddingBytes));\n+\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes));\n+      } else {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes));\n+      }\n+    } else {\n+      RETURN_NOT_OK(fixed_len_buf_->Resize(\n+          (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes));\n+    }\n+  }\n+\n+  num_rows_allocated_ = num_rows_allocated_new;\n+  num_rows_ = num_rows_new;\n+\n+  return Status::OK();\n+}\n+\n+Status ResizableArrayData::ResizeVaryingLengthBuffer() {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (!column_metadata.is_fixed_length) {\n+    int min_new_size = static_cast<int>(\n+        reinterpret_cast<const uint32_t*>(fixed_len_buf_->data())[num_rows_]);\n+    ARROW_DCHECK(var_len_buf_size_ > 0);\n+    if (var_len_buf_size_ < min_new_size) {\n+      int new_size = var_len_buf_size_;\n+      while (new_size < min_new_size) {\n+        new_size *= 2;\n+      }\n+      RETURN_NOT_OK(var_len_buf_->Resize(new_size + kNumPaddingBytes));\n+      var_len_buf_size_ = new_size;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+KeyEncoder::KeyColumnArray ResizableArrayData::column_array() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+  return KeyEncoder::KeyColumnArray(\n+      column_metadata, num_rows_, non_null_buf_->mutable_data(),\n+      fixed_len_buf_->mutable_data(), var_len_buf_->mutable_data());\n+}\n+\n+std::shared_ptr<ArrayData> ResizableArrayData::array_data() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  auto valid_count = arrow::internal::CountSetBits(non_null_buf_->data(), /*offset=*/0,\n+                                                   static_cast<int64_t>(num_rows_));\n+  int null_count = static_cast<int>(num_rows_) - static_cast<int>(valid_count);\n+\n+  if (column_metadata.is_fixed_length) {\n+    return ArrayData::Make(data_type_, num_rows_, {non_null_buf_, fixed_len_buf_},\n+                           null_count);\n+  } else {\n+    return ArrayData::Make(data_type_, num_rows_,\n+                           {non_null_buf_, fixed_len_buf_, var_len_buf_}, null_count);\n+  }\n+}\n+\n+int ExecBatchBuilder::NumRowsToSkip(const std::shared_ptr<ArrayData>& column,\n+                                    int num_rows, const uint16_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+#ifndef NDEBUG\n+  // Ids must be in non-decreasing order\n+  //\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(row_ids[i] >= row_ids[i - 1]);\n+  }\n+#endif\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(column->type);\n+\n+  int num_rows_left = num_rows;\n+  int num_bytes_skipped = 0;\n+  while (num_rows_left > 0 && num_bytes_skipped < num_tail_bytes_to_skip) {\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        num_rows_left = std::max(num_rows_left, 8) - 8;\n+        ++num_bytes_skipped;\n+      } else {\n+        --num_rows_left;\n+        num_bytes_skipped += column_metadata.fixed_length;\n+      }\n+    } else {\n+      --num_rows_left;\n+      int row_id_removed = row_ids[num_rows_left];\n+      const uint32_t* offsets =\n+          reinterpret_cast<const uint32_t*>(column->buffers[1]->data());\n+      num_bytes_skipped += offsets[row_id_removed + 1] - offsets[row_id_removed];\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <bool OUTPUT_BYTE_ALIGNED>\n+void ExecBatchBuilder::CollectBitsImp(const uint8_t* input_bits,\n+                                      int64_t input_bits_offset, uint8_t* output_bits,\n+                                      int64_t output_bits_offset, int num_rows,\n+                                      const uint16_t* row_ids) {\n+  if (!OUTPUT_BYTE_ALIGNED) {\n+    ARROW_DCHECK(output_bits_offset % 8 > 0);\n+    output_bits[output_bits_offset / 8] &=\n+        static_cast<uint8_t>((1 << (output_bits_offset % 8)) - 1);\n+  } else {\n+    ARROW_DCHECK(output_bits_offset % 8 == 0);\n+  }\n+  constexpr int unroll = 8;\n+  for (int i = 0; i < num_rows / unroll; ++i) {\n+    const uint16_t* row_ids_base = row_ids + unroll * i;\n+    uint8_t result;\n+    result = bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[0]) ? 1 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[1]) ? 2 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[2]) ? 4 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[3]) ? 8 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[4]) ? 16 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[5]) ? 32 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[6]) ? 64 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[7]) ? 128 : 0;\n+    if (OUTPUT_BYTE_ALIGNED) {\n+      output_bits[output_bits_offset / 8 + i] = result;\n+    } else {\n+      output_bits[output_bits_offset / 8 + i] |=\n+          static_cast<uint8_t>(result << (output_bits_offset % 8));\n+      output_bits[output_bits_offset / 8 + i + 1] =\n+          static_cast<uint8_t>(result >> (8 - (output_bits_offset % 8)));\n+    }\n+  }\n+  if (num_rows % unroll > 0) {\n+    for (int i = num_rows - (num_rows % unroll); i < num_rows; ++i) {\n+      bit_util::SetBitTo(output_bits, output_bits_offset + i,\n+                         bit_util::GetBit(input_bits, input_bits_offset + row_ids[i]));\n+    }\n+  }\n+}\n+\n+void ExecBatchBuilder::CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                                   uint8_t* output_bits, int64_t output_bits_offset,\n+                                   int num_rows, const uint16_t* row_ids) {\n+  if (output_bits_offset % 8 > 0) {\n+    CollectBitsImp<false>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                          num_rows, row_ids);\n+  } else {\n+    CollectBitsImp<true>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                         num_rows, row_ids);\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void ExecBatchBuilder::Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                             const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  KeyEncoder::KeyColumnMetadata metadata = ColumnMetadataFromDataType(column->type);\n+\n+  if (!metadata.is_fixed_length) {\n+    const uint8_t* ptr_base = column->buffers[2]->data();\n+    const uint32_t* offsets =\n+        reinterpret_cast<const uint32_t*>(column->buffers[1]->data()) + column->offset;\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr = ptr_base + offsets[row_id];\n+      uint32_t field_length = offsets[row_id + 1] - offsets[row_id];\n+      process_value_fn(i, field_ptr, field_length);\n+    }\n+  } else {\n+    ARROW_DCHECK(metadata.fixed_length > 0);\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr =\n+          column->buffers[1]->data() +\n+          (column->offset + row_id) * static_cast<int64_t>(metadata.fixed_length);\n+      process_value_fn(i, field_ptr, metadata.fixed_length);\n+    }\n+  }\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                                        ResizableArrayData& target,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  ARROW_DCHECK(num_rows_before >= 0);\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(source->type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(source->type);\n+\n+  if (column_metadata.is_fixed_length) {\n+    // Fixed length column\n+    //\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        CollectBits(source->buffers[1]->data(), source->offset, target.mutable_data(1),\n+                    num_rows_before, num_rows_to_append, row_ids);\n+        break;\n+      case 1:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                target.mutable_data(1)[num_rows_before + i] = *ptr;\n+              });\n+        break;\n+      case 2:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint16_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint16_t*>(ptr);\n+              });\n+        break;\n+      case 4:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint32_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint32_t*>(ptr);\n+              });\n+        break;\n+      case 8:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint64_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint64_t*>(ptr);\n+              });\n+        break;\n+      default: {\n+        int num_rows_to_process =\n+            num_rows_to_append -\n+            NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+        Visit(source, num_rows_to_process, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                    target.mutable_data(1) +\n+                    static_cast<int64_t>(num_bytes) * (num_rows_before + i));\n+                const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                for (uint32_t word_id = 0;\n+                     word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t));\n+                     ++word_id) {\n+                  util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+                }\n+              });\n+        if (num_rows_to_append > num_rows_to_process) {\n+          Visit(source, num_rows_to_append - num_rows_to_process,\n+                row_ids + num_rows_to_process,\n+                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                  uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                      target.mutable_data(1) +\n+                      static_cast<int64_t>(num_bytes) *\n+                          (num_rows_before + num_rows_to_process + i));\n+                  const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                  memcpy(dst, src, num_bytes);\n+                });\n+        }\n+      }\n+    }\n+  } else {\n+    // Varying length column\n+    //\n+\n+    // Step 1: calculate target offsets\n+    //\n+    uint32_t* offsets = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[num_rows_before];\n+    Visit(source, num_rows_to_append, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            offsets[num_rows_before + i] = num_bytes;\n+          });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[num_rows_before + i];\n+      offsets[num_rows_before + i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_before + num_rows_to_append] = sum;\n+\n+    // Step 2: resize output buffers\n+    //\n+    RETURN_NOT_OK(target.ResizeVaryingLengthBuffer());\n+\n+    // Step 3: copy varying-length data\n+    //\n+    int num_rows_to_process =\n+        num_rows_to_append -\n+        NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+    Visit(source, num_rows_to_process, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(target.mutable_data(2) +\n+                                                        offsets[num_rows_before + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            for (uint32_t word_id = 0;\n+                 word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+              util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+            }\n+          });\n+    Visit(source, num_rows_to_append - num_rows_to_process, row_ids + num_rows_to_process,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                target.mutable_data(2) +\n+                offsets[num_rows_before + num_rows_to_process + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            memcpy(dst, src, num_bytes);\n+          });\n+  }\n+\n+  // Process nulls\n+  //\n+  if (source->buffers[0] == NULLPTR) {\n+    uint8_t* dst = target.mutable_data(0);\n+    dst[num_rows_before / 8] |= static_cast<uint8_t>(~0ULL << (num_rows_before & 7));\n+    for (int i = num_rows_before / 8 + 1;\n+         i < bit_util::BytesForBits(num_rows_before + num_rows_to_append); ++i) {\n+      dst[i] = 0xff;\n+    }\n+  } else {\n+    CollectBits(source->buffers[0]->data(), source->offset, target.mutable_data(0),\n+                num_rows_before, num_rows_to_append, row_ids);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(const std::shared_ptr<DataType>& type,\n+                                     ResizableArrayData& target, int num_rows_to_append,\n+                                     MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(type);\n+\n+  // Process fixed length buffer\n+  //\n+  if (column_metadata.is_fixed_length) {\n+    uint8_t* dst = target.mutable_data(1);\n+    if (column_metadata.fixed_length == 0) {\n+      dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+      int64_t offset_begin = num_rows_before / 8 + 1;\n+      int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+      if (offset_end > offset_begin) {\n+        memset(dst + offset_begin, 0, offset_end - offset_begin);\n+      }\n+    } else {\n+      memset(dst + num_rows_before * static_cast<int64_t>(column_metadata.fixed_length),\n+             0, static_cast<int64_t>(column_metadata.fixed_length) * num_rows_to_append);\n+    }\n+  } else {\n+    uint32_t* dst = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : dst[num_rows_before];\n+    for (int64_t i = num_rows_before; i <= num_rows_after; ++i) {\n+      dst[i] = sum;\n+    }\n+  }\n+\n+  // Process nulls\n+  //\n+  uint8_t* dst = target.mutable_data(0);\n+  dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+  int64_t offset_begin = num_rows_before / 8 + 1;\n+  int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+  if (offset_end > offset_begin) {\n+    memset(dst + offset_begin, 0, offset_end - offset_begin);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int num_cols, const int* col_ids) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(num_cols);\n+    for (int i = 0; i < num_cols; ++i) {\n+      const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+      ARROW_DCHECK(data.is_array());\n+      const std::shared_ptr<ArrayData>& array_data = data.array();\n+      values_[i].Init(array_data->type, pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    RETURN_NOT_OK(\n+        AppendSelected(array_data, values_[i], num_rows_to_append, row_ids, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int* num_appended, int num_cols,\n+                                        const int* col_ids) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendSelected(pool, batch, num_rows_next, row_ids, num_cols, col_ids));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(types.size());\n+    for (size_t i = 0; i < types.size(); ++i) {\n+      values_[i].Init(types[i], pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    RETURN_NOT_OK(AppendNulls(types[i], values_[i], num_rows_to_append, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append, int* num_appended) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendNulls(pool, types, num_rows_next));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+ExecBatch ExecBatchBuilder::Flush() {\n+  ARROW_DCHECK(num_rows() > 0);\n+  ExecBatch out({}, num_rows());\n+  out.values.resize(values_.size());\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    out.values[i] = values_[i].array_data();\n\nReview comment:\n       I think `std::move(values_[i].array_data())` would be better? you can avoid incrementing refcount \n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.cc\n##########\n@@ -0,0 +1,3279 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_compare.h\"\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+void ResizableArrayData::Init(const std::shared_ptr<DataType>& data_type,\n+                              MemoryPool* pool, int log_num_rows_min) {\n+#ifndef NDEBUG\n+  if (num_rows_allocated_ > 0) {\n+    ARROW_DCHECK(data_type_ != NULLPTR);\n+    KeyEncoder::KeyColumnMetadata metadata_before =\n+        ColumnMetadataFromDataType(data_type_);\n+    KeyEncoder::KeyColumnMetadata metadata_after = ColumnMetadataFromDataType(data_type);\n+    ARROW_DCHECK(metadata_before.is_fixed_length == metadata_after.is_fixed_length &&\n+                 metadata_before.fixed_length == metadata_after.fixed_length);\n+  }\n+#endif\n+  Clear(/*release_buffers=*/false);\n+  log_num_rows_min_ = log_num_rows_min;\n+  data_type_ = data_type;\n+  pool_ = pool;\n+}\n+\n+void ResizableArrayData::Clear(bool release_buffers) {\n+  num_rows_ = 0;\n+  if (release_buffers) {\n+    non_null_buf_.reset();\n+    fixed_len_buf_.reset();\n+    var_len_buf_.reset();\n+    num_rows_allocated_ = 0;\n+    var_len_buf_size_ = 0;\n+  }\n+}\n+\n+Status ResizableArrayData::ResizeFixedLengthBuffers(int num_rows_new) {\n+  ARROW_DCHECK(num_rows_new >= 0);\n+  if (num_rows_new <= num_rows_allocated_) {\n+    num_rows_ = num_rows_new;\n+    return Status::OK();\n+  }\n+\n+  int num_rows_allocated_new = 1 << log_num_rows_min_;\n+  while (num_rows_allocated_new < num_rows_new) {\n+    num_rows_allocated_new *= 2;\n+  }\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (fixed_len_buf_ == NULLPTR) {\n+    ARROW_DCHECK(non_null_buf_ == NULLPTR && var_len_buf_ == NULLPTR);\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        non_null_buf_,\n+        AllocateResizableBuffer(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes, pool_));\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes,\n+                pool_));\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes,\n+                pool_));\n+      }\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(\n+          fixed_len_buf_,\n+          AllocateResizableBuffer(\n+              (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes, pool_));\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(var_len_buf_, AllocateResizableBuffer(\n+                                            sizeof(uint64_t) + kNumPaddingBytes, pool_));\n+\n+    var_len_buf_size_ = sizeof(uint64_t);\n+  } else {\n+    ARROW_DCHECK(non_null_buf_ != NULLPTR && var_len_buf_ != NULLPTR);\n+\n+    RETURN_NOT_OK(non_null_buf_->Resize(bit_util::BytesForBits(num_rows_allocated_new) +\n+                                        kNumPaddingBytes));\n+\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes));\n+      } else {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes));\n+      }\n+    } else {\n+      RETURN_NOT_OK(fixed_len_buf_->Resize(\n+          (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes));\n+    }\n+  }\n+\n+  num_rows_allocated_ = num_rows_allocated_new;\n+  num_rows_ = num_rows_new;\n+\n+  return Status::OK();\n+}\n+\n+Status ResizableArrayData::ResizeVaryingLengthBuffer() {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (!column_metadata.is_fixed_length) {\n+    int min_new_size = static_cast<int>(\n+        reinterpret_cast<const uint32_t*>(fixed_len_buf_->data())[num_rows_]);\n+    ARROW_DCHECK(var_len_buf_size_ > 0);\n+    if (var_len_buf_size_ < min_new_size) {\n+      int new_size = var_len_buf_size_;\n+      while (new_size < min_new_size) {\n+        new_size *= 2;\n+      }\n+      RETURN_NOT_OK(var_len_buf_->Resize(new_size + kNumPaddingBytes));\n+      var_len_buf_size_ = new_size;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+KeyEncoder::KeyColumnArray ResizableArrayData::column_array() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+  return KeyEncoder::KeyColumnArray(\n+      column_metadata, num_rows_, non_null_buf_->mutable_data(),\n+      fixed_len_buf_->mutable_data(), var_len_buf_->mutable_data());\n+}\n+\n+std::shared_ptr<ArrayData> ResizableArrayData::array_data() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  auto valid_count = arrow::internal::CountSetBits(non_null_buf_->data(), /*offset=*/0,\n+                                                   static_cast<int64_t>(num_rows_));\n+  int null_count = static_cast<int>(num_rows_) - static_cast<int>(valid_count);\n+\n+  if (column_metadata.is_fixed_length) {\n+    return ArrayData::Make(data_type_, num_rows_, {non_null_buf_, fixed_len_buf_},\n+                           null_count);\n+  } else {\n+    return ArrayData::Make(data_type_, num_rows_,\n+                           {non_null_buf_, fixed_len_buf_, var_len_buf_}, null_count);\n+  }\n+}\n+\n+int ExecBatchBuilder::NumRowsToSkip(const std::shared_ptr<ArrayData>& column,\n+                                    int num_rows, const uint16_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+#ifndef NDEBUG\n+  // Ids must be in non-decreasing order\n+  //\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(row_ids[i] >= row_ids[i - 1]);\n+  }\n+#endif\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(column->type);\n+\n+  int num_rows_left = num_rows;\n+  int num_bytes_skipped = 0;\n+  while (num_rows_left > 0 && num_bytes_skipped < num_tail_bytes_to_skip) {\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        num_rows_left = std::max(num_rows_left, 8) - 8;\n+        ++num_bytes_skipped;\n+      } else {\n+        --num_rows_left;\n+        num_bytes_skipped += column_metadata.fixed_length;\n+      }\n+    } else {\n+      --num_rows_left;\n+      int row_id_removed = row_ids[num_rows_left];\n+      const uint32_t* offsets =\n+          reinterpret_cast<const uint32_t*>(column->buffers[1]->data());\n+      num_bytes_skipped += offsets[row_id_removed + 1] - offsets[row_id_removed];\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <bool OUTPUT_BYTE_ALIGNED>\n+void ExecBatchBuilder::CollectBitsImp(const uint8_t* input_bits,\n+                                      int64_t input_bits_offset, uint8_t* output_bits,\n+                                      int64_t output_bits_offset, int num_rows,\n+                                      const uint16_t* row_ids) {\n+  if (!OUTPUT_BYTE_ALIGNED) {\n+    ARROW_DCHECK(output_bits_offset % 8 > 0);\n+    output_bits[output_bits_offset / 8] &=\n+        static_cast<uint8_t>((1 << (output_bits_offset % 8)) - 1);\n+  } else {\n+    ARROW_DCHECK(output_bits_offset % 8 == 0);\n+  }\n+  constexpr int unroll = 8;\n+  for (int i = 0; i < num_rows / unroll; ++i) {\n+    const uint16_t* row_ids_base = row_ids + unroll * i;\n+    uint8_t result;\n+    result = bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[0]) ? 1 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[1]) ? 2 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[2]) ? 4 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[3]) ? 8 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[4]) ? 16 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[5]) ? 32 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[6]) ? 64 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[7]) ? 128 : 0;\n+    if (OUTPUT_BYTE_ALIGNED) {\n+      output_bits[output_bits_offset / 8 + i] = result;\n+    } else {\n+      output_bits[output_bits_offset / 8 + i] |=\n+          static_cast<uint8_t>(result << (output_bits_offset % 8));\n+      output_bits[output_bits_offset / 8 + i + 1] =\n+          static_cast<uint8_t>(result >> (8 - (output_bits_offset % 8)));\n+    }\n+  }\n+  if (num_rows % unroll > 0) {\n+    for (int i = num_rows - (num_rows % unroll); i < num_rows; ++i) {\n+      bit_util::SetBitTo(output_bits, output_bits_offset + i,\n+                         bit_util::GetBit(input_bits, input_bits_offset + row_ids[i]));\n+    }\n+  }\n+}\n+\n+void ExecBatchBuilder::CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                                   uint8_t* output_bits, int64_t output_bits_offset,\n+                                   int num_rows, const uint16_t* row_ids) {\n+  if (output_bits_offset % 8 > 0) {\n+    CollectBitsImp<false>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                          num_rows, row_ids);\n+  } else {\n+    CollectBitsImp<true>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                         num_rows, row_ids);\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void ExecBatchBuilder::Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                             const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  KeyEncoder::KeyColumnMetadata metadata = ColumnMetadataFromDataType(column->type);\n+\n+  if (!metadata.is_fixed_length) {\n+    const uint8_t* ptr_base = column->buffers[2]->data();\n+    const uint32_t* offsets =\n+        reinterpret_cast<const uint32_t*>(column->buffers[1]->data()) + column->offset;\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr = ptr_base + offsets[row_id];\n+      uint32_t field_length = offsets[row_id + 1] - offsets[row_id];\n+      process_value_fn(i, field_ptr, field_length);\n+    }\n+  } else {\n+    ARROW_DCHECK(metadata.fixed_length > 0);\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr =\n+          column->buffers[1]->data() +\n+          (column->offset + row_id) * static_cast<int64_t>(metadata.fixed_length);\n+      process_value_fn(i, field_ptr, metadata.fixed_length);\n+    }\n+  }\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                                        ResizableArrayData& target,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  ARROW_DCHECK(num_rows_before >= 0);\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(source->type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(source->type);\n+\n+  if (column_metadata.is_fixed_length) {\n+    // Fixed length column\n+    //\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        CollectBits(source->buffers[1]->data(), source->offset, target.mutable_data(1),\n+                    num_rows_before, num_rows_to_append, row_ids);\n+        break;\n+      case 1:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                target.mutable_data(1)[num_rows_before + i] = *ptr;\n+              });\n+        break;\n+      case 2:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint16_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint16_t*>(ptr);\n+              });\n+        break;\n+      case 4:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint32_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint32_t*>(ptr);\n+              });\n+        break;\n+      case 8:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint64_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint64_t*>(ptr);\n+              });\n+        break;\n+      default: {\n+        int num_rows_to_process =\n+            num_rows_to_append -\n+            NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+        Visit(source, num_rows_to_process, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                    target.mutable_data(1) +\n+                    static_cast<int64_t>(num_bytes) * (num_rows_before + i));\n+                const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                for (uint32_t word_id = 0;\n+                     word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t));\n+                     ++word_id) {\n+                  util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+                }\n+              });\n+        if (num_rows_to_append > num_rows_to_process) {\n+          Visit(source, num_rows_to_append - num_rows_to_process,\n+                row_ids + num_rows_to_process,\n+                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                  uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                      target.mutable_data(1) +\n+                      static_cast<int64_t>(num_bytes) *\n+                          (num_rows_before + num_rows_to_process + i));\n+                  const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                  memcpy(dst, src, num_bytes);\n+                });\n+        }\n+      }\n+    }\n+  } else {\n+    // Varying length column\n+    //\n+\n+    // Step 1: calculate target offsets\n+    //\n+    uint32_t* offsets = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[num_rows_before];\n+    Visit(source, num_rows_to_append, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            offsets[num_rows_before + i] = num_bytes;\n+          });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[num_rows_before + i];\n+      offsets[num_rows_before + i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_before + num_rows_to_append] = sum;\n+\n+    // Step 2: resize output buffers\n+    //\n+    RETURN_NOT_OK(target.ResizeVaryingLengthBuffer());\n+\n+    // Step 3: copy varying-length data\n+    //\n+    int num_rows_to_process =\n+        num_rows_to_append -\n+        NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+    Visit(source, num_rows_to_process, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(target.mutable_data(2) +\n+                                                        offsets[num_rows_before + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            for (uint32_t word_id = 0;\n+                 word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+              util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+            }\n+          });\n+    Visit(source, num_rows_to_append - num_rows_to_process, row_ids + num_rows_to_process,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                target.mutable_data(2) +\n+                offsets[num_rows_before + num_rows_to_process + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            memcpy(dst, src, num_bytes);\n+          });\n+  }\n+\n+  // Process nulls\n+  //\n+  if (source->buffers[0] == NULLPTR) {\n+    uint8_t* dst = target.mutable_data(0);\n+    dst[num_rows_before / 8] |= static_cast<uint8_t>(~0ULL << (num_rows_before & 7));\n+    for (int i = num_rows_before / 8 + 1;\n+         i < bit_util::BytesForBits(num_rows_before + num_rows_to_append); ++i) {\n+      dst[i] = 0xff;\n+    }\n+  } else {\n+    CollectBits(source->buffers[0]->data(), source->offset, target.mutable_data(0),\n+                num_rows_before, num_rows_to_append, row_ids);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(const std::shared_ptr<DataType>& type,\n+                                     ResizableArrayData& target, int num_rows_to_append,\n+                                     MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(type);\n+\n+  // Process fixed length buffer\n+  //\n+  if (column_metadata.is_fixed_length) {\n+    uint8_t* dst = target.mutable_data(1);\n+    if (column_metadata.fixed_length == 0) {\n+      dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+      int64_t offset_begin = num_rows_before / 8 + 1;\n+      int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+      if (offset_end > offset_begin) {\n+        memset(dst + offset_begin, 0, offset_end - offset_begin);\n+      }\n+    } else {\n+      memset(dst + num_rows_before * static_cast<int64_t>(column_metadata.fixed_length),\n+             0, static_cast<int64_t>(column_metadata.fixed_length) * num_rows_to_append);\n+    }\n+  } else {\n+    uint32_t* dst = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : dst[num_rows_before];\n+    for (int64_t i = num_rows_before; i <= num_rows_after; ++i) {\n+      dst[i] = sum;\n+    }\n+  }\n+\n+  // Process nulls\n+  //\n+  uint8_t* dst = target.mutable_data(0);\n+  dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+  int64_t offset_begin = num_rows_before / 8 + 1;\n+  int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+  if (offset_end > offset_begin) {\n+    memset(dst + offset_begin, 0, offset_end - offset_begin);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int num_cols, const int* col_ids) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(num_cols);\n+    for (int i = 0; i < num_cols; ++i) {\n+      const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+      ARROW_DCHECK(data.is_array());\n+      const std::shared_ptr<ArrayData>& array_data = data.array();\n+      values_[i].Init(array_data->type, pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    RETURN_NOT_OK(\n+        AppendSelected(array_data, values_[i], num_rows_to_append, row_ids, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int* num_appended, int num_cols,\n+                                        const int* col_ids) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendSelected(pool, batch, num_rows_next, row_ids, num_cols, col_ids));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(types.size());\n+    for (size_t i = 0; i < types.size(); ++i) {\n+      values_[i].Init(types[i], pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    RETURN_NOT_OK(AppendNulls(types[i], values_[i], num_rows_to_append, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append, int* num_appended) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendNulls(pool, types, num_rows_next));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+ExecBatch ExecBatchBuilder::Flush() {\n+  ARROW_DCHECK(num_rows() > 0);\n+  ExecBatch out({}, num_rows());\n+  out.values.resize(values_.size());\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    out.values[i] = values_[i].array_data();\n+    values_[i].Clear(true);\n+  }\n+  return out;\n+}\n+\n+int RowArrayAccessor::VarbinaryColumnId(const KeyEncoder::KeyRowMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                    int num_rows, const uint32_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                             int num_rows, const uint32_t* row_ids,\n+                             PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                  int num_rows, const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool,\n+                              const KeyEncoder::KeyRowMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyEncoder::KeyColumnMetadata> column_metadatas;\n+  ColumnMetadatasFromExecBatch(batch, column_metadatas);\n+  KeyEncoder::KeyRowMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(\n+    MemoryPool* pool, const ExecBatch& batch, int begin_row_id, int end_row_id,\n+    int num_row_ids, const uint16_t* row_ids,\n+    std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+\n+  KeyEncoder::KeyEncoderContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = output->column_metadata();\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const KeyEncoder::KeyRowMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(KeyEncoder::KeyRowArray* target,\n+                                    const KeyEncoder::KeyRowArray& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(KeyEncoder::KeyRowArray* target,\n+                                      const KeyEncoder::KeyRowArray& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(KeyEncoder::KeyRowArray* target,\n+                              const KeyEncoder::KeyRowArray& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays,\n+    std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Hashing32::HashBatch(*input->batch, input->batch_start_row,\n+                       input->batch_end_row - input->batch_start_row, hashes,\n+                       *input->temp_column_arrays, hardware_flags, input->temp_stack);\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n\nReview comment:\n       Should we `DCHECK_OK` this? Why is it valid to ignore return status here? \n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.cc\n##########\n@@ -0,0 +1,3279 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_compare.h\"\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+void ResizableArrayData::Init(const std::shared_ptr<DataType>& data_type,\n+                              MemoryPool* pool, int log_num_rows_min) {\n+#ifndef NDEBUG\n+  if (num_rows_allocated_ > 0) {\n+    ARROW_DCHECK(data_type_ != NULLPTR);\n+    KeyEncoder::KeyColumnMetadata metadata_before =\n+        ColumnMetadataFromDataType(data_type_);\n+    KeyEncoder::KeyColumnMetadata metadata_after = ColumnMetadataFromDataType(data_type);\n+    ARROW_DCHECK(metadata_before.is_fixed_length == metadata_after.is_fixed_length &&\n+                 metadata_before.fixed_length == metadata_after.fixed_length);\n+  }\n+#endif\n+  Clear(/*release_buffers=*/false);\n+  log_num_rows_min_ = log_num_rows_min;\n+  data_type_ = data_type;\n+  pool_ = pool;\n+}\n+\n+void ResizableArrayData::Clear(bool release_buffers) {\n+  num_rows_ = 0;\n+  if (release_buffers) {\n+    non_null_buf_.reset();\n+    fixed_len_buf_.reset();\n+    var_len_buf_.reset();\n+    num_rows_allocated_ = 0;\n+    var_len_buf_size_ = 0;\n+  }\n+}\n+\n+Status ResizableArrayData::ResizeFixedLengthBuffers(int num_rows_new) {\n+  ARROW_DCHECK(num_rows_new >= 0);\n+  if (num_rows_new <= num_rows_allocated_) {\n+    num_rows_ = num_rows_new;\n+    return Status::OK();\n+  }\n+\n+  int num_rows_allocated_new = 1 << log_num_rows_min_;\n+  while (num_rows_allocated_new < num_rows_new) {\n+    num_rows_allocated_new *= 2;\n+  }\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (fixed_len_buf_ == NULLPTR) {\n+    ARROW_DCHECK(non_null_buf_ == NULLPTR && var_len_buf_ == NULLPTR);\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        non_null_buf_,\n+        AllocateResizableBuffer(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes, pool_));\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes,\n+                pool_));\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes,\n+                pool_));\n+      }\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(\n+          fixed_len_buf_,\n+          AllocateResizableBuffer(\n+              (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes, pool_));\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(var_len_buf_, AllocateResizableBuffer(\n+                                            sizeof(uint64_t) + kNumPaddingBytes, pool_));\n+\n+    var_len_buf_size_ = sizeof(uint64_t);\n+  } else {\n+    ARROW_DCHECK(non_null_buf_ != NULLPTR && var_len_buf_ != NULLPTR);\n+\n+    RETURN_NOT_OK(non_null_buf_->Resize(bit_util::BytesForBits(num_rows_allocated_new) +\n+                                        kNumPaddingBytes));\n+\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes));\n+      } else {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes));\n+      }\n+    } else {\n+      RETURN_NOT_OK(fixed_len_buf_->Resize(\n+          (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes));\n+    }\n+  }\n+\n+  num_rows_allocated_ = num_rows_allocated_new;\n+  num_rows_ = num_rows_new;\n+\n+  return Status::OK();\n+}\n+\n+Status ResizableArrayData::ResizeVaryingLengthBuffer() {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (!column_metadata.is_fixed_length) {\n+    int min_new_size = static_cast<int>(\n+        reinterpret_cast<const uint32_t*>(fixed_len_buf_->data())[num_rows_]);\n+    ARROW_DCHECK(var_len_buf_size_ > 0);\n+    if (var_len_buf_size_ < min_new_size) {\n+      int new_size = var_len_buf_size_;\n+      while (new_size < min_new_size) {\n+        new_size *= 2;\n+      }\n+      RETURN_NOT_OK(var_len_buf_->Resize(new_size + kNumPaddingBytes));\n+      var_len_buf_size_ = new_size;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+KeyEncoder::KeyColumnArray ResizableArrayData::column_array() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+  return KeyEncoder::KeyColumnArray(\n+      column_metadata, num_rows_, non_null_buf_->mutable_data(),\n+      fixed_len_buf_->mutable_data(), var_len_buf_->mutable_data());\n+}\n+\n+std::shared_ptr<ArrayData> ResizableArrayData::array_data() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  auto valid_count = arrow::internal::CountSetBits(non_null_buf_->data(), /*offset=*/0,\n+                                                   static_cast<int64_t>(num_rows_));\n+  int null_count = static_cast<int>(num_rows_) - static_cast<int>(valid_count);\n+\n+  if (column_metadata.is_fixed_length) {\n+    return ArrayData::Make(data_type_, num_rows_, {non_null_buf_, fixed_len_buf_},\n+                           null_count);\n+  } else {\n+    return ArrayData::Make(data_type_, num_rows_,\n+                           {non_null_buf_, fixed_len_buf_, var_len_buf_}, null_count);\n+  }\n+}\n+\n+int ExecBatchBuilder::NumRowsToSkip(const std::shared_ptr<ArrayData>& column,\n+                                    int num_rows, const uint16_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+#ifndef NDEBUG\n+  // Ids must be in non-decreasing order\n+  //\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(row_ids[i] >= row_ids[i - 1]);\n+  }\n+#endif\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(column->type);\n+\n+  int num_rows_left = num_rows;\n+  int num_bytes_skipped = 0;\n+  while (num_rows_left > 0 && num_bytes_skipped < num_tail_bytes_to_skip) {\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        num_rows_left = std::max(num_rows_left, 8) - 8;\n+        ++num_bytes_skipped;\n+      } else {\n+        --num_rows_left;\n+        num_bytes_skipped += column_metadata.fixed_length;\n+      }\n+    } else {\n+      --num_rows_left;\n+      int row_id_removed = row_ids[num_rows_left];\n+      const uint32_t* offsets =\n+          reinterpret_cast<const uint32_t*>(column->buffers[1]->data());\n+      num_bytes_skipped += offsets[row_id_removed + 1] - offsets[row_id_removed];\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <bool OUTPUT_BYTE_ALIGNED>\n+void ExecBatchBuilder::CollectBitsImp(const uint8_t* input_bits,\n+                                      int64_t input_bits_offset, uint8_t* output_bits,\n+                                      int64_t output_bits_offset, int num_rows,\n+                                      const uint16_t* row_ids) {\n+  if (!OUTPUT_BYTE_ALIGNED) {\n+    ARROW_DCHECK(output_bits_offset % 8 > 0);\n+    output_bits[output_bits_offset / 8] &=\n+        static_cast<uint8_t>((1 << (output_bits_offset % 8)) - 1);\n+  } else {\n+    ARROW_DCHECK(output_bits_offset % 8 == 0);\n+  }\n+  constexpr int unroll = 8;\n+  for (int i = 0; i < num_rows / unroll; ++i) {\n+    const uint16_t* row_ids_base = row_ids + unroll * i;\n+    uint8_t result;\n+    result = bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[0]) ? 1 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[1]) ? 2 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[2]) ? 4 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[3]) ? 8 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[4]) ? 16 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[5]) ? 32 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[6]) ? 64 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[7]) ? 128 : 0;\n+    if (OUTPUT_BYTE_ALIGNED) {\n+      output_bits[output_bits_offset / 8 + i] = result;\n+    } else {\n+      output_bits[output_bits_offset / 8 + i] |=\n+          static_cast<uint8_t>(result << (output_bits_offset % 8));\n+      output_bits[output_bits_offset / 8 + i + 1] =\n+          static_cast<uint8_t>(result >> (8 - (output_bits_offset % 8)));\n+    }\n+  }\n+  if (num_rows % unroll > 0) {\n+    for (int i = num_rows - (num_rows % unroll); i < num_rows; ++i) {\n+      bit_util::SetBitTo(output_bits, output_bits_offset + i,\n+                         bit_util::GetBit(input_bits, input_bits_offset + row_ids[i]));\n+    }\n+  }\n+}\n+\n+void ExecBatchBuilder::CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                                   uint8_t* output_bits, int64_t output_bits_offset,\n+                                   int num_rows, const uint16_t* row_ids) {\n+  if (output_bits_offset % 8 > 0) {\n+    CollectBitsImp<false>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                          num_rows, row_ids);\n+  } else {\n+    CollectBitsImp<true>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                         num_rows, row_ids);\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void ExecBatchBuilder::Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                             const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  KeyEncoder::KeyColumnMetadata metadata = ColumnMetadataFromDataType(column->type);\n+\n+  if (!metadata.is_fixed_length) {\n+    const uint8_t* ptr_base = column->buffers[2]->data();\n+    const uint32_t* offsets =\n+        reinterpret_cast<const uint32_t*>(column->buffers[1]->data()) + column->offset;\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr = ptr_base + offsets[row_id];\n+      uint32_t field_length = offsets[row_id + 1] - offsets[row_id];\n+      process_value_fn(i, field_ptr, field_length);\n+    }\n+  } else {\n+    ARROW_DCHECK(metadata.fixed_length > 0);\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr =\n+          column->buffers[1]->data() +\n+          (column->offset + row_id) * static_cast<int64_t>(metadata.fixed_length);\n+      process_value_fn(i, field_ptr, metadata.fixed_length);\n+    }\n+  }\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                                        ResizableArrayData& target,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  ARROW_DCHECK(num_rows_before >= 0);\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(source->type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(source->type);\n+\n+  if (column_metadata.is_fixed_length) {\n+    // Fixed length column\n+    //\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        CollectBits(source->buffers[1]->data(), source->offset, target.mutable_data(1),\n+                    num_rows_before, num_rows_to_append, row_ids);\n+        break;\n+      case 1:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                target.mutable_data(1)[num_rows_before + i] = *ptr;\n+              });\n+        break;\n+      case 2:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint16_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint16_t*>(ptr);\n+              });\n+        break;\n+      case 4:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint32_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint32_t*>(ptr);\n+              });\n+        break;\n+      case 8:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint64_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint64_t*>(ptr);\n+              });\n+        break;\n+      default: {\n+        int num_rows_to_process =\n+            num_rows_to_append -\n+            NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+        Visit(source, num_rows_to_process, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                    target.mutable_data(1) +\n+                    static_cast<int64_t>(num_bytes) * (num_rows_before + i));\n+                const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                for (uint32_t word_id = 0;\n+                     word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t));\n+                     ++word_id) {\n+                  util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+                }\n+              });\n+        if (num_rows_to_append > num_rows_to_process) {\n+          Visit(source, num_rows_to_append - num_rows_to_process,\n+                row_ids + num_rows_to_process,\n+                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                  uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                      target.mutable_data(1) +\n+                      static_cast<int64_t>(num_bytes) *\n+                          (num_rows_before + num_rows_to_process + i));\n+                  const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                  memcpy(dst, src, num_bytes);\n+                });\n+        }\n+      }\n+    }\n+  } else {\n+    // Varying length column\n+    //\n+\n+    // Step 1: calculate target offsets\n+    //\n+    uint32_t* offsets = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[num_rows_before];\n+    Visit(source, num_rows_to_append, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            offsets[num_rows_before + i] = num_bytes;\n+          });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[num_rows_before + i];\n+      offsets[num_rows_before + i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_before + num_rows_to_append] = sum;\n+\n+    // Step 2: resize output buffers\n+    //\n+    RETURN_NOT_OK(target.ResizeVaryingLengthBuffer());\n+\n+    // Step 3: copy varying-length data\n+    //\n+    int num_rows_to_process =\n+        num_rows_to_append -\n+        NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+    Visit(source, num_rows_to_process, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(target.mutable_data(2) +\n+                                                        offsets[num_rows_before + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            for (uint32_t word_id = 0;\n+                 word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+              util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+            }\n+          });\n+    Visit(source, num_rows_to_append - num_rows_to_process, row_ids + num_rows_to_process,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                target.mutable_data(2) +\n+                offsets[num_rows_before + num_rows_to_process + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            memcpy(dst, src, num_bytes);\n+          });\n+  }\n+\n+  // Process nulls\n+  //\n+  if (source->buffers[0] == NULLPTR) {\n+    uint8_t* dst = target.mutable_data(0);\n+    dst[num_rows_before / 8] |= static_cast<uint8_t>(~0ULL << (num_rows_before & 7));\n+    for (int i = num_rows_before / 8 + 1;\n+         i < bit_util::BytesForBits(num_rows_before + num_rows_to_append); ++i) {\n+      dst[i] = 0xff;\n+    }\n+  } else {\n+    CollectBits(source->buffers[0]->data(), source->offset, target.mutable_data(0),\n+                num_rows_before, num_rows_to_append, row_ids);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(const std::shared_ptr<DataType>& type,\n+                                     ResizableArrayData& target, int num_rows_to_append,\n+                                     MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(type);\n+\n+  // Process fixed length buffer\n+  //\n+  if (column_metadata.is_fixed_length) {\n+    uint8_t* dst = target.mutable_data(1);\n+    if (column_metadata.fixed_length == 0) {\n+      dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+      int64_t offset_begin = num_rows_before / 8 + 1;\n+      int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+      if (offset_end > offset_begin) {\n+        memset(dst + offset_begin, 0, offset_end - offset_begin);\n+      }\n+    } else {\n+      memset(dst + num_rows_before * static_cast<int64_t>(column_metadata.fixed_length),\n+             0, static_cast<int64_t>(column_metadata.fixed_length) * num_rows_to_append);\n+    }\n+  } else {\n+    uint32_t* dst = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : dst[num_rows_before];\n+    for (int64_t i = num_rows_before; i <= num_rows_after; ++i) {\n+      dst[i] = sum;\n+    }\n+  }\n+\n+  // Process nulls\n+  //\n+  uint8_t* dst = target.mutable_data(0);\n+  dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+  int64_t offset_begin = num_rows_before / 8 + 1;\n+  int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+  if (offset_end > offset_begin) {\n+    memset(dst + offset_begin, 0, offset_end - offset_begin);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int num_cols, const int* col_ids) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(num_cols);\n+    for (int i = 0; i < num_cols; ++i) {\n+      const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+      ARROW_DCHECK(data.is_array());\n+      const std::shared_ptr<ArrayData>& array_data = data.array();\n+      values_[i].Init(array_data->type, pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    RETURN_NOT_OK(\n+        AppendSelected(array_data, values_[i], num_rows_to_append, row_ids, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int* num_appended, int num_cols,\n+                                        const int* col_ids) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendSelected(pool, batch, num_rows_next, row_ids, num_cols, col_ids));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(types.size());\n+    for (size_t i = 0; i < types.size(); ++i) {\n+      values_[i].Init(types[i], pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    RETURN_NOT_OK(AppendNulls(types[i], values_[i], num_rows_to_append, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append, int* num_appended) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendNulls(pool, types, num_rows_next));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+ExecBatch ExecBatchBuilder::Flush() {\n+  ARROW_DCHECK(num_rows() > 0);\n+  ExecBatch out({}, num_rows());\n+  out.values.resize(values_.size());\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    out.values[i] = values_[i].array_data();\n+    values_[i].Clear(true);\n+  }\n+  return out;\n+}\n+\n+int RowArrayAccessor::VarbinaryColumnId(const KeyEncoder::KeyRowMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                    int num_rows, const uint32_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                             int num_rows, const uint32_t* row_ids,\n+                             PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                  int num_rows, const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool,\n+                              const KeyEncoder::KeyRowMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyEncoder::KeyColumnMetadata> column_metadatas;\n+  ColumnMetadatasFromExecBatch(batch, column_metadatas);\n+  KeyEncoder::KeyRowMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(\n+    MemoryPool* pool, const ExecBatch& batch, int begin_row_id, int end_row_id,\n+    int num_row_ids, const uint16_t* row_ids,\n+    std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+\n+  KeyEncoder::KeyEncoderContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = output->column_metadata();\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const KeyEncoder::KeyRowMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(KeyEncoder::KeyRowArray* target,\n+                                    const KeyEncoder::KeyRowArray& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(KeyEncoder::KeyRowArray* target,\n+                                      const KeyEncoder::KeyRowArray& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(KeyEncoder::KeyRowArray* target,\n+                              const KeyEncoder::KeyRowArray& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays,\n+    std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Hashing32::HashBatch(*input->batch, input->batch_start_row,\n+                       input->batch_end_row - input->batch_start_row, hashes,\n+                       *input->temp_column_arrays, hardware_flags, input->temp_stack);\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(\n+    const ExecBatch& batch, int start_row, int num_rows, uint8_t* out_has_match_bitvector,\n+    uint32_t* out_key_ids, util::TempVectorStack* temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(\n+    SwissTableForJoin* target, int dop, int64_t num_rows, bool reject_duplicate_keys,\n+    bool no_payload, const std::vector<KeyEncoder::KeyColumnMetadata>& key_types,\n+    const std::vector<KeyEncoder::KeyColumnMetadata>& payload_types, MemoryPool* pool,\n+    int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(num_prtns_);\n+\n+  KeyEncoder::KeyRowMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  KeyEncoder::KeyRowMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  Hashing32::HashBatch(key_batch, /*start_row=*/0, static_cast<int>(key_batch.length),\n+                       locals.batch_hashes.data(), locals.temp_column_arrays,\n+                       hardware_flags_, temp_stack);\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int i, int pos) { locals.batch_prtn_row_ids[pos] = i; });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n+  }\n+}\n+\n+void SwissTableForJoinBuild::FinishPrtnMerge(util::TempVectorStack* temp_stack) {\n+  // Process overflow key ids\n+  //\n+  for (int prtn_id = 0; prtn_id < num_prtns_; ++prtn_id) {\n+    SwissTableMerge::InsertNewGroups(target_->map_.swiss_table(),\n+                                     prtn_states_[prtn_id].overflow_key_ids,\n+                                     prtn_states_[prtn_id].overflow_hashes);\n+  }\n+\n+  // Calculate whether we have nulls in hash table keys\n+  // (it is lazily evaluated but since we will be accessing it from multiple\n+  // threads we need to make sure that the value gets calculated here).\n+  //\n+  KeyEncoder::KeyEncoderContext ctx;\n+  ctx.hardware_flags = hardware_flags_;\n+  ctx.stack = temp_stack;\n+  std::ignore = target_->map_.keys()->rows_.has_any_nulls(&ctx);\n+}\n+\n+void JoinResultMaterialize::Init(MemoryPool* pool,\n+                                 const HashJoinProjectionMaps* probe_schemas,\n+                                 const HashJoinProjectionMaps* build_schemas) {\n+  pool_ = pool;\n+  probe_schemas_ = probe_schemas;\n+  build_schemas_ = build_schemas;\n+  num_rows_ = 0;\n+  null_ranges_.clear();\n+  num_produced_batches_ = 0;\n+\n+  // Initialize mapping of columns from output batch column index to key and\n+  // payload batch column index.\n+  //\n+  probe_output_to_key_and_payload_.resize(\n+      probe_schemas_->num_cols(HashJoinProjection::OUTPUT));\n+  int num_key_cols = probe_schemas_->num_cols(HashJoinProjection::KEY);\n+  auto to_key = probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  auto to_payload =\n+      probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; static_cast<size_t>(i) < probe_output_to_key_and_payload_.size(); ++i) {\n+    probe_output_to_key_and_payload_[i] =\n+        to_key.get(i) == SchemaProjectionMap::kMissingField\n+            ? to_payload.get(i) + num_key_cols\n+            : to_key.get(i);\n+  }\n+}\n+\n+void JoinResultMaterialize::SetBuildSide(const RowArray* build_keys,\n+                                         const RowArray* build_payloads,\n+                                         bool payload_id_same_as_key_id) {\n+  build_keys_ = build_keys;\n+  build_payloads_ = build_payloads;\n+  payload_id_same_as_key_id_ = payload_id_same_as_key_id;\n+}\n+\n+bool JoinResultMaterialize::HasProbeOutput() const {\n+  return probe_schemas_->num_cols(HashJoinProjection::OUTPUT) > 0;\n+}\n+\n+bool JoinResultMaterialize::HasBuildKeyOutput() const {\n+  auto to_key = build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_key.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::HasBuildPayloadOutput() const {\n+  auto to_payload =\n+      build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_payload.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::NeedsKeyId() const {\n+  return HasBuildKeyOutput() || (HasBuildPayloadOutput() && payload_id_same_as_key_id_);\n+}\n+\n+bool JoinResultMaterialize::NeedsPayloadId() const {\n+  return HasBuildPayloadOutput() && !payload_id_same_as_key_id_;\n+}\n+\n+Status JoinResultMaterialize::AppendProbeOnly(const ExecBatch& key_and_payload,\n+                                              int num_rows_to_append,\n+                                              const uint16_t* row_ids,\n+                                              int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendSelected(\n+        pool_, key_and_payload, num_rows_to_append, row_ids,\n+        static_cast<int>(probe_output_to_key_and_payload_.size()),\n+        probe_output_to_key_and_payload_.data()));\n+  }\n+  if (!null_ranges_.empty() &&\n+      null_ranges_.back().first + null_ranges_.back().second == num_rows_) {\n+    // We can extend the last range of null rows on build side.\n+    //\n+    null_ranges_.back().second += num_rows_to_append;\n+  } else {\n+    null_ranges_.push_back(\n+        std::make_pair(static_cast<int>(num_rows_), num_rows_to_append));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Status JoinResultMaterialize::AppendBuildOnly(int num_rows_to_append,\n+                                              const uint32_t* key_ids,\n+                                              const uint32_t* payload_ids,\n+                                              int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendNulls(\n+        pool_, probe_schemas_->data_types(HashJoinProjection::OUTPUT),\n+        num_rows_to_append));\n+  }\n+  if (NeedsKeyId()) {\n+    ARROW_DCHECK(key_ids != nullptr);\n+    key_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(key_ids_.data() + num_rows_, key_ids, num_rows_to_append * sizeof(uint32_t));\n+  }\n+  if (NeedsPayloadId()) {\n+    ARROW_DCHECK(payload_ids != nullptr);\n+    payload_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(payload_ids_.data() + num_rows_, payload_ids,\n+           num_rows_to_append * sizeof(uint32_t));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Status JoinResultMaterialize::Append(const ExecBatch& key_and_payload,\n+                                     int num_rows_to_append, const uint16_t* row_ids,\n+                                     const uint32_t* key_ids, const uint32_t* payload_ids,\n+                                     int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendSelected(\n+        pool_, key_and_payload, num_rows_to_append, row_ids,\n+        static_cast<int>(probe_output_to_key_and_payload_.size()),\n+        probe_output_to_key_and_payload_.data()));\n+  }\n+  if (NeedsKeyId()) {\n+    ARROW_DCHECK(key_ids != nullptr);\n+    key_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(key_ids_.data() + num_rows_, key_ids, num_rows_to_append * sizeof(uint32_t));\n+  }\n+  if (NeedsPayloadId()) {\n+    ARROW_DCHECK(payload_ids != nullptr);\n+    payload_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(payload_ids_.data() + num_rows_, payload_ids,\n+           num_rows_to_append * sizeof(uint32_t));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Result<std::shared_ptr<ArrayData>> JoinResultMaterialize::FlushBuildColumn(\n+    const std::shared_ptr<DataType>& data_type, const RowArray* row_array, int column_id,\n+    uint32_t* row_ids) {\n+  ResizableArrayData output;\n+  output.Init(data_type, pool_, bit_util::Log2(num_rows_));\n+\n+  for (size_t i = 0; i <= null_ranges_.size(); ++i) {\n+    int row_id_begin =\n+        i == 0 ? 0 : null_ranges_[i - 1].first + null_ranges_[i - 1].second;\n+    int row_id_end = i == null_ranges_.size() ? num_rows_ : null_ranges_[i].first;\n+    if (row_id_end > row_id_begin) {\n+      RETURN_NOT_OK(row_array->DecodeSelected(\n+          &output, column_id, row_id_end - row_id_begin, row_ids + row_id_begin, pool_));\n+    }\n+    int num_nulls = i == null_ranges_.size() ? 0 : null_ranges_[i].second;\n+    if (num_nulls > 0) {\n+      RETURN_NOT_OK(ExecBatchBuilder::AppendNulls(data_type, output, num_nulls, pool_));\n+    }\n+  }\n+\n+  return output.array_data();\n+}\n+\n+Status JoinResultMaterialize::Flush(ExecBatch* out) {\n\nReview comment:\n       Why not `Result<ExecBatch>`?\n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.cc\n##########\n@@ -0,0 +1,3279 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_compare.h\"\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+void ResizableArrayData::Init(const std::shared_ptr<DataType>& data_type,\n+                              MemoryPool* pool, int log_num_rows_min) {\n+#ifndef NDEBUG\n+  if (num_rows_allocated_ > 0) {\n+    ARROW_DCHECK(data_type_ != NULLPTR);\n+    KeyEncoder::KeyColumnMetadata metadata_before =\n+        ColumnMetadataFromDataType(data_type_);\n+    KeyEncoder::KeyColumnMetadata metadata_after = ColumnMetadataFromDataType(data_type);\n+    ARROW_DCHECK(metadata_before.is_fixed_length == metadata_after.is_fixed_length &&\n+                 metadata_before.fixed_length == metadata_after.fixed_length);\n+  }\n+#endif\n+  Clear(/*release_buffers=*/false);\n+  log_num_rows_min_ = log_num_rows_min;\n+  data_type_ = data_type;\n+  pool_ = pool;\n+}\n+\n+void ResizableArrayData::Clear(bool release_buffers) {\n+  num_rows_ = 0;\n+  if (release_buffers) {\n+    non_null_buf_.reset();\n+    fixed_len_buf_.reset();\n+    var_len_buf_.reset();\n+    num_rows_allocated_ = 0;\n+    var_len_buf_size_ = 0;\n+  }\n+}\n+\n+Status ResizableArrayData::ResizeFixedLengthBuffers(int num_rows_new) {\n+  ARROW_DCHECK(num_rows_new >= 0);\n+  if (num_rows_new <= num_rows_allocated_) {\n+    num_rows_ = num_rows_new;\n+    return Status::OK();\n+  }\n+\n+  int num_rows_allocated_new = 1 << log_num_rows_min_;\n+  while (num_rows_allocated_new < num_rows_new) {\n+    num_rows_allocated_new *= 2;\n+  }\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (fixed_len_buf_ == NULLPTR) {\n+    ARROW_DCHECK(non_null_buf_ == NULLPTR && var_len_buf_ == NULLPTR);\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        non_null_buf_,\n+        AllocateResizableBuffer(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes, pool_));\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes,\n+                pool_));\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes,\n+                pool_));\n+      }\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(\n+          fixed_len_buf_,\n+          AllocateResizableBuffer(\n+              (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes, pool_));\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(var_len_buf_, AllocateResizableBuffer(\n+                                            sizeof(uint64_t) + kNumPaddingBytes, pool_));\n+\n+    var_len_buf_size_ = sizeof(uint64_t);\n+  } else {\n+    ARROW_DCHECK(non_null_buf_ != NULLPTR && var_len_buf_ != NULLPTR);\n+\n+    RETURN_NOT_OK(non_null_buf_->Resize(bit_util::BytesForBits(num_rows_allocated_new) +\n+                                        kNumPaddingBytes));\n+\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes));\n+      } else {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes));\n+      }\n+    } else {\n+      RETURN_NOT_OK(fixed_len_buf_->Resize(\n+          (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes));\n+    }\n+  }\n+\n+  num_rows_allocated_ = num_rows_allocated_new;\n+  num_rows_ = num_rows_new;\n+\n+  return Status::OK();\n+}\n+\n+Status ResizableArrayData::ResizeVaryingLengthBuffer() {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (!column_metadata.is_fixed_length) {\n+    int min_new_size = static_cast<int>(\n+        reinterpret_cast<const uint32_t*>(fixed_len_buf_->data())[num_rows_]);\n+    ARROW_DCHECK(var_len_buf_size_ > 0);\n+    if (var_len_buf_size_ < min_new_size) {\n+      int new_size = var_len_buf_size_;\n+      while (new_size < min_new_size) {\n+        new_size *= 2;\n+      }\n+      RETURN_NOT_OK(var_len_buf_->Resize(new_size + kNumPaddingBytes));\n+      var_len_buf_size_ = new_size;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+KeyEncoder::KeyColumnArray ResizableArrayData::column_array() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+  return KeyEncoder::KeyColumnArray(\n+      column_metadata, num_rows_, non_null_buf_->mutable_data(),\n+      fixed_len_buf_->mutable_data(), var_len_buf_->mutable_data());\n+}\n+\n+std::shared_ptr<ArrayData> ResizableArrayData::array_data() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  auto valid_count = arrow::internal::CountSetBits(non_null_buf_->data(), /*offset=*/0,\n+                                                   static_cast<int64_t>(num_rows_));\n+  int null_count = static_cast<int>(num_rows_) - static_cast<int>(valid_count);\n+\n+  if (column_metadata.is_fixed_length) {\n+    return ArrayData::Make(data_type_, num_rows_, {non_null_buf_, fixed_len_buf_},\n+                           null_count);\n+  } else {\n+    return ArrayData::Make(data_type_, num_rows_,\n+                           {non_null_buf_, fixed_len_buf_, var_len_buf_}, null_count);\n+  }\n+}\n+\n+int ExecBatchBuilder::NumRowsToSkip(const std::shared_ptr<ArrayData>& column,\n+                                    int num_rows, const uint16_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+#ifndef NDEBUG\n+  // Ids must be in non-decreasing order\n+  //\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(row_ids[i] >= row_ids[i - 1]);\n+  }\n+#endif\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(column->type);\n+\n+  int num_rows_left = num_rows;\n+  int num_bytes_skipped = 0;\n+  while (num_rows_left > 0 && num_bytes_skipped < num_tail_bytes_to_skip) {\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        num_rows_left = std::max(num_rows_left, 8) - 8;\n+        ++num_bytes_skipped;\n+      } else {\n+        --num_rows_left;\n+        num_bytes_skipped += column_metadata.fixed_length;\n+      }\n+    } else {\n+      --num_rows_left;\n+      int row_id_removed = row_ids[num_rows_left];\n+      const uint32_t* offsets =\n+          reinterpret_cast<const uint32_t*>(column->buffers[1]->data());\n+      num_bytes_skipped += offsets[row_id_removed + 1] - offsets[row_id_removed];\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <bool OUTPUT_BYTE_ALIGNED>\n+void ExecBatchBuilder::CollectBitsImp(const uint8_t* input_bits,\n+                                      int64_t input_bits_offset, uint8_t* output_bits,\n+                                      int64_t output_bits_offset, int num_rows,\n+                                      const uint16_t* row_ids) {\n+  if (!OUTPUT_BYTE_ALIGNED) {\n+    ARROW_DCHECK(output_bits_offset % 8 > 0);\n+    output_bits[output_bits_offset / 8] &=\n+        static_cast<uint8_t>((1 << (output_bits_offset % 8)) - 1);\n+  } else {\n+    ARROW_DCHECK(output_bits_offset % 8 == 0);\n+  }\n+  constexpr int unroll = 8;\n+  for (int i = 0; i < num_rows / unroll; ++i) {\n+    const uint16_t* row_ids_base = row_ids + unroll * i;\n+    uint8_t result;\n+    result = bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[0]) ? 1 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[1]) ? 2 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[2]) ? 4 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[3]) ? 8 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[4]) ? 16 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[5]) ? 32 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[6]) ? 64 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[7]) ? 128 : 0;\n+    if (OUTPUT_BYTE_ALIGNED) {\n+      output_bits[output_bits_offset / 8 + i] = result;\n+    } else {\n+      output_bits[output_bits_offset / 8 + i] |=\n+          static_cast<uint8_t>(result << (output_bits_offset % 8));\n+      output_bits[output_bits_offset / 8 + i + 1] =\n+          static_cast<uint8_t>(result >> (8 - (output_bits_offset % 8)));\n+    }\n+  }\n+  if (num_rows % unroll > 0) {\n+    for (int i = num_rows - (num_rows % unroll); i < num_rows; ++i) {\n+      bit_util::SetBitTo(output_bits, output_bits_offset + i,\n+                         bit_util::GetBit(input_bits, input_bits_offset + row_ids[i]));\n+    }\n+  }\n+}\n+\n+void ExecBatchBuilder::CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                                   uint8_t* output_bits, int64_t output_bits_offset,\n+                                   int num_rows, const uint16_t* row_ids) {\n+  if (output_bits_offset % 8 > 0) {\n+    CollectBitsImp<false>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                          num_rows, row_ids);\n+  } else {\n+    CollectBitsImp<true>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                         num_rows, row_ids);\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void ExecBatchBuilder::Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                             const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  KeyEncoder::KeyColumnMetadata metadata = ColumnMetadataFromDataType(column->type);\n+\n+  if (!metadata.is_fixed_length) {\n+    const uint8_t* ptr_base = column->buffers[2]->data();\n+    const uint32_t* offsets =\n+        reinterpret_cast<const uint32_t*>(column->buffers[1]->data()) + column->offset;\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr = ptr_base + offsets[row_id];\n+      uint32_t field_length = offsets[row_id + 1] - offsets[row_id];\n+      process_value_fn(i, field_ptr, field_length);\n+    }\n+  } else {\n+    ARROW_DCHECK(metadata.fixed_length > 0);\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr =\n+          column->buffers[1]->data() +\n+          (column->offset + row_id) * static_cast<int64_t>(metadata.fixed_length);\n+      process_value_fn(i, field_ptr, metadata.fixed_length);\n+    }\n+  }\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                                        ResizableArrayData& target,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  ARROW_DCHECK(num_rows_before >= 0);\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(source->type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(source->type);\n+\n+  if (column_metadata.is_fixed_length) {\n+    // Fixed length column\n+    //\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        CollectBits(source->buffers[1]->data(), source->offset, target.mutable_data(1),\n+                    num_rows_before, num_rows_to_append, row_ids);\n+        break;\n+      case 1:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                target.mutable_data(1)[num_rows_before + i] = *ptr;\n+              });\n+        break;\n+      case 2:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint16_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint16_t*>(ptr);\n+              });\n+        break;\n+      case 4:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint32_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint32_t*>(ptr);\n+              });\n+        break;\n+      case 8:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint64_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint64_t*>(ptr);\n+              });\n+        break;\n+      default: {\n+        int num_rows_to_process =\n+            num_rows_to_append -\n+            NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+        Visit(source, num_rows_to_process, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                    target.mutable_data(1) +\n+                    static_cast<int64_t>(num_bytes) * (num_rows_before + i));\n+                const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                for (uint32_t word_id = 0;\n+                     word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t));\n+                     ++word_id) {\n+                  util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+                }\n+              });\n+        if (num_rows_to_append > num_rows_to_process) {\n+          Visit(source, num_rows_to_append - num_rows_to_process,\n+                row_ids + num_rows_to_process,\n+                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                  uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                      target.mutable_data(1) +\n+                      static_cast<int64_t>(num_bytes) *\n+                          (num_rows_before + num_rows_to_process + i));\n+                  const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                  memcpy(dst, src, num_bytes);\n+                });\n+        }\n+      }\n+    }\n+  } else {\n+    // Varying length column\n+    //\n+\n+    // Step 1: calculate target offsets\n+    //\n+    uint32_t* offsets = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[num_rows_before];\n+    Visit(source, num_rows_to_append, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            offsets[num_rows_before + i] = num_bytes;\n+          });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[num_rows_before + i];\n+      offsets[num_rows_before + i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_before + num_rows_to_append] = sum;\n+\n+    // Step 2: resize output buffers\n+    //\n+    RETURN_NOT_OK(target.ResizeVaryingLengthBuffer());\n+\n+    // Step 3: copy varying-length data\n+    //\n+    int num_rows_to_process =\n+        num_rows_to_append -\n+        NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+    Visit(source, num_rows_to_process, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(target.mutable_data(2) +\n+                                                        offsets[num_rows_before + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            for (uint32_t word_id = 0;\n+                 word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+              util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+            }\n+          });\n+    Visit(source, num_rows_to_append - num_rows_to_process, row_ids + num_rows_to_process,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                target.mutable_data(2) +\n+                offsets[num_rows_before + num_rows_to_process + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            memcpy(dst, src, num_bytes);\n+          });\n+  }\n+\n+  // Process nulls\n+  //\n+  if (source->buffers[0] == NULLPTR) {\n+    uint8_t* dst = target.mutable_data(0);\n+    dst[num_rows_before / 8] |= static_cast<uint8_t>(~0ULL << (num_rows_before & 7));\n+    for (int i = num_rows_before / 8 + 1;\n+         i < bit_util::BytesForBits(num_rows_before + num_rows_to_append); ++i) {\n+      dst[i] = 0xff;\n+    }\n+  } else {\n+    CollectBits(source->buffers[0]->data(), source->offset, target.mutable_data(0),\n+                num_rows_before, num_rows_to_append, row_ids);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(const std::shared_ptr<DataType>& type,\n+                                     ResizableArrayData& target, int num_rows_to_append,\n+                                     MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(type);\n+\n+  // Process fixed length buffer\n+  //\n+  if (column_metadata.is_fixed_length) {\n+    uint8_t* dst = target.mutable_data(1);\n+    if (column_metadata.fixed_length == 0) {\n+      dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+      int64_t offset_begin = num_rows_before / 8 + 1;\n+      int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+      if (offset_end > offset_begin) {\n+        memset(dst + offset_begin, 0, offset_end - offset_begin);\n+      }\n+    } else {\n+      memset(dst + num_rows_before * static_cast<int64_t>(column_metadata.fixed_length),\n+             0, static_cast<int64_t>(column_metadata.fixed_length) * num_rows_to_append);\n+    }\n+  } else {\n+    uint32_t* dst = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : dst[num_rows_before];\n+    for (int64_t i = num_rows_before; i <= num_rows_after; ++i) {\n+      dst[i] = sum;\n+    }\n+  }\n+\n+  // Process nulls\n+  //\n+  uint8_t* dst = target.mutable_data(0);\n+  dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+  int64_t offset_begin = num_rows_before / 8 + 1;\n+  int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+  if (offset_end > offset_begin) {\n+    memset(dst + offset_begin, 0, offset_end - offset_begin);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int num_cols, const int* col_ids) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(num_cols);\n+    for (int i = 0; i < num_cols; ++i) {\n+      const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+      ARROW_DCHECK(data.is_array());\n+      const std::shared_ptr<ArrayData>& array_data = data.array();\n+      values_[i].Init(array_data->type, pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    RETURN_NOT_OK(\n+        AppendSelected(array_data, values_[i], num_rows_to_append, row_ids, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int* num_appended, int num_cols,\n+                                        const int* col_ids) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendSelected(pool, batch, num_rows_next, row_ids, num_cols, col_ids));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(types.size());\n+    for (size_t i = 0; i < types.size(); ++i) {\n+      values_[i].Init(types[i], pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    RETURN_NOT_OK(AppendNulls(types[i], values_[i], num_rows_to_append, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append, int* num_appended) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendNulls(pool, types, num_rows_next));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+ExecBatch ExecBatchBuilder::Flush() {\n+  ARROW_DCHECK(num_rows() > 0);\n+  ExecBatch out({}, num_rows());\n+  out.values.resize(values_.size());\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    out.values[i] = values_[i].array_data();\n+    values_[i].Clear(true);\n+  }\n+  return out;\n+}\n+\n+int RowArrayAccessor::VarbinaryColumnId(const KeyEncoder::KeyRowMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                    int num_rows, const uint32_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                             int num_rows, const uint32_t* row_ids,\n+                             PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                  int num_rows, const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool,\n+                              const KeyEncoder::KeyRowMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyEncoder::KeyColumnMetadata> column_metadatas;\n+  ColumnMetadatasFromExecBatch(batch, column_metadatas);\n+  KeyEncoder::KeyRowMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(\n+    MemoryPool* pool, const ExecBatch& batch, int begin_row_id, int end_row_id,\n+    int num_row_ids, const uint16_t* row_ids,\n+    std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+\n+  KeyEncoder::KeyEncoderContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = output->column_metadata();\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const KeyEncoder::KeyRowMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(KeyEncoder::KeyRowArray* target,\n+                                    const KeyEncoder::KeyRowArray& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(KeyEncoder::KeyRowArray* target,\n+                                      const KeyEncoder::KeyRowArray& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(KeyEncoder::KeyRowArray* target,\n+                              const KeyEncoder::KeyRowArray& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays,\n+    std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Hashing32::HashBatch(*input->batch, input->batch_start_row,\n+                       input->batch_end_row - input->batch_start_row, hashes,\n+                       *input->temp_column_arrays, hardware_flags, input->temp_stack);\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(\n+    const ExecBatch& batch, int start_row, int num_rows, uint8_t* out_has_match_bitvector,\n+    uint32_t* out_key_ids, util::TempVectorStack* temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(\n+    SwissTableForJoin* target, int dop, int64_t num_rows, bool reject_duplicate_keys,\n+    bool no_payload, const std::vector<KeyEncoder::KeyColumnMetadata>& key_types,\n+    const std::vector<KeyEncoder::KeyColumnMetadata>& payload_types, MemoryPool* pool,\n+    int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(num_prtns_);\n+\n+  KeyEncoder::KeyRowMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  KeyEncoder::KeyRowMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  Hashing32::HashBatch(key_batch, /*start_row=*/0, static_cast<int>(key_batch.length),\n+                       locals.batch_hashes.data(), locals.temp_column_arrays,\n+                       hardware_flags_, temp_stack);\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int i, int pos) { locals.batch_prtn_row_ids[pos] = i; });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n\nReview comment:\n       This can be removed I think\n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.cc\n##########\n@@ -0,0 +1,3279 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_compare.h\"\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+void ResizableArrayData::Init(const std::shared_ptr<DataType>& data_type,\n+                              MemoryPool* pool, int log_num_rows_min) {\n+#ifndef NDEBUG\n+  if (num_rows_allocated_ > 0) {\n+    ARROW_DCHECK(data_type_ != NULLPTR);\n+    KeyEncoder::KeyColumnMetadata metadata_before =\n+        ColumnMetadataFromDataType(data_type_);\n+    KeyEncoder::KeyColumnMetadata metadata_after = ColumnMetadataFromDataType(data_type);\n+    ARROW_DCHECK(metadata_before.is_fixed_length == metadata_after.is_fixed_length &&\n+                 metadata_before.fixed_length == metadata_after.fixed_length);\n+  }\n+#endif\n+  Clear(/*release_buffers=*/false);\n+  log_num_rows_min_ = log_num_rows_min;\n+  data_type_ = data_type;\n+  pool_ = pool;\n+}\n+\n+void ResizableArrayData::Clear(bool release_buffers) {\n+  num_rows_ = 0;\n+  if (release_buffers) {\n+    non_null_buf_.reset();\n+    fixed_len_buf_.reset();\n+    var_len_buf_.reset();\n+    num_rows_allocated_ = 0;\n+    var_len_buf_size_ = 0;\n+  }\n+}\n+\n+Status ResizableArrayData::ResizeFixedLengthBuffers(int num_rows_new) {\n+  ARROW_DCHECK(num_rows_new >= 0);\n+  if (num_rows_new <= num_rows_allocated_) {\n+    num_rows_ = num_rows_new;\n+    return Status::OK();\n+  }\n+\n+  int num_rows_allocated_new = 1 << log_num_rows_min_;\n+  while (num_rows_allocated_new < num_rows_new) {\n+    num_rows_allocated_new *= 2;\n+  }\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (fixed_len_buf_ == NULLPTR) {\n+    ARROW_DCHECK(non_null_buf_ == NULLPTR && var_len_buf_ == NULLPTR);\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        non_null_buf_,\n+        AllocateResizableBuffer(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes, pool_));\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes,\n+                pool_));\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes,\n+                pool_));\n+      }\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(\n+          fixed_len_buf_,\n+          AllocateResizableBuffer(\n+              (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes, pool_));\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(var_len_buf_, AllocateResizableBuffer(\n+                                            sizeof(uint64_t) + kNumPaddingBytes, pool_));\n+\n+    var_len_buf_size_ = sizeof(uint64_t);\n+  } else {\n+    ARROW_DCHECK(non_null_buf_ != NULLPTR && var_len_buf_ != NULLPTR);\n+\n+    RETURN_NOT_OK(non_null_buf_->Resize(bit_util::BytesForBits(num_rows_allocated_new) +\n+                                        kNumPaddingBytes));\n+\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes));\n+      } else {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes));\n+      }\n+    } else {\n+      RETURN_NOT_OK(fixed_len_buf_->Resize(\n+          (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes));\n+    }\n+  }\n+\n+  num_rows_allocated_ = num_rows_allocated_new;\n+  num_rows_ = num_rows_new;\n+\n+  return Status::OK();\n+}\n+\n+Status ResizableArrayData::ResizeVaryingLengthBuffer() {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (!column_metadata.is_fixed_length) {\n+    int min_new_size = static_cast<int>(\n+        reinterpret_cast<const uint32_t*>(fixed_len_buf_->data())[num_rows_]);\n+    ARROW_DCHECK(var_len_buf_size_ > 0);\n+    if (var_len_buf_size_ < min_new_size) {\n+      int new_size = var_len_buf_size_;\n+      while (new_size < min_new_size) {\n+        new_size *= 2;\n+      }\n+      RETURN_NOT_OK(var_len_buf_->Resize(new_size + kNumPaddingBytes));\n+      var_len_buf_size_ = new_size;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+KeyEncoder::KeyColumnArray ResizableArrayData::column_array() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+  return KeyEncoder::KeyColumnArray(\n+      column_metadata, num_rows_, non_null_buf_->mutable_data(),\n+      fixed_len_buf_->mutable_data(), var_len_buf_->mutable_data());\n+}\n+\n+std::shared_ptr<ArrayData> ResizableArrayData::array_data() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  auto valid_count = arrow::internal::CountSetBits(non_null_buf_->data(), /*offset=*/0,\n+                                                   static_cast<int64_t>(num_rows_));\n+  int null_count = static_cast<int>(num_rows_) - static_cast<int>(valid_count);\n+\n+  if (column_metadata.is_fixed_length) {\n+    return ArrayData::Make(data_type_, num_rows_, {non_null_buf_, fixed_len_buf_},\n+                           null_count);\n+  } else {\n+    return ArrayData::Make(data_type_, num_rows_,\n+                           {non_null_buf_, fixed_len_buf_, var_len_buf_}, null_count);\n+  }\n+}\n+\n+int ExecBatchBuilder::NumRowsToSkip(const std::shared_ptr<ArrayData>& column,\n+                                    int num_rows, const uint16_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+#ifndef NDEBUG\n+  // Ids must be in non-decreasing order\n+  //\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(row_ids[i] >= row_ids[i - 1]);\n+  }\n+#endif\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(column->type);\n+\n+  int num_rows_left = num_rows;\n+  int num_bytes_skipped = 0;\n+  while (num_rows_left > 0 && num_bytes_skipped < num_tail_bytes_to_skip) {\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        num_rows_left = std::max(num_rows_left, 8) - 8;\n+        ++num_bytes_skipped;\n+      } else {\n+        --num_rows_left;\n+        num_bytes_skipped += column_metadata.fixed_length;\n+      }\n+    } else {\n+      --num_rows_left;\n+      int row_id_removed = row_ids[num_rows_left];\n+      const uint32_t* offsets =\n+          reinterpret_cast<const uint32_t*>(column->buffers[1]->data());\n+      num_bytes_skipped += offsets[row_id_removed + 1] - offsets[row_id_removed];\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <bool OUTPUT_BYTE_ALIGNED>\n+void ExecBatchBuilder::CollectBitsImp(const uint8_t* input_bits,\n+                                      int64_t input_bits_offset, uint8_t* output_bits,\n+                                      int64_t output_bits_offset, int num_rows,\n+                                      const uint16_t* row_ids) {\n+  if (!OUTPUT_BYTE_ALIGNED) {\n+    ARROW_DCHECK(output_bits_offset % 8 > 0);\n+    output_bits[output_bits_offset / 8] &=\n+        static_cast<uint8_t>((1 << (output_bits_offset % 8)) - 1);\n+  } else {\n+    ARROW_DCHECK(output_bits_offset % 8 == 0);\n+  }\n+  constexpr int unroll = 8;\n+  for (int i = 0; i < num_rows / unroll; ++i) {\n+    const uint16_t* row_ids_base = row_ids + unroll * i;\n+    uint8_t result;\n+    result = bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[0]) ? 1 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[1]) ? 2 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[2]) ? 4 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[3]) ? 8 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[4]) ? 16 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[5]) ? 32 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[6]) ? 64 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[7]) ? 128 : 0;\n+    if (OUTPUT_BYTE_ALIGNED) {\n+      output_bits[output_bits_offset / 8 + i] = result;\n+    } else {\n+      output_bits[output_bits_offset / 8 + i] |=\n+          static_cast<uint8_t>(result << (output_bits_offset % 8));\n+      output_bits[output_bits_offset / 8 + i + 1] =\n+          static_cast<uint8_t>(result >> (8 - (output_bits_offset % 8)));\n+    }\n+  }\n+  if (num_rows % unroll > 0) {\n+    for (int i = num_rows - (num_rows % unroll); i < num_rows; ++i) {\n+      bit_util::SetBitTo(output_bits, output_bits_offset + i,\n+                         bit_util::GetBit(input_bits, input_bits_offset + row_ids[i]));\n+    }\n+  }\n+}\n+\n+void ExecBatchBuilder::CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                                   uint8_t* output_bits, int64_t output_bits_offset,\n+                                   int num_rows, const uint16_t* row_ids) {\n+  if (output_bits_offset % 8 > 0) {\n+    CollectBitsImp<false>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                          num_rows, row_ids);\n+  } else {\n+    CollectBitsImp<true>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                         num_rows, row_ids);\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void ExecBatchBuilder::Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                             const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  KeyEncoder::KeyColumnMetadata metadata = ColumnMetadataFromDataType(column->type);\n+\n+  if (!metadata.is_fixed_length) {\n+    const uint8_t* ptr_base = column->buffers[2]->data();\n+    const uint32_t* offsets =\n+        reinterpret_cast<const uint32_t*>(column->buffers[1]->data()) + column->offset;\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr = ptr_base + offsets[row_id];\n+      uint32_t field_length = offsets[row_id + 1] - offsets[row_id];\n+      process_value_fn(i, field_ptr, field_length);\n+    }\n+  } else {\n+    ARROW_DCHECK(metadata.fixed_length > 0);\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr =\n+          column->buffers[1]->data() +\n+          (column->offset + row_id) * static_cast<int64_t>(metadata.fixed_length);\n+      process_value_fn(i, field_ptr, metadata.fixed_length);\n+    }\n+  }\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                                        ResizableArrayData& target,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  ARROW_DCHECK(num_rows_before >= 0);\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(source->type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(source->type);\n+\n+  if (column_metadata.is_fixed_length) {\n+    // Fixed length column\n+    //\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        CollectBits(source->buffers[1]->data(), source->offset, target.mutable_data(1),\n+                    num_rows_before, num_rows_to_append, row_ids);\n+        break;\n+      case 1:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                target.mutable_data(1)[num_rows_before + i] = *ptr;\n+              });\n+        break;\n+      case 2:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint16_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint16_t*>(ptr);\n+              });\n+        break;\n+      case 4:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint32_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint32_t*>(ptr);\n+              });\n+        break;\n+      case 8:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint64_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint64_t*>(ptr);\n+              });\n+        break;\n+      default: {\n+        int num_rows_to_process =\n+            num_rows_to_append -\n+            NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+        Visit(source, num_rows_to_process, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                    target.mutable_data(1) +\n+                    static_cast<int64_t>(num_bytes) * (num_rows_before + i));\n+                const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                for (uint32_t word_id = 0;\n+                     word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t));\n+                     ++word_id) {\n+                  util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+                }\n+              });\n+        if (num_rows_to_append > num_rows_to_process) {\n+          Visit(source, num_rows_to_append - num_rows_to_process,\n+                row_ids + num_rows_to_process,\n+                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                  uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                      target.mutable_data(1) +\n+                      static_cast<int64_t>(num_bytes) *\n+                          (num_rows_before + num_rows_to_process + i));\n+                  const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                  memcpy(dst, src, num_bytes);\n+                });\n+        }\n+      }\n+    }\n+  } else {\n+    // Varying length column\n+    //\n+\n+    // Step 1: calculate target offsets\n+    //\n+    uint32_t* offsets = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[num_rows_before];\n+    Visit(source, num_rows_to_append, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            offsets[num_rows_before + i] = num_bytes;\n+          });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[num_rows_before + i];\n+      offsets[num_rows_before + i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_before + num_rows_to_append] = sum;\n+\n+    // Step 2: resize output buffers\n+    //\n+    RETURN_NOT_OK(target.ResizeVaryingLengthBuffer());\n+\n+    // Step 3: copy varying-length data\n+    //\n+    int num_rows_to_process =\n+        num_rows_to_append -\n+        NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+    Visit(source, num_rows_to_process, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(target.mutable_data(2) +\n+                                                        offsets[num_rows_before + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            for (uint32_t word_id = 0;\n+                 word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+              util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+            }\n+          });\n+    Visit(source, num_rows_to_append - num_rows_to_process, row_ids + num_rows_to_process,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                target.mutable_data(2) +\n+                offsets[num_rows_before + num_rows_to_process + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            memcpy(dst, src, num_bytes);\n+          });\n+  }\n+\n+  // Process nulls\n+  //\n+  if (source->buffers[0] == NULLPTR) {\n+    uint8_t* dst = target.mutable_data(0);\n+    dst[num_rows_before / 8] |= static_cast<uint8_t>(~0ULL << (num_rows_before & 7));\n+    for (int i = num_rows_before / 8 + 1;\n+         i < bit_util::BytesForBits(num_rows_before + num_rows_to_append); ++i) {\n+      dst[i] = 0xff;\n+    }\n+  } else {\n+    CollectBits(source->buffers[0]->data(), source->offset, target.mutable_data(0),\n+                num_rows_before, num_rows_to_append, row_ids);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(const std::shared_ptr<DataType>& type,\n+                                     ResizableArrayData& target, int num_rows_to_append,\n+                                     MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(type);\n+\n+  // Process fixed length buffer\n+  //\n+  if (column_metadata.is_fixed_length) {\n+    uint8_t* dst = target.mutable_data(1);\n+    if (column_metadata.fixed_length == 0) {\n+      dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+      int64_t offset_begin = num_rows_before / 8 + 1;\n+      int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+      if (offset_end > offset_begin) {\n+        memset(dst + offset_begin, 0, offset_end - offset_begin);\n+      }\n+    } else {\n+      memset(dst + num_rows_before * static_cast<int64_t>(column_metadata.fixed_length),\n+             0, static_cast<int64_t>(column_metadata.fixed_length) * num_rows_to_append);\n+    }\n+  } else {\n+    uint32_t* dst = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : dst[num_rows_before];\n+    for (int64_t i = num_rows_before; i <= num_rows_after; ++i) {\n+      dst[i] = sum;\n+    }\n+  }\n+\n+  // Process nulls\n+  //\n+  uint8_t* dst = target.mutable_data(0);\n+  dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+  int64_t offset_begin = num_rows_before / 8 + 1;\n+  int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+  if (offset_end > offset_begin) {\n+    memset(dst + offset_begin, 0, offset_end - offset_begin);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int num_cols, const int* col_ids) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(num_cols);\n+    for (int i = 0; i < num_cols; ++i) {\n+      const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+      ARROW_DCHECK(data.is_array());\n+      const std::shared_ptr<ArrayData>& array_data = data.array();\n+      values_[i].Init(array_data->type, pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    RETURN_NOT_OK(\n+        AppendSelected(array_data, values_[i], num_rows_to_append, row_ids, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int* num_appended, int num_cols,\n+                                        const int* col_ids) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendSelected(pool, batch, num_rows_next, row_ids, num_cols, col_ids));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(types.size());\n+    for (size_t i = 0; i < types.size(); ++i) {\n+      values_[i].Init(types[i], pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    RETURN_NOT_OK(AppendNulls(types[i], values_[i], num_rows_to_append, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append, int* num_appended) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendNulls(pool, types, num_rows_next));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+ExecBatch ExecBatchBuilder::Flush() {\n+  ARROW_DCHECK(num_rows() > 0);\n+  ExecBatch out({}, num_rows());\n+  out.values.resize(values_.size());\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    out.values[i] = values_[i].array_data();\n+    values_[i].Clear(true);\n+  }\n+  return out;\n+}\n+\n+int RowArrayAccessor::VarbinaryColumnId(const KeyEncoder::KeyRowMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                    int num_rows, const uint32_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                             int num_rows, const uint32_t* row_ids,\n+                             PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                  int num_rows, const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool,\n+                              const KeyEncoder::KeyRowMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyEncoder::KeyColumnMetadata> column_metadatas;\n+  ColumnMetadatasFromExecBatch(batch, column_metadatas);\n+  KeyEncoder::KeyRowMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(\n+    MemoryPool* pool, const ExecBatch& batch, int begin_row_id, int end_row_id,\n+    int num_row_ids, const uint16_t* row_ids,\n+    std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+\n+  KeyEncoder::KeyEncoderContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = output->column_metadata();\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const KeyEncoder::KeyRowMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(KeyEncoder::KeyRowArray* target,\n+                                    const KeyEncoder::KeyRowArray& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(KeyEncoder::KeyRowArray* target,\n+                                      const KeyEncoder::KeyRowArray& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(KeyEncoder::KeyRowArray* target,\n+                              const KeyEncoder::KeyRowArray& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays,\n+    std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Hashing32::HashBatch(*input->batch, input->batch_start_row,\n+                       input->batch_end_row - input->batch_start_row, hashes,\n+                       *input->temp_column_arrays, hardware_flags, input->temp_stack);\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(\n+    const ExecBatch& batch, int start_row, int num_rows, uint8_t* out_has_match_bitvector,\n+    uint32_t* out_key_ids, util::TempVectorStack* temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(\n+    SwissTableForJoin* target, int dop, int64_t num_rows, bool reject_duplicate_keys,\n+    bool no_payload, const std::vector<KeyEncoder::KeyColumnMetadata>& key_types,\n+    const std::vector<KeyEncoder::KeyColumnMetadata>& payload_types, MemoryPool* pool,\n+    int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(num_prtns_);\n+\n+  KeyEncoder::KeyRowMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  KeyEncoder::KeyRowMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  Hashing32::HashBatch(key_batch, /*start_row=*/0, static_cast<int>(key_batch.length),\n+                       locals.batch_hashes.data(), locals.temp_column_arrays,\n+                       hardware_flags_, temp_stack);\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int i, int pos) { locals.batch_prtn_row_ids[pos] = i; });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n+  }\n+}\n+\n+void SwissTableForJoinBuild::FinishPrtnMerge(util::TempVectorStack* temp_stack) {\n+  // Process overflow key ids\n+  //\n+  for (int prtn_id = 0; prtn_id < num_prtns_; ++prtn_id) {\n+    SwissTableMerge::InsertNewGroups(target_->map_.swiss_table(),\n+                                     prtn_states_[prtn_id].overflow_key_ids,\n+                                     prtn_states_[prtn_id].overflow_hashes);\n+  }\n+\n+  // Calculate whether we have nulls in hash table keys\n+  // (it is lazily evaluated but since we will be accessing it from multiple\n+  // threads we need to make sure that the value gets calculated here).\n+  //\n+  KeyEncoder::KeyEncoderContext ctx;\n+  ctx.hardware_flags = hardware_flags_;\n+  ctx.stack = temp_stack;\n+  std::ignore = target_->map_.keys()->rows_.has_any_nulls(&ctx);\n+}\n+\n+void JoinResultMaterialize::Init(MemoryPool* pool,\n+                                 const HashJoinProjectionMaps* probe_schemas,\n+                                 const HashJoinProjectionMaps* build_schemas) {\n+  pool_ = pool;\n+  probe_schemas_ = probe_schemas;\n+  build_schemas_ = build_schemas;\n+  num_rows_ = 0;\n+  null_ranges_.clear();\n+  num_produced_batches_ = 0;\n+\n+  // Initialize mapping of columns from output batch column index to key and\n+  // payload batch column index.\n+  //\n+  probe_output_to_key_and_payload_.resize(\n+      probe_schemas_->num_cols(HashJoinProjection::OUTPUT));\n+  int num_key_cols = probe_schemas_->num_cols(HashJoinProjection::KEY);\n+  auto to_key = probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  auto to_payload =\n+      probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; static_cast<size_t>(i) < probe_output_to_key_and_payload_.size(); ++i) {\n+    probe_output_to_key_and_payload_[i] =\n+        to_key.get(i) == SchemaProjectionMap::kMissingField\n+            ? to_payload.get(i) + num_key_cols\n+            : to_key.get(i);\n+  }\n+}\n+\n+void JoinResultMaterialize::SetBuildSide(const RowArray* build_keys,\n+                                         const RowArray* build_payloads,\n+                                         bool payload_id_same_as_key_id) {\n+  build_keys_ = build_keys;\n+  build_payloads_ = build_payloads;\n+  payload_id_same_as_key_id_ = payload_id_same_as_key_id;\n+}\n+\n+bool JoinResultMaterialize::HasProbeOutput() const {\n+  return probe_schemas_->num_cols(HashJoinProjection::OUTPUT) > 0;\n+}\n+\n+bool JoinResultMaterialize::HasBuildKeyOutput() const {\n+  auto to_key = build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_key.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::HasBuildPayloadOutput() const {\n+  auto to_payload =\n+      build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_payload.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::NeedsKeyId() const {\n+  return HasBuildKeyOutput() || (HasBuildPayloadOutput() && payload_id_same_as_key_id_);\n+}\n+\n+bool JoinResultMaterialize::NeedsPayloadId() const {\n+  return HasBuildPayloadOutput() && !payload_id_same_as_key_id_;\n+}\n+\n+Status JoinResultMaterialize::AppendProbeOnly(const ExecBatch& key_and_payload,\n+                                              int num_rows_to_append,\n+                                              const uint16_t* row_ids,\n+                                              int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendSelected(\n+        pool_, key_and_payload, num_rows_to_append, row_ids,\n+        static_cast<int>(probe_output_to_key_and_payload_.size()),\n+        probe_output_to_key_and_payload_.data()));\n+  }\n+  if (!null_ranges_.empty() &&\n+      null_ranges_.back().first + null_ranges_.back().second == num_rows_) {\n+    // We can extend the last range of null rows on build side.\n+    //\n+    null_ranges_.back().second += num_rows_to_append;\n+  } else {\n+    null_ranges_.push_back(\n+        std::make_pair(static_cast<int>(num_rows_), num_rows_to_append));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Status JoinResultMaterialize::AppendBuildOnly(int num_rows_to_append,\n+                                              const uint32_t* key_ids,\n+                                              const uint32_t* payload_ids,\n+                                              int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendNulls(\n+        pool_, probe_schemas_->data_types(HashJoinProjection::OUTPUT),\n+        num_rows_to_append));\n+  }\n+  if (NeedsKeyId()) {\n+    ARROW_DCHECK(key_ids != nullptr);\n+    key_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(key_ids_.data() + num_rows_, key_ids, num_rows_to_append * sizeof(uint32_t));\n+  }\n+  if (NeedsPayloadId()) {\n+    ARROW_DCHECK(payload_ids != nullptr);\n+    payload_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(payload_ids_.data() + num_rows_, payload_ids,\n+           num_rows_to_append * sizeof(uint32_t));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Status JoinResultMaterialize::Append(const ExecBatch& key_and_payload,\n+                                     int num_rows_to_append, const uint16_t* row_ids,\n+                                     const uint32_t* key_ids, const uint32_t* payload_ids,\n+                                     int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendSelected(\n+        pool_, key_and_payload, num_rows_to_append, row_ids,\n+        static_cast<int>(probe_output_to_key_and_payload_.size()),\n+        probe_output_to_key_and_payload_.data()));\n+  }\n+  if (NeedsKeyId()) {\n+    ARROW_DCHECK(key_ids != nullptr);\n+    key_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(key_ids_.data() + num_rows_, key_ids, num_rows_to_append * sizeof(uint32_t));\n+  }\n+  if (NeedsPayloadId()) {\n+    ARROW_DCHECK(payload_ids != nullptr);\n+    payload_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(payload_ids_.data() + num_rows_, payload_ids,\n+           num_rows_to_append * sizeof(uint32_t));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Result<std::shared_ptr<ArrayData>> JoinResultMaterialize::FlushBuildColumn(\n+    const std::shared_ptr<DataType>& data_type, const RowArray* row_array, int column_id,\n+    uint32_t* row_ids) {\n+  ResizableArrayData output;\n+  output.Init(data_type, pool_, bit_util::Log2(num_rows_));\n+\n+  for (size_t i = 0; i <= null_ranges_.size(); ++i) {\n+    int row_id_begin =\n+        i == 0 ? 0 : null_ranges_[i - 1].first + null_ranges_[i - 1].second;\n+    int row_id_end = i == null_ranges_.size() ? num_rows_ : null_ranges_[i].first;\n+    if (row_id_end > row_id_begin) {\n+      RETURN_NOT_OK(row_array->DecodeSelected(\n+          &output, column_id, row_id_end - row_id_begin, row_ids + row_id_begin, pool_));\n+    }\n+    int num_nulls = i == null_ranges_.size() ? 0 : null_ranges_[i].second;\n+    if (num_nulls > 0) {\n+      RETURN_NOT_OK(ExecBatchBuilder::AppendNulls(data_type, output, num_nulls, pool_));\n+    }\n+  }\n+\n+  return output.array_data();\n+}\n+\n+Status JoinResultMaterialize::Flush(ExecBatch* out) {\n+  ARROW_DCHECK(num_rows_ > 0);\n+  out->length = num_rows_;\n+  out->values.clear();\n+\n+  int num_probe_cols = probe_schemas_->num_cols(HashJoinProjection::OUTPUT);\n+  int num_build_cols = build_schemas_->num_cols(HashJoinProjection::OUTPUT);\n+  out->values.resize(num_probe_cols + num_build_cols);\n+\n+  if (HasProbeOutput()) {\n+    ExecBatch probe_batch = batch_builder_.Flush();\n+    ARROW_DCHECK(static_cast<int>(probe_batch.values.size()) == num_probe_cols);\n+    for (size_t i = 0; i < probe_batch.values.size(); ++i) {\n+      out->values[i] = std::move(probe_batch.values[i]);\n+    }\n+  }\n+  auto to_key = build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  auto to_payload =\n+      build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; i < num_build_cols; ++i) {\n+    if (to_key.get(i) != SchemaProjectionMap::kMissingField) {\n+      std::shared_ptr<ArrayData> column;\n+      ARROW_ASSIGN_OR_RAISE(\n+          column,\n+          FlushBuildColumn(build_schemas_->data_type(HashJoinProjection::OUTPUT, i),\n+                           build_keys_, to_key.get(i), key_ids_.data()));\n+      out->values[num_probe_cols + i] = std::move(column);\n+    } else if (to_payload.get(i) != SchemaProjectionMap::kMissingField) {\n+      std::shared_ptr<ArrayData> column;\n+      ARROW_ASSIGN_OR_RAISE(\n+          column,\n+          FlushBuildColumn(\n+              build_schemas_->data_type(HashJoinProjection::OUTPUT, i), build_payloads_,\n+              to_payload.get(i),\n+              payload_id_same_as_key_id_ ? key_ids_.data() : payload_ids_.data()));\n+      out->values[num_probe_cols + i] = std::move(column);\n+    } else {\n+      ARROW_DCHECK(false);\n+    }\n+  }\n+\n+  num_rows_ = 0;\n+  key_ids_.clear();\n+  payload_ids_.clear();\n+  null_ranges_.clear();\n+\n+  ++num_produced_batches_;\n+\n+  return Status::OK();\n+}\n+\n+void JoinNullFilter::Filter(const ExecBatch& key_batch, int batch_start_row,\n+                            int num_batch_rows, const std::vector<JoinKeyCmp>& cmp,\n+                            bool* all_valid, bool and_with_input,\n+                            uint8_t* inout_bit_vector) {\n+  // AND together validity vectors for columns that use equality comparison.\n+  //\n+  bool is_output_initialized = and_with_input;\n+  for (size_t i = 0; i < cmp.size(); ++i) {\n+    // No null filtering if null == null is true\n+    //\n+    if (cmp[i] != JoinKeyCmp::EQ) {\n+      continue;\n+    }\n+\n+    // No null filtering when there are no nulls\n+    //\n+    const Datum& data = key_batch.values[i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    if (!array_data->buffers[0]) {\n+      continue;\n+    }\n+\n+    const uint8_t* non_null_buffer = array_data->buffers[0]->data();\n+    int64_t offset = array_data->offset + batch_start_row;\n+\n+    // Filter out nulls for this column\n+    //\n+    if (!is_output_initialized) {\n+      memset(inout_bit_vector, 0xff, bit_util::BytesForBits(num_batch_rows));\n+      is_output_initialized = true;\n+    }\n+    arrow::internal::BitmapAnd(inout_bit_vector, 0, non_null_buffer, offset,\n+                               num_batch_rows, 0, inout_bit_vector);\n+  }\n+  *all_valid = !is_output_initialized;\n+}\n+\n+void JoinMatchIterator::SetLookupResult(int num_batch_rows, int start_batch_row,\n+                                        const uint8_t* batch_has_match,\n+                                        const uint32_t* key_ids, bool no_duplicate_keys,\n+                                        const uint32_t* key_to_payload) {\n+  num_batch_rows_ = num_batch_rows;\n+  start_batch_row_ = start_batch_row;\n+  batch_has_match_ = batch_has_match;\n+  key_ids_ = key_ids;\n+\n+  no_duplicate_keys_ = no_duplicate_keys;\n+  key_to_payload_ = key_to_payload;\n+\n+  current_row_ = 0;\n+  current_match_for_row_ = 0;\n+}\n+\n+bool JoinMatchIterator::GetNextBatch(int num_rows_max, int* out_num_rows,\n+                                     uint16_t* batch_row_ids, uint32_t* key_ids,\n+                                     uint32_t* payload_ids) {\n+  *out_num_rows = 0;\n+\n+  if (no_duplicate_keys_) {\n+    // When every input key can have at most one match,\n+    // then we only need to filter according to has match bit vector.\n+    //\n+    // We stop when either we produce a full batch or when we reach the end of\n+    // matches to output.\n+    //\n+    while (current_row_ < num_batch_rows_ && *out_num_rows < num_rows_max) {\n+      batch_row_ids[*out_num_rows] = start_batch_row_ + current_row_;\n+      key_ids[*out_num_rows] = payload_ids[*out_num_rows] = key_ids_[current_row_];\n+      (*out_num_rows) += bit_util::GetBit(batch_has_match_, current_row_) ? 1 : 0;\n+      ++current_row_;\n+    }\n+  } else {\n+    // When every input key can have zero, one or many matches,\n+    // then we need to filter out ones with no match and\n+    // iterate over all matches for the remaining ones.\n+    //\n+    // We stop when either we produce a full batch or when we reach the end of\n+    // matches to output.\n+    //\n+    while (current_row_ < num_batch_rows_ && *out_num_rows < num_rows_max) {\n+      if (!bit_util::GetBit(batch_has_match_, current_row_)) {\n+        ++current_row_;\n+        current_match_for_row_ = 0;\n+        continue;\n+      }\n+      uint32_t base_payload_id = key_to_payload_[key_ids_[current_row_]];\n+\n+      // Total number of matches for the currently selected input row\n+      //\n+      int num_matches_total =\n+          key_to_payload_[key_ids_[current_row_] + 1] - base_payload_id;\n+\n+      // Number of remaining matches for the currently selected input row\n+      //\n+      int num_matches_left = num_matches_total - current_match_for_row_;\n+\n+      // Number of matches for the currently selected input row that will fit\n+      // into the next batch\n+      //\n+      int num_matches_next = std::min(num_matches_left, num_rows_max - *out_num_rows);\n+\n+      for (int imatch = 0; imatch < num_matches_next; ++imatch) {\n+        batch_row_ids[*out_num_rows] = start_batch_row_ + current_row_;\n+        key_ids[*out_num_rows] = key_ids_[current_row_];\n+        payload_ids[*out_num_rows] = base_payload_id + current_match_for_row_ + imatch;\n+        ++(*out_num_rows);\n+      }\n+      current_match_for_row_ += num_matches_next;\n+\n+      if (current_match_for_row_ == num_matches_total) {\n+        ++current_row_;\n+        current_match_for_row_ = 0;\n+      }\n+    }\n+  }\n+\n+  return (*out_num_rows) > 0;\n+}\n+\n+void JoinProbeProcessor::Init(int num_key_columns, JoinType join_type,\n+                              SwissTableForJoin* hash_table,\n+                              std::vector<JoinResultMaterialize*> materialize,\n+                              const std::vector<JoinKeyCmp>* cmp,\n+                              OutputBatchFn output_batch_fn) {\n+  num_key_columns_ = num_key_columns;\n+  join_type_ = join_type;\n+  hash_table_ = hash_table;\n+  materialize_.resize(materialize.size());\n+  for (size_t i = 0; i < materialize.size(); ++i) {\n+    materialize_[i] = materialize[i];\n+  }\n+  cmp_ = cmp;\n+  output_batch_fn_ = output_batch_fn;\n+}\n+\n+Status JoinProbeProcessor::OnNextBatch(\n+    int64_t thread_id, const ExecBatch& keypayload_batch,\n+    util::TempVectorStack* temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* temp_column_arrays) {\n+  const SwissTable* swiss_table = hash_table_->keys()->swiss_table();\n+  int64_t hardware_flags = swiss_table->hardware_flags();\n+  int minibatch_size = swiss_table->minibatch_size();\n+  int num_rows = static_cast<int>(keypayload_batch.length);\n+\n+  ExecBatch key_batch({}, keypayload_batch.length);\n+  key_batch.values.resize(num_key_columns_);\n+  for (int i = 0; i < num_key_columns_; ++i) {\n+    key_batch.values[i] = keypayload_batch.values[i];\n+  }\n+\n+  // Break into mini-batches\n+  //\n+  // Start by allocating mini-batch buffers\n+  //\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack, static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)));\n+  auto key_ids_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto materialize_batch_ids_buf =\n+      util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size);\n+  auto materialize_key_ids_buf =\n+      util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto materialize_payload_ids_buf =\n+      util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input input(&key_batch, minibatch_start,\n+                                    minibatch_start + minibatch_size_next, temp_stack,\n+                                    temp_column_arrays);\n+    hash_table_->keys()->Hash(&input, hashes_buf.mutable_data(), hardware_flags);\n+    hash_table_->keys()->MapReadOnly(&input, hashes_buf.mutable_data(),\n+                                     match_bitvector_buf.mutable_data(),\n+                                     key_ids_buf.mutable_data());\n+\n+    // AND bit vector with null key filter for join\n+    //\n+    bool ignored;\n+    JoinNullFilter::Filter(key_batch, minibatch_start, minibatch_size_next, *cmp_,\n+                           &ignored,\n+                           /*and_with_input=*/true, match_bitvector_buf.mutable_data());\n+    // Semi-joins\n+    //\n+    if (join_type_ == JoinType::LEFT_SEMI || join_type_ == JoinType::LEFT_ANTI ||\n+        join_type_ == JoinType::RIGHT_SEMI || join_type_ == JoinType::RIGHT_ANTI) {\n+      int num_passing_ids = 0;\n+      util::bit_util::bits_to_indexes(\n+          (join_type_ == JoinType::LEFT_ANTI) ? 0 : 1, hardware_flags,\n+          minibatch_size_next, match_bitvector_buf.mutable_data(), &num_passing_ids,\n+          materialize_batch_ids_buf.mutable_data());\n+\n+      // For right-semi, right-anti joins: update has-match flags for the rows\n+      // in hash table.\n+      //\n+      if (join_type_ == JoinType::RIGHT_SEMI || join_type_ == JoinType::RIGHT_ANTI) {\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          uint16_t id = materialize_batch_ids_buf.mutable_data()[i];\n+          key_ids_buf.mutable_data()[i] = key_ids_buf.mutable_data()[id];\n+        }\n+        hash_table_->UpdateHasMatchForKeys(thread_id, num_passing_ids,\n+                                           key_ids_buf.mutable_data());\n+      } else {\n+        // For left-semi, left-anti joins: call materialize using match\n+        // bit-vector.\n+        //\n+\n+        // Add base batch row index.\n+        //\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          materialize_batch_ids_buf.mutable_data()[i] +=\n+              static_cast<uint16_t>(minibatch_start);\n+        }\n+\n+        RETURN_NOT_OK(materialize_[thread_id]->AppendProbeOnly(\n+            keypayload_batch, num_passing_ids, materialize_batch_ids_buf.mutable_data(),\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+    } else {\n+      // We need to output matching pairs of rows from both sides of the join.\n+      // Since every hash table lookup for an input row might have multiple\n+      // matches we use a helper class that implements enumerating all of them.\n+      //\n+      bool no_duplicate_keys = (hash_table_->key_to_payload() == nullptr);\n+      bool no_payload_columns = (hash_table_->payloads() == nullptr);\n+      JoinMatchIterator match_iterator;\n+      match_iterator.SetLookupResult(\n+          minibatch_size_next, minibatch_start, match_bitvector_buf.mutable_data(),\n+          key_ids_buf.mutable_data(), no_duplicate_keys, hash_table_->key_to_payload());\n+      int num_matches_next;\n+      while (match_iterator.GetNextBatch(minibatch_size, &num_matches_next,\n+                                         materialize_batch_ids_buf.mutable_data(),\n+                                         materialize_key_ids_buf.mutable_data(),\n+                                         materialize_payload_ids_buf.mutable_data())) {\n+        const uint16_t* materialize_batch_ids = materialize_batch_ids_buf.mutable_data();\n+        const uint32_t* materialize_key_ids = materialize_key_ids_buf.mutable_data();\n+        const uint32_t* materialize_payload_ids =\n+            no_duplicate_keys || no_payload_columns\n+                ? materialize_key_ids_buf.mutable_data()\n+                : materialize_payload_ids_buf.mutable_data();\n+\n+        // For right-outer, full-outer joins we need to update has-match flags\n+        // for the rows in hash table.\n+        //\n+        if (join_type_ == JoinType::RIGHT_OUTER || join_type_ == JoinType::FULL_OUTER) {\n+          hash_table_->UpdateHasMatchForKeys(thread_id, num_matches_next,\n+                                             materialize_key_ids);\n+        }\n+\n+        // Call materialize for resulting id tuples pointing to matching pairs\n+        // of rows.\n+        //\n+        RETURN_NOT_OK(materialize_[thread_id]->Append(\n+            keypayload_batch, num_matches_next, materialize_batch_ids,\n+            materialize_key_ids, materialize_payload_ids,\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+\n+      // For left-outer and full-outer joins output non-matches.\n+      //\n+      // Call materialize. Nulls will be output in all columns that come from\n+      // the other side of the join.\n+      //\n+      if (join_type_ == JoinType::LEFT_OUTER || join_type_ == JoinType::FULL_OUTER) {\n+        int num_passing_ids = 0;\n+        util::bit_util::bits_to_indexes(\n+            /*bit_to_search=*/0, hardware_flags, minibatch_size_next,\n+            match_bitvector_buf.mutable_data(), &num_passing_ids,\n+            materialize_batch_ids_buf.mutable_data());\n+\n+        // Add base batch row index.\n+        //\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          materialize_batch_ids_buf.mutable_data()[i] +=\n+              static_cast<uint16_t>(minibatch_start);\n+        }\n+\n+        RETURN_NOT_OK(materialize_[thread_id]->AppendProbeOnly(\n+            keypayload_batch, num_passing_ids, materialize_batch_ids_buf.mutable_data(),\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status JoinProbeProcessor::OnFinished() {\n+  // Flush all instances of materialize that have non-zero accumulated output\n+  // rows.\n+  //\n+  for (size_t i = 0; i < materialize_.size(); ++i) {\n+    JoinResultMaterialize& materialize = *materialize_[i];\n+    if (materialize.num_rows() > 0) {\n+      RETURN_NOT_OK(materialize.Flush(\n+          [&](ExecBatch batch) { output_batch_fn_(i, std::move(batch)); }));\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+class ExecBatchQueue {\n+ public:\n+  void Init(int dop);\n+  bool Append(int64_t thread_id, ExecBatch* batch);\n+  void CloseAll();\n+  int64_t num_shared_rows() const { return num_shared_rows_; }\n+  int64_t num_shared_batches() const { return num_shared_batches_; }\n+  ExecBatch* shared_batch(int64_t batch_id);\n+\n+ private:\n+  struct ThreadLocalState {\n+    ThreadLocalState() {}\n+    ThreadLocalState(const ThreadLocalState&) {}\n+    std::mutex mutex_;\n+    // Protected by mutex:\n+    bool is_closed_;\n+    std::vector<ExecBatch> queue_;\n+    int64_t num_rows_;\n+    // Not protected by mutex:\n+    std::vector<ExecBatch> queue_shared_;\n+  };\n+  std::vector<ThreadLocalState> thread_states_;\n+  int64_t num_shared_rows_;\n+  int64_t num_shared_batches_;\n+};\n+\n+void ExecBatchQueue::Init(int dop) {\n+  num_shared_rows_ = 0;\n+  num_shared_batches_ = 0;\n+  thread_states_.resize(dop);\n+  for (int i = 0; i < dop; ++i) {\n+    std::lock_guard<std::mutex> lock(thread_states_[i].mutex_);\n+    thread_states_[i].is_closed_ = false;\n+    thread_states_[i].num_rows_ = 0;\n+  }\n+}\n+\n+bool ExecBatchQueue::Append(int64_t thread_id, ExecBatch* batch) {\n+  int64_t num_rows = batch->length;\n+  std::lock_guard<std::mutex> lock(thread_states_[thread_id].mutex_);\n+  if (thread_states_[thread_id].is_closed_) {\n+    return false;\n+  }\n+  thread_states_[thread_id].queue_.push_back(*batch);\n+  thread_states_[thread_id].num_rows_ += num_rows;\n+  return true;\n+}\n+\n+void ExecBatchQueue::CloseAll() {\n+  for (size_t i = 0; i < thread_states_.size(); ++i) {\n+    std::vector<ExecBatch> queue_copy;\n+    int64_t num_rows;\n+    {\n+      std::lock_guard<std::mutex> lock(thread_states_[i].mutex_);\n+      if (thread_states_[i].is_closed_) {\n+        continue;\n+      }\n+      thread_states_[i].is_closed_ = true;\n+      queue_copy = std::move(thread_states_[i].queue_);\n+      num_rows = thread_states_[i].num_rows_;\n+    }\n+    num_shared_batches_ += queue_copy.size();\n+    num_shared_rows_ += num_rows;\n+    thread_states_[i].queue_shared_ = std::move(queue_copy);\n+  }\n+}\n+\n+ExecBatch* ExecBatchQueue::shared_batch(int64_t batch_id) {\n+  for (size_t i = 0; i < thread_states_.size(); ++i) {\n+    int64_t queue_size = static_cast<int64_t>(thread_states_[i].queue_shared_.size());\n+    if (batch_id < queue_size) {\n+      return &(thread_states_[i].queue_shared_[batch_id]);\n+    }\n+    batch_id -= queue_size;\n+  }\n+  // We should never get here\n+  //\n+  ARROW_DCHECK(false);\n+  return nullptr;\n+}\n+\n+class SwissJoin : public HashJoinImpl {\n+ public:\n+  Status Init(ExecContext* ctx, JoinType join_type, bool use_sync_execution,\n+              size_t num_threads, const HashJoinProjectionMaps* proj_map_left,\n+              const HashJoinProjectionMaps* proj_map_right,\n+              std::vector<JoinKeyCmp> key_cmp, Expression filter,\n+              OutputBatchCallback output_batch_callback,\n+              FinishedCallback finished_callback,\n+              TaskScheduler::ScheduleImpl schedule_task_callback) override {\n+    num_threads_ = static_cast<int>(std::max(num_threads, static_cast<size_t>(1)));\n+\n+    START_SPAN(span_, \"HashJoinBasicImpl\",\n\nReview comment:\n       Should probably switch span to SwissJoin\n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.h\n##########\n@@ -0,0 +1,874 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class ResizableArrayData {\n+ public:\n+  ResizableArrayData()\n+      : log_num_rows_min_(0),\n+        pool_(NULLPTR),\n+        num_rows_(0),\n+        num_rows_allocated_(0),\n+        var_len_buf_size_(0) {}\n+  ~ResizableArrayData() { Clear(true); }\n+  void Init(const std::shared_ptr<DataType>& data_type, MemoryPool* pool,\n+            int log_num_rows_min);\n+  void Clear(bool release_buffers);\n+  Status ResizeFixedLengthBuffers(int num_rows_new);\n+  Status ResizeVaryingLengthBuffer();\n+  int num_rows() const { return num_rows_; }\n+  KeyEncoder::KeyColumnArray column_array() const;\n+  KeyEncoder::KeyColumnMetadata column_metadata() const {\n+    return ColumnMetadataFromDataType(data_type_);\n+  }\n+  std::shared_ptr<ArrayData> array_data() const;\n+  uint8_t* mutable_data(int i) {\n+    return i == 0   ? non_null_buf_->mutable_data()\n+           : i == 1 ? fixed_len_buf_->mutable_data()\n+                    : var_len_buf_->mutable_data();\n+  }\n+\n+ private:\n+  static constexpr int64_t kNumPaddingBytes = 64;\n+  int log_num_rows_min_;\n+  std::shared_ptr<DataType> data_type_;\n+  MemoryPool* pool_;\n+  int num_rows_;\n+  int num_rows_allocated_;\n+  int var_len_buf_size_;\n+  std::shared_ptr<ResizableBuffer> non_null_buf_;\n+  std::shared_ptr<ResizableBuffer> fixed_len_buf_;\n+  std::shared_ptr<ResizableBuffer> var_len_buf_;\n+};\n+\n+class ExecBatchBuilder {\n+ public:\n+  static Status AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                               ResizableArrayData& target, int num_rows_to_append,\n+                               const uint16_t* row_ids, MemoryPool* pool);\n+\n+  static Status AppendNulls(const std::shared_ptr<DataType>& type,\n+                            ResizableArrayData& target, int num_rows_to_append,\n+                            MemoryPool* pool);\n+\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int num_cols,\n+                        const int* col_ids = NULLPTR);\n+\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int* num_appended, int num_cols,\n+                        const int* col_ids = NULLPTR);\n+\n+  Status AppendNulls(MemoryPool* pool,\n+                     const std::vector<std::shared_ptr<DataType>>& types,\n+                     int num_rows_to_append);\n+\n+  Status AppendNulls(MemoryPool* pool,\n+                     const std::vector<std::shared_ptr<DataType>>& types,\n+                     int num_rows_to_append, int* num_appended);\n+\n+  // Should only be called if num_rows() returns non-zero.\n+  //\n+  ExecBatch Flush();\n+\n+  int num_rows() const { return values_.empty() ? 0 : values_[0].num_rows(); }\n+\n+  static int num_rows_max() { return 1 << kLogNumRows; }\n+\n+ private:\n+  static constexpr int kLogNumRows = 15;\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  // The sequence of row_ids provided must be non-decreasing.\n+  //\n+  static int NumRowsToSkip(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                           const uint16_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                    const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  template <bool OUTPUT_BYTE_ALIGNED>\n+  static void CollectBitsImp(const uint8_t* input_bits, int64_t input_bits_offset,\n+                             uint8_t* output_bits, int64_t output_bits_offset,\n+                             int num_rows, const uint16_t* row_ids);\n+  static void CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                          uint8_t* output_bits, int64_t output_bits_offset, int num_rows,\n+                          const uint16_t* row_ids);\n+\n+  std::vector<ResizableArrayData> values_;\n+};\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const KeyEncoder::KeyRowMetadata& row_metadata,\n+                               int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                           int num_rows, const uint32_t* row_ids,\n+                           int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const KeyEncoder::KeyRowArray& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const KeyEncoder::KeyRowArray& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const KeyEncoder::KeyRowArray& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                             int num_rows, const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const KeyEncoder::KeyRowMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(\n+      MemoryPool* pool, const ExecBatch& batch, int begin_row_id, int end_row_id,\n+      int num_row_ids, const uint16_t* row_ids,\n+      std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  KeyEncoder encoder_;\n+  KeyEncoder::KeyRowArray rows_;\n+  KeyEncoder::KeyRowArray rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(KeyEncoder::KeyRowArray* target,\n+                              const KeyEncoder::KeyRowArray& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(KeyEncoder::KeyRowArray* target,\n+                                const KeyEncoder::KeyRowArray& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(KeyEncoder::KeyRowArray* target,\n+                        const KeyEncoder::KeyRowArray& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyEncoder::KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n+\n+  // If input uses selection, then hashes array must have one element for every\n+  // row in the whole (unfiltered and not spliced) input exec batch. Otherwise,\n+  // there must be one element in hashes array for every value in the window of\n+  // the exec batch specified by input.\n+  //\n+  // Output arrays will contain one element for every selected batch row in\n+  // input (selected either by selection vector if provided or input window\n+  // otherwise).\n+  //\n+  void MapReadOnly(Input* input, const uint32_t* hashes, uint8_t* match_bitvector,\n+                   uint32_t* key_ids);\n+  Status MapWithInserts(Input* input, const uint32_t* hashes, uint32_t* key_ids);\n+\n+  SwissTable* swiss_table() { return &swiss_table_; }\n+  const SwissTable* swiss_table() const { return &swiss_table_; }\n+  RowArray* keys() { return &keys_; }\n+  const RowArray* keys() const { return &keys_; }\n+\n+ private:\n+  void EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                     const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                     uint16_t* out_selection_mismatch, void* callback_ctx);\n+  Status AppendCallback(int num_keys, const uint16_t* selection, void* callback_ctx);\n+  Status Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+             uint8_t* match_bitvector_maybe_null, uint32_t* key_ids);\n+\n+  SwissTable::EqualImpl equal_impl_;\n+  SwissTable::AppendImpl append_impl_;\n+\n+  SwissTable swiss_table_;\n+  RowArray keys_;\n+};\n+\n+// Enhances SwissTableWithKeys with the following structures used by hash join:\n+// - storage of payloads (that unlike keys do not have to be unique)\n+// - mapping from a key to all inserted payloads corresponding to it (we can\n+// store multiple rows corresponding to a single key)\n+// - bit-vectors for keeping track of whether each payload had a match during\n+// evaluation of join.\n+//\n+class SwissTableForJoin {\n+  friend class SwissTableForJoinBuild;\n+\n+ public:\n+  void Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+              uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+              util::TempVectorStack* temp_stack,\n+              std::vector<KeyEncoder::KeyColumnArray>* temp_column_arrays);\n+  void UpdateHasMatchForKeys(int64_t thread_id, int num_rows, const uint32_t* key_ids);\n+  void MergeHasMatch();\n+\n+  const SwissTableWithKeys* keys() const { return &map_; }\n+  SwissTableWithKeys* keys() { return &map_; }\n+  const RowArray* payloads() const { return no_payload_columns_ ? NULLPTR : &payloads_; }\n+  const uint32_t* key_to_payload() const {\n+    return no_duplicate_keys_ ? NULLPTR : row_offset_for_key_.data();\n+  }\n+  const uint8_t* has_match() const {\n+    return has_match_.empty() ? NULLPTR : has_match_.data();\n+  }\n+  int64_t num_keys() const { return map_.keys()->num_rows(); }\n+  int64_t num_rows() const {\n+    return no_duplicate_keys_ ? num_keys() : row_offset_for_key_[num_keys()];\n+  }\n+\n+  uint32_t payload_id_to_key_id(uint32_t payload_id) const;\n+  // Input payload ids must form an increasing sequence.\n+  //\n+  void payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                              uint32_t* key_ids) const;\n+\n+ private:\n+  uint8_t* local_has_match(int64_t thread_id);\n+\n+  // Degree of parallelism (number of threads)\n+  int dop_;\n+\n+  struct ThreadLocalState {\n+    std::vector<uint8_t> has_match;\n+  };\n+  std::vector<ThreadLocalState> local_states_;\n+  std::vector<uint8_t> has_match_;\n+\n+  SwissTableWithKeys map_;\n+\n+  bool no_duplicate_keys_;\n+  // Not used if no_duplicate_keys_ is true.\n+  std::vector<uint32_t> row_offset_for_key_;\n+\n+  bool no_payload_columns_;\n+  // Not used if no_payload_columns_ is true.\n+  RowArray payloads_;\n+};\n+\n+// Implements parallel build process for hash table for join from a sequence of\n+// exec batches with input rows.\n+//\n+class SwissTableForJoinBuild {\n+ public:\n+  Status Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+              bool reject_duplicate_keys, bool no_payload,\n+              const std::vector<KeyEncoder::KeyColumnMetadata>& key_types,\n+              const std::vector<KeyEncoder::KeyColumnMetadata>& payload_types,\n+              MemoryPool* pool, int64_t hardware_flags);\n+\n+  // In the first phase of parallel hash table build, threads pick unprocessed\n+  // exec batches, partition the rows based on hash, and update all of the\n+  // partitions with information related to that batch of rows.\n+  //\n+  Status PushNextBatch(int64_t thread_id, const ExecBatch& key_batch,\n+                       const ExecBatch* payload_batch_maybe_null,\n+                       util::TempVectorStack* temp_stack);\n+\n+  // Allocate memory and initialize counters required for parallel merging of\n+  // hash table partitions.\n+  // Single-threaded.\n+  //\n+  Status PreparePrtnMerge();\n+\n+  // Second phase of parallel hash table build.\n+  // Each partition can be processed by a different thread.\n+  // Parallel step.\n+  //\n+  void PrtnMerge(int prtn_id);\n+\n+  // Single-threaded processing of the rows that have been skipped during\n+  // parallel merging phase, due to hash table search resulting in crossing\n+  // partition boundaries.\n+  //\n+  void FinishPrtnMerge(util::TempVectorStack* temp_stack);\n+\n+  // The number of partitions is the number of parallel tasks to execute during\n+  // the final phase of hash table build process.\n+  //\n+  int num_prtns() const { return num_prtns_; }\n+\n+  bool no_payload() const { return no_payload_; }\n+\n+ private:\n+  void InitRowArray();\n+  Status ProcessPartition(int64_t thread_id, const ExecBatch& key_batch,\n+                          const ExecBatch* payload_batch_maybe_null,\n+                          util::TempVectorStack* temp_stack, int prtn_id);\n+\n+  SwissTableForJoin* target_;\n+  // DOP stands for Degree Of Parallelism - the maximum number of participating\n+  // threads.\n+  //\n+  int dop_;\n+  // Partition is a unit of parallel work.\n+  //\n+  // There must be power of 2 partitions (bits of hash will be used to\n+  // identify them).\n+  //\n+  // Pick number of partitions at least equal to the number of threads (degree\n+  // of parallelism).\n+  //\n+  int log_num_prtns_;\n+  int num_prtns_;\n+  int64_t num_rows_;\n+  // Left-semi and left-anti-semi joins do not need more than one copy of the\n+  // same key in the hash table.\n+  // This flag, if set, will result in filtering rows with duplicate keys before\n+  // inserting them into hash table.\n+  //\n+  // Since left-semi and left-anti-semi joins also do not need payload, when\n+  // this flag is set there also will not be any processing of payload.\n+  //\n+  bool reject_duplicate_keys_;\n+  // This flag, when set, will result in skipping any processing of the payload.\n+  //\n+  // The flag for rejecting duplicate keys (which should be set for left-semi\n+  // and left-anti joins), when set, will force this flag to also be set, but\n+  // other join flavors may set it to true as well if no payload columns are\n+  // needed for join output.\n+  //\n+  bool no_payload_;\n+  MemoryPool* pool_;\n+  int64_t hardware_flags_;\n+\n+  // One per partition.\n+  //\n+  struct PartitionState {\n+    SwissTableWithKeys keys;\n+    RowArray payloads;\n+    std::vector<uint32_t> key_ids;\n+    std::vector<uint32_t> overflow_key_ids;\n+    std::vector<uint32_t> overflow_hashes;\n+  };\n+\n+  // One per thread.\n+  //\n+  // Buffers for storing temporary intermediate results when processing input\n+  // batches.\n+  //\n+  struct ThreadState {\n+    std::vector<uint32_t> batch_hashes;\n+    std::vector<uint16_t> batch_prtn_ranges;\n+    std::vector<uint16_t> batch_prtn_row_ids;\n+    std::vector<int> temp_prtn_ids;\n+    std::vector<uint32_t> temp_group_ids;\n+    std::vector<KeyEncoder::KeyColumnArray> temp_column_arrays;\n+  };\n+\n+  std::vector<PartitionState> prtn_states_;\n+  std::vector<ThreadState> thread_states_;\n+  PartitionLocks prtn_locks_;\n+\n+  std::vector<int64_t> partition_keys_first_row_id_;\n+  std::vector<int64_t> partition_payloads_first_row_id_;\n+};\n+\n+class JoinResultMaterialize {\n\nReview comment:\n       Can you add a quick comment to summarize what this class does? \n\n##########\nFile path: cpp/src/arrow/compute/exec/swiss_join.cc\n##########\n@@ -0,0 +1,3279 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_compare.h\"\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+void ResizableArrayData::Init(const std::shared_ptr<DataType>& data_type,\n+                              MemoryPool* pool, int log_num_rows_min) {\n+#ifndef NDEBUG\n+  if (num_rows_allocated_ > 0) {\n+    ARROW_DCHECK(data_type_ != NULLPTR);\n+    KeyEncoder::KeyColumnMetadata metadata_before =\n+        ColumnMetadataFromDataType(data_type_);\n+    KeyEncoder::KeyColumnMetadata metadata_after = ColumnMetadataFromDataType(data_type);\n+    ARROW_DCHECK(metadata_before.is_fixed_length == metadata_after.is_fixed_length &&\n+                 metadata_before.fixed_length == metadata_after.fixed_length);\n+  }\n+#endif\n+  Clear(/*release_buffers=*/false);\n+  log_num_rows_min_ = log_num_rows_min;\n+  data_type_ = data_type;\n+  pool_ = pool;\n+}\n+\n+void ResizableArrayData::Clear(bool release_buffers) {\n+  num_rows_ = 0;\n+  if (release_buffers) {\n+    non_null_buf_.reset();\n+    fixed_len_buf_.reset();\n+    var_len_buf_.reset();\n+    num_rows_allocated_ = 0;\n+    var_len_buf_size_ = 0;\n+  }\n+}\n+\n+Status ResizableArrayData::ResizeFixedLengthBuffers(int num_rows_new) {\n+  ARROW_DCHECK(num_rows_new >= 0);\n+  if (num_rows_new <= num_rows_allocated_) {\n+    num_rows_ = num_rows_new;\n+    return Status::OK();\n+  }\n+\n+  int num_rows_allocated_new = 1 << log_num_rows_min_;\n+  while (num_rows_allocated_new < num_rows_new) {\n+    num_rows_allocated_new *= 2;\n+  }\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (fixed_len_buf_ == NULLPTR) {\n+    ARROW_DCHECK(non_null_buf_ == NULLPTR && var_len_buf_ == NULLPTR);\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        non_null_buf_,\n+        AllocateResizableBuffer(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes, pool_));\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes,\n+                pool_));\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes,\n+                pool_));\n+      }\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(\n+          fixed_len_buf_,\n+          AllocateResizableBuffer(\n+              (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes, pool_));\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(var_len_buf_, AllocateResizableBuffer(\n+                                            sizeof(uint64_t) + kNumPaddingBytes, pool_));\n+\n+    var_len_buf_size_ = sizeof(uint64_t);\n+  } else {\n+    ARROW_DCHECK(non_null_buf_ != NULLPTR && var_len_buf_ != NULLPTR);\n+\n+    RETURN_NOT_OK(non_null_buf_->Resize(bit_util::BytesForBits(num_rows_allocated_new) +\n+                                        kNumPaddingBytes));\n+\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes));\n+      } else {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes));\n+      }\n+    } else {\n+      RETURN_NOT_OK(fixed_len_buf_->Resize(\n+          (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes));\n+    }\n+  }\n+\n+  num_rows_allocated_ = num_rows_allocated_new;\n+  num_rows_ = num_rows_new;\n+\n+  return Status::OK();\n+}\n+\n+Status ResizableArrayData::ResizeVaryingLengthBuffer() {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (!column_metadata.is_fixed_length) {\n+    int min_new_size = static_cast<int>(\n+        reinterpret_cast<const uint32_t*>(fixed_len_buf_->data())[num_rows_]);\n+    ARROW_DCHECK(var_len_buf_size_ > 0);\n+    if (var_len_buf_size_ < min_new_size) {\n+      int new_size = var_len_buf_size_;\n+      while (new_size < min_new_size) {\n+        new_size *= 2;\n+      }\n+      RETURN_NOT_OK(var_len_buf_->Resize(new_size + kNumPaddingBytes));\n+      var_len_buf_size_ = new_size;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+KeyEncoder::KeyColumnArray ResizableArrayData::column_array() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+  return KeyEncoder::KeyColumnArray(\n+      column_metadata, num_rows_, non_null_buf_->mutable_data(),\n+      fixed_len_buf_->mutable_data(), var_len_buf_->mutable_data());\n+}\n+\n+std::shared_ptr<ArrayData> ResizableArrayData::array_data() const {\n+  KeyEncoder::KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  auto valid_count = arrow::internal::CountSetBits(non_null_buf_->data(), /*offset=*/0,\n+                                                   static_cast<int64_t>(num_rows_));\n+  int null_count = static_cast<int>(num_rows_) - static_cast<int>(valid_count);\n+\n+  if (column_metadata.is_fixed_length) {\n+    return ArrayData::Make(data_type_, num_rows_, {non_null_buf_, fixed_len_buf_},\n+                           null_count);\n+  } else {\n+    return ArrayData::Make(data_type_, num_rows_,\n+                           {non_null_buf_, fixed_len_buf_, var_len_buf_}, null_count);\n+  }\n+}\n+\n+int ExecBatchBuilder::NumRowsToSkip(const std::shared_ptr<ArrayData>& column,\n+                                    int num_rows, const uint16_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+#ifndef NDEBUG\n+  // Ids must be in non-decreasing order\n+  //\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(row_ids[i] >= row_ids[i - 1]);\n+  }\n+#endif\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(column->type);\n+\n+  int num_rows_left = num_rows;\n+  int num_bytes_skipped = 0;\n+  while (num_rows_left > 0 && num_bytes_skipped < num_tail_bytes_to_skip) {\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        num_rows_left = std::max(num_rows_left, 8) - 8;\n+        ++num_bytes_skipped;\n+      } else {\n+        --num_rows_left;\n+        num_bytes_skipped += column_metadata.fixed_length;\n+      }\n+    } else {\n+      --num_rows_left;\n+      int row_id_removed = row_ids[num_rows_left];\n+      const uint32_t* offsets =\n+          reinterpret_cast<const uint32_t*>(column->buffers[1]->data());\n+      num_bytes_skipped += offsets[row_id_removed + 1] - offsets[row_id_removed];\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <bool OUTPUT_BYTE_ALIGNED>\n+void ExecBatchBuilder::CollectBitsImp(const uint8_t* input_bits,\n+                                      int64_t input_bits_offset, uint8_t* output_bits,\n+                                      int64_t output_bits_offset, int num_rows,\n+                                      const uint16_t* row_ids) {\n+  if (!OUTPUT_BYTE_ALIGNED) {\n+    ARROW_DCHECK(output_bits_offset % 8 > 0);\n+    output_bits[output_bits_offset / 8] &=\n+        static_cast<uint8_t>((1 << (output_bits_offset % 8)) - 1);\n+  } else {\n+    ARROW_DCHECK(output_bits_offset % 8 == 0);\n+  }\n+  constexpr int unroll = 8;\n+  for (int i = 0; i < num_rows / unroll; ++i) {\n+    const uint16_t* row_ids_base = row_ids + unroll * i;\n+    uint8_t result;\n+    result = bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[0]) ? 1 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[1]) ? 2 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[2]) ? 4 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[3]) ? 8 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[4]) ? 16 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[5]) ? 32 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[6]) ? 64 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[7]) ? 128 : 0;\n+    if (OUTPUT_BYTE_ALIGNED) {\n+      output_bits[output_bits_offset / 8 + i] = result;\n+    } else {\n+      output_bits[output_bits_offset / 8 + i] |=\n+          static_cast<uint8_t>(result << (output_bits_offset % 8));\n+      output_bits[output_bits_offset / 8 + i + 1] =\n+          static_cast<uint8_t>(result >> (8 - (output_bits_offset % 8)));\n+    }\n+  }\n+  if (num_rows % unroll > 0) {\n+    for (int i = num_rows - (num_rows % unroll); i < num_rows; ++i) {\n+      bit_util::SetBitTo(output_bits, output_bits_offset + i,\n+                         bit_util::GetBit(input_bits, input_bits_offset + row_ids[i]));\n+    }\n+  }\n+}\n+\n+void ExecBatchBuilder::CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                                   uint8_t* output_bits, int64_t output_bits_offset,\n+                                   int num_rows, const uint16_t* row_ids) {\n+  if (output_bits_offset % 8 > 0) {\n+    CollectBitsImp<false>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                          num_rows, row_ids);\n+  } else {\n+    CollectBitsImp<true>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                         num_rows, row_ids);\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void ExecBatchBuilder::Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                             const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  KeyEncoder::KeyColumnMetadata metadata = ColumnMetadataFromDataType(column->type);\n+\n+  if (!metadata.is_fixed_length) {\n+    const uint8_t* ptr_base = column->buffers[2]->data();\n+    const uint32_t* offsets =\n+        reinterpret_cast<const uint32_t*>(column->buffers[1]->data()) + column->offset;\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr = ptr_base + offsets[row_id];\n+      uint32_t field_length = offsets[row_id + 1] - offsets[row_id];\n+      process_value_fn(i, field_ptr, field_length);\n+    }\n+  } else {\n+    ARROW_DCHECK(metadata.fixed_length > 0);\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr =\n+          column->buffers[1]->data() +\n+          (column->offset + row_id) * static_cast<int64_t>(metadata.fixed_length);\n+      process_value_fn(i, field_ptr, metadata.fixed_length);\n+    }\n+  }\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                                        ResizableArrayData& target,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  ARROW_DCHECK(num_rows_before >= 0);\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(source->type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata =\n+      ColumnMetadataFromDataType(source->type);\n+\n+  if (column_metadata.is_fixed_length) {\n+    // Fixed length column\n+    //\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        CollectBits(source->buffers[1]->data(), source->offset, target.mutable_data(1),\n+                    num_rows_before, num_rows_to_append, row_ids);\n+        break;\n+      case 1:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                target.mutable_data(1)[num_rows_before + i] = *ptr;\n+              });\n+        break;\n+      case 2:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint16_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint16_t*>(ptr);\n+              });\n+        break;\n+      case 4:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint32_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint32_t*>(ptr);\n+              });\n+        break;\n+      case 8:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                reinterpret_cast<uint64_t*>(target.mutable_data(1))[num_rows_before + i] =\n+                    *reinterpret_cast<const uint64_t*>(ptr);\n+              });\n+        break;\n+      default: {\n+        int num_rows_to_process =\n+            num_rows_to_append -\n+            NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+        Visit(source, num_rows_to_process, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                    target.mutable_data(1) +\n+                    static_cast<int64_t>(num_bytes) * (num_rows_before + i));\n+                const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                for (uint32_t word_id = 0;\n+                     word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t));\n+                     ++word_id) {\n+                  util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+                }\n+              });\n+        if (num_rows_to_append > num_rows_to_process) {\n+          Visit(source, num_rows_to_append - num_rows_to_process,\n+                row_ids + num_rows_to_process,\n+                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                  uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                      target.mutable_data(1) +\n+                      static_cast<int64_t>(num_bytes) *\n+                          (num_rows_before + num_rows_to_process + i));\n+                  const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                  memcpy(dst, src, num_bytes);\n+                });\n+        }\n+      }\n+    }\n+  } else {\n+    // Varying length column\n+    //\n+\n+    // Step 1: calculate target offsets\n+    //\n+    uint32_t* offsets = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[num_rows_before];\n+    Visit(source, num_rows_to_append, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            offsets[num_rows_before + i] = num_bytes;\n+          });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[num_rows_before + i];\n+      offsets[num_rows_before + i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_before + num_rows_to_append] = sum;\n+\n+    // Step 2: resize output buffers\n+    //\n+    RETURN_NOT_OK(target.ResizeVaryingLengthBuffer());\n+\n+    // Step 3: copy varying-length data\n+    //\n+    int num_rows_to_process =\n+        num_rows_to_append -\n+        NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+    Visit(source, num_rows_to_process, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(target.mutable_data(2) +\n+                                                        offsets[num_rows_before + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            for (uint32_t word_id = 0;\n+                 word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+              util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+            }\n+          });\n+    Visit(source, num_rows_to_append - num_rows_to_process, row_ids + num_rows_to_process,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                target.mutable_data(2) +\n+                offsets[num_rows_before + num_rows_to_process + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            memcpy(dst, src, num_bytes);\n+          });\n+  }\n+\n+  // Process nulls\n+  //\n+  if (source->buffers[0] == NULLPTR) {\n+    uint8_t* dst = target.mutable_data(0);\n+    dst[num_rows_before / 8] |= static_cast<uint8_t>(~0ULL << (num_rows_before & 7));\n+    for (int i = num_rows_before / 8 + 1;\n+         i < bit_util::BytesForBits(num_rows_before + num_rows_to_append); ++i) {\n+      dst[i] = 0xff;\n+    }\n+  } else {\n+    CollectBits(source->buffers[0]->data(), source->offset, target.mutable_data(0),\n+                num_rows_before, num_rows_to_append, row_ids);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(const std::shared_ptr<DataType>& type,\n+                                     ResizableArrayData& target, int num_rows_to_append,\n+                                     MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(type);\n+\n+  // Process fixed length buffer\n+  //\n+  if (column_metadata.is_fixed_length) {\n+    uint8_t* dst = target.mutable_data(1);\n+    if (column_metadata.fixed_length == 0) {\n+      dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+      int64_t offset_begin = num_rows_before / 8 + 1;\n+      int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+      if (offset_end > offset_begin) {\n+        memset(dst + offset_begin, 0, offset_end - offset_begin);\n+      }\n+    } else {\n+      memset(dst + num_rows_before * static_cast<int64_t>(column_metadata.fixed_length),\n+             0, static_cast<int64_t>(column_metadata.fixed_length) * num_rows_to_append);\n+    }\n+  } else {\n+    uint32_t* dst = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : dst[num_rows_before];\n+    for (int64_t i = num_rows_before; i <= num_rows_after; ++i) {\n+      dst[i] = sum;\n+    }\n+  }\n+\n+  // Process nulls\n+  //\n+  uint8_t* dst = target.mutable_data(0);\n+  dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+  int64_t offset_begin = num_rows_before / 8 + 1;\n+  int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+  if (offset_end > offset_begin) {\n+    memset(dst + offset_begin, 0, offset_end - offset_begin);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int num_cols, const int* col_ids) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(num_cols);\n+    for (int i = 0; i < num_cols; ++i) {\n+      const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+      ARROW_DCHECK(data.is_array());\n+      const std::shared_ptr<ArrayData>& array_data = data.array();\n+      values_[i].Init(array_data->type, pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    RETURN_NOT_OK(\n+        AppendSelected(array_data, values_[i], num_rows_to_append, row_ids, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int* num_appended, int num_cols,\n+                                        const int* col_ids) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendSelected(pool, batch, num_rows_next, row_ids, num_cols, col_ids));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(types.size());\n+    for (size_t i = 0; i < types.size(); ++i) {\n+      values_[i].Init(types[i], pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    RETURN_NOT_OK(AppendNulls(types[i], values_[i], num_rows_to_append, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append, int* num_appended) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendNulls(pool, types, num_rows_next));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+ExecBatch ExecBatchBuilder::Flush() {\n+  ARROW_DCHECK(num_rows() > 0);\n+  ExecBatch out({}, num_rows());\n+  out.values.resize(values_.size());\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    out.values[i] = values_[i].array_data();\n+    values_[i].Clear(true);\n+  }\n+  return out;\n+}\n+\n+int RowArrayAccessor::VarbinaryColumnId(const KeyEncoder::KeyRowMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                    int num_rows, const uint32_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                             int num_rows, const uint32_t* row_ids,\n+                             PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const KeyEncoder::KeyRowArray& rows, int column_id,\n+                                  int num_rows, const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool,\n+                              const KeyEncoder::KeyRowMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyEncoder::KeyColumnMetadata> column_metadatas;\n+  ColumnMetadatasFromExecBatch(batch, column_metadatas);\n+  KeyEncoder::KeyRowMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(\n+    MemoryPool* pool, const ExecBatch& batch, int begin_row_id, int end_row_id,\n+    int num_row_ids, const uint16_t* row_ids,\n+    std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyEncoder::KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                            temp_column_arrays);\n+\n+  KeyEncoder::KeyEncoderContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  KeyEncoder::KeyColumnMetadata column_metadata = output->column_metadata();\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const KeyEncoder::KeyRowMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(KeyEncoder::KeyRowArray* target,\n+                                    const KeyEncoder::KeyRowArray& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(KeyEncoder::KeyRowArray* target,\n+                                      const KeyEncoder::KeyRowArray& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(KeyEncoder::KeyRowArray* target,\n+                              const KeyEncoder::KeyRowArray& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(\n+    const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+    util::TempVectorStack* in_temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* in_temp_column_arrays,\n+    std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Hashing32::HashBatch(*input->batch, input->batch_start_row,\n+                       input->batch_end_row - input->batch_start_row, hashes,\n+                       *input->temp_column_arrays, hardware_flags, input->temp_stack);\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(\n+    const ExecBatch& batch, int start_row, int num_rows, uint8_t* out_has_match_bitvector,\n+    uint32_t* out_key_ids, util::TempVectorStack* temp_stack,\n+    std::vector<KeyEncoder::KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(\n+    SwissTableForJoin* target, int dop, int64_t num_rows, bool reject_duplicate_keys,\n+    bool no_payload, const std::vector<KeyEncoder::KeyColumnMetadata>& key_types,\n+    const std::vector<KeyEncoder::KeyColumnMetadata>& payload_types, MemoryPool* pool,\n+    int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(num_prtns_);\n+\n+  KeyEncoder::KeyRowMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  KeyEncoder::KeyRowMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  Hashing32::HashBatch(key_batch, /*start_row=*/0, static_cast<int>(key_batch.length),\n+                       locals.batch_hashes.data(), locals.temp_column_arrays,\n+                       hardware_flags_, temp_stack);\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int i, int pos) { locals.batch_prtn_row_ids[pos] = i; });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n+  }\n+}\n+\n+void SwissTableForJoinBuild::FinishPrtnMerge(util::TempVectorStack* temp_stack) {\n\nReview comment:\n       This could return a Status, and then you don't have to ignore `has_any_nulls`\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-16T19:36:07.148+0000",
                    "updated": "2022-03-16T19:36:07.148+0000",
                    "started": "2022-03-16T19:36:07.147+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "742720",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/750278",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #12326:\nURL: https://github.com/apache/arrow/pull/12326#issuecomment-1083406094\n\n\n   Can we rebase this now that the bloom filter (and key_hash) changes have been merged in.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-30T17:13:30.926+0000",
                    "updated": "2022-03-30T17:13:30.926+0000",
                    "started": "2022-03-30T17:13:30.925+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "750278",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/752883",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zagto commented on code in PR #12326:\nURL: https://github.com/apache/arrow/pull/12326#discussion_r842784590\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,874 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_encode.h\"\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class ResizableArrayData {\n+ public:\n+  ResizableArrayData()\n+      : log_num_rows_min_(0),\n+        pool_(NULLPTR),\n+        num_rows_(0),\n+        num_rows_allocated_(0),\n+        var_len_buf_size_(0) {}\n+  ~ResizableArrayData() { Clear(true); }\n+  void Init(const std::shared_ptr<DataType>& data_type, MemoryPool* pool,\n+            int log_num_rows_min);\n+  void Clear(bool release_buffers);\n+  Status ResizeFixedLengthBuffers(int num_rows_new);\n+  Status ResizeVaryingLengthBuffer();\n+  int num_rows() const { return num_rows_; }\n+  KeyEncoder::KeyColumnArray column_array() const;\n+  KeyEncoder::KeyColumnMetadata column_metadata() const {\n+    return ColumnMetadataFromDataType(data_type_);\n+  }\n+  std::shared_ptr<ArrayData> array_data() const;\n+  uint8_t* mutable_data(int i) {\n+    return i == 0   ? non_null_buf_->mutable_data()\n+           : i == 1 ? fixed_len_buf_->mutable_data()\n+                    : var_len_buf_->mutable_data();\n+  }\n+\n+ private:\n+  static constexpr int64_t kNumPaddingBytes = 64;\n+  int log_num_rows_min_;\n+  std::shared_ptr<DataType> data_type_;\n+  MemoryPool* pool_;\n+  int num_rows_;\n+  int num_rows_allocated_;\n+  int var_len_buf_size_;\n+  std::shared_ptr<ResizableBuffer> non_null_buf_;\n+  std::shared_ptr<ResizableBuffer> fixed_len_buf_;\n+  std::shared_ptr<ResizableBuffer> var_len_buf_;\n+};\n+\n+class ExecBatchBuilder {\n+ public:\n+  static Status AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                               ResizableArrayData& target, int num_rows_to_append,\n+                               const uint16_t* row_ids, MemoryPool* pool);\n+\n+  static Status AppendNulls(const std::shared_ptr<DataType>& type,\n+                            ResizableArrayData& target, int num_rows_to_append,\n+                            MemoryPool* pool);\n+\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int num_cols,\n+                        const int* col_ids = NULLPTR);\n+\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int* num_appended, int num_cols,\n+                        const int* col_ids = NULLPTR);\n+\n+  Status AppendNulls(MemoryPool* pool,\n+                     const std::vector<std::shared_ptr<DataType>>& types,\n+                     int num_rows_to_append);\n+\n+  Status AppendNulls(MemoryPool* pool,\n+                     const std::vector<std::shared_ptr<DataType>>& types,\n+                     int num_rows_to_append, int* num_appended);\n\nReview Comment:\n   It looks like this overload is no longer used anywhere\n\n\n\n",
                    "created": "2022-04-05T13:27:46.938+0000",
                    "updated": "2022-04-05T13:27:46.938+0000",
                    "started": "2022-04-05T13:27:46.937+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "752883",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/787258",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#issuecomment-1172788542\n\n   https://issues.apache.org/jira/browse/ARROW-14182\n\n\n",
                    "created": "2022-07-01T23:52:48.433+0000",
                    "updated": "2022-07-01T23:52:48.433+0000",
                    "started": "2022-07-01T23:52:48.432+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "787258",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/788444",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r915355867\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n\nReview Comment:\n   It seems odd that I have to supply `hardware_flags` for this method but not for the `MapXyz` variants.  I think the Map methods are using the hardware flags from `swiss_table_`.  Can we do that here too?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n\nReview Comment:\n   So it took me a while to realize that this method is actually static (and thus doesn't affect the table state).  Does this really belong here?  How is this different than something like `Hashing32::HashBatch`?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n+\n+  // If input uses selection, then hashes array must have one element for every\n+  // row in the whole (unfiltered and not spliced) input exec batch. Otherwise,\n+  // there must be one element in hashes array for every value in the window of\n+  // the exec batch specified by input.\n+  //\n+  // Output arrays will contain one element for every selected batch row in\n+  // input (selected either by selection vector if provided or input window\n+  // otherwise).\n+  //\n+  void MapReadOnly(Input* input, const uint32_t* hashes, uint8_t* match_bitvector,\n+                   uint32_t* key_ids);\n+  Status MapWithInserts(Input* input, const uint32_t* hashes, uint32_t* key_ids);\n\nReview Comment:\n   It's not obvious at a first glance what the difference is between the `Hash` call and the `MapXyz` call.  It's also not obvious what a key id is.\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n\nReview Comment:\n   Why does this need to be called separately from `Init`?  Can `Init` call `InitCallbacks`?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n\nReview Comment:\n   Can you add a note about the thread safety of the hash/map methods?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n\nReview Comment:\n   What's the rationale for these belonging to the input instead of belonging to the table?  Is it because these need to belong to the calling thread and the table is shared across all threads?\n\n\n\n",
                    "created": "2022-07-07T01:22:07.703+0000",
                    "updated": "2022-07-07T01:22:07.703+0000",
                    "started": "2022-07-07T01:22:07.702+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "788444",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/788835",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r916460870\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n\nReview Comment:\n   There are two ways SwissTableWithKeys is used:\r\n   a) There is per partition object used by one thread at a time, that grows from zero rows as new keys are inserted.\r\n   b) There is a shared, read-only copy created during the process of merging per partition objects.\r\n   \r\n   In a) Init is called, it calls InitCallback and then initializes SwissTable object with default number of buckets.\r\n   In b) only InitCallback is called, and SwissTableMerge initializes SwissTable object by allocating a pre-computed number of buckets that is different than the default. SwissTableMerge does not interact with SwissTableWithKeys, only with SwissTable, so it cannot call SwissTableWithKeys::Init. \r\n   \r\n   One thing could be to rename InitCallbacks as Init, remove the other Init, call map_.init() instead in the appropriate place in the code. \n\n\n\n",
                    "created": "2022-07-08T04:51:18.016+0000",
                    "updated": "2022-07-08T04:51:18.016+0000",
                    "started": "2022-07-08T04:51:18.016+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "788835",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/788836",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r916463537\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n+\n+  // If input uses selection, then hashes array must have one element for every\n+  // row in the whole (unfiltered and not spliced) input exec batch. Otherwise,\n+  // there must be one element in hashes array for every value in the window of\n+  // the exec batch specified by input.\n+  //\n+  // Output arrays will contain one element for every selected batch row in\n+  // input (selected either by selection vector if provided or input window\n+  // otherwise).\n+  //\n+  void MapReadOnly(Input* input, const uint32_t* hashes, uint8_t* match_bitvector,\n+                   uint32_t* key_ids);\n+  Status MapWithInserts(Input* input, const uint32_t* hashes, uint32_t* key_ids);\n\nReview Comment:\n   SwissTableWithKeys maps keys (made of arbitrary number of columns of supported data types) into integer ids from 0 to N for N + 1 inserted keys. Key id is the integer assigned to a given key that has been inserted into the hash table.\r\n   \r\n   Hash() computes hashes for input keys. Hash() needs to be called before calling MapReadOnly() or MapWithInserts() but the computed hashes can be reused outside of MapReadOnly and MapWithInserts or can be shared between both of these calls.\r\n   \r\n   MapReadOnly() uses both input keys and hashes to lookup existing keys in the hash table. Keys that are not in a hash table will be marked in a bit-vector. \r\n   \r\n   MapWithInserts() does everything MapReadOnly() does, but it will also allocate new key ids for new keys that are not present in the hash table yet and insert these new keys. Not thread-safe, unlike MapReadOnly().\n\n\n\n",
                    "created": "2022-07-08T04:58:39.797+0000",
                    "updated": "2022-07-08T04:58:39.797+0000",
                    "started": "2022-07-08T04:58:39.796+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "788836",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/788837",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r916463860\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n\nReview Comment:\n   Hash is a static method, so it cannot get flags from swiss_table_.\n\n\n\n",
                    "created": "2022-07-08T04:59:39.280+0000",
                    "updated": "2022-07-08T04:59:39.280+0000",
                    "started": "2022-07-08T04:59:39.279+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "788837",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/788838",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r916464162\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n\nReview Comment:\n   Hash is a static method, so there is no state.\r\n   For Maps I can add a comment similar to the one in one of the previous answers.\n\n\n\n",
                    "created": "2022-07-08T05:00:24.614+0000",
                    "updated": "2022-07-08T05:00:24.614+0000",
                    "started": "2022-07-08T05:00:24.614+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "788838",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/788839",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r916465683\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n\nReview Comment:\n   Exactly, these are all thread specific states (temporary buffers) that are passed to a shared hash table, a scratch space. They have to be provided from outside of the shared object when doing operations that use temporary buffers.  That makes them part of the input for Hash, MapReadOnly and MapWithInserts. \n\n\n\n",
                    "created": "2022-07-08T05:04:29.759+0000",
                    "updated": "2022-07-08T05:04:29.759+0000",
                    "started": "2022-07-08T05:04:29.758+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "788839",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/789181",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r917111224\n\n\n##########\ncpp/src/arrow/compute/exec/hash_join_node.cc:\n##########\n@@ -654,6 +655,33 @@ struct BloomFilterPushdownContext {\n     FilterFinishedCallback on_finished_;\n   } eval_;\n };\n+bool HashJoinSchema::HasDictionaries() const {\n+  for (int side = 0; side <= 1; ++side) {\n+    for (int icol = 0; icol < proj_maps[side].num_cols(HashJoinProjection::INPUT);\n+         ++icol) {\n+      const std::shared_ptr<DataType>& column_type =\n+          proj_maps[side].data_type(HashJoinProjection::INPUT, icol);\n+      if (column_type->id() == Type::DICTIONARY) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+bool HashJoinSchema::HasLargeBinary() const {\n+  for (int side = 0; side <= 1; ++side) {\n+    for (int icol = 0; icol < proj_maps[side].num_cols(HashJoinProjection::INPUT);\n+         ++icol) {\n+      const std::shared_ptr<DataType>& column_type =\n+          proj_maps[side].data_type(HashJoinProjection::INPUT, icol);\n+      if (is_large_binary_like(column_type->id())) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n\nReview Comment:\n   Can we move these methods (and the other HashJoinSchema methods) into `hash_join.cc` since they are defined in `hash_join.h`?\n\n\n\n##########\ncpp/src/arrow/compute/exec/key_map.h:\n##########\n@@ -28,29 +28,44 @@ namespace arrow {\n namespace compute {\n \n class SwissTable {\n+  friend class SwissTableMerge;\n\nReview Comment:\n   Can we add a class comment that gives a general overview (or a pointer to an overview) of a swiss table?  In particular, what is a group?  What is a block?  These concepts are referenced elsewhere in the swiss merge and it is hard to understand what is going on without a better understanding of these concepts.\n\n\n\n##########\ncpp/src/arrow/compute/exec/hash_join_benchmark.cc:\n##########\n@@ -132,7 +134,7 @@ class JoinBenchmark {\n                                 left_keys, *r_batches_with_schema.schema, right_keys,\n                                 filter, \"l_\", \"r_\"));\n \n-    join_ = *HashJoinImpl::MakeBasic();\n\nReview Comment:\n   Since we are still using both (depending on the output type) it might be nice if we could include results for both hash types instead of replacing the existing benchmarks completely.  That would also help demonstrate the differences between the two approaches.\n\n\n\n##########\ncpp/src/arrow/compute/exec/hash_join_node.cc:\n##########\n@@ -708,8 +736,26 @@ class HashJoinNode : public ExecNode {\n     // Generate output schema\n     std::shared_ptr<Schema> output_schema = schema_mgr->MakeOutputSchema(\n         join_options.output_suffix_for_left, join_options.output_suffix_for_right);\n+\n     // Create hash join implementation object\n-    ARROW_ASSIGN_OR_RAISE(std::unique_ptr<HashJoinImpl> impl, HashJoinImpl::MakeBasic());\n+    // SwissJoin does not support:\n+    // a) 64-bit string offsets\n+    // b) residual predicates\n+    // c) dictionaries\n+    //\n+    bool use_swiss_join;\n+#if ARROW_LITTLE_ENDIAN\n+    use_swiss_join = (filter == literal(true)) && !schema_mgr->HasDictionaries() &&\n+                     !schema_mgr->HasLargeBinary();\n+#else\n+    use_swiss_join = false;\n+#endif\n+    std::unique_ptr<HashJoinImpl> impl;\n+    if (use_swiss_join) {\n+      ARROW_ASSIGN_OR_RAISE(impl, HashJoinImpl::MakeSwiss());\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(impl, HashJoinImpl::MakeBasic());\n+    }\n\nReview Comment:\n   When debugging / profiling I think it would be pretty valuable to know whether we are using a swiss join or not.  Can we add a ToStringExtra method to HashJoinNode (look at the other nodes for examples) that includes this field?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.cc:\n##########\n@@ -0,0 +1,2545 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/row/compare_internal.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+int RowArrayAccessor::VarbinaryColumnId(const RowTableMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                                    const uint32_t* row_ids, int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                                  const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyColumnMetadata> column_metadatas;\n+  RETURN_NOT_OK(ColumnMetadatasFromExecBatch(batch, &column_metadatas));\n+  RowTableMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch,\n+                                      int begin_row_id, int end_row_id, int num_row_ids,\n+                                      const uint16_t* row_ids,\n+                                      std::vector<KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  RETURN_NOT_OK(ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                                          &temp_column_arrays));\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  Status status = ColumnArraysFromExecBatch(\n+      batch, begin_row_id, end_row_id - begin_row_id, &temp_column_arrays);\n+  ARROW_DCHECK(status.ok());\n+\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  ARROW_ASSIGN_OR_RAISE(KeyColumnMetadata column_metadata, output->column_metadata());\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const RowTableMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_batch_start_row,\n+                                 int in_batch_end_row,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_num_selected,\n+                                 const uint16_t* in_selection,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays,\n+                                 std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Status status =\n+      Hashing32::HashBatch(*input->batch, hashes, *input->temp_column_arrays,\n+                           hardware_flags, input->temp_stack, input->batch_start_row,\n+                           input->batch_end_row - input->batch_start_row);\n+  ARROW_DCHECK(status.ok());\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+                               uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+                               util::TempVectorStack* temp_stack,\n+                               std::vector<KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+                                    bool reject_duplicate_keys, bool no_payload,\n+                                    const std::vector<KeyColumnMetadata>& key_types,\n+                                    const std::vector<KeyColumnMetadata>& payload_types,\n+                                    MemoryPool* pool, int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(dop_, num_prtns_);\n+\n+  RowTableMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  RowTableMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  RETURN_NOT_OK(Hashing32::HashBatch(\n+      key_batch, locals.batch_hashes.data(), locals.temp_column_arrays, hardware_flags_,\n+      temp_stack, /*start_row=*/0, static_cast<int>(key_batch.length)));\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int64_t i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int64_t i, int pos) {\n+          locals.batch_prtn_row_ids[pos] = static_cast<uint16_t>(i);\n+        });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      thread_id, locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n\nReview Comment:\n   ```suggestion\r\n   ```\n\n\n\n##########\ncpp/src/arrow/compute/exec/partition_util.h:\n##########\n@@ -118,6 +118,43 @@ class PartitionLocks {\n   /// \\brief Release a partition so that other threads can work on it\n   void ReleasePartitionLock(int prtn_id);\n \n+  template <typename IS_PRTN_EMPTY_FN, typename PROCESS_PRTN_FN>\n+  Status ForEachPartition(size_t thread_id, int* temp_unprocessed_prtns,\n\nReview Comment:\n   Can you add notes on the general threading that's expected here?  I'm assuming the intent is to spread the work of `process_prtn_fn` across multiple threads?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.cc:\n##########\n@@ -0,0 +1,2545 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/row/compare_internal.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+int RowArrayAccessor::VarbinaryColumnId(const RowTableMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                                    const uint32_t* row_ids, int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                                  const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyColumnMetadata> column_metadatas;\n+  RETURN_NOT_OK(ColumnMetadatasFromExecBatch(batch, &column_metadatas));\n+  RowTableMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch,\n+                                      int begin_row_id, int end_row_id, int num_row_ids,\n+                                      const uint16_t* row_ids,\n+                                      std::vector<KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  RETURN_NOT_OK(ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                                          &temp_column_arrays));\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  Status status = ColumnArraysFromExecBatch(\n+      batch, begin_row_id, end_row_id - begin_row_id, &temp_column_arrays);\n+  ARROW_DCHECK(status.ok());\n+\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  ARROW_ASSIGN_OR_RAISE(KeyColumnMetadata column_metadata, output->column_metadata());\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const RowTableMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_batch_start_row,\n+                                 int in_batch_end_row,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_num_selected,\n+                                 const uint16_t* in_selection,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays,\n+                                 std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Status status =\n+      Hashing32::HashBatch(*input->batch, hashes, *input->temp_column_arrays,\n+                           hardware_flags, input->temp_stack, input->batch_start_row,\n+                           input->batch_end_row - input->batch_start_row);\n+  ARROW_DCHECK(status.ok());\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+                               uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+                               util::TempVectorStack* temp_stack,\n+                               std::vector<KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+                                    bool reject_duplicate_keys, bool no_payload,\n+                                    const std::vector<KeyColumnMetadata>& key_types,\n+                                    const std::vector<KeyColumnMetadata>& payload_types,\n+                                    MemoryPool* pool, int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(dop_, num_prtns_);\n+\n+  RowTableMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  RowTableMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  RETURN_NOT_OK(Hashing32::HashBatch(\n+      key_batch, locals.batch_hashes.data(), locals.temp_column_arrays, hardware_flags_,\n+      temp_stack, /*start_row=*/0, static_cast<int>(key_batch.length)));\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int64_t i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int64_t i, int pos) {\n+          locals.batch_prtn_row_ids[pos] = static_cast<uint16_t>(i);\n+        });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      thread_id, locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n\nReview Comment:\n   It appears this is calculated but never used.  This is the only place that PrepareForMerge is called.  Is there any need to continue calculating this?  I don't think it's really harmful to leave it in if needed.\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n+\n+  // If input uses selection, then hashes array must have one element for every\n+  // row in the whole (unfiltered and not spliced) input exec batch. Otherwise,\n+  // there must be one element in hashes array for every value in the window of\n+  // the exec batch specified by input.\n+  //\n+  // Output arrays will contain one element for every selected batch row in\n+  // input (selected either by selection vector if provided or input window\n+  // otherwise).\n+  //\n+  void MapReadOnly(Input* input, const uint32_t* hashes, uint8_t* match_bitvector,\n+                   uint32_t* key_ids);\n+  Status MapWithInserts(Input* input, const uint32_t* hashes, uint32_t* key_ids);\n+\n+  SwissTable* swiss_table() { return &swiss_table_; }\n+  const SwissTable* swiss_table() const { return &swiss_table_; }\n+  RowArray* keys() { return &keys_; }\n+  const RowArray* keys() const { return &keys_; }\n+\n+ private:\n+  void EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                     const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                     uint16_t* out_selection_mismatch, void* callback_ctx);\n+  Status AppendCallback(int num_keys, const uint16_t* selection, void* callback_ctx);\n+  Status Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+             uint8_t* match_bitvector_maybe_null, uint32_t* key_ids);\n+\n+  SwissTable::EqualImpl equal_impl_;\n+  SwissTable::AppendImpl append_impl_;\n+\n+  SwissTable swiss_table_;\n+  RowArray keys_;\n+};\n+\n+// Enhances SwissTableWithKeys with the following structures used by hash join:\n+// - storage of payloads (that unlike keys do not have to be unique)\n+// - mapping from a key to all inserted payloads corresponding to it (we can\n+// store multiple rows corresponding to a single key)\n+// - bit-vectors for keeping track of whether each payload had a match during\n+// evaluation of join.\n+//\n+class SwissTableForJoin {\n+  friend class SwissTableForJoinBuild;\n+\n+ public:\n+  void Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+              uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+              util::TempVectorStack* temp_stack,\n+              std::vector<KeyColumnArray>* temp_column_arrays);\n\nReview Comment:\n   This method is never used since all access to the table goes through the `JoinMatchIterator`.  Do you want to just get rid of it?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n+\n+  // If input uses selection, then hashes array must have one element for every\n+  // row in the whole (unfiltered and not spliced) input exec batch. Otherwise,\n+  // there must be one element in hashes array for every value in the window of\n+  // the exec batch specified by input.\n+  //\n+  // Output arrays will contain one element for every selected batch row in\n+  // input (selected either by selection vector if provided or input window\n+  // otherwise).\n+  //\n+  void MapReadOnly(Input* input, const uint32_t* hashes, uint8_t* match_bitvector,\n+                   uint32_t* key_ids);\n+  Status MapWithInserts(Input* input, const uint32_t* hashes, uint32_t* key_ids);\n+\n+  SwissTable* swiss_table() { return &swiss_table_; }\n+  const SwissTable* swiss_table() const { return &swiss_table_; }\n+  RowArray* keys() { return &keys_; }\n+  const RowArray* keys() const { return &keys_; }\n+\n+ private:\n+  void EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                     const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                     uint16_t* out_selection_mismatch, void* callback_ctx);\n+  Status AppendCallback(int num_keys, const uint16_t* selection, void* callback_ctx);\n+  Status Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+             uint8_t* match_bitvector_maybe_null, uint32_t* key_ids);\n+\n+  SwissTable::EqualImpl equal_impl_;\n+  SwissTable::AppendImpl append_impl_;\n+\n+  SwissTable swiss_table_;\n+  RowArray keys_;\n+};\n+\n+// Enhances SwissTableWithKeys with the following structures used by hash join:\n+// - storage of payloads (that unlike keys do not have to be unique)\n+// - mapping from a key to all inserted payloads corresponding to it (we can\n+// store multiple rows corresponding to a single key)\n+// - bit-vectors for keeping track of whether each payload had a match during\n+// evaluation of join.\n+//\n+class SwissTableForJoin {\n+  friend class SwissTableForJoinBuild;\n+\n+ public:\n+  void Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+              uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+              util::TempVectorStack* temp_stack,\n+              std::vector<KeyColumnArray>* temp_column_arrays);\n+  void UpdateHasMatchForKeys(int64_t thread_id, int num_rows, const uint32_t* key_ids);\n+  void MergeHasMatch();\n+\n+  const SwissTableWithKeys* keys() const { return &map_; }\n+  SwissTableWithKeys* keys() { return &map_; }\n+  const RowArray* payloads() const { return no_payload_columns_ ? NULLPTR : &payloads_; }\n+  const uint32_t* key_to_payload() const {\n+    return no_duplicate_keys_ ? NULLPTR : row_offset_for_key_.data();\n+  }\n+  const uint8_t* has_match() const {\n+    return has_match_.empty() ? NULLPTR : has_match_.data();\n+  }\n+  int64_t num_keys() const { return map_.keys()->num_rows(); }\n+  int64_t num_rows() const {\n+    return no_duplicate_keys_ ? num_keys() : row_offset_for_key_[num_keys()];\n+  }\n+\n+  uint32_t payload_id_to_key_id(uint32_t payload_id) const;\n+  // Input payload ids must form an increasing sequence.\n+  //\n+  void payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                              uint32_t* key_ids) const;\n+\n+ private:\n+  uint8_t* local_has_match(int64_t thread_id);\n+\n+  // Degree of parallelism (number of threads)\n+  int dop_;\n+\n+  struct ThreadLocalState {\n+    std::vector<uint8_t> has_match;\n+  };\n+  std::vector<ThreadLocalState> local_states_;\n+  std::vector<uint8_t> has_match_;\n+\n+  SwissTableWithKeys map_;\n+\n+  bool no_duplicate_keys_;\n+  // Not used if no_duplicate_keys_ is true.\n+  std::vector<uint32_t> row_offset_for_key_;\n+\n+  bool no_payload_columns_;\n+  // Not used if no_payload_columns_ is true.\n+  RowArray payloads_;\n+};\n+\n+// Implements parallel build process for hash table for join from a sequence of\n+// exec batches with input rows.\n+//\n+class SwissTableForJoinBuild {\n+ public:\n+  Status Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+              bool reject_duplicate_keys, bool no_payload,\n+              const std::vector<KeyColumnMetadata>& key_types,\n+              const std::vector<KeyColumnMetadata>& payload_types, MemoryPool* pool,\n+              int64_t hardware_flags);\n+\n+  // In the first phase of parallel hash table build, threads pick unprocessed\n+  // exec batches, partition the rows based on hash, and update all of the\n+  // partitions with information related to that batch of rows.\n+  //\n+  Status PushNextBatch(int64_t thread_id, const ExecBatch& key_batch,\n+                       const ExecBatch* payload_batch_maybe_null,\n+                       util::TempVectorStack* temp_stack);\n+\n+  // Allocate memory and initialize counters required for parallel merging of\n+  // hash table partitions.\n+  // Single-threaded.\n+  //\n+  Status PreparePrtnMerge();\n+\n+  // Second phase of parallel hash table build.\n+  // Each partition can be processed by a different thread.\n+  // Parallel step.\n+  //\n+  void PrtnMerge(int prtn_id);\n+\n+  // Single-threaded processing of the rows that have been skipped during\n+  // parallel merging phase, due to hash table search resulting in crossing\n+  // partition boundaries.\n+  //\n+  void FinishPrtnMerge(util::TempVectorStack* temp_stack);\n+\n+  // The number of partitions is the number of parallel tasks to execute during\n+  // the final phase of hash table build process.\n+  //\n+  int num_prtns() const { return num_prtns_; }\n+\n+  bool no_payload() const { return no_payload_; }\n+\n+ private:\n+  void InitRowArray();\n+  Status ProcessPartition(int64_t thread_id, const ExecBatch& key_batch,\n+                          const ExecBatch* payload_batch_maybe_null,\n+                          util::TempVectorStack* temp_stack, int prtn_id);\n+\n+  SwissTableForJoin* target_;\n+  // DOP stands for Degree Of Parallelism - the maximum number of participating\n+  // threads.\n+  //\n+  int dop_;\n+  // Partition is a unit of parallel work.\n+  //\n+  // There must be power of 2 partitions (bits of hash will be used to\n+  // identify them).\n+  //\n+  // Pick number of partitions at least equal to the number of threads (degree\n+  // of parallelism).\n+  //\n+  int log_num_prtns_;\n+  int num_prtns_;\n+  int64_t num_rows_;\n+  // Left-semi and left-anti-semi joins do not need more than one copy of the\n+  // same key in the hash table.\n+  // This flag, if set, will result in filtering rows with duplicate keys before\n+  // inserting them into hash table.\n+  //\n+  // Since left-semi and left-anti-semi joins also do not need payload, when\n+  // this flag is set there also will not be any processing of payload.\n+  //\n+  bool reject_duplicate_keys_;\n+  // This flag, when set, will result in skipping any processing of the payload.\n+  //\n+  // The flag for rejecting duplicate keys (which should be set for left-semi\n+  // and left-anti joins), when set, will force this flag to also be set, but\n+  // other join flavors may set it to true as well if no payload columns are\n+  // needed for join output.\n+  //\n+  bool no_payload_;\n+  MemoryPool* pool_;\n+  int64_t hardware_flags_;\n+\n+  // One per partition.\n+  //\n+  struct PartitionState {\n+    SwissTableWithKeys keys;\n+    RowArray payloads;\n+    std::vector<uint32_t> key_ids;\n+    std::vector<uint32_t> overflow_key_ids;\n+    std::vector<uint32_t> overflow_hashes;\n+  };\n+\n+  // One per thread.\n+  //\n+  // Buffers for storing temporary intermediate results when processing input\n+  // batches.\n+  //\n+  struct ThreadState {\n+    std::vector<uint32_t> batch_hashes;\n+    std::vector<uint16_t> batch_prtn_ranges;\n+    std::vector<uint16_t> batch_prtn_row_ids;\n+    std::vector<int> temp_prtn_ids;\n+    std::vector<uint32_t> temp_group_ids;\n+    std::vector<KeyColumnArray> temp_column_arrays;\n+  };\n+\n+  std::vector<PartitionState> prtn_states_;\n+  std::vector<ThreadState> thread_states_;\n+  PartitionLocks prtn_locks_;\n+\n+  std::vector<int64_t> partition_keys_first_row_id_;\n+  std::vector<int64_t> partition_payloads_first_row_id_;\n+};\n+\n+class JoinResultMaterialize {\n+ public:\n+  void Init(MemoryPool* pool, const HashJoinProjectionMaps* probe_schemas,\n+            const HashJoinProjectionMaps* build_schemas);\n+\n+  void SetBuildSide(const RowArray* build_keys, const RowArray* build_payloads,\n+                    bool payload_id_same_as_key_id);\n+\n+  // Input probe side batches should contain all key columns followed by all\n+  // payload columns.\n+  //\n+  Status AppendProbeOnly(const ExecBatch& key_and_payload, int num_rows_to_append,\n+                         const uint16_t* row_ids, int* num_rows_appended);\n+\n+  Status AppendBuildOnly(int num_rows_to_append, const uint32_t* key_ids,\n+                         const uint32_t* payload_ids, int* num_rows_appended);\n+\n+  Status Append(const ExecBatch& key_and_payload, int num_rows_to_append,\n+                const uint16_t* row_ids, const uint32_t* key_ids,\n+                const uint32_t* payload_ids, int* num_rows_appended);\n+\n+  // Should only be called if num_rows() returns non-zero.\n+  //\n+  Status Flush(ExecBatch* out);\n+\n+  int num_rows() const { return num_rows_; }\n+\n+  template <class APPEND_ROWS_FN, class OUTPUT_BATCH_FN>\n+  Status AppendAndOutput(int num_rows_to_append, const APPEND_ROWS_FN& append_rows_fn,\n+                         const OUTPUT_BATCH_FN& output_batch_fn) {\n+    int offset = 0;\n+    for (;;) {\n+      int num_rows_appended = 0;\n+      ARROW_RETURN_NOT_OK(append_rows_fn(num_rows_to_append, offset, &num_rows_appended));\n+      if (num_rows_appended < num_rows_to_append) {\n+        ExecBatch batch;\n+        ARROW_RETURN_NOT_OK(Flush(&batch));\n+        output_batch_fn(batch);\n+        num_rows_to_append -= num_rows_appended;\n+        offset += num_rows_appended;\n+      } else {\n+        break;\n+      }\n+    }\n+    return Status::OK();\n+  }\n+\n+  template <class OUTPUT_BATCH_FN>\n+  Status AppendProbeOnly(const ExecBatch& key_and_payload, int num_rows_to_append,\n+                         const uint16_t* row_ids, OUTPUT_BATCH_FN output_batch_fn) {\n+    return AppendAndOutput(\n+        num_rows_to_append,\n+        [&](int num_rows_to_append_left, int offset, int* num_rows_appended) {\n+          return AppendProbeOnly(key_and_payload, num_rows_to_append_left,\n+                                 row_ids + offset, num_rows_appended);\n+        },\n+        output_batch_fn);\n+  }\n+\n+  template <class OUTPUT_BATCH_FN>\n+  Status AppendBuildOnly(int num_rows_to_append, const uint32_t* key_ids,\n+                         const uint32_t* payload_ids, OUTPUT_BATCH_FN output_batch_fn) {\n+    return AppendAndOutput(\n+        num_rows_to_append,\n+        [&](int num_rows_to_append_left, int offset, int* num_rows_appended) {\n+          return AppendBuildOnly(\n+              num_rows_to_append_left, key_ids ? key_ids + offset : NULLPTR,\n+              payload_ids ? payload_ids + offset : NULLPTR, num_rows_appended);\n+        },\n+        output_batch_fn);\n+  }\n+\n+  template <class OUTPUT_BATCH_FN>\n+  Status Append(const ExecBatch& key_and_payload, int num_rows_to_append,\n+                const uint16_t* row_ids, const uint32_t* key_ids,\n+                const uint32_t* payload_ids, OUTPUT_BATCH_FN output_batch_fn) {\n+    return AppendAndOutput(\n+        num_rows_to_append,\n+        [&](int num_rows_to_append_left, int offset, int* num_rows_appended) {\n+          return Append(key_and_payload, num_rows_to_append_left,\n+                        row_ids ? row_ids + offset : NULLPTR,\n+                        key_ids ? key_ids + offset : NULLPTR,\n+                        payload_ids ? payload_ids + offset : NULLPTR, num_rows_appended);\n+        },\n+        output_batch_fn);\n+  }\n+\n+  template <class OUTPUT_BATCH_FN>\n+  Status Flush(OUTPUT_BATCH_FN output_batch_fn) {\n+    if (num_rows_ > 0) {\n+      ExecBatch batch({}, num_rows_);\n+      ARROW_RETURN_NOT_OK(Flush(&batch));\n+      output_batch_fn(std::move(batch));\n+    }\n+    return Status::OK();\n+  }\n+\n+  int64_t num_produced_batches() const { return num_produced_batches_; }\n+\n+ private:\n+  bool HasProbeOutput() const;\n+  bool HasBuildKeyOutput() const;\n+  bool HasBuildPayloadOutput() const;\n+  bool NeedsKeyId() const;\n+  bool NeedsPayloadId() const;\n+  Result<std::shared_ptr<ArrayData>> FlushBuildColumn(\n+      const std::shared_ptr<DataType>& data_type, const RowArray* row_array,\n+      int column_id, uint32_t* row_ids);\n+\n+  MemoryPool* pool_;\n+  const HashJoinProjectionMaps* probe_schemas_;\n+  const HashJoinProjectionMaps* build_schemas_;\n+  const RowArray* build_keys_;\n+  // Payload array pointer may be left as null, if no payload columns are\n+  // in the output column set.\n+  //\n+  const RowArray* build_payloads_;\n+  // If true, then ignore updating payload ids and use key ids instead when\n+  // reading.\n+  //\n+  bool payload_id_same_as_key_id_;\n+  std::vector<int> probe_output_to_key_and_payload_;\n+\n+  // Number of accumulated rows (since last flush)\n+  //\n+  int num_rows_;\n+  // Accumulated output columns from probe side batches.\n+  //\n+  ExecBatchBuilder batch_builder_;\n+  // Accumulated build side row references.\n+  //\n+  std::vector<uint32_t> key_ids_;\n+  std::vector<uint32_t> payload_ids_;\n+  // Information about ranges of rows from build side,\n+  // that in the accumulated materialized results have all fields set to null.\n+  //\n+  // Each pair contains index of the first output row in the range and the\n+  // length of the range. Only rows outside of these ranges have data present in\n+  // the key_ids_ and payload_ids_ arrays.\n+  //\n+  std::vector<std::pair<int, int>> null_ranges_;\n+\n+  int64_t num_produced_batches_;\n+};\n+\n+// Implements evaluating filter bit vector eliminating rows that do not have\n+// join matches due to nulls in key columns.\n\nReview Comment:\n   Can you expand on this comment (or on the filter method) to explain that this only applies to columns that are using the EQ comparison method?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.cc:\n##########\n@@ -0,0 +1,2545 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/row/compare_internal.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+int RowArrayAccessor::VarbinaryColumnId(const RowTableMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                                    const uint32_t* row_ids, int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                                  const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyColumnMetadata> column_metadatas;\n+  RETURN_NOT_OK(ColumnMetadatasFromExecBatch(batch, &column_metadatas));\n+  RowTableMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch,\n+                                      int begin_row_id, int end_row_id, int num_row_ids,\n+                                      const uint16_t* row_ids,\n+                                      std::vector<KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  RETURN_NOT_OK(ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                                          &temp_column_arrays));\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  Status status = ColumnArraysFromExecBatch(\n+      batch, begin_row_id, end_row_id - begin_row_id, &temp_column_arrays);\n+  ARROW_DCHECK(status.ok());\n+\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  ARROW_ASSIGN_OR_RAISE(KeyColumnMetadata column_metadata, output->column_metadata());\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const RowTableMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_batch_start_row,\n+                                 int in_batch_end_row,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_num_selected,\n+                                 const uint16_t* in_selection,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays,\n+                                 std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Status status =\n+      Hashing32::HashBatch(*input->batch, hashes, *input->temp_column_arrays,\n+                           hardware_flags, input->temp_stack, input->batch_start_row,\n+                           input->batch_end_row - input->batch_start_row);\n+  ARROW_DCHECK(status.ok());\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+                               uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+                               util::TempVectorStack* temp_stack,\n+                               std::vector<KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+                                    bool reject_duplicate_keys, bool no_payload,\n+                                    const std::vector<KeyColumnMetadata>& key_types,\n+                                    const std::vector<KeyColumnMetadata>& payload_types,\n+                                    MemoryPool* pool, int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(dop_, num_prtns_);\n+\n+  RowTableMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  RowTableMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  RETURN_NOT_OK(Hashing32::HashBatch(\n+      key_batch, locals.batch_hashes.data(), locals.temp_column_arrays, hardware_flags_,\n+      temp_stack, /*start_row=*/0, static_cast<int>(key_batch.length)));\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int64_t i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int64_t i, int pos) {\n+          locals.batch_prtn_row_ids[pos] = static_cast<uint16_t>(i);\n+        });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      thread_id, locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n+  }\n+}\n+\n+void SwissTableForJoinBuild::FinishPrtnMerge(util::TempVectorStack* temp_stack) {\n+  // Process overflow key ids\n+  //\n+  for (int prtn_id = 0; prtn_id < num_prtns_; ++prtn_id) {\n+    SwissTableMerge::InsertNewGroups(target_->map_.swiss_table(),\n+                                     prtn_states_[prtn_id].overflow_key_ids,\n+                                     prtn_states_[prtn_id].overflow_hashes);\n+  }\n+\n+  // Calculate whether we have nulls in hash table keys\n+  // (it is lazily evaluated but since we will be accessing it from multiple\n+  // threads we need to make sure that the value gets calculated here).\n+  //\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags_;\n+  ctx.stack = temp_stack;\n+  std::ignore = target_->map_.keys()->rows_.has_any_nulls(&ctx);\n+}\n+\n+void JoinResultMaterialize::Init(MemoryPool* pool,\n+                                 const HashJoinProjectionMaps* probe_schemas,\n+                                 const HashJoinProjectionMaps* build_schemas) {\n+  pool_ = pool;\n+  probe_schemas_ = probe_schemas;\n+  build_schemas_ = build_schemas;\n+  num_rows_ = 0;\n+  null_ranges_.clear();\n+  num_produced_batches_ = 0;\n+\n+  // Initialize mapping of columns from output batch column index to key and\n+  // payload batch column index.\n+  //\n+  probe_output_to_key_and_payload_.resize(\n+      probe_schemas_->num_cols(HashJoinProjection::OUTPUT));\n+  int num_key_cols = probe_schemas_->num_cols(HashJoinProjection::KEY);\n+  auto to_key = probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  auto to_payload =\n+      probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; static_cast<size_t>(i) < probe_output_to_key_and_payload_.size(); ++i) {\n+    probe_output_to_key_and_payload_[i] =\n+        to_key.get(i) == SchemaProjectionMap::kMissingField\n+            ? to_payload.get(i) + num_key_cols\n+            : to_key.get(i);\n+  }\n+}\n+\n+void JoinResultMaterialize::SetBuildSide(const RowArray* build_keys,\n+                                         const RowArray* build_payloads,\n+                                         bool payload_id_same_as_key_id) {\n+  build_keys_ = build_keys;\n+  build_payloads_ = build_payloads;\n+  payload_id_same_as_key_id_ = payload_id_same_as_key_id;\n+}\n+\n+bool JoinResultMaterialize::HasProbeOutput() const {\n+  return probe_schemas_->num_cols(HashJoinProjection::OUTPUT) > 0;\n+}\n+\n+bool JoinResultMaterialize::HasBuildKeyOutput() const {\n+  auto to_key = build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_key.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::HasBuildPayloadOutput() const {\n+  auto to_payload =\n+      build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_payload.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::NeedsKeyId() const {\n+  return HasBuildKeyOutput() || (HasBuildPayloadOutput() && payload_id_same_as_key_id_);\n+}\n+\n+bool JoinResultMaterialize::NeedsPayloadId() const {\n+  return HasBuildPayloadOutput() && !payload_id_same_as_key_id_;\n+}\n+\n+Status JoinResultMaterialize::AppendProbeOnly(const ExecBatch& key_and_payload,\n\nReview Comment:\n   Why is there no `if (NeedsPayloadId())` like there are in the other appends?  Wouldn't it be possible to want the payload id and payload in something like a probe side semi join?\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.cc:\n##########\n@@ -0,0 +1,2545 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/row/compare_internal.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+int RowArrayAccessor::VarbinaryColumnId(const RowTableMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                                    const uint32_t* row_ids, int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                                  const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyColumnMetadata> column_metadatas;\n+  RETURN_NOT_OK(ColumnMetadatasFromExecBatch(batch, &column_metadatas));\n+  RowTableMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch,\n+                                      int begin_row_id, int end_row_id, int num_row_ids,\n+                                      const uint16_t* row_ids,\n+                                      std::vector<KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  RETURN_NOT_OK(ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                                          &temp_column_arrays));\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  Status status = ColumnArraysFromExecBatch(\n+      batch, begin_row_id, end_row_id - begin_row_id, &temp_column_arrays);\n+  ARROW_DCHECK(status.ok());\n+\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  ARROW_ASSIGN_OR_RAISE(KeyColumnMetadata column_metadata, output->column_metadata());\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const RowTableMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_batch_start_row,\n+                                 int in_batch_end_row,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_num_selected,\n+                                 const uint16_t* in_selection,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays,\n+                                 std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Status status =\n+      Hashing32::HashBatch(*input->batch, hashes, *input->temp_column_arrays,\n+                           hardware_flags, input->temp_stack, input->batch_start_row,\n+                           input->batch_end_row - input->batch_start_row);\n+  ARROW_DCHECK(status.ok());\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+                               uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+                               util::TempVectorStack* temp_stack,\n+                               std::vector<KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+                                    bool reject_duplicate_keys, bool no_payload,\n+                                    const std::vector<KeyColumnMetadata>& key_types,\n+                                    const std::vector<KeyColumnMetadata>& payload_types,\n+                                    MemoryPool* pool, int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(dop_, num_prtns_);\n+\n+  RowTableMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  RowTableMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  RETURN_NOT_OK(Hashing32::HashBatch(\n+      key_batch, locals.batch_hashes.data(), locals.temp_column_arrays, hardware_flags_,\n+      temp_stack, /*start_row=*/0, static_cast<int>(key_batch.length)));\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int64_t i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int64_t i, int pos) {\n+          locals.batch_prtn_row_ids[pos] = static_cast<uint16_t>(i);\n+        });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      thread_id, locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n+  }\n+}\n+\n+void SwissTableForJoinBuild::FinishPrtnMerge(util::TempVectorStack* temp_stack) {\n+  // Process overflow key ids\n+  //\n+  for (int prtn_id = 0; prtn_id < num_prtns_; ++prtn_id) {\n+    SwissTableMerge::InsertNewGroups(target_->map_.swiss_table(),\n+                                     prtn_states_[prtn_id].overflow_key_ids,\n+                                     prtn_states_[prtn_id].overflow_hashes);\n+  }\n+\n+  // Calculate whether we have nulls in hash table keys\n+  // (it is lazily evaluated but since we will be accessing it from multiple\n+  // threads we need to make sure that the value gets calculated here).\n+  //\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags_;\n+  ctx.stack = temp_stack;\n+  std::ignore = target_->map_.keys()->rows_.has_any_nulls(&ctx);\n+}\n+\n+void JoinResultMaterialize::Init(MemoryPool* pool,\n+                                 const HashJoinProjectionMaps* probe_schemas,\n+                                 const HashJoinProjectionMaps* build_schemas) {\n+  pool_ = pool;\n+  probe_schemas_ = probe_schemas;\n+  build_schemas_ = build_schemas;\n+  num_rows_ = 0;\n+  null_ranges_.clear();\n+  num_produced_batches_ = 0;\n+\n+  // Initialize mapping of columns from output batch column index to key and\n+  // payload batch column index.\n+  //\n+  probe_output_to_key_and_payload_.resize(\n+      probe_schemas_->num_cols(HashJoinProjection::OUTPUT));\n+  int num_key_cols = probe_schemas_->num_cols(HashJoinProjection::KEY);\n+  auto to_key = probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  auto to_payload =\n+      probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; static_cast<size_t>(i) < probe_output_to_key_and_payload_.size(); ++i) {\n+    probe_output_to_key_and_payload_[i] =\n+        to_key.get(i) == SchemaProjectionMap::kMissingField\n+            ? to_payload.get(i) + num_key_cols\n+            : to_key.get(i);\n+  }\n+}\n+\n+void JoinResultMaterialize::SetBuildSide(const RowArray* build_keys,\n+                                         const RowArray* build_payloads,\n+                                         bool payload_id_same_as_key_id) {\n+  build_keys_ = build_keys;\n+  build_payloads_ = build_payloads;\n+  payload_id_same_as_key_id_ = payload_id_same_as_key_id;\n+}\n+\n+bool JoinResultMaterialize::HasProbeOutput() const {\n+  return probe_schemas_->num_cols(HashJoinProjection::OUTPUT) > 0;\n+}\n+\n+bool JoinResultMaterialize::HasBuildKeyOutput() const {\n+  auto to_key = build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_key.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::HasBuildPayloadOutput() const {\n+  auto to_payload =\n+      build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_payload.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::NeedsKeyId() const {\n+  return HasBuildKeyOutput() || (HasBuildPayloadOutput() && payload_id_same_as_key_id_);\n+}\n+\n+bool JoinResultMaterialize::NeedsPayloadId() const {\n+  return HasBuildPayloadOutput() && !payload_id_same_as_key_id_;\n+}\n+\n+Status JoinResultMaterialize::AppendProbeOnly(const ExecBatch& key_and_payload,\n+                                              int num_rows_to_append,\n+                                              const uint16_t* row_ids,\n+                                              int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendSelected(\n+        pool_, key_and_payload, num_rows_to_append, row_ids,\n+        static_cast<int>(probe_output_to_key_and_payload_.size()),\n+        probe_output_to_key_and_payload_.data()));\n+  }\n+  if (!null_ranges_.empty() &&\n+      null_ranges_.back().first + null_ranges_.back().second == num_rows_) {\n+    // We can extend the last range of null rows on build side.\n+    //\n+    null_ranges_.back().second += num_rows_to_append;\n+  } else {\n+    null_ranges_.push_back(\n+        std::make_pair(static_cast<int>(num_rows_), num_rows_to_append));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Status JoinResultMaterialize::AppendBuildOnly(int num_rows_to_append,\n+                                              const uint32_t* key_ids,\n+                                              const uint32_t* payload_ids,\n+                                              int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendNulls(\n+        pool_, probe_schemas_->data_types(HashJoinProjection::OUTPUT),\n+        num_rows_to_append));\n+  }\n+  if (NeedsKeyId()) {\n+    ARROW_DCHECK(key_ids != nullptr);\n+    key_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(key_ids_.data() + num_rows_, key_ids, num_rows_to_append * sizeof(uint32_t));\n+  }\n+  if (NeedsPayloadId()) {\n+    ARROW_DCHECK(payload_ids != nullptr);\n+    payload_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(payload_ids_.data() + num_rows_, payload_ids,\n+           num_rows_to_append * sizeof(uint32_t));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Status JoinResultMaterialize::Append(const ExecBatch& key_and_payload,\n+                                     int num_rows_to_append, const uint16_t* row_ids,\n+                                     const uint32_t* key_ids, const uint32_t* payload_ids,\n+                                     int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendSelected(\n+        pool_, key_and_payload, num_rows_to_append, row_ids,\n+        static_cast<int>(probe_output_to_key_and_payload_.size()),\n+        probe_output_to_key_and_payload_.data()));\n+  }\n+  if (NeedsKeyId()) {\n+    ARROW_DCHECK(key_ids != nullptr);\n+    key_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(key_ids_.data() + num_rows_, key_ids, num_rows_to_append * sizeof(uint32_t));\n+  }\n+  if (NeedsPayloadId()) {\n+    ARROW_DCHECK(payload_ids != nullptr);\n+    payload_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(payload_ids_.data() + num_rows_, payload_ids,\n+           num_rows_to_append * sizeof(uint32_t));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Result<std::shared_ptr<ArrayData>> JoinResultMaterialize::FlushBuildColumn(\n+    const std::shared_ptr<DataType>& data_type, const RowArray* row_array, int column_id,\n+    uint32_t* row_ids) {\n+  ResizableArrayData output;\n+  output.Init(data_type, pool_, bit_util::Log2(num_rows_));\n+\n+  for (size_t i = 0; i <= null_ranges_.size(); ++i) {\n+    int row_id_begin =\n+        i == 0 ? 0 : null_ranges_[i - 1].first + null_ranges_[i - 1].second;\n+    int row_id_end = i == null_ranges_.size() ? num_rows_ : null_ranges_[i].first;\n+    if (row_id_end > row_id_begin) {\n+      RETURN_NOT_OK(row_array->DecodeSelected(\n+          &output, column_id, row_id_end - row_id_begin, row_ids + row_id_begin, pool_));\n+    }\n+    int num_nulls = i == null_ranges_.size() ? 0 : null_ranges_[i].second;\n+    if (num_nulls > 0) {\n+      RETURN_NOT_OK(ExecBatchBuilder::AppendNulls(data_type, output, num_nulls, pool_));\n+    }\n+  }\n+\n+  return output.array_data();\n+}\n+\n+Status JoinResultMaterialize::Flush(ExecBatch* out) {\n+  ARROW_DCHECK(num_rows_ > 0);\n+  out->length = num_rows_;\n+  out->values.clear();\n+\n+  int num_probe_cols = probe_schemas_->num_cols(HashJoinProjection::OUTPUT);\n+  int num_build_cols = build_schemas_->num_cols(HashJoinProjection::OUTPUT);\n+  out->values.resize(num_probe_cols + num_build_cols);\n+\n+  if (HasProbeOutput()) {\n+    ExecBatch probe_batch = batch_builder_.Flush();\n+    ARROW_DCHECK(static_cast<int>(probe_batch.values.size()) == num_probe_cols);\n+    for (size_t i = 0; i < probe_batch.values.size(); ++i) {\n+      out->values[i] = std::move(probe_batch.values[i]);\n+    }\n+  }\n+  auto to_key = build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  auto to_payload =\n+      build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; i < num_build_cols; ++i) {\n+    if (to_key.get(i) != SchemaProjectionMap::kMissingField) {\n+      std::shared_ptr<ArrayData> column;\n+      ARROW_ASSIGN_OR_RAISE(\n+          column,\n+          FlushBuildColumn(build_schemas_->data_type(HashJoinProjection::OUTPUT, i),\n+                           build_keys_, to_key.get(i), key_ids_.data()));\n+      out->values[num_probe_cols + i] = std::move(column);\n+    } else if (to_payload.get(i) != SchemaProjectionMap::kMissingField) {\n+      std::shared_ptr<ArrayData> column;\n+      ARROW_ASSIGN_OR_RAISE(\n+          column,\n+          FlushBuildColumn(\n+              build_schemas_->data_type(HashJoinProjection::OUTPUT, i), build_payloads_,\n+              to_payload.get(i),\n+              payload_id_same_as_key_id_ ? key_ids_.data() : payload_ids_.data()));\n+      out->values[num_probe_cols + i] = std::move(column);\n+    } else {\n+      ARROW_DCHECK(false);\n+    }\n+  }\n+\n+  num_rows_ = 0;\n+  key_ids_.clear();\n+  payload_ids_.clear();\n+  null_ranges_.clear();\n+\n+  ++num_produced_batches_;\n+\n+  return Status::OK();\n+}\n+\n+void JoinNullFilter::Filter(const ExecBatch& key_batch, int batch_start_row,\n+                            int num_batch_rows, const std::vector<JoinKeyCmp>& cmp,\n+                            bool* all_valid, bool and_with_input,\n+                            uint8_t* inout_bit_vector) {\n+  // AND together validity vectors for columns that use equality comparison.\n+  //\n+  bool is_output_initialized = and_with_input;\n+  for (size_t i = 0; i < cmp.size(); ++i) {\n+    // No null filtering if null == null is true\n+    //\n+    if (cmp[i] != JoinKeyCmp::EQ) {\n+      continue;\n+    }\n+\n+    // No null filtering when there are no nulls\n+    //\n+    const Datum& data = key_batch.values[i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    if (!array_data->buffers[0]) {\n+      continue;\n+    }\n+\n+    const uint8_t* non_null_buffer = array_data->buffers[0]->data();\n+    int64_t offset = array_data->offset + batch_start_row;\n+\n+    // Filter out nulls for this column\n+    //\n+    if (!is_output_initialized) {\n+      memset(inout_bit_vector, 0xff, bit_util::BytesForBits(num_batch_rows));\n+      is_output_initialized = true;\n+    }\n+    arrow::internal::BitmapAnd(inout_bit_vector, 0, non_null_buffer, offset,\n+                               num_batch_rows, 0, inout_bit_vector);\n+  }\n+  *all_valid = !is_output_initialized;\n+}\n+\n+void JoinMatchIterator::SetLookupResult(int num_batch_rows, int start_batch_row,\n+                                        const uint8_t* batch_has_match,\n+                                        const uint32_t* key_ids, bool no_duplicate_keys,\n+                                        const uint32_t* key_to_payload) {\n+  num_batch_rows_ = num_batch_rows;\n+  start_batch_row_ = start_batch_row;\n+  batch_has_match_ = batch_has_match;\n+  key_ids_ = key_ids;\n+\n+  no_duplicate_keys_ = no_duplicate_keys;\n+  key_to_payload_ = key_to_payload;\n+\n+  current_row_ = 0;\n+  current_match_for_row_ = 0;\n+}\n+\n+bool JoinMatchIterator::GetNextBatch(int num_rows_max, int* out_num_rows,\n+                                     uint16_t* batch_row_ids, uint32_t* key_ids,\n+                                     uint32_t* payload_ids) {\n+  *out_num_rows = 0;\n+\n+  if (no_duplicate_keys_) {\n+    // When every input key can have at most one match,\n+    // then we only need to filter according to has match bit vector.\n+    //\n+    // We stop when either we produce a full batch or when we reach the end of\n+    // matches to output.\n+    //\n+    while (current_row_ < num_batch_rows_ && *out_num_rows < num_rows_max) {\n+      batch_row_ids[*out_num_rows] = start_batch_row_ + current_row_;\n+      key_ids[*out_num_rows] = payload_ids[*out_num_rows] = key_ids_[current_row_];\n+      (*out_num_rows) += bit_util::GetBit(batch_has_match_, current_row_) ? 1 : 0;\n+      ++current_row_;\n+    }\n+  } else {\n+    // When every input key can have zero, one or many matches,\n+    // then we need to filter out ones with no match and\n+    // iterate over all matches for the remaining ones.\n+    //\n+    // We stop when either we produce a full batch or when we reach the end of\n+    // matches to output.\n+    //\n+    while (current_row_ < num_batch_rows_ && *out_num_rows < num_rows_max) {\n+      if (!bit_util::GetBit(batch_has_match_, current_row_)) {\n+        ++current_row_;\n+        current_match_for_row_ = 0;\n+        continue;\n+      }\n+      uint32_t base_payload_id = key_to_payload_[key_ids_[current_row_]];\n+\n+      // Total number of matches for the currently selected input row\n+      //\n+      int num_matches_total =\n+          key_to_payload_[key_ids_[current_row_] + 1] - base_payload_id;\n+\n+      // Number of remaining matches for the currently selected input row\n+      //\n+      int num_matches_left = num_matches_total - current_match_for_row_;\n+\n+      // Number of matches for the currently selected input row that will fit\n+      // into the next batch\n+      //\n+      int num_matches_next = std::min(num_matches_left, num_rows_max - *out_num_rows);\n+\n+      for (int imatch = 0; imatch < num_matches_next; ++imatch) {\n+        batch_row_ids[*out_num_rows] = start_batch_row_ + current_row_;\n+        key_ids[*out_num_rows] = key_ids_[current_row_];\n+        payload_ids[*out_num_rows] = base_payload_id + current_match_for_row_ + imatch;\n+        ++(*out_num_rows);\n+      }\n+      current_match_for_row_ += num_matches_next;\n+\n+      if (current_match_for_row_ == num_matches_total) {\n+        ++current_row_;\n+        current_match_for_row_ = 0;\n+      }\n+    }\n+  }\n+\n+  return (*out_num_rows) > 0;\n+}\n+\n+void JoinProbeProcessor::Init(int num_key_columns, JoinType join_type,\n+                              SwissTableForJoin* hash_table,\n+                              std::vector<JoinResultMaterialize*> materialize,\n+                              const std::vector<JoinKeyCmp>* cmp,\n+                              OutputBatchFn output_batch_fn) {\n+  num_key_columns_ = num_key_columns;\n+  join_type_ = join_type;\n+  hash_table_ = hash_table;\n+  materialize_.resize(materialize.size());\n+  for (size_t i = 0; i < materialize.size(); ++i) {\n+    materialize_[i] = materialize[i];\n+  }\n+  cmp_ = cmp;\n+  output_batch_fn_ = output_batch_fn;\n+}\n+\n+Status JoinProbeProcessor::OnNextBatch(int64_t thread_id,\n+                                       const ExecBatch& keypayload_batch,\n+                                       util::TempVectorStack* temp_stack,\n+                                       std::vector<KeyColumnArray>* temp_column_arrays) {\n+  const SwissTable* swiss_table = hash_table_->keys()->swiss_table();\n+  int64_t hardware_flags = swiss_table->hardware_flags();\n+  int minibatch_size = swiss_table->minibatch_size();\n+  int num_rows = static_cast<int>(keypayload_batch.length);\n+\n+  ExecBatch key_batch({}, keypayload_batch.length);\n+  key_batch.values.resize(num_key_columns_);\n+  for (int i = 0; i < num_key_columns_; ++i) {\n+    key_batch.values[i] = keypayload_batch.values[i];\n+  }\n+\n+  // Break into mini-batches\n+  //\n+  // Start by allocating mini-batch buffers\n+  //\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack, static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)));\n+  auto key_ids_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto materialize_batch_ids_buf =\n+      util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size);\n+  auto materialize_key_ids_buf =\n+      util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto materialize_payload_ids_buf =\n+      util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input input(&key_batch, minibatch_start,\n+                                    minibatch_start + minibatch_size_next, temp_stack,\n+                                    temp_column_arrays);\n+    hash_table_->keys()->Hash(&input, hashes_buf.mutable_data(), hardware_flags);\n+    hash_table_->keys()->MapReadOnly(&input, hashes_buf.mutable_data(),\n+                                     match_bitvector_buf.mutable_data(),\n+                                     key_ids_buf.mutable_data());\n+\n+    // AND bit vector with null key filter for join\n+    //\n+    bool ignored;\n+    JoinNullFilter::Filter(key_batch, minibatch_start, minibatch_size_next, *cmp_,\n+                           &ignored,\n+                           /*and_with_input=*/true, match_bitvector_buf.mutable_data());\n+    // Semi-joins\n+    //\n+    if (join_type_ == JoinType::LEFT_SEMI || join_type_ == JoinType::LEFT_ANTI ||\n+        join_type_ == JoinType::RIGHT_SEMI || join_type_ == JoinType::RIGHT_ANTI) {\n+      int num_passing_ids = 0;\n+      util::bit_util::bits_to_indexes(\n+          (join_type_ == JoinType::LEFT_ANTI) ? 0 : 1, hardware_flags,\n+          minibatch_size_next, match_bitvector_buf.mutable_data(), &num_passing_ids,\n+          materialize_batch_ids_buf.mutable_data());\n+\n+      // For right-semi, right-anti joins: update has-match flags for the rows\n+      // in hash table.\n+      //\n+      if (join_type_ == JoinType::RIGHT_SEMI || join_type_ == JoinType::RIGHT_ANTI) {\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          uint16_t id = materialize_batch_ids_buf.mutable_data()[i];\n+          key_ids_buf.mutable_data()[i] = key_ids_buf.mutable_data()[id];\n+        }\n+        hash_table_->UpdateHasMatchForKeys(thread_id, num_passing_ids,\n+                                           key_ids_buf.mutable_data());\n+      } else {\n+        // For left-semi, left-anti joins: call materialize using match\n+        // bit-vector.\n+        //\n+\n+        // Add base batch row index.\n+        //\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          materialize_batch_ids_buf.mutable_data()[i] +=\n+              static_cast<uint16_t>(minibatch_start);\n+        }\n+\n+        RETURN_NOT_OK(materialize_[thread_id]->AppendProbeOnly(\n+            keypayload_batch, num_passing_ids, materialize_batch_ids_buf.mutable_data(),\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+    } else {\n+      // We need to output matching pairs of rows from both sides of the join.\n+      // Since every hash table lookup for an input row might have multiple\n+      // matches we use a helper class that implements enumerating all of them.\n+      //\n+      bool no_duplicate_keys = (hash_table_->key_to_payload() == nullptr);\n+      bool no_payload_columns = (hash_table_->payloads() == nullptr);\n+      JoinMatchIterator match_iterator;\n+      match_iterator.SetLookupResult(\n+          minibatch_size_next, minibatch_start, match_bitvector_buf.mutable_data(),\n+          key_ids_buf.mutable_data(), no_duplicate_keys, hash_table_->key_to_payload());\n+      int num_matches_next;\n+      while (match_iterator.GetNextBatch(minibatch_size, &num_matches_next,\n+                                         materialize_batch_ids_buf.mutable_data(),\n+                                         materialize_key_ids_buf.mutable_data(),\n+                                         materialize_payload_ids_buf.mutable_data())) {\n+        const uint16_t* materialize_batch_ids = materialize_batch_ids_buf.mutable_data();\n+        const uint32_t* materialize_key_ids = materialize_key_ids_buf.mutable_data();\n+        const uint32_t* materialize_payload_ids =\n+            no_duplicate_keys || no_payload_columns\n+                ? materialize_key_ids_buf.mutable_data()\n+                : materialize_payload_ids_buf.mutable_data();\n+\n+        // For right-outer, full-outer joins we need to update has-match flags\n+        // for the rows in hash table.\n+        //\n+        if (join_type_ == JoinType::RIGHT_OUTER || join_type_ == JoinType::FULL_OUTER) {\n+          hash_table_->UpdateHasMatchForKeys(thread_id, num_matches_next,\n+                                             materialize_key_ids);\n+        }\n+\n+        // Call materialize for resulting id tuples pointing to matching pairs\n+        // of rows.\n+        //\n+        RETURN_NOT_OK(materialize_[thread_id]->Append(\n+            keypayload_batch, num_matches_next, materialize_batch_ids,\n+            materialize_key_ids, materialize_payload_ids,\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+\n+      // For left-outer and full-outer joins output non-matches.\n+      //\n+      // Call materialize. Nulls will be output in all columns that come from\n+      // the other side of the join.\n+      //\n+      if (join_type_ == JoinType::LEFT_OUTER || join_type_ == JoinType::FULL_OUTER) {\n+        int num_passing_ids = 0;\n+        util::bit_util::bits_to_indexes(\n+            /*bit_to_search=*/0, hardware_flags, minibatch_size_next,\n+            match_bitvector_buf.mutable_data(), &num_passing_ids,\n+            materialize_batch_ids_buf.mutable_data());\n+\n+        // Add base batch row index.\n+        //\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          materialize_batch_ids_buf.mutable_data()[i] +=\n+              static_cast<uint16_t>(minibatch_start);\n+        }\n+\n+        RETURN_NOT_OK(materialize_[thread_id]->AppendProbeOnly(\n+            keypayload_batch, num_passing_ids, materialize_batch_ids_buf.mutable_data(),\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status JoinProbeProcessor::OnFinished() {\n+  // Flush all instances of materialize that have non-zero accumulated output\n+  // rows.\n+  //\n+  for (size_t i = 0; i < materialize_.size(); ++i) {\n+    JoinResultMaterialize& materialize = *materialize_[i];\n+    if (materialize.num_rows() > 0) {\n+      RETURN_NOT_OK(materialize.Flush(\n+          [&](ExecBatch batch) { output_batch_fn_(i, std::move(batch)); }));\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+class SwissJoin : public HashJoinImpl {\n+ public:\n+  Status Init(ExecContext* ctx, JoinType join_type, size_t num_threads,\n+              const HashJoinProjectionMaps* proj_map_left,\n+              const HashJoinProjectionMaps* proj_map_right,\n+              std::vector<JoinKeyCmp> key_cmp, Expression filter,\n+              OutputBatchCallback output_batch_callback,\n+              FinishedCallback finished_callback, TaskScheduler* scheduler) override {\n+    START_COMPUTE_SPAN(span_, \"SwissJoinImpl\",\n+                       {{\"detail\", filter.ToString()},\n+                        {\"join.kind\", ToString(join_type)},\n+                        {\"join.threads\", static_cast<uint32_t>(num_threads)}});\n+\n+    num_threads_ = static_cast<int>(num_threads);\n+    ctx_ = ctx;\n+    hardware_flags_ = ctx->cpu_info()->hardware_flags();\n+    pool_ = ctx->memory_pool();\n+\n+    join_type_ = join_type;\n+    key_cmp_.resize(key_cmp.size());\n+    for (size_t i = 0; i < key_cmp.size(); ++i) {\n+      key_cmp_[i] = key_cmp[i];\n+    }\n+    schema_[0] = proj_map_left;\n+    schema_[1] = proj_map_right;\n+    output_batch_callback_ = output_batch_callback;\n+    finished_callback_ = finished_callback;\n+    scheduler_ = scheduler;\n+    hash_table_ready_.store(false);\n+    cancelled_.store(false);\n+    {\n+      std::lock_guard<std::mutex> lock(state_mutex_);\n+      left_side_finished_ = false;\n+      right_side_finished_ = false;\n+      error_status_ = Status::OK();\n+    }\n+\n+    local_states_.resize(num_threads_);\n+    for (int i = 0; i < num_threads_; ++i) {\n+      local_states_[i].hash_table_ready = false;\n+      local_states_[i].num_output_batches = 0;\n+      RETURN_NOT_OK(CancelIfNotOK(local_states_[i].temp_stack.Init(\n+          pool_, 1024 + 64 * util::MiniBatch::kMiniBatchLength)));\n+      local_states_[i].materialize.Init(pool_, proj_map_left, proj_map_right);\n+    }\n+\n+    std::vector<JoinResultMaterialize*> materialize;\n+    materialize.resize(num_threads_);\n+    for (int i = 0; i < num_threads_; ++i) {\n+      materialize[i] = &local_states_[i].materialize;\n+    }\n+\n+    probe_processor_.Init(proj_map_left->num_cols(HashJoinProjection::KEY), join_type_,\n+                          &hash_table_, materialize, &key_cmp_, output_batch_callback_);\n+\n+    InitTaskGroups();\n+\n+    return Status::OK();\n+  }\n+\n+  void InitTaskGroups() {\n+    task_group_build_ = scheduler_->RegisterTaskGroup(\n+        [this](size_t thread_index, int64_t task_id) -> Status {\n+          return BuildTask(thread_index, task_id);\n+        },\n+        [this](size_t thread_index) -> Status { return BuildFinished(thread_index); });\n+    task_group_merge_ = scheduler_->RegisterTaskGroup(\n+        [this](size_t thread_index, int64_t task_id) -> Status {\n+          return MergeTask(thread_index, task_id);\n+        },\n+        [this](size_t thread_index) -> Status { return MergeFinished(thread_index); });\n+    task_group_scan_ = scheduler_->RegisterTaskGroup(\n+        [this](size_t thread_index, int64_t task_id) -> Status {\n+          return ScanTask(thread_index, task_id);\n+        },\n+        [this](size_t thread_index) -> Status { return ScanFinished(thread_index); });\n+  }\n+\n+  Status ProbeSingleBatch(size_t thread_index, ExecBatch batch) override {\n+    if (IsCancelled()) {\n+      return status();\n+    }\n+\n+    if (!local_states_[thread_index].hash_table_ready) {\n+      local_states_[thread_index].hash_table_ready = hash_table_ready_.load();\n+    }\n+    ARROW_DCHECK(local_states_[thread_index].hash_table_ready);\n+\n+    ExecBatch keypayload_batch;\n+    ARROW_ASSIGN_OR_RAISE(keypayload_batch, KeyPayloadFromInput(/*side=*/0, &batch));\n+\n+    return CancelIfNotOK(probe_processor_.OnNextBatch(\n+        thread_index, keypayload_batch, &local_states_[thread_index].temp_stack,\n+        &local_states_[thread_index].temp_column_arrays));\n+  }\n+\n+  Status ProbingFinished(size_t thread_index) override {\n+    if (IsCancelled()) {\n+      return status();\n+    }\n+\n+    return CancelIfNotOK(ScanHashTableAsync(static_cast<int64_t>(thread_index)));\n+  }\n+\n+  Status BuildHashTable(size_t thread_id, AccumulationQueue batches,\n+                        BuildFinishedCallback on_finished) override {\n+    if (IsCancelled()) {\n+      return status();\n+    }\n+\n+    build_side_batches_ = std::move(batches);\n+    build_finished_callback_ = on_finished;\n+\n+    return CancelIfNotOK(BuildHashTableAsync(static_cast<int64_t>(thread_id)));\n+  }\n+\n+  void Abort(TaskScheduler::AbortContinuationImpl pos_abort_callback) override {\n+    EVENT(span_, \"Abort\");\n+    END_SPAN(span_);\n+    std::ignore = CancelIfNotOK(Status::Cancelled(\"Hash Join Cancelled\"));\n+    scheduler_->Abort(std::move(pos_abort_callback));\n+  }\n+\n+ private:\n+  Status BuildHashTableAsync(int64_t thread_id) {\n\nReview Comment:\n   Minor nit: I kind of prefer something like `StartBuildHashTable` over `BuildHashTableAsync` since we use the `Async` suffix elsewhere to indicate:\r\n   \r\n    * The work is not done on the CPU thread pool\r\n    * The method returns a future\r\n   \r\n   Same comment for ScanHashTableAsync\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n\nReview Comment:\n   Minor nit: Everywhere else we generally refer to these ids as \"key ids\" and here we refer to them as \"group ids\" which are (I think) the same thing?  I'm a little confused on the point.\n\n\n\n##########\ncpp/src/arrow/compute/exec/partition_util.h:\n##########\n@@ -118,6 +118,43 @@ class PartitionLocks {\n   /// \\brief Release a partition so that other threads can work on it\n   void ReleasePartitionLock(int prtn_id);\n \n+  template <typename IS_PRTN_EMPTY_FN, typename PROCESS_PRTN_FN>\n+  Status ForEachPartition(size_t thread_id, int* temp_unprocessed_prtns,\n\nReview Comment:\n   Minor nit: I'm not sure we need `temp_` in `temp_unprocessed_prtns`.\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n\nReview Comment:\n   Do we want to move SwissTableWithKeys and SwissTableMerge into `key_map.h`?  These seem like general purpose utilities that aren't necessarily specific to join.  The rest of this file though seems very specific to hash join.  Although I suppose both types are oriented around a model of \"do inserts first then do reads\"\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.cc:\n##########\n@@ -0,0 +1,2545 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/row/compare_internal.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+int RowArrayAccessor::VarbinaryColumnId(const RowTableMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                                    const uint32_t* row_ids, int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                                  const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyColumnMetadata> column_metadatas;\n+  RETURN_NOT_OK(ColumnMetadatasFromExecBatch(batch, &column_metadatas));\n+  RowTableMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch,\n+                                      int begin_row_id, int end_row_id, int num_row_ids,\n+                                      const uint16_t* row_ids,\n+                                      std::vector<KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  RETURN_NOT_OK(ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                                          &temp_column_arrays));\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  Status status = ColumnArraysFromExecBatch(\n+      batch, begin_row_id, end_row_id - begin_row_id, &temp_column_arrays);\n+  ARROW_DCHECK(status.ok());\n+\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  ARROW_ASSIGN_OR_RAISE(KeyColumnMetadata column_metadata, output->column_metadata());\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const RowTableMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_batch_start_row,\n+                                 int in_batch_end_row,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_num_selected,\n+                                 const uint16_t* in_selection,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays,\n+                                 std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Status status =\n+      Hashing32::HashBatch(*input->batch, hashes, *input->temp_column_arrays,\n+                           hardware_flags, input->temp_stack, input->batch_start_row,\n+                           input->batch_end_row - input->batch_start_row);\n+  ARROW_DCHECK(status.ok());\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+                               uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+                               util::TempVectorStack* temp_stack,\n+                               std::vector<KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+                                    bool reject_duplicate_keys, bool no_payload,\n+                                    const std::vector<KeyColumnMetadata>& key_types,\n+                                    const std::vector<KeyColumnMetadata>& payload_types,\n+                                    MemoryPool* pool, int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(dop_, num_prtns_);\n+\n+  RowTableMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  RowTableMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  RETURN_NOT_OK(Hashing32::HashBatch(\n+      key_batch, locals.batch_hashes.data(), locals.temp_column_arrays, hardware_flags_,\n+      temp_stack, /*start_row=*/0, static_cast<int>(key_batch.length)));\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int64_t i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int64_t i, int pos) {\n+          locals.batch_prtn_row_ids[pos] = static_cast<uint16_t>(i);\n+        });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      thread_id, locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n+  }\n+}\n+\n+void SwissTableForJoinBuild::FinishPrtnMerge(util::TempVectorStack* temp_stack) {\n+  // Process overflow key ids\n+  //\n+  for (int prtn_id = 0; prtn_id < num_prtns_; ++prtn_id) {\n+    SwissTableMerge::InsertNewGroups(target_->map_.swiss_table(),\n+                                     prtn_states_[prtn_id].overflow_key_ids,\n+                                     prtn_states_[prtn_id].overflow_hashes);\n+  }\n+\n+  // Calculate whether we have nulls in hash table keys\n+  // (it is lazily evaluated but since we will be accessing it from multiple\n+  // threads we need to make sure that the value gets calculated here).\n+  //\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags_;\n+  ctx.stack = temp_stack;\n+  std::ignore = target_->map_.keys()->rows_.has_any_nulls(&ctx);\n+}\n+\n+void JoinResultMaterialize::Init(MemoryPool* pool,\n+                                 const HashJoinProjectionMaps* probe_schemas,\n+                                 const HashJoinProjectionMaps* build_schemas) {\n+  pool_ = pool;\n+  probe_schemas_ = probe_schemas;\n+  build_schemas_ = build_schemas;\n+  num_rows_ = 0;\n+  null_ranges_.clear();\n+  num_produced_batches_ = 0;\n+\n+  // Initialize mapping of columns from output batch column index to key and\n+  // payload batch column index.\n+  //\n+  probe_output_to_key_and_payload_.resize(\n+      probe_schemas_->num_cols(HashJoinProjection::OUTPUT));\n+  int num_key_cols = probe_schemas_->num_cols(HashJoinProjection::KEY);\n+  auto to_key = probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  auto to_payload =\n+      probe_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; static_cast<size_t>(i) < probe_output_to_key_and_payload_.size(); ++i) {\n+    probe_output_to_key_and_payload_[i] =\n+        to_key.get(i) == SchemaProjectionMap::kMissingField\n+            ? to_payload.get(i) + num_key_cols\n+            : to_key.get(i);\n+  }\n+}\n+\n+void JoinResultMaterialize::SetBuildSide(const RowArray* build_keys,\n+                                         const RowArray* build_payloads,\n+                                         bool payload_id_same_as_key_id) {\n+  build_keys_ = build_keys;\n+  build_payloads_ = build_payloads;\n+  payload_id_same_as_key_id_ = payload_id_same_as_key_id;\n+}\n+\n+bool JoinResultMaterialize::HasProbeOutput() const {\n+  return probe_schemas_->num_cols(HashJoinProjection::OUTPUT) > 0;\n+}\n+\n+bool JoinResultMaterialize::HasBuildKeyOutput() const {\n+  auto to_key = build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_key.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::HasBuildPayloadOutput() const {\n+  auto to_payload =\n+      build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; i < build_schemas_->num_cols(HashJoinProjection::OUTPUT); ++i) {\n+    if (to_payload.get(i) != SchemaProjectionMap::kMissingField) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool JoinResultMaterialize::NeedsKeyId() const {\n+  return HasBuildKeyOutput() || (HasBuildPayloadOutput() && payload_id_same_as_key_id_);\n+}\n+\n+bool JoinResultMaterialize::NeedsPayloadId() const {\n+  return HasBuildPayloadOutput() && !payload_id_same_as_key_id_;\n+}\n+\n+Status JoinResultMaterialize::AppendProbeOnly(const ExecBatch& key_and_payload,\n+                                              int num_rows_to_append,\n+                                              const uint16_t* row_ids,\n+                                              int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendSelected(\n+        pool_, key_and_payload, num_rows_to_append, row_ids,\n+        static_cast<int>(probe_output_to_key_and_payload_.size()),\n+        probe_output_to_key_and_payload_.data()));\n+  }\n+  if (!null_ranges_.empty() &&\n+      null_ranges_.back().first + null_ranges_.back().second == num_rows_) {\n+    // We can extend the last range of null rows on build side.\n+    //\n+    null_ranges_.back().second += num_rows_to_append;\n+  } else {\n+    null_ranges_.push_back(\n+        std::make_pair(static_cast<int>(num_rows_), num_rows_to_append));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Status JoinResultMaterialize::AppendBuildOnly(int num_rows_to_append,\n+                                              const uint32_t* key_ids,\n+                                              const uint32_t* payload_ids,\n+                                              int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendNulls(\n+        pool_, probe_schemas_->data_types(HashJoinProjection::OUTPUT),\n+        num_rows_to_append));\n+  }\n+  if (NeedsKeyId()) {\n+    ARROW_DCHECK(key_ids != nullptr);\n+    key_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(key_ids_.data() + num_rows_, key_ids, num_rows_to_append * sizeof(uint32_t));\n+  }\n+  if (NeedsPayloadId()) {\n+    ARROW_DCHECK(payload_ids != nullptr);\n+    payload_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(payload_ids_.data() + num_rows_, payload_ids,\n+           num_rows_to_append * sizeof(uint32_t));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Status JoinResultMaterialize::Append(const ExecBatch& key_and_payload,\n+                                     int num_rows_to_append, const uint16_t* row_ids,\n+                                     const uint32_t* key_ids, const uint32_t* payload_ids,\n+                                     int* num_rows_appended) {\n+  num_rows_to_append =\n+      std::min(ExecBatchBuilder::num_rows_max() - num_rows_, num_rows_to_append);\n+  if (HasProbeOutput()) {\n+    RETURN_NOT_OK(batch_builder_.AppendSelected(\n+        pool_, key_and_payload, num_rows_to_append, row_ids,\n+        static_cast<int>(probe_output_to_key_and_payload_.size()),\n+        probe_output_to_key_and_payload_.data()));\n+  }\n+  if (NeedsKeyId()) {\n+    ARROW_DCHECK(key_ids != nullptr);\n+    key_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(key_ids_.data() + num_rows_, key_ids, num_rows_to_append * sizeof(uint32_t));\n+  }\n+  if (NeedsPayloadId()) {\n+    ARROW_DCHECK(payload_ids != nullptr);\n+    payload_ids_.resize(num_rows_ + num_rows_to_append);\n+    memcpy(payload_ids_.data() + num_rows_, payload_ids,\n+           num_rows_to_append * sizeof(uint32_t));\n+  }\n+  num_rows_ += num_rows_to_append;\n+  *num_rows_appended = num_rows_to_append;\n+  return Status::OK();\n+}\n+\n+Result<std::shared_ptr<ArrayData>> JoinResultMaterialize::FlushBuildColumn(\n+    const std::shared_ptr<DataType>& data_type, const RowArray* row_array, int column_id,\n+    uint32_t* row_ids) {\n+  ResizableArrayData output;\n+  output.Init(data_type, pool_, bit_util::Log2(num_rows_));\n+\n+  for (size_t i = 0; i <= null_ranges_.size(); ++i) {\n+    int row_id_begin =\n+        i == 0 ? 0 : null_ranges_[i - 1].first + null_ranges_[i - 1].second;\n+    int row_id_end = i == null_ranges_.size() ? num_rows_ : null_ranges_[i].first;\n+    if (row_id_end > row_id_begin) {\n+      RETURN_NOT_OK(row_array->DecodeSelected(\n+          &output, column_id, row_id_end - row_id_begin, row_ids + row_id_begin, pool_));\n+    }\n+    int num_nulls = i == null_ranges_.size() ? 0 : null_ranges_[i].second;\n+    if (num_nulls > 0) {\n+      RETURN_NOT_OK(ExecBatchBuilder::AppendNulls(data_type, output, num_nulls, pool_));\n+    }\n+  }\n+\n+  return output.array_data();\n+}\n+\n+Status JoinResultMaterialize::Flush(ExecBatch* out) {\n+  ARROW_DCHECK(num_rows_ > 0);\n+  out->length = num_rows_;\n+  out->values.clear();\n+\n+  int num_probe_cols = probe_schemas_->num_cols(HashJoinProjection::OUTPUT);\n+  int num_build_cols = build_schemas_->num_cols(HashJoinProjection::OUTPUT);\n+  out->values.resize(num_probe_cols + num_build_cols);\n+\n+  if (HasProbeOutput()) {\n+    ExecBatch probe_batch = batch_builder_.Flush();\n+    ARROW_DCHECK(static_cast<int>(probe_batch.values.size()) == num_probe_cols);\n+    for (size_t i = 0; i < probe_batch.values.size(); ++i) {\n+      out->values[i] = std::move(probe_batch.values[i]);\n+    }\n+  }\n+  auto to_key = build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::KEY);\n+  auto to_payload =\n+      build_schemas_->map(HashJoinProjection::OUTPUT, HashJoinProjection::PAYLOAD);\n+  for (int i = 0; i < num_build_cols; ++i) {\n+    if (to_key.get(i) != SchemaProjectionMap::kMissingField) {\n+      std::shared_ptr<ArrayData> column;\n+      ARROW_ASSIGN_OR_RAISE(\n+          column,\n+          FlushBuildColumn(build_schemas_->data_type(HashJoinProjection::OUTPUT, i),\n+                           build_keys_, to_key.get(i), key_ids_.data()));\n+      out->values[num_probe_cols + i] = std::move(column);\n+    } else if (to_payload.get(i) != SchemaProjectionMap::kMissingField) {\n+      std::shared_ptr<ArrayData> column;\n+      ARROW_ASSIGN_OR_RAISE(\n+          column,\n+          FlushBuildColumn(\n+              build_schemas_->data_type(HashJoinProjection::OUTPUT, i), build_payloads_,\n+              to_payload.get(i),\n+              payload_id_same_as_key_id_ ? key_ids_.data() : payload_ids_.data()));\n+      out->values[num_probe_cols + i] = std::move(column);\n+    } else {\n+      ARROW_DCHECK(false);\n+    }\n+  }\n+\n+  num_rows_ = 0;\n+  key_ids_.clear();\n+  payload_ids_.clear();\n+  null_ranges_.clear();\n+\n+  ++num_produced_batches_;\n+\n+  return Status::OK();\n+}\n+\n+void JoinNullFilter::Filter(const ExecBatch& key_batch, int batch_start_row,\n+                            int num_batch_rows, const std::vector<JoinKeyCmp>& cmp,\n+                            bool* all_valid, bool and_with_input,\n+                            uint8_t* inout_bit_vector) {\n+  // AND together validity vectors for columns that use equality comparison.\n+  //\n+  bool is_output_initialized = and_with_input;\n+  for (size_t i = 0; i < cmp.size(); ++i) {\n+    // No null filtering if null == null is true\n+    //\n+    if (cmp[i] != JoinKeyCmp::EQ) {\n+      continue;\n+    }\n+\n+    // No null filtering when there are no nulls\n+    //\n+    const Datum& data = key_batch.values[i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    if (!array_data->buffers[0]) {\n+      continue;\n+    }\n+\n+    const uint8_t* non_null_buffer = array_data->buffers[0]->data();\n+    int64_t offset = array_data->offset + batch_start_row;\n+\n+    // Filter out nulls for this column\n+    //\n+    if (!is_output_initialized) {\n+      memset(inout_bit_vector, 0xff, bit_util::BytesForBits(num_batch_rows));\n+      is_output_initialized = true;\n+    }\n+    arrow::internal::BitmapAnd(inout_bit_vector, 0, non_null_buffer, offset,\n+                               num_batch_rows, 0, inout_bit_vector);\n+  }\n+  *all_valid = !is_output_initialized;\n+}\n+\n+void JoinMatchIterator::SetLookupResult(int num_batch_rows, int start_batch_row,\n+                                        const uint8_t* batch_has_match,\n+                                        const uint32_t* key_ids, bool no_duplicate_keys,\n+                                        const uint32_t* key_to_payload) {\n+  num_batch_rows_ = num_batch_rows;\n+  start_batch_row_ = start_batch_row;\n+  batch_has_match_ = batch_has_match;\n+  key_ids_ = key_ids;\n+\n+  no_duplicate_keys_ = no_duplicate_keys;\n+  key_to_payload_ = key_to_payload;\n+\n+  current_row_ = 0;\n+  current_match_for_row_ = 0;\n+}\n+\n+bool JoinMatchIterator::GetNextBatch(int num_rows_max, int* out_num_rows,\n+                                     uint16_t* batch_row_ids, uint32_t* key_ids,\n+                                     uint32_t* payload_ids) {\n+  *out_num_rows = 0;\n+\n+  if (no_duplicate_keys_) {\n+    // When every input key can have at most one match,\n+    // then we only need to filter according to has match bit vector.\n+    //\n+    // We stop when either we produce a full batch or when we reach the end of\n+    // matches to output.\n+    //\n+    while (current_row_ < num_batch_rows_ && *out_num_rows < num_rows_max) {\n+      batch_row_ids[*out_num_rows] = start_batch_row_ + current_row_;\n+      key_ids[*out_num_rows] = payload_ids[*out_num_rows] = key_ids_[current_row_];\n+      (*out_num_rows) += bit_util::GetBit(batch_has_match_, current_row_) ? 1 : 0;\n+      ++current_row_;\n+    }\n+  } else {\n+    // When every input key can have zero, one or many matches,\n+    // then we need to filter out ones with no match and\n+    // iterate over all matches for the remaining ones.\n+    //\n+    // We stop when either we produce a full batch or when we reach the end of\n+    // matches to output.\n+    //\n+    while (current_row_ < num_batch_rows_ && *out_num_rows < num_rows_max) {\n+      if (!bit_util::GetBit(batch_has_match_, current_row_)) {\n+        ++current_row_;\n+        current_match_for_row_ = 0;\n+        continue;\n+      }\n+      uint32_t base_payload_id = key_to_payload_[key_ids_[current_row_]];\n+\n+      // Total number of matches for the currently selected input row\n+      //\n+      int num_matches_total =\n+          key_to_payload_[key_ids_[current_row_] + 1] - base_payload_id;\n+\n+      // Number of remaining matches for the currently selected input row\n+      //\n+      int num_matches_left = num_matches_total - current_match_for_row_;\n+\n+      // Number of matches for the currently selected input row that will fit\n+      // into the next batch\n+      //\n+      int num_matches_next = std::min(num_matches_left, num_rows_max - *out_num_rows);\n+\n+      for (int imatch = 0; imatch < num_matches_next; ++imatch) {\n+        batch_row_ids[*out_num_rows] = start_batch_row_ + current_row_;\n+        key_ids[*out_num_rows] = key_ids_[current_row_];\n+        payload_ids[*out_num_rows] = base_payload_id + current_match_for_row_ + imatch;\n+        ++(*out_num_rows);\n+      }\n+      current_match_for_row_ += num_matches_next;\n+\n+      if (current_match_for_row_ == num_matches_total) {\n+        ++current_row_;\n+        current_match_for_row_ = 0;\n+      }\n+    }\n+  }\n+\n+  return (*out_num_rows) > 0;\n+}\n+\n+void JoinProbeProcessor::Init(int num_key_columns, JoinType join_type,\n+                              SwissTableForJoin* hash_table,\n+                              std::vector<JoinResultMaterialize*> materialize,\n+                              const std::vector<JoinKeyCmp>* cmp,\n+                              OutputBatchFn output_batch_fn) {\n+  num_key_columns_ = num_key_columns;\n+  join_type_ = join_type;\n+  hash_table_ = hash_table;\n+  materialize_.resize(materialize.size());\n+  for (size_t i = 0; i < materialize.size(); ++i) {\n+    materialize_[i] = materialize[i];\n+  }\n+  cmp_ = cmp;\n+  output_batch_fn_ = output_batch_fn;\n+}\n+\n+Status JoinProbeProcessor::OnNextBatch(int64_t thread_id,\n+                                       const ExecBatch& keypayload_batch,\n+                                       util::TempVectorStack* temp_stack,\n+                                       std::vector<KeyColumnArray>* temp_column_arrays) {\n+  const SwissTable* swiss_table = hash_table_->keys()->swiss_table();\n+  int64_t hardware_flags = swiss_table->hardware_flags();\n+  int minibatch_size = swiss_table->minibatch_size();\n+  int num_rows = static_cast<int>(keypayload_batch.length);\n+\n+  ExecBatch key_batch({}, keypayload_batch.length);\n+  key_batch.values.resize(num_key_columns_);\n+  for (int i = 0; i < num_key_columns_; ++i) {\n+    key_batch.values[i] = keypayload_batch.values[i];\n+  }\n+\n+  // Break into mini-batches\n+  //\n+  // Start by allocating mini-batch buffers\n+  //\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack, static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)));\n+  auto key_ids_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto materialize_batch_ids_buf =\n+      util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size);\n+  auto materialize_key_ids_buf =\n+      util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto materialize_payload_ids_buf =\n+      util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input input(&key_batch, minibatch_start,\n+                                    minibatch_start + minibatch_size_next, temp_stack,\n+                                    temp_column_arrays);\n+    hash_table_->keys()->Hash(&input, hashes_buf.mutable_data(), hardware_flags);\n+    hash_table_->keys()->MapReadOnly(&input, hashes_buf.mutable_data(),\n+                                     match_bitvector_buf.mutable_data(),\n+                                     key_ids_buf.mutable_data());\n+\n+    // AND bit vector with null key filter for join\n+    //\n+    bool ignored;\n+    JoinNullFilter::Filter(key_batch, minibatch_start, minibatch_size_next, *cmp_,\n+                           &ignored,\n+                           /*and_with_input=*/true, match_bitvector_buf.mutable_data());\n+    // Semi-joins\n+    //\n+    if (join_type_ == JoinType::LEFT_SEMI || join_type_ == JoinType::LEFT_ANTI ||\n+        join_type_ == JoinType::RIGHT_SEMI || join_type_ == JoinType::RIGHT_ANTI) {\n+      int num_passing_ids = 0;\n+      util::bit_util::bits_to_indexes(\n+          (join_type_ == JoinType::LEFT_ANTI) ? 0 : 1, hardware_flags,\n+          minibatch_size_next, match_bitvector_buf.mutable_data(), &num_passing_ids,\n+          materialize_batch_ids_buf.mutable_data());\n+\n+      // For right-semi, right-anti joins: update has-match flags for the rows\n+      // in hash table.\n+      //\n+      if (join_type_ == JoinType::RIGHT_SEMI || join_type_ == JoinType::RIGHT_ANTI) {\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          uint16_t id = materialize_batch_ids_buf.mutable_data()[i];\n+          key_ids_buf.mutable_data()[i] = key_ids_buf.mutable_data()[id];\n+        }\n+        hash_table_->UpdateHasMatchForKeys(thread_id, num_passing_ids,\n+                                           key_ids_buf.mutable_data());\n+      } else {\n+        // For left-semi, left-anti joins: call materialize using match\n+        // bit-vector.\n+        //\n+\n+        // Add base batch row index.\n+        //\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          materialize_batch_ids_buf.mutable_data()[i] +=\n+              static_cast<uint16_t>(minibatch_start);\n+        }\n+\n+        RETURN_NOT_OK(materialize_[thread_id]->AppendProbeOnly(\n+            keypayload_batch, num_passing_ids, materialize_batch_ids_buf.mutable_data(),\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+    } else {\n+      // We need to output matching pairs of rows from both sides of the join.\n+      // Since every hash table lookup for an input row might have multiple\n+      // matches we use a helper class that implements enumerating all of them.\n+      //\n+      bool no_duplicate_keys = (hash_table_->key_to_payload() == nullptr);\n+      bool no_payload_columns = (hash_table_->payloads() == nullptr);\n+      JoinMatchIterator match_iterator;\n+      match_iterator.SetLookupResult(\n+          minibatch_size_next, minibatch_start, match_bitvector_buf.mutable_data(),\n+          key_ids_buf.mutable_data(), no_duplicate_keys, hash_table_->key_to_payload());\n+      int num_matches_next;\n+      while (match_iterator.GetNextBatch(minibatch_size, &num_matches_next,\n+                                         materialize_batch_ids_buf.mutable_data(),\n+                                         materialize_key_ids_buf.mutable_data(),\n+                                         materialize_payload_ids_buf.mutable_data())) {\n+        const uint16_t* materialize_batch_ids = materialize_batch_ids_buf.mutable_data();\n+        const uint32_t* materialize_key_ids = materialize_key_ids_buf.mutable_data();\n+        const uint32_t* materialize_payload_ids =\n+            no_duplicate_keys || no_payload_columns\n+                ? materialize_key_ids_buf.mutable_data()\n+                : materialize_payload_ids_buf.mutable_data();\n+\n+        // For right-outer, full-outer joins we need to update has-match flags\n+        // for the rows in hash table.\n+        //\n+        if (join_type_ == JoinType::RIGHT_OUTER || join_type_ == JoinType::FULL_OUTER) {\n+          hash_table_->UpdateHasMatchForKeys(thread_id, num_matches_next,\n+                                             materialize_key_ids);\n+        }\n+\n+        // Call materialize for resulting id tuples pointing to matching pairs\n+        // of rows.\n+        //\n+        RETURN_NOT_OK(materialize_[thread_id]->Append(\n+            keypayload_batch, num_matches_next, materialize_batch_ids,\n+            materialize_key_ids, materialize_payload_ids,\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+\n+      // For left-outer and full-outer joins output non-matches.\n+      //\n+      // Call materialize. Nulls will be output in all columns that come from\n+      // the other side of the join.\n+      //\n+      if (join_type_ == JoinType::LEFT_OUTER || join_type_ == JoinType::FULL_OUTER) {\n+        int num_passing_ids = 0;\n+        util::bit_util::bits_to_indexes(\n+            /*bit_to_search=*/0, hardware_flags, minibatch_size_next,\n+            match_bitvector_buf.mutable_data(), &num_passing_ids,\n+            materialize_batch_ids_buf.mutable_data());\n+\n+        // Add base batch row index.\n+        //\n+        for (int i = 0; i < num_passing_ids; ++i) {\n+          materialize_batch_ids_buf.mutable_data()[i] +=\n+              static_cast<uint16_t>(minibatch_start);\n+        }\n+\n+        RETURN_NOT_OK(materialize_[thread_id]->AppendProbeOnly(\n+            keypayload_batch, num_passing_ids, materialize_batch_ids_buf.mutable_data(),\n+            [&](ExecBatch batch) { output_batch_fn_(thread_id, std::move(batch)); }));\n+      }\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status JoinProbeProcessor::OnFinished() {\n+  // Flush all instances of materialize that have non-zero accumulated output\n+  // rows.\n+  //\n+  for (size_t i = 0; i < materialize_.size(); ++i) {\n+    JoinResultMaterialize& materialize = *materialize_[i];\n+    if (materialize.num_rows() > 0) {\n\nReview Comment:\n   Minor nit: Given that `JoinResultMaterialize::Flush` is only called here you could just move this if check inside of `JoinResultMaterialize::Flush` and get rid of the `DCHECK` that is there.\n\n\n\n",
                    "created": "2022-07-08T22:26:01.448+0000",
                    "updated": "2022-07-08T22:26:01.448+0000",
                    "started": "2022-07-08T22:26:01.448+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "789181",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/789586",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on PR #12326:\nURL: https://github.com/apache/arrow/pull/12326#issuecomment-1180490399\n\n   @michalursa I think this can be closed in favor of #13493 \n\n\n",
                    "created": "2022-07-11T14:35:32.743+0000",
                    "updated": "2022-07-11T14:35:32.743+0000",
                    "started": "2022-07-11T14:35:32.743+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "789586",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/789788",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r918446714\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n+ public:\n+  // Calculate total number of blocks for merged table.\n+  // Allocate buffers sized accordingly and initialize empty target table.\n+  //\n+  // All input sources must be initialized, but they can be empty.\n+  //\n+  // Output in a vector the first target group id for each source (exclusive\n+  // cummulative sum of number of groups in sources).\n+  //\n+  static Status PrepareForMerge(SwissTable* target,\n+                                const std::vector<SwissTable*>& sources,\n+                                std::vector<uint32_t>* first_target_group_id,\n+                                MemoryPool* pool);\n+\n+  // Copy all entries from source to a range of blocks (partition) of target.\n+  //\n+  // During copy, adjust group ids from source by adding provided base id.\n+  //\n+  // Skip entries from source that would cross partition boundaries (range of\n+  // blocks) when inserted into target. Save their data in output vector for\n+  // processing later. We postpone inserting these overflow entries in order to\n+  // allow concurrent processing of all partitions. Overflow entries will be\n+  // handled by a single-thread afterwards.\n+  //\n+  static void MergePartition(SwissTable* target, const SwissTable* source,\n+                             uint32_t partition_id, int num_partition_bits,\n+                             uint32_t base_group_id,\n+                             std::vector<uint32_t>* overflow_group_ids,\n+                             std::vector<uint32_t>* overflow_hashes);\n+\n+  // Single-threaded processing of remaining groups, that could not be\n+  // inserted in partition merge phase\n+  // (due to entries from one partition spilling over due to full blocks into\n+  // the next partition).\n+  //\n+  static void InsertNewGroups(SwissTable* target, const std::vector<uint32_t>& group_ids,\n+                              const std::vector<uint32_t>& hashes);\n+\n+ private:\n+  // Insert a new group id.\n+  //\n+  // Assumes that there are enough slots in the target\n+  // and there is no need to resize it.\n+  //\n+  // Max block id can be provided, in which case the search for an empty slot to\n+  // insert new entry to will stop after visiting that block.\n+  //\n+  // Max block id value greater or equal to the number of blocks guarantees that\n+  // the search will not be stopped.\n+  //\n+  static inline bool InsertNewGroup(SwissTable* target, uint64_t group_id, uint32_t hash,\n+                                    int64_t max_block_id);\n+};\n+\n+struct SwissTableWithKeys {\n+  struct Input {\n+    Input(const ExecBatch* in_batch, int in_batch_start_row, int in_batch_end_row,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays);\n+\n+    Input(const ExecBatch* in_batch, int in_num_selected, const uint16_t* in_selection,\n+          util::TempVectorStack* in_temp_stack,\n+          std::vector<KeyColumnArray>* in_temp_column_arrays,\n+          std::vector<uint32_t>* in_temp_group_ids);\n+\n+    Input(const Input& base, int num_rows_to_skip, int num_rows_to_include);\n+\n+    const ExecBatch* batch;\n+    // Window of the batch to operate on.\n+    // The window information is only used if row selection is null.\n+    //\n+    int batch_start_row;\n+    int batch_end_row;\n+    // Optional selection.\n+    // Used instead of window of the batch if not null.\n+    //\n+    int num_selected;\n+    const uint16_t* selection_maybe_null;\n+    // Thread specific scratch buffers for storing temporary data.\n+    //\n+    util::TempVectorStack* temp_stack;\n+    std::vector<KeyColumnArray>* temp_column_arrays;\n+    std::vector<uint32_t>* temp_group_ids;\n+  };\n+\n+  Status Init(int64_t hardware_flags, MemoryPool* pool);\n+\n+  void InitCallbacks();\n+\n+  static void Hash(Input* input, uint32_t* hashes, int64_t hardware_flags);\n\nReview Comment:\n   It is a simple wrapper on top of Hashing32::HashBatch that uses differently structured input.\r\n   It is useful, because the input to it is the same as for MapReadOnly and MapWithInserts and Hash() and these functions are called in the same block of code.\n\n\n\n",
                    "created": "2022-07-12T00:27:36.630+0000",
                    "updated": "2022-07-12T00:27:36.630+0000",
                    "started": "2022-07-12T00:27:36.630+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "789788",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/789789",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r918448090\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.cc:\n##########\n@@ -0,0 +1,2545 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/row/compare_internal.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+int RowArrayAccessor::VarbinaryColumnId(const RowTableMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                                    const uint32_t* row_ids, int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                                  const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyColumnMetadata> column_metadatas;\n+  RETURN_NOT_OK(ColumnMetadatasFromExecBatch(batch, &column_metadatas));\n+  RowTableMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch,\n+                                      int begin_row_id, int end_row_id, int num_row_ids,\n+                                      const uint16_t* row_ids,\n+                                      std::vector<KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  RETURN_NOT_OK(ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                                          &temp_column_arrays));\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  Status status = ColumnArraysFromExecBatch(\n+      batch, begin_row_id, end_row_id - begin_row_id, &temp_column_arrays);\n+  ARROW_DCHECK(status.ok());\n+\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  ARROW_ASSIGN_OR_RAISE(KeyColumnMetadata column_metadata, output->column_metadata());\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const RowTableMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_batch_start_row,\n+                                 int in_batch_end_row,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_num_selected,\n+                                 const uint16_t* in_selection,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays,\n+                                 std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Status status =\n+      Hashing32::HashBatch(*input->batch, hashes, *input->temp_column_arrays,\n+                           hardware_flags, input->temp_stack, input->batch_start_row,\n+                           input->batch_end_row - input->batch_start_row);\n+  ARROW_DCHECK(status.ok());\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+                               uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+                               util::TempVectorStack* temp_stack,\n+                               std::vector<KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+                                    bool reject_duplicate_keys, bool no_payload,\n+                                    const std::vector<KeyColumnMetadata>& key_types,\n+                                    const std::vector<KeyColumnMetadata>& payload_types,\n+                                    MemoryPool* pool, int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(dop_, num_prtns_);\n+\n+  RowTableMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  RowTableMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  RETURN_NOT_OK(Hashing32::HashBatch(\n+      key_batch, locals.batch_hashes.data(), locals.temp_column_arrays, hardware_flags_,\n+      temp_stack, /*start_row=*/0, static_cast<int>(key_batch.length)));\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int64_t i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int64_t i, int pos) {\n+          locals.batch_prtn_row_ids[pos] = static_cast<uint16_t>(i);\n+        });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      thread_id, locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n+  RETURN_NOT_OK(SwissTableMerge::PrepareForMerge(\n+      target_->map_.swiss_table(), partition_tables, &partition_first_group_id, pool_));\n+\n+  // 3. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    std::vector<RowArray*> partition_payloads;\n+    partition_payloads.resize(num_prtns_);\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      partition_payloads[i] = &prtn_states_[i].payloads;\n+    }\n+    RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(&target_->payloads_, partition_payloads,\n+                                                 &partition_payloads_first_row_id_,\n+                                                 pool_));\n+  }\n+\n+  // Check if we have duplicate keys\n+  //\n+  int64_t num_keys = partition_keys_first_row_id_[num_prtns_];\n+  int64_t num_rows = 0;\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+  }\n+  bool no_duplicate_keys = reject_duplicate_keys_ || num_keys == num_rows;\n+\n+  // 4. Mapping from key id to first payload id:\n+  //\n+  target_->no_duplicate_keys_ = no_duplicate_keys;\n+  if (!no_duplicate_keys) {\n+    target_->row_offset_for_key_.resize(num_keys + 1);\n+    int64_t num_rows = 0;\n+    for (int i = 0; i < num_prtns_; ++i) {\n+      int64_t first_key = partition_keys_first_row_id_[i];\n+      target_->row_offset_for_key_[first_key] = static_cast<uint32_t>(num_rows);\n+      num_rows += static_cast<int64_t>(prtn_states_[i].key_ids.size());\n+    }\n+    target_->row_offset_for_key_[num_keys] = static_cast<uint32_t>(num_rows);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoinBuild::PrtnMerge(int prtn_id) {\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  // 4. array of payload rows (only when no_payload_ is false)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  RowArrayMerge::MergeSingle(target_->map_.keys(), *prtn_state.keys.keys(),\n+                             partition_keys_first_row_id_[prtn_id],\n+                             /*source_rows_permutation=*/nullptr);\n+\n+  // 2. SwissTable:\n+  //\n+  SwissTableMerge::MergePartition(\n+      target_->map_.swiss_table(), prtn_state.keys.swiss_table(), prtn_id, log_num_prtns_,\n+      static_cast<uint32_t>(partition_keys_first_row_id_[prtn_id]),\n+      &prtn_state.overflow_key_ids, &prtn_state.overflow_hashes);\n+\n+  std::vector<int64_t> source_payload_ids;\n+\n+  // 3. mapping from key id to first payload id\n+  //\n+  if (!target_->no_duplicate_keys_) {\n+    // Count for each local (within partition) key id how many times it appears\n+    // in input rows.\n+    //\n+    // For convenience, we use an array in merged hash table mapping key ids to\n+    // first payload ids to collect the counters.\n+    //\n+    int64_t first_key = partition_keys_first_row_id_[prtn_id];\n+    int64_t num_keys = partition_keys_first_row_id_[prtn_id + 1] - first_key;\n+    uint32_t* counters = target_->row_offset_for_key_.data() + first_key;\n+    uint32_t first_payload = counters[0];\n+    for (int64_t i = 0; i < num_keys; ++i) {\n+      counters[i] = 0;\n+    }\n+    for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+      uint32_t key_id = prtn_state.key_ids[i];\n+      ++counters[key_id];\n+    }\n+\n+    if (!no_payload_) {\n+      // Count sort payloads on key id\n+      //\n+      // Start by computing inclusive cummulative sum of counters.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        sum += counters[i];\n+        counters[i] = sum;\n+      }\n+      // Now use cummulative sum of counters to obtain the target position in\n+      // the sorted order for each row. At the end of this process the counters\n+      // will contain exclusive cummulative sum (instead of inclusive that is\n+      // there at the beginning).\n+      //\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        int64_t position = --counters[key_id];\n+        source_payload_ids[position] = static_cast<int64_t>(i);\n+      }\n+      // Add base payload id to all of the counters.\n+      //\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        counters[i] += first_payload;\n+      }\n+    } else {\n+      // When there is no payload to process, we just need to compute exclusive\n+      // cummulative sum of counters and add the base payload id to all of them.\n+      //\n+      uint32_t sum = 0;\n+      for (int64_t i = 0; i < num_keys; ++i) {\n+        uint32_t sum_next = sum + counters[i];\n+        counters[i] = sum + first_payload;\n+        sum = sum_next;\n+      }\n+    }\n+  }\n+\n+  // 4. Array of payload rows:\n+  //\n+  if (!no_payload_) {\n+    // If there are duplicate keys, then we have already initialized permutation\n+    // of payloads for this partition.\n+    //\n+    if (target_->no_duplicate_keys_) {\n+      source_payload_ids.resize(prtn_state.key_ids.size());\n+      for (size_t i = 0; i < prtn_state.key_ids.size(); ++i) {\n+        uint32_t key_id = prtn_state.key_ids[i];\n+        source_payload_ids[key_id] = static_cast<int64_t>(i);\n+      }\n+    }\n+    // Merge partition payloads into target array using the permutation.\n+    //\n+    RowArrayMerge::MergeSingle(&target_->payloads_, prtn_state.payloads,\n+                               partition_payloads_first_row_id_[prtn_id],\n+                               source_payload_ids.data());\n+\n+    // TODO: Uncomment for debugging\n+    // prtn_state.payloads.DebugPrintToFile(\"payload_local.txt\", false);\n\nReview Comment:\n   done\n\n\n\n",
                    "created": "2022-07-12T00:31:52.536+0000",
                    "updated": "2022-07-12T00:31:52.536+0000",
                    "started": "2022-07-12T00:31:52.536+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "789789",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/789800",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r918452643\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.cc:\n##########\n@@ -0,0 +1,2545 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+#include <sys/stat.h>\n+#include <algorithm>  // std::upper_bound\n+#include <cstdio>\n+#include <cstdlib>\n+#include <mutex>\n+#include \"arrow/array/util.h\"  // MakeArrayFromScalar\n+#include \"arrow/compute/exec/hash_join.h\"\n+#include \"arrow/compute/exec/key_hash.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/row/compare_internal.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+int RowArrayAccessor::VarbinaryColumnId(const RowTableMetadata& row_metadata,\n+                                        int column_id) {\n+  ARROW_DCHECK(row_metadata.num_cols() > static_cast<uint32_t>(column_id));\n+  ARROW_DCHECK(!row_metadata.is_fixed_length);\n+  ARROW_DCHECK(!row_metadata.column_metadatas[column_id].is_fixed_length);\n+\n+  int varbinary_column_id = 0;\n+  for (int i = 0; i < column_id; ++i) {\n+    if (!row_metadata.column_metadatas[i].is_fixed_length) {\n+      ++varbinary_column_id;\n+    }\n+  }\n+  return varbinary_column_id;\n+}\n+\n+int RowArrayAccessor::NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                                    const uint32_t* row_ids, int num_tail_bytes_to_skip) {\n+  uint32_t num_bytes_skipped = 0;\n+  int num_rows_left = num_rows;\n+\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  if (!is_fixed_length_column) {\n+    // Varying length column\n+    //\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      // Find the pointer to the last requested row\n+      //\n+      uint32_t last_row_id = row_ids[num_rows_left - 1];\n+      const uint8_t* row_ptr = rows.data(2) + rows.offsets()[last_row_id];\n+\n+      // Find the length of the requested varying length field in that row\n+      //\n+      uint32_t field_offset_within_row, field_length;\n+      if (varbinary_column_id == 0) {\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+      } else {\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+      }\n+\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  } else {\n+    // Fixed length column\n+    //\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    uint32_t num_bytes_skipped = 0;\n+    while (num_rows_left > 0 &&\n+           num_bytes_skipped < static_cast<uint32_t>(num_tail_bytes_to_skip)) {\n+      num_bytes_skipped += field_length;\n+      --num_rows_left;\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  bool is_fixed_length_column =\n+      rows.metadata().column_metadatas[column_id].is_fixed_length;\n+\n+  // There are 4 cases, each requiring different steps:\n+  // 1. Varying length column that is the first varying length column in a row\n+  // 2. Varying length column that is not the first varying length column in a\n+  // row\n+  // 3. Fixed length column in a fixed length row\n+  // 4. Fixed length column in a varying length row\n+\n+  if (!is_fixed_length_column) {\n+    int varbinary_column_id = VarbinaryColumnId(rows.metadata(), column_id);\n+    const uint8_t* row_ptr_base = rows.data(2);\n+    const uint32_t* row_offsets = rows.offsets();\n+    uint32_t field_offset_within_row, field_length;\n+\n+    if (varbinary_column_id == 0) {\n+      // Case 1: This is the first varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().first_varbinary_offset_and_length(\n+            row_ptr, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    } else {\n+      // Case 2: This is second or later varbinary column\n+      //\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        rows.metadata().nth_varbinary_offset_and_length(\n+            row_ptr, varbinary_column_id, &field_offset_within_row, &field_length);\n+        process_value_fn(i, row_ptr + field_offset_within_row, field_length);\n+      }\n+    }\n+  }\n+\n+  if (is_fixed_length_column) {\n+    uint32_t field_offset_within_row = rows.metadata().encoded_field_offset(\n+        rows.metadata().pos_after_encoding(column_id));\n+    uint32_t field_length = rows.metadata().column_metadatas[column_id].fixed_length;\n+    // Bit column is encoded as a single byte\n+    //\n+    if (field_length == 0) {\n+      field_length = 1;\n+    }\n+    uint32_t row_length = rows.metadata().fixed_length;\n+\n+    bool is_fixed_length_row = rows.metadata().is_fixed_length;\n+    if (is_fixed_length_row) {\n+      // Case 3: This is a fixed length column in a fixed length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(1) + field_offset_within_row;\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_length * row_id;\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    } else {\n+      // Case 4: This is a fixed length column in a varying length row\n+      //\n+      const uint8_t* row_ptr_base = rows.data(2) + field_offset_within_row;\n+      const uint32_t* row_offsets = rows.offsets();\n+      for (int i = 0; i < num_rows; ++i) {\n+        uint32_t row_id = row_ids[i];\n+        const uint8_t* row_ptr = row_ptr_base + row_offsets[row_id];\n+        process_value_fn(i, row_ptr, field_length);\n+      }\n+    }\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void RowArrayAccessor::VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                                  const uint32_t* row_ids,\n+                                  PROCESS_VALUE_FN process_value_fn) {\n+  const uint8_t* null_masks = rows.null_masks();\n+  uint32_t null_mask_num_bytes = rows.metadata().null_masks_bytes_per_row;\n+  uint32_t pos_after_encoding = rows.metadata().pos_after_encoding(column_id);\n+  for (int i = 0; i < num_rows; ++i) {\n+    uint32_t row_id = row_ids[i];\n+    int64_t bit_id = row_id * null_mask_num_bytes * 8 + pos_after_encoding;\n+    process_value_fn(i, bit_util::GetBit(null_masks, bit_id) ? 0xff : 0);\n+  }\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  encoder_.Init(row_metadata.column_metadatas, sizeof(uint64_t), sizeof(uint64_t));\n+  RETURN_NOT_OK(rows_temp_.Init(pool, row_metadata));\n+  RETURN_NOT_OK(rows_.Init(pool, row_metadata));\n+  is_initialized_ = true;\n+  return Status::OK();\n+}\n+\n+Status RowArray::InitIfNeeded(MemoryPool* pool, const ExecBatch& batch) {\n+  if (is_initialized_) {\n+    return Status::OK();\n+  }\n+  std::vector<KeyColumnMetadata> column_metadatas;\n+  RETURN_NOT_OK(ColumnMetadatasFromExecBatch(batch, &column_metadatas));\n+  RowTableMetadata row_metadata;\n+  row_metadata.FromColumnMetadataVector(column_metadatas, sizeof(uint64_t),\n+                                        sizeof(uint64_t));\n+\n+  return InitIfNeeded(pool, row_metadata);\n+}\n+\n+Status RowArray::AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch,\n+                                      int begin_row_id, int end_row_id, int num_row_ids,\n+                                      const uint16_t* row_ids,\n+                                      std::vector<KeyColumnArray>& temp_column_arrays) {\n+  RETURN_NOT_OK(InitIfNeeded(pool, batch));\n+  RETURN_NOT_OK(ColumnArraysFromExecBatch(batch, begin_row_id, end_row_id - begin_row_id,\n+                                          &temp_column_arrays));\n+  encoder_.PrepareEncodeSelected(\n+      /*start_row=*/0, end_row_id - begin_row_id, temp_column_arrays);\n+  RETURN_NOT_OK(encoder_.EncodeSelected(&rows_temp_, num_row_ids, row_ids));\n+  RETURN_NOT_OK(rows_.AppendSelectionFrom(rows_temp_, num_row_ids, nullptr));\n+  return Status::OK();\n+}\n+\n+void RowArray::Compare(const ExecBatch& batch, int begin_row_id, int end_row_id,\n+                       int num_selected, const uint16_t* batch_selection_maybe_null,\n+                       const uint32_t* array_row_ids, uint32_t* out_num_not_equal,\n+                       uint16_t* out_not_equal_selection, int64_t hardware_flags,\n+                       util::TempVectorStack* temp_stack,\n+                       std::vector<KeyColumnArray>& temp_column_arrays,\n+                       uint8_t* out_match_bitvector_maybe_null) {\n+  Status status = ColumnArraysFromExecBatch(\n+      batch, begin_row_id, end_row_id - begin_row_id, &temp_column_arrays);\n+  ARROW_DCHECK(status.ok());\n+\n+  LightContext ctx;\n+  ctx.hardware_flags = hardware_flags;\n+  ctx.stack = temp_stack;\n+  KeyCompare::CompareColumnsToRows(\n+      num_selected, batch_selection_maybe_null, array_row_ids, &ctx, out_num_not_equal,\n+      out_not_equal_selection, temp_column_arrays, rows_,\n+      /*are_cols_in_encoding_order=*/false, out_match_bitvector_maybe_null);\n+}\n+\n+Status RowArray::DecodeSelected(ResizableArrayData* output, int column_id,\n+                                int num_rows_to_append, const uint32_t* row_ids,\n+                                MemoryPool* pool) const {\n+  int num_rows_before = output->num_rows();\n+  RETURN_NOT_OK(output->ResizeFixedLengthBuffers(num_rows_before + num_rows_to_append));\n+\n+  // Both input (KeyRowArray) and output (ResizableArrayData) have buffers with\n+  // extra bytes added at the end to avoid buffer overruns when using wide load\n+  // instructions.\n+  //\n+\n+  ARROW_ASSIGN_OR_RAISE(KeyColumnMetadata column_metadata, output->column_metadata());\n+\n+  if (column_metadata.is_fixed_length) {\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  bit_util::SetBitTo(output->mutable_data(1),\n+                                                     num_rows_before + i, *ptr != 0);\n+                                });\n+        break;\n+      case 1:\n+        RowArrayAccessor::Visit(rows_, column_id, num_rows_to_append, row_ids,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  output->mutable_data(1)[num_rows_before + i] = *ptr;\n+                                });\n+        break;\n+      case 2:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(output->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default:\n+        RowArrayAccessor::Visit(\n+            rows_, column_id, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                  output->mutable_data(1) + num_bytes * (num_rows_before + i));\n+              const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+              for (uint32_t word_id = 0;\n+                   word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+                util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+              }\n+            });\n+        break;\n+    }\n+  } else {\n+    uint32_t* offsets =\n+        reinterpret_cast<uint32_t*>(output->mutable_data(1)) + num_rows_before;\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[0];\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) { offsets[i] = num_bytes; });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[i];\n+      offsets[i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_to_append] = sum;\n+    RETURN_NOT_OK(output->ResizeVaryingLengthBuffer());\n+    RowArrayAccessor::Visit(\n+        rows_, column_id, num_rows_to_append, row_ids,\n+        [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+          uint64_t* dst = reinterpret_cast<uint64_t*>(\n+              output->mutable_data(2) +\n+              reinterpret_cast<const uint32_t*>(\n+                  output->mutable_data(1))[num_rows_before + i]);\n+          const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+          for (uint32_t word_id = 0;\n+               word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+            util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+          }\n+        });\n+  }\n+\n+  // Process nulls\n+  //\n+  RowArrayAccessor::VisitNulls(\n+      rows_, column_id, num_rows_to_append, row_ids, [&](int i, uint8_t value) {\n+        bit_util::SetBitTo(output->mutable_data(0), num_rows_before + i, value == 0);\n+      });\n+\n+  return Status::OK();\n+}\n+\n+void RowArray::DebugPrintToFile(const char* filename, bool print_sorted) const {\n+  FILE* fout;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+  fopen_s(&fout, filename, \"wt\");\n+#else\n+  fout = fopen(filename, \"wt\");\n+#endif\n+  if (!fout) {\n+    return;\n+  }\n+\n+  for (int64_t row_id = 0; row_id < rows_.length(); ++row_id) {\n+    for (uint32_t column_id = 0; column_id < rows_.metadata().num_cols(); ++column_id) {\n+      bool is_null;\n+      uint32_t row_id_cast = static_cast<uint32_t>(row_id);\n+      RowArrayAccessor::VisitNulls(rows_, column_id, 1, &row_id_cast,\n+                                   [&](int i, uint8_t value) { is_null = (value != 0); });\n+      if (is_null) {\n+        fprintf(fout, \"null\");\n+      } else {\n+        RowArrayAccessor::Visit(rows_, column_id, 1, &row_id_cast,\n+                                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                                  fprintf(fout, \"\\\"\");\n+                                  for (uint32_t ibyte = 0; ibyte < num_bytes; ++ibyte) {\n+                                    fprintf(fout, \"%02x\", ptr[ibyte]);\n+                                  }\n+                                  fprintf(fout, \"\\\"\");\n+                                });\n+      }\n+      fprintf(fout, \"\\t\");\n+    }\n+    fprintf(fout, \"\\n\");\n+  }\n+  fclose(fout);\n+\n+  if (print_sorted) {\n+    struct stat sb;\n+    if (stat(filename, &sb) == -1) {\n+      ARROW_DCHECK(false);\n+      return;\n+    }\n+    std::vector<char> buffer;\n+    buffer.resize(sb.st_size);\n+    std::vector<std::string> lines;\n+    FILE* fin;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fin, filename, \"rt\");\n+#else\n+    fin = fopen(filename, \"rt\");\n+#endif\n+    if (!fin) {\n+      return;\n+    }\n+    while (fgets(buffer.data(), static_cast<int>(buffer.size()), fin)) {\n+      lines.push_back(std::string(buffer.data()));\n+    }\n+    fclose(fin);\n+    std::sort(lines.begin(), lines.end());\n+    FILE* fout2;\n+#if defined(_MSC_VER) && _MSC_VER >= 1400\n+    fopen_s(&fout2, filename, \"wt\");\n+#else\n+    fout2 = fopen(filename, \"wt\");\n+#endif\n+    if (!fout2) {\n+      return;\n+    }\n+    for (size_t i = 0; i < lines.size(); ++i) {\n+      fprintf(fout2, \"%s\\n\", lines[i].c_str());\n+    }\n+    fclose(fout2);\n+  }\n+}\n+\n+Status RowArrayMerge::PrepareForMerge(RowArray* target,\n+                                      const std::vector<RowArray*>& sources,\n+                                      std::vector<int64_t>* first_target_row_id,\n+                                      MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  ARROW_DCHECK(sources[0]->is_initialized_);\n+  const RowTableMetadata& metadata = sources[0]->rows_.metadata();\n+  ARROW_DCHECK(!target->is_initialized_);\n+  RETURN_NOT_OK(target->InitIfNeeded(pool, metadata));\n+\n+  // Sum the number of rows from all input sources and calculate their total\n+  // size.\n+  //\n+  int64_t num_rows = 0;\n+  int64_t num_bytes = 0;\n+  first_target_row_id->resize(sources.size() + 1);\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    // All input sources must be initialized and have the same row format.\n+    //\n+    ARROW_DCHECK(sources[i]->is_initialized_);\n+    ARROW_DCHECK(metadata.is_compatible(sources[i]->rows_.metadata()));\n+    (*first_target_row_id)[i] = num_rows;\n+    num_rows += sources[i]->rows_.length();\n+    if (!metadata.is_fixed_length) {\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+  }\n+  (*first_target_row_id)[sources.size()] = num_rows;\n+\n+  // Allocate target memory\n+  //\n+  target->rows_.Clean();\n+  RETURN_NOT_OK(target->rows_.AppendEmpty(static_cast<uint32_t>(num_rows),\n+                                          static_cast<uint32_t>(num_bytes)));\n+\n+  // In case of varying length rows,\n+  // initialize the first row offset for each range of rows corresponding to a\n+  // single source.\n+  //\n+  if (!metadata.is_fixed_length) {\n+    num_rows = 0;\n+    num_bytes = 0;\n+    for (size_t i = 0; i < sources.size(); ++i) {\n+      target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+      num_rows += sources[i]->rows_.length();\n+      num_bytes += sources[i]->rows_.offsets()[sources[i]->rows_.length()];\n+    }\n+    target->rows_.mutable_offsets()[num_rows] = static_cast<uint32_t>(num_bytes);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void RowArrayMerge::MergeSingle(RowArray* target, const RowArray& source,\n+                                int64_t first_target_row_id,\n+                                const int64_t* source_rows_permutation) {\n+  // Source and target must:\n+  // - be initialized\n+  // - use the same row format\n+  // - use 64-bit alignment\n+  //\n+  ARROW_DCHECK(source.is_initialized_ && target->is_initialized_);\n+  ARROW_DCHECK(target->rows_.metadata().is_compatible(source.rows_.metadata()));\n+  ARROW_DCHECK(target->rows_.metadata().row_alignment == sizeof(uint64_t));\n+\n+  if (target->rows_.metadata().is_fixed_length) {\n+    CopyFixedLength(&target->rows_, source.rows_, first_target_row_id,\n+                    source_rows_permutation);\n+  } else {\n+    CopyVaryingLength(&target->rows_, source.rows_, first_target_row_id,\n+                      target->rows_.offsets()[first_target_row_id],\n+                      source_rows_permutation);\n+  }\n+  CopyNulls(&target->rows_, source.rows_, first_target_row_id, source_rows_permutation);\n+}\n+\n+void RowArrayMerge::CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                                    int64_t first_target_row_id,\n+                                    const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+\n+  int64_t fixed_length = target->metadata().fixed_length;\n+\n+  // Permutation of source rows is optional. Without permutation all that is\n+  // needed is memcpy.\n+  //\n+  if (!source_rows_permutation) {\n+    memcpy(target->mutable_data(1) + fixed_length * first_target_row_id, source.data(1),\n+           fixed_length * num_source_rows);\n+  } else {\n+    // Row length must be a multiple of 64-bits due to enforced alignment.\n+    // Loop for each output row copying a fixed number of 64-bit words.\n+    //\n+    ARROW_DCHECK(fixed_length % sizeof(uint64_t) == 0);\n+\n+    int64_t num_words_per_row = fixed_length / sizeof(uint64_t);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(1) + fixed_length * source_row_id);\n+      uint64_t* target_row_ptr = reinterpret_cast<uint64_t*>(\n+          target->mutable_data(1) + fixed_length * (first_target_row_id + i));\n+\n+      for (int64_t word = 0; word < num_words_per_row; ++word) {\n+        target_row_ptr[word] = source_row_ptr[word];\n+      }\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                      int64_t first_target_row_id,\n+                                      int64_t first_target_row_offset,\n+                                      const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  uint32_t* target_offsets = target->mutable_offsets();\n+  const uint32_t* source_offsets = source.offsets();\n+\n+  // Permutation of source rows is optional.\n+  //\n+  if (!source_rows_permutation) {\n+    int64_t target_row_offset = first_target_row_offset;\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += source_offsets[i + 1] - source_offsets[i];\n+    }\n+    // We purposefully skip outputting of N+1 offset, to allow concurrent\n+    // copies of rows done to adjacent ranges in target array.\n+    // It should have already been initialized during preparation for merge.\n+    //\n+\n+    // We can simply memcpy bytes of rows if their order has not changed.\n+    //\n+    memcpy(target->mutable_data(2) + target_offsets[first_target_row_id], source.data(2),\n+           source_offsets[num_source_rows] - source_offsets[0]);\n+  } else {\n+    int64_t target_row_offset = first_target_row_offset;\n+    uint64_t* target_row_ptr =\n+        reinterpret_cast<uint64_t*>(target->mutable_data(2) + target_row_offset);\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint64_t* source_row_ptr = reinterpret_cast<const uint64_t*>(\n+          source.data(2) + source_offsets[source_row_id]);\n+      uint32_t length = source_offsets[source_row_id + 1] - source_offsets[source_row_id];\n+\n+      // Rows should be 64-bit aligned.\n+      // In that case we can copy them using a sequence of 64-bit read/writes.\n+      //\n+      ARROW_DCHECK(length % sizeof(uint64_t) == 0);\n+\n+      for (uint32_t word = 0; word < length / sizeof(uint64_t); ++word) {\n+        *target_row_ptr++ = *source_row_ptr++;\n+      }\n+\n+      target_offsets[first_target_row_id + i] = static_cast<uint32_t>(target_row_offset);\n+      target_row_offset += length;\n+    }\n+  }\n+}\n+\n+void RowArrayMerge::CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation) {\n+  int64_t num_source_rows = source.length();\n+  int num_bytes_per_row = target->metadata().null_masks_bytes_per_row;\n+  uint8_t* target_nulls = target->null_masks() + num_bytes_per_row * first_target_row_id;\n+  if (!source_rows_permutation) {\n+    memcpy(target_nulls, source.null_masks(), num_bytes_per_row * num_source_rows);\n+  } else {\n+    for (int64_t i = 0; i < num_source_rows; ++i) {\n+      int64_t source_row_id = source_rows_permutation[i];\n+      const uint8_t* source_nulls =\n+          source.null_masks() + num_bytes_per_row * source_row_id;\n+      for (int64_t byte = 0; byte < num_bytes_per_row; ++byte) {\n+        *target_nulls++ = *source_nulls++;\n+      }\n+    }\n+  }\n+}\n+\n+Status SwissTableMerge::PrepareForMerge(SwissTable* target,\n+                                        const std::vector<SwissTable*>& sources,\n+                                        std::vector<uint32_t>* first_target_group_id,\n+                                        MemoryPool* pool) {\n+  ARROW_DCHECK(!sources.empty());\n+\n+  // Each source should correspond to a range of hashes.\n+  // A row belongs to a source with index determined by K highest bits of hash.\n+  // That means that the number of sources must be a power of 2.\n+  //\n+  int log_num_sources = bit_util::Log2(sources.size());\n+  ARROW_DCHECK((1 << log_num_sources) == static_cast<int>(sources.size()));\n+\n+  // Determine the number of blocks in the target table.\n+  // We will use max of numbers of blocks in any of the sources multiplied by\n+  // the number of sources.\n+  //\n+  int log_blocks_max = 1;\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    log_blocks_max = std::max(log_blocks_max, sources[i]->log_blocks_);\n+  }\n+  int log_blocks = log_num_sources + log_blocks_max;\n+\n+  // Allocate target blocks and mark all slots as empty\n+  //\n+  // We will skip allocating the array of hash values in target table.\n+  // Target will be used in read-only mode and that array is only needed when\n+  // resizing table which may occur only after new inserts.\n+  //\n+  RETURN_NOT_OK(target->init(sources[0]->hardware_flags_, pool, log_blocks,\n+                             /*no_hash_array=*/true));\n+\n+  // Calculate and output the first group id index for each source.\n+  //\n+  uint32_t num_groups = 0;\n+  first_target_group_id->resize(sources.size());\n+  for (size_t i = 0; i < sources.size(); ++i) {\n+    (*first_target_group_id)[i] = num_groups;\n+    num_groups += sources[i]->num_inserted_;\n+  }\n+  target->num_inserted_ = num_groups;\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableMerge::MergePartition(SwissTable* target, const SwissTable* source,\n+                                     uint32_t partition_id, int num_partition_bits,\n+                                     uint32_t base_group_id,\n+                                     std::vector<uint32_t>* overflow_group_ids,\n+                                     std::vector<uint32_t>* overflow_hashes) {\n+  // Prepare parameters needed for scanning full slots in source.\n+  //\n+  int source_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(source->log_blocks_);\n+  uint64_t source_group_id_mask = ~0ULL >> (64 - source_group_id_bits);\n+  int64_t source_block_bytes = source_group_id_bits + 8;\n+  ARROW_DCHECK(source_block_bytes % sizeof(uint64_t) == 0);\n+\n+  // Compute index of the last block in target that corresponds to the given\n+  // partition.\n+  //\n+  ARROW_DCHECK(num_partition_bits <= target->log_blocks_);\n+  int64_t target_max_block_id =\n+      ((partition_id + 1) << (target->log_blocks_ - num_partition_bits)) - 1;\n+\n+  overflow_group_ids->clear();\n+  overflow_hashes->clear();\n+\n+  // For each source block...\n+  int64_t source_blocks = 1LL << source->log_blocks_;\n+  for (int64_t block_id = 0; block_id < source_blocks; ++block_id) {\n+    uint8_t* block_bytes = source->blocks_ + block_id * source_block_bytes;\n+    uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+    // For each non-empty source slot...\n+    constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+    constexpr int kSlotsPerBlock = 8;\n+    int num_full_slots =\n+        kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+    for (int local_slot_id = 0; local_slot_id < num_full_slots; ++local_slot_id) {\n+      // Read group id and hash for this slot.\n+      //\n+      uint64_t group_id =\n+          source->extract_group_id(block_bytes, local_slot_id, source_group_id_mask);\n+      int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+      uint32_t hash = source->hashes_[global_slot_id];\n+      // Insert partition id into the highest bits of hash, shifting the\n+      // remaining hash bits right.\n+      //\n+      hash >>= num_partition_bits;\n+      hash |= (partition_id << (SwissTable::bits_hash_ - 1 - num_partition_bits) << 1);\n+      // Add base group id\n+      //\n+      group_id += base_group_id;\n+\n+      // Insert new entry into target. Store in overflow vectors if not\n+      // successful.\n+      //\n+      bool was_inserted = InsertNewGroup(target, group_id, hash, target_max_block_id);\n+      if (!was_inserted) {\n+        overflow_group_ids->push_back(static_cast<uint32_t>(group_id));\n+        overflow_hashes->push_back(hash);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool SwissTableMerge::InsertNewGroup(SwissTable* target, uint64_t group_id,\n+                                            uint32_t hash, int64_t max_block_id) {\n+  // Load the first block to visit for this hash\n+  //\n+  int64_t block_id = hash >> (SwissTable::bits_hash_ - target->log_blocks_);\n+  int64_t block_id_mask = ((1LL << target->log_blocks_) - 1);\n+  int num_group_id_bits =\n+      SwissTable::num_groupid_bits_from_log_blocks(target->log_blocks_);\n+  int64_t num_block_bytes = num_group_id_bits + sizeof(uint64_t);\n+  ARROW_DCHECK(num_block_bytes % sizeof(uint64_t) == 0);\n+  uint8_t* block_bytes = target->blocks_ + block_id * num_block_bytes;\n+  uint64_t block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+\n+  // Search for the first block with empty slots.\n+  // Stop after reaching max block id.\n+  //\n+  constexpr uint64_t kHighBitOfEachByte = 0x8080808080808080ULL;\n+  while ((block & kHighBitOfEachByte) == 0 && block_id < max_block_id) {\n+    block_id = (block_id + 1) & block_id_mask;\n+    block_bytes = target->blocks_ + block_id * num_block_bytes;\n+    block = *reinterpret_cast<const uint64_t*>(block_bytes);\n+  }\n+  if ((block & kHighBitOfEachByte) == 0) {\n+    return false;\n+  }\n+  constexpr int kSlotsPerBlock = 8;\n+  int local_slot_id =\n+      kSlotsPerBlock - static_cast<int>(ARROW_POPCOUNT64(block & kHighBitOfEachByte));\n+  int64_t global_slot_id = block_id * kSlotsPerBlock + local_slot_id;\n+  target->insert_into_empty_slot(static_cast<uint32_t>(global_slot_id), hash,\n+                                 static_cast<uint32_t>(group_id));\n+  return true;\n+}\n+\n+void SwissTableMerge::InsertNewGroups(SwissTable* target,\n+                                      const std::vector<uint32_t>& group_ids,\n+                                      const std::vector<uint32_t>& hashes) {\n+  int64_t num_blocks = 1LL << target->log_blocks_;\n+  for (size_t i = 0; i < group_ids.size(); ++i) {\n+    std::ignore = InsertNewGroup(target, group_ids[i], hashes[i], num_blocks);\n+  }\n+}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_batch_start_row,\n+                                 int in_batch_end_row,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(in_batch_start_row),\n+      batch_end_row(in_batch_end_row),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(0),\n+      selection_maybe_null(nullptr),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(nullptr) {}\n+\n+SwissTableWithKeys::Input::Input(const ExecBatch* in_batch, int in_num_selected,\n+                                 const uint16_t* in_selection,\n+                                 util::TempVectorStack* in_temp_stack,\n+                                 std::vector<KeyColumnArray>* in_temp_column_arrays,\n+                                 std::vector<uint32_t>* in_temp_group_ids)\n+    : batch(in_batch),\n+      batch_start_row(0),\n+      batch_end_row(static_cast<int>(in_batch->length)),\n+      num_selected(in_num_selected),\n+      selection_maybe_null(in_selection),\n+      temp_stack(in_temp_stack),\n+      temp_column_arrays(in_temp_column_arrays),\n+      temp_group_ids(in_temp_group_ids) {}\n+\n+SwissTableWithKeys::Input::Input(const Input& base, int num_rows_to_skip,\n+                                 int num_rows_to_include)\n+    : batch(base.batch),\n+      temp_stack(base.temp_stack),\n+      temp_column_arrays(base.temp_column_arrays),\n+      temp_group_ids(base.temp_group_ids) {\n+  if (base.selection_maybe_null) {\n+    batch_start_row = 0;\n+    batch_end_row = static_cast<int>(batch->length);\n+    ARROW_DCHECK(num_rows_to_skip + num_rows_to_include <= base.num_selected);\n+    num_selected = num_rows_to_include;\n+    selection_maybe_null = base.selection_maybe_null + num_rows_to_skip;\n+  } else {\n+    ARROW_DCHECK(base.batch_start_row + num_rows_to_skip + num_rows_to_include <=\n+                 base.batch_end_row);\n+    batch_start_row = base.batch_start_row + num_rows_to_skip;\n+    batch_end_row = base.batch_start_row + num_rows_to_skip + num_rows_to_include;\n+    num_selected = 0;\n+    selection_maybe_null = nullptr;\n+  }\n+}\n+\n+Status SwissTableWithKeys::Init(int64_t hardware_flags, MemoryPool* pool) {\n+  InitCallbacks();\n+  return swiss_table_.init(hardware_flags, pool);\n+}\n+\n+void SwissTableWithKeys::EqualCallback(int num_keys, const uint16_t* selection_maybe_null,\n+                                       const uint32_t* group_ids,\n+                                       uint32_t* out_num_keys_mismatch,\n+                                       uint16_t* out_selection_mismatch,\n+                                       void* callback_ctx) {\n+  if (num_keys == 0) {\n+    *out_num_keys_mismatch = 0;\n+    return;\n+  }\n+\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int64_t hardware_flags = swiss_table_.hardware_flags();\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+  const uint32_t* group_ids_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    ARROW_DCHECK(in->temp_group_ids);\n+    in->temp_group_ids->resize(in->batch->length);\n+\n+    if (selection_maybe_null) {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t local_row_id = selection_maybe_null[i];\n+        uint16_t global_row_id = in->selection_maybe_null[local_row_id];\n+        selection_to_use_buf.mutable_data()[i] = global_row_id;\n+        (*in->temp_group_ids)[global_row_id] = group_ids[local_row_id];\n+      }\n+      selection_to_use = selection_to_use_buf.mutable_data();\n+    } else {\n+      for (int i = 0; i < num_keys; ++i) {\n+        uint16_t global_row_id = in->selection_maybe_null[i];\n+        (*in->temp_group_ids)[global_row_id] = group_ids[i];\n+      }\n+      selection_to_use = in->selection_maybe_null;\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    group_ids_to_use = in->temp_group_ids->data();\n+\n+    auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(in->temp_stack, num_keys);\n+    uint8_t* match_bitvector = match_bitvector_buf.mutable_data();\n+\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, nullptr, nullptr, hardware_flags,\n+                  in->temp_stack, *in->temp_column_arrays, match_bitvector);\n+\n+    if (selection_maybe_null) {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_filter_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                          selection_maybe_null, &num_keys_mismatch,\n+                                          out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    } else {\n+      int num_keys_mismatch = 0;\n+      util::bit_util::bits_to_indexes(0, hardware_flags, num_keys, match_bitvector,\n+                                      &num_keys_mismatch, out_selection_mismatch);\n+      *out_num_keys_mismatch = num_keys_mismatch;\n+    }\n+\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection_maybe_null;\n+    group_ids_to_use = group_ids;\n+    keys_.Compare(*in->batch, batch_start_to_use, batch_end_to_use, num_keys,\n+                  selection_to_use, group_ids_to_use, out_num_keys_mismatch,\n+                  out_selection_mismatch, hardware_flags, in->temp_stack,\n+                  *in->temp_column_arrays);\n+  }\n+}\n+\n+Status SwissTableWithKeys::AppendCallback(int num_keys, const uint16_t* selection,\n+                                          void* callback_ctx) {\n+  ARROW_DCHECK(num_keys <= swiss_table_.minibatch_size());\n+  ARROW_DCHECK(selection);\n+\n+  Input* in = reinterpret_cast<Input*>(callback_ctx);\n+\n+  int batch_start_to_use;\n+  int batch_end_to_use;\n+  const uint16_t* selection_to_use;\n+\n+  if (in->selection_maybe_null) {\n+    auto selection_to_use_buf =\n+        util::TempVectorHolder<uint16_t>(in->temp_stack, num_keys);\n+    for (int i = 0; i < num_keys; ++i) {\n+      selection_to_use_buf.mutable_data()[i] = in->selection_maybe_null[selection[i]];\n+    }\n+    batch_start_to_use = 0;\n+    batch_end_to_use = static_cast<int>(in->batch->length);\n+    selection_to_use = selection_to_use_buf.mutable_data();\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  } else {\n+    batch_start_to_use = in->batch_start_row;\n+    batch_end_to_use = in->batch_end_row;\n+    selection_to_use = selection;\n+\n+    return keys_.AppendBatchSelection(swiss_table_.pool(), *in->batch, batch_start_to_use,\n+                                      batch_end_to_use, num_keys, selection_to_use,\n+                                      *in->temp_column_arrays);\n+  }\n+}\n+\n+void SwissTableWithKeys::InitCallbacks() {\n+  equal_impl_ = [&](int num_keys, const uint16_t* selection_maybe_null,\n+                    const uint32_t* group_ids, uint32_t* out_num_keys_mismatch,\n+                    uint16_t* out_selection_mismatch, void* callback_ctx) {\n+    EqualCallback(num_keys, selection_maybe_null, group_ids, out_num_keys_mismatch,\n+                  out_selection_mismatch, callback_ctx);\n+  };\n+  append_impl_ = [&](int num_keys, const uint16_t* selection, void* callback_ctx) {\n+    return AppendCallback(num_keys, selection, callback_ctx);\n+  };\n+}\n+\n+void SwissTableWithKeys::Hash(Input* input, uint32_t* hashes, int64_t hardware_flags) {\n+  // Hashing does not support selection of rows\n+  //\n+  ARROW_DCHECK(input->selection_maybe_null == nullptr);\n+\n+  Status status =\n+      Hashing32::HashBatch(*input->batch, hashes, *input->temp_column_arrays,\n+                           hardware_flags, input->temp_stack, input->batch_start_row,\n+                           input->batch_end_row - input->batch_start_row);\n+  ARROW_DCHECK(status.ok());\n+}\n+\n+void SwissTableWithKeys::MapReadOnly(Input* input, const uint32_t* hashes,\n+                                     uint8_t* match_bitvector, uint32_t* key_ids) {\n+  std::ignore = Map(input, /*insert_missing=*/false, hashes, match_bitvector, key_ids);\n+}\n+\n+Status SwissTableWithKeys::MapWithInserts(Input* input, const uint32_t* hashes,\n+                                          uint32_t* key_ids) {\n+  return Map(input, /*insert_missing=*/true, hashes, nullptr, key_ids);\n+}\n+\n+Status SwissTableWithKeys::Map(Input* input, bool insert_missing, const uint32_t* hashes,\n+                               uint8_t* match_bitvector_maybe_null, uint32_t* key_ids) {\n+  util::TempVectorStack* temp_stack = input->temp_stack;\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = swiss_table_.minibatch_size();\n+  int num_rows_to_process = input->selection_maybe_null\n+                                ? input->num_selected\n+                                : input->batch_end_row - input->batch_start_row;\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  auto match_bitvector_buf = util::TempVectorHolder<uint8_t>(\n+      temp_stack,\n+      static_cast<uint32_t>(bit_util::BytesForBits(minibatch_size)) + sizeof(uint64_t));\n+  for (int minibatch_start = 0; minibatch_start < num_rows_to_process;) {\n+    int minibatch_size_next =\n+        std::min(minibatch_size, num_rows_to_process - minibatch_start);\n+\n+    // Prepare updated input buffers that represent the current minibatch.\n+    //\n+    Input minibatch_input(*input, minibatch_start, minibatch_size_next);\n+    uint8_t* minibatch_match_bitvector =\n+        insert_missing ? match_bitvector_buf.mutable_data()\n+                       : match_bitvector_maybe_null + minibatch_start / 8;\n+    const uint32_t* minibatch_hashes;\n+    if (input->selection_maybe_null) {\n+      minibatch_hashes = hashes_buf.mutable_data();\n+      for (int i = 0; i < minibatch_size_next; ++i) {\n+        hashes_buf.mutable_data()[i] = hashes[minibatch_input.selection_maybe_null[i]];\n+      }\n+    } else {\n+      minibatch_hashes = hashes + minibatch_start;\n+    }\n+    uint32_t* minibatch_key_ids = key_ids + minibatch_start;\n+\n+    // Lookup existing keys.\n+    {\n+      auto slots = util::TempVectorHolder<uint8_t>(temp_stack, minibatch_size_next);\n+      swiss_table_.early_filter(minibatch_size_next, minibatch_hashes,\n+                                minibatch_match_bitvector, slots.mutable_data());\n+      swiss_table_.find(minibatch_size_next, minibatch_hashes, minibatch_match_bitvector,\n+                        slots.mutable_data(), minibatch_key_ids, temp_stack, equal_impl_,\n+                        &minibatch_input);\n+    }\n+\n+    // Perform inserts of missing keys if required.\n+    //\n+    if (insert_missing) {\n+      auto ids_buf = util::TempVectorHolder<uint16_t>(temp_stack, minibatch_size_next);\n+      int num_ids;\n+      util::bit_util::bits_to_indexes(0, swiss_table_.hardware_flags(),\n+                                      minibatch_size_next, minibatch_match_bitvector,\n+                                      &num_ids, ids_buf.mutable_data());\n+\n+      RETURN_NOT_OK(swiss_table_.map_new_keys(\n+          num_ids, ids_buf.mutable_data(), minibatch_hashes, minibatch_key_ids,\n+          temp_stack, equal_impl_, append_impl_, &minibatch_input));\n+    }\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+\n+  return Status::OK();\n+}\n+\n+void SwissTableForJoin::Lookup(const ExecBatch& batch, int start_row, int num_rows,\n+                               uint8_t* out_has_match_bitvector, uint32_t* out_key_ids,\n+                               util::TempVectorStack* temp_stack,\n+                               std::vector<KeyColumnArray>* temp_column_arrays) {\n+  SwissTableWithKeys::Input input(&batch, start_row, start_row + num_rows, temp_stack,\n+                                  temp_column_arrays);\n+\n+  // Split into smaller mini-batches\n+  //\n+  int minibatch_size = map_.swiss_table()->minibatch_size();\n+  auto hashes_buf = util::TempVectorHolder<uint32_t>(temp_stack, minibatch_size);\n+  for (int minibatch_start = 0; minibatch_start < num_rows;) {\n+    uint32_t minibatch_size_next = std::min(minibatch_size, num_rows - minibatch_start);\n+\n+    SwissTableWithKeys::Input minibatch_input(input, minibatch_start,\n+                                              minibatch_size_next);\n+\n+    SwissTableWithKeys::Hash(&minibatch_input, hashes_buf.mutable_data(),\n+                             map_.swiss_table()->hardware_flags());\n+    map_.MapReadOnly(&minibatch_input, hashes_buf.mutable_data(),\n+                     out_has_match_bitvector + minibatch_start / 8,\n+                     out_key_ids + minibatch_start);\n+\n+    minibatch_start += minibatch_size_next;\n+  }\n+}\n+\n+uint8_t* SwissTableForJoin::local_has_match(int64_t thread_id) {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return nullptr;\n+  }\n+\n+  ThreadLocalState& local_state = local_states_[thread_id];\n+  if (local_state.has_match.empty() && num_rows_hash_table > 0) {\n+    local_state.has_match.resize(bit_util::BytesForBits(num_rows_hash_table) +\n+                                 sizeof(uint64_t));\n+    memset(local_state.has_match.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+  }\n+\n+  return local_states_[thread_id].has_match.data();\n+}\n+\n+void SwissTableForJoin::UpdateHasMatchForKeys(int64_t thread_id, int num_ids,\n+                                              const uint32_t* key_ids) {\n+  uint8_t* bit_vector = local_has_match(thread_id);\n+  if (num_ids == 0 || !bit_vector) {\n+    return;\n+  }\n+  for (int i = 0; i < num_ids; ++i) {\n+    // Mark row in hash table as having a match\n+    //\n+    bit_util::SetBit(bit_vector, key_ids[i]);\n+  }\n+}\n+\n+void SwissTableForJoin::MergeHasMatch() {\n+  int64_t num_rows_hash_table = num_rows();\n+  if (num_rows_hash_table == 0) {\n+    return;\n+  }\n+\n+  has_match_.resize(bit_util::BytesForBits(num_rows_hash_table) + sizeof(uint64_t));\n+  memset(has_match_.data(), 0, bit_util::BytesForBits(num_rows_hash_table));\n+\n+  for (size_t tid = 0; tid < local_states_.size(); ++tid) {\n+    if (!local_states_[tid].has_match.empty()) {\n+      arrow::internal::BitmapOr(has_match_.data(), 0, local_states_[tid].has_match.data(),\n+                                0, num_rows_hash_table, 0, has_match_.data());\n+    }\n+  }\n+}\n+\n+uint32_t SwissTableForJoin::payload_id_to_key_id(uint32_t payload_id) const {\n+  if (no_duplicate_keys_) {\n+    return payload_id;\n+  }\n+  int64_t num_entries = num_keys();\n+  const uint32_t* entries = key_to_payload();\n+  ARROW_DCHECK(entries);\n+  ARROW_DCHECK(entries[num_entries] > payload_id);\n+  const uint32_t* first_greater =\n+      std::upper_bound(entries, entries + num_entries + 1, payload_id);\n+  ARROW_DCHECK(first_greater > entries);\n+  return static_cast<uint32_t>(first_greater - entries) - 1;\n+}\n+\n+void SwissTableForJoin::payload_ids_to_key_ids(int num_rows, const uint32_t* payload_ids,\n+                                               uint32_t* key_ids) const {\n+  if (num_rows == 0) {\n+    return;\n+  }\n+  if (no_duplicate_keys_) {\n+    memcpy(key_ids, payload_ids, num_rows * sizeof(uint32_t));\n+    return;\n+  }\n+\n+  const uint32_t* entries = key_to_payload();\n+  uint32_t key_id = payload_id_to_key_id(payload_ids[0]);\n+  key_ids[0] = key_id;\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(payload_ids[i] > payload_ids[i - 1]);\n+    while (entries[key_id + 1] <= payload_ids[i]) {\n+      ++key_id;\n+      ARROW_DCHECK(key_id < num_keys());\n+    }\n+    key_ids[i] = key_id;\n+  }\n+}\n+\n+Status SwissTableForJoinBuild::Init(SwissTableForJoin* target, int dop, int64_t num_rows,\n+                                    bool reject_duplicate_keys, bool no_payload,\n+                                    const std::vector<KeyColumnMetadata>& key_types,\n+                                    const std::vector<KeyColumnMetadata>& payload_types,\n+                                    MemoryPool* pool, int64_t hardware_flags) {\n+  target_ = target;\n+  dop_ = dop;\n+  num_rows_ = num_rows;\n+\n+  // Make sure that we do not use many partitions if there are not enough rows.\n+  //\n+  constexpr int64_t min_num_rows_per_prtn = 1 << 18;\n+  log_num_prtns_ =\n+      std::min(bit_util::Log2(dop_),\n+               bit_util::Log2(bit_util::CeilDiv(num_rows, min_num_rows_per_prtn)));\n+  num_prtns_ = 1 << log_num_prtns_;\n+\n+  reject_duplicate_keys_ = reject_duplicate_keys;\n+  no_payload_ = no_payload;\n+  pool_ = pool;\n+  hardware_flags_ = hardware_flags;\n+\n+  prtn_states_.resize(num_prtns_);\n+  thread_states_.resize(dop_);\n+  prtn_locks_.Init(dop_, num_prtns_);\n+\n+  RowTableMetadata key_row_metadata;\n+  key_row_metadata.FromColumnMetadataVector(key_types,\n+                                            /*row_alignment=*/sizeof(uint64_t),\n+                                            /*string_alignment=*/sizeof(uint64_t));\n+  RowTableMetadata payload_row_metadata;\n+  payload_row_metadata.FromColumnMetadataVector(payload_types,\n+                                                /*row_alignment=*/sizeof(uint64_t),\n+                                                /*string_alignment=*/sizeof(uint64_t));\n+\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    PartitionState& prtn_state = prtn_states_[i];\n+    RETURN_NOT_OK(prtn_state.keys.Init(hardware_flags_, pool_));\n+    RETURN_NOT_OK(prtn_state.keys.keys()->InitIfNeeded(pool, key_row_metadata));\n+    RETURN_NOT_OK(prtn_state.payloads.InitIfNeeded(pool, payload_row_metadata));\n+  }\n+\n+  target_->dop_ = dop_;\n+  target_->local_states_.resize(dop_);\n+  target_->no_payload_columns_ = no_payload;\n+  target_->no_duplicate_keys_ = reject_duplicate_keys;\n+  target_->map_.InitCallbacks();\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PushNextBatch(int64_t thread_id,\n+                                             const ExecBatch& key_batch,\n+                                             const ExecBatch* payload_batch_maybe_null,\n+                                             util::TempVectorStack* temp_stack) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  // Compute hash\n+  //\n+  locals.batch_hashes.resize(key_batch.length);\n+  RETURN_NOT_OK(Hashing32::HashBatch(\n+      key_batch, locals.batch_hashes.data(), locals.temp_column_arrays, hardware_flags_,\n+      temp_stack, /*start_row=*/0, static_cast<int>(key_batch.length)));\n+\n+  // Partition on hash\n+  //\n+  locals.batch_prtn_row_ids.resize(locals.batch_hashes.size());\n+  locals.batch_prtn_ranges.resize(num_prtns_ + 1);\n+  int num_rows = static_cast<int>(locals.batch_hashes.size());\n+  if (num_prtns_ == 1) {\n+    // We treat single partition case separately to avoid extra checks in row\n+    // partitioning implementation for general case.\n+    //\n+    locals.batch_prtn_ranges[0] = 0;\n+    locals.batch_prtn_ranges[1] = num_rows;\n+    for (int i = 0; i < num_rows; ++i) {\n+      locals.batch_prtn_row_ids[i] = i;\n+    }\n+  } else {\n+    PartitionSort::Eval(\n+        static_cast<int>(locals.batch_hashes.size()), num_prtns_,\n+        locals.batch_prtn_ranges.data(),\n+        [this, &locals](int64_t i) {\n+          // SwissTable uses the highest bits of the hash for block index.\n+          // We want each partition to correspond to a range of block indices,\n+          // so we also partition on the highest bits of the hash.\n+          //\n+          return locals.batch_hashes[i] >> (31 - log_num_prtns_) >> 1;\n+        },\n+        [&locals](int64_t i, int pos) {\n+          locals.batch_prtn_row_ids[pos] = static_cast<uint16_t>(i);\n+        });\n+  }\n+\n+  // Update hashes, shifting left to get rid of the bits that were already used\n+  // for partitioning.\n+  //\n+  for (size_t i = 0; i < locals.batch_hashes.size(); ++i) {\n+    locals.batch_hashes[i] <<= log_num_prtns_;\n+  }\n+\n+  // For each partition:\n+  // - map keys to unique integers using (this partition's) hash table\n+  // - append payloads (if present) to (this partition's) row array\n+  //\n+  locals.temp_prtn_ids.resize(num_prtns_);\n+\n+  RETURN_NOT_OK(prtn_locks_.ForEachPartition(\n+      thread_id, locals.temp_prtn_ids.data(),\n+      /*is_prtn_empty_fn=*/\n+      [&](int prtn_id) {\n+        return locals.batch_prtn_ranges[prtn_id + 1] == locals.batch_prtn_ranges[prtn_id];\n+      },\n+      /*process_prtn_fn=*/\n+      [&](int prtn_id) {\n+        return ProcessPartition(thread_id, key_batch, payload_batch_maybe_null,\n+                                temp_stack, prtn_id);\n+      }));\n+\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::ProcessPartition(int64_t thread_id,\n+                                                const ExecBatch& key_batch,\n+                                                const ExecBatch* payload_batch_maybe_null,\n+                                                util::TempVectorStack* temp_stack,\n+                                                int prtn_id) {\n+  ARROW_DCHECK(thread_id < dop_);\n+  ThreadState& locals = thread_states_[thread_id];\n+\n+  int num_rows_new =\n+      locals.batch_prtn_ranges[prtn_id + 1] - locals.batch_prtn_ranges[prtn_id];\n+  const uint16_t* row_ids =\n+      locals.batch_prtn_row_ids.data() + locals.batch_prtn_ranges[prtn_id];\n+  PartitionState& prtn_state = prtn_states_[prtn_id];\n+  size_t num_rows_before = prtn_state.key_ids.size();\n+  // Insert new keys into hash table associated with the current partition\n+  // and map existing keys to integer ids.\n+  //\n+  prtn_state.key_ids.resize(num_rows_before + num_rows_new);\n+  SwissTableWithKeys::Input input(&key_batch, num_rows_new, row_ids, temp_stack,\n+                                  &locals.temp_column_arrays, &locals.temp_group_ids);\n+  RETURN_NOT_OK(prtn_state.keys.MapWithInserts(\n+      &input, locals.batch_hashes.data(), prtn_state.key_ids.data() + num_rows_before));\n+  // Append input batch rows from current partition to an array of payload\n+  // rows for this partition.\n+  //\n+  // The order of payloads is the same as the order of key ids accumulated\n+  // in a vector (we will use the vector of key ids later on to sort\n+  // payload on key ids before merging into the final row array).\n+  //\n+  if (!no_payload_) {\n+    ARROW_DCHECK(payload_batch_maybe_null);\n+    RETURN_NOT_OK(prtn_state.payloads.AppendBatchSelection(\n+        pool_, *payload_batch_maybe_null, 0,\n+        static_cast<int>(payload_batch_maybe_null->length), num_rows_new, row_ids,\n+        locals.temp_column_arrays));\n+  }\n+  // We do not need to keep track of key ids if we reject rows with\n+  // duplicate keys.\n+  //\n+  if (reject_duplicate_keys_) {\n+    prtn_state.key_ids.clear();\n+  }\n+  return Status::OK();\n+}\n+\n+Status SwissTableForJoinBuild::PreparePrtnMerge() {\n+  // There are 4 data structures that require partition merging:\n+  // 1. array of key rows\n+  // 2. SwissTable\n+  // 3. array of payload rows (only when no_payload_ is false)\n+  // 4. mapping from key id to first payload id (only when\n+  // reject_duplicate_keys_ is false and there are duplicate keys)\n+  //\n+\n+  // 1. Array of key rows:\n+  //\n+  std::vector<RowArray*> partition_keys;\n+  partition_keys.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_keys[i] = prtn_states_[i].keys.keys();\n+  }\n+  RETURN_NOT_OK(RowArrayMerge::PrepareForMerge(target_->map_.keys(), partition_keys,\n+                                               &partition_keys_first_row_id_, pool_));\n+\n+  // 2. SwissTable:\n+  //\n+  std::vector<SwissTable*> partition_tables;\n+  partition_tables.resize(num_prtns_);\n+  for (int i = 0; i < num_prtns_; ++i) {\n+    partition_tables[i] = prtn_states_[i].keys.swiss_table();\n+  }\n+  std::vector<uint32_t> partition_first_group_id;\n\nReview Comment:\n   So both RowArrayMerge::PrepareForMerge (called for encoded keys) and SwissTableMerge::PrepareForMerge calculate the same array of base offsets for key id. That is why this one is ignored. But from the point of view of SwissTableMerge, PrepareForMerge method computes base offsets for key id that MergePartition needs to use. \r\n   \r\n   Maybe I'll just allow SwissTableMerge::PrepareForMerge to take optionally null pointer for the output and skip writing it. Then hash join code would pass null explaining that this array would come out identical to the one from  RowArrayMerge::PrepareForMerge.\n\n\n\n",
                    "created": "2022-07-12T00:45:03.703+0000",
                    "updated": "2022-07-12T00:45:03.703+0000",
                    "started": "2022-07-12T00:45:03.703+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "789800",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/789801",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r918455139\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -0,0 +1,758 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+#include \"arrow/compute/exec/key_map.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/partition_util.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/exec/task_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/compute/light_array.h\"\n+#include \"arrow/compute/row/encode_internal.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+class RowArrayAccessor {\n+ public:\n+  // Find the index of this varbinary column within the sequence of all\n+  // varbinary columns encoded in rows.\n+  //\n+  static int VarbinaryColumnId(const RowTableMetadata& row_metadata, int column_id);\n+\n+  // Calculate how many rows to skip from the tail of the\n+  // sequence of selected rows, such that the total size of skipped rows is at\n+  // least equal to the size specified by the caller. Skipping of the tail rows\n+  // is used to allow for faster processing by the caller of remaining rows\n+  // without checking buffer bounds (useful with SIMD or fixed size memory loads\n+  // and stores).\n+  //\n+  static int NumRowsToSkip(const RowTableImpl& rows, int column_id, int num_rows,\n+                           const uint32_t* row_ids, int num_tail_bytes_to_skip);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - pointer to the value,\n+  // - byte length of the value.\n+  //\n+  // The information about nulls (validity bitmap) is not used in this call and\n+  // has to be processed separately.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void Visit(const RowTableImpl& rows, int column_id, int num_rows,\n+                    const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+  // The supplied lambda will be called for each row in the given list of rows.\n+  // The arguments given to it will be:\n+  // - index of a row (within the set of selected rows),\n+  // - byte 0xFF if the null is set for the row or 0x00 otherwise.\n+  //\n+  template <class PROCESS_VALUE_FN>\n+  static void VisitNulls(const RowTableImpl& rows, int column_id, int num_rows,\n+                         const uint32_t* row_ids, PROCESS_VALUE_FN process_value_fn);\n+\n+ private:\n+#if defined(ARROW_HAVE_AVX2)\n+  // This is equivalent to Visit method, but processing 8 rows at a time in a\n+  // loop.\n+  // Returns the number of processed rows, which may be less than requested (up\n+  // to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int Visit_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                        const uint32_t* row_ids, PROCESS_8_VALUES_FN process_8_values_fn);\n+\n+  // This is equivalent to VisitNulls method, but processing 8 rows at a time in\n+  // a loop. Returns the number of processed rows, which may be less than\n+  // requested (up to 7 rows at the end may be skipped).\n+  //\n+  template <class PROCESS_8_VALUES_FN>\n+  static int VisitNulls_avx2(const RowTableImpl& rows, int column_id, int num_rows,\n+                             const uint32_t* row_ids,\n+                             PROCESS_8_VALUES_FN process_8_values_fn);\n+#endif\n+};\n+\n+// Write operations (appending batch rows) must not be called by more than one\n+// thread at the same time.\n+//\n+// Read operations (row comparison, column decoding)\n+// can be called by multiple threads concurrently.\n+//\n+struct RowArray {\n+  RowArray() : is_initialized_(false) {}\n+\n+  Status InitIfNeeded(MemoryPool* pool, const ExecBatch& batch);\n+  Status InitIfNeeded(MemoryPool* pool, const RowTableMetadata& row_metadata);\n+\n+  Status AppendBatchSelection(MemoryPool* pool, const ExecBatch& batch, int begin_row_id,\n+                              int end_row_id, int num_row_ids, const uint16_t* row_ids,\n+                              std::vector<KeyColumnArray>& temp_column_arrays);\n+\n+  // This can only be called for a minibatch.\n+  //\n+  void Compare(const ExecBatch& batch, int begin_row_id, int end_row_id, int num_selected,\n+               const uint16_t* batch_selection_maybe_null, const uint32_t* array_row_ids,\n+               uint32_t* out_num_not_equal, uint16_t* out_not_equal_selection,\n+               int64_t hardware_flags, util::TempVectorStack* temp_stack,\n+               std::vector<KeyColumnArray>& temp_column_arrays,\n+               uint8_t* out_match_bitvector_maybe_null = NULLPTR);\n+\n+  // TODO: add AVX2 version\n+  //\n+  Status DecodeSelected(ResizableArrayData* target, int column_id, int num_rows_to_append,\n+                        const uint32_t* row_ids, MemoryPool* pool) const;\n+\n+  void DebugPrintToFile(const char* filename, bool print_sorted) const;\n+\n+  int64_t num_rows() const { return is_initialized_ ? rows_.length() : 0; }\n+\n+  bool is_initialized_;\n+  RowTableEncoder encoder_;\n+  RowTableImpl rows_;\n+  RowTableImpl rows_temp_;\n+};\n+\n+// Implements concatenating multiple row arrays into a single one, using\n+// potentially multiple threads, each processing a single input row array.\n+//\n+class RowArrayMerge {\n+ public:\n+  // Calculate total number of rows and size in bytes for merged sequence of\n+  // rows and allocate memory for it.\n+  //\n+  // If the rows are of varying length, initialize in the offset array the first\n+  // entry for the write area for each input row array. Leave all other\n+  // offsets and buffers uninitialized.\n+  //\n+  // All input sources must be initialized, but they can contain zero rows.\n+  //\n+  // Output in vector the first target row id for each source (exclusive\n+  // cummulative sum of number of rows in sources).\n+  //\n+  static Status PrepareForMerge(RowArray* target, const std::vector<RowArray*>& sources,\n+                                std::vector<int64_t>* first_target_row_id,\n+                                MemoryPool* pool);\n+\n+  // Copy rows from source array to target array.\n+  // Both arrays must have the same row metadata.\n+  // Target array must already have the memory reserved in all internal buffers\n+  // for the copy of the rows.\n+  //\n+  // Copy of the rows will occupy the same amount of space in the target array\n+  // buffers as in the source array, but in the target array we pick at what row\n+  // position and offset we start writing.\n+  //\n+  // Optionally, the rows may be reordered during copy according to the\n+  // provided permutation, which represents some sorting order of source rows.\n+  // Nth element of the permutation array is the source row index for the Nth\n+  // row written into target array. If permutation is missing (null), then the\n+  // order of source rows will remain unchanged.\n+  //\n+  // In case of varying length rows, we purposefully skip outputting of N+1 (one\n+  // after last) offset, to allow concurrent copies of rows done to adjacent\n+  // ranges in the target array. This offset should already contain the right\n+  // value after calling the method preparing target array for merge (which\n+  // initializes boundary offsets for target row ranges for each source).\n+  //\n+  static void MergeSingle(RowArray* target, const RowArray& source,\n+                          int64_t first_target_row_id,\n+                          const int64_t* source_rows_permutation);\n+\n+ private:\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for fixed length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyFixedLength(RowTableImpl* target, const RowTableImpl& source,\n+                              int64_t first_target_row_id,\n+                              const int64_t* source_rows_permutation);\n+\n+  // Copy rows from source array to a region of the target array.\n+  // This implementation is for varying length rows.\n+  // Null information needs to be handled separately.\n+  //\n+  static void CopyVaryingLength(RowTableImpl* target, const RowTableImpl& source,\n+                                int64_t first_target_row_id,\n+                                int64_t first_target_row_offset,\n+                                const int64_t* source_rows_permutation);\n+\n+  // Copy null information from rows from source array to a region of the target\n+  // array.\n+  //\n+  static void CopyNulls(RowTableImpl* target, const RowTableImpl& source,\n+                        int64_t first_target_row_id,\n+                        const int64_t* source_rows_permutation);\n+};\n+\n+// Implements merging of multiple SwissTables into a single one, using\n+// potentially multiple threads, each processing a single input source.\n+//\n+// Each source should correspond to a range of original hashes.\n+// A row belongs to a source with index determined by K highest bits of\n+// original hash. That means that the number of sources must be a power of 2.\n+//\n+// We assume that the hash values used and stored inside source tables\n+// have K highest bits removed from the original hash in order to avoid huge\n+// number of hash collisions that would occur otherwise.\n+// These bits will be reinserted back (original hashes will be used) when\n+// merging into target.\n+//\n+class SwissTableMerge {\n\nReview Comment:\n   SwissTable code refers to them as group ids and hash join code refers to them as key ids. So this piece got stuck in the middle of two naming words. I would say that group id is more generic, that is in the context of grouping on partitioning key in partitioning operation group ids may be called partition ids, in the context of grouping on join key in hash join operation they are called key ids (join key = grouping key).\r\n   \r\n   I can change to key ids for the consistency with the rest of the source file.\n\n\n\n",
                    "created": "2022-07-12T00:51:54.541+0000",
                    "updated": "2022-07-12T00:51:54.541+0000",
                    "started": "2022-07-12T00:51:54.541+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "789801",
                    "issueId": "13404218"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/worklog/789804",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #13493:\nURL: https://github.com/apache/arrow/pull/13493#discussion_r918457704\n\n\n##########\ncpp/src/arrow/compute/exec/hash_join_node.cc:\n##########\n@@ -654,6 +655,33 @@ struct BloomFilterPushdownContext {\n     FilterFinishedCallback on_finished_;\n   } eval_;\n };\n+bool HashJoinSchema::HasDictionaries() const {\n+  for (int side = 0; side <= 1; ++side) {\n+    for (int icol = 0; icol < proj_maps[side].num_cols(HashJoinProjection::INPUT);\n+         ++icol) {\n+      const std::shared_ptr<DataType>& column_type =\n+          proj_maps[side].data_type(HashJoinProjection::INPUT, icol);\n+      if (column_type->id() == Type::DICTIONARY) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+bool HashJoinSchema::HasLargeBinary() const {\n+  for (int side = 0; side <= 1; ++side) {\n+    for (int icol = 0; icol < proj_maps[side].num_cols(HashJoinProjection::INPUT);\n+         ++icol) {\n+      const std::shared_ptr<DataType>& column_type =\n+          proj_maps[side].data_type(HashJoinProjection::INPUT, icol);\n+      if (is_large_binary_like(column_type->id())) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n\nReview Comment:\n   I would rather move the class declaration from hash_join.h to hash_join_node.h.\r\n   HashJoinBasicImpl in hash_join.cc doesn't need it anymore (and same for SwissJoin), so HashJoinNode is the only user of that class now.\n\n\n\n",
                    "created": "2022-07-12T00:59:15.961+0000",
                    "updated": "2022-07-12T00:59:15.961+0000",
                    "started": "2022-07-12T00:59:15.961+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "789804",
                    "issueId": "13404218"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 24600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@4d7e5447[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6badc85f[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7cb4d717[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@795e17ee[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3cf3a8a2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@44a64d25[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1ef026c9[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@1718a39e[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@368faf67[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@403603f6[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5f6fa357[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@db9ef77[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 61200,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Jul 13 19:52:57 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-07-13T19:52:57.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-14182/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-09-30T07:56:50.000+0000",
        "updated": "2022-08-04T19:58:29.000+0000",
        "timeoriginalestimate": null,
        "description": "Add micro-benchmarks for hash join exec node.\r\nWrite a new implementation of the interface HashJoinImpl making sure that it is efficient for all types of join.\u00a0Current implementation, based on unordered map, trades performance for a simpler code and is likely not as fast as it could be.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "6h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 24600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Compute] Hash Join performance improvement",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/comment/17477182",
                    "id": "17477182",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "body": "Postponing it to 8.0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "created": "2022-01-17T12:40:42.366+0000",
                    "updated": "2022-01-17T12:40:42.366+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/comment/17525978",
                    "id": "17525978",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "body": "Postponing to 9.0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "created": "2022-04-21T18:47:37.894+0000",
                    "updated": "2022-04-21T18:47:37.894+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218/comment/17566477",
                    "id": "17566477",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Issue resolved by pull request 13493\n[https://github.com/apache/arrow/pull/13493]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2022-07-13T19:52:57.199+0000",
                    "updated": "2022-07-13T19:52:57.199+0000"
                }
            ],
            "maxResults": 3,
            "total": 3,
            "startAt": 0
        },
        "customfield_12311820": "0|z0vg0g:",
        "customfield_12314139": null
    }
}