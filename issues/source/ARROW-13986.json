{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13400825",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825",
    "key": "ARROW-13986",
    "fields": {
        "parent": {
            "id": "13286806",
            "key": "ARROW-7905",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13286806",
            "fields": {
                "summary": "[Go][Parquet] Port the C++ Parquet implementation to Go",
                "status": {
                    "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                    "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                    "name": "Resolved",
                    "id": "5",
                    "statusCategory": {
                        "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                        "id": 3,
                        "key": "done",
                        "colorName": "green",
                        "name": "Done"
                    }
                },
                "priority": {
                    "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                    "name": "Minor",
                    "id": "4"
                },
                "issuetype": {
                    "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                    "id": "2",
                    "description": "A new feature of the product, which has yet to be developed.",
                    "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                    "name": "New Feature",
                    "subtask": false,
                    "avatarId": 21141
                }
            }
        },
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350591",
                "id": "12350591",
                "description": "",
                "name": "7.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-02-03"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12622933",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12622933",
                "type": {
                    "id": "10020",
                    "name": "Cloners",
                    "inward": "is cloned by",
                    "outward": "is a clone of",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10020"
                },
                "outwardIssue": {
                    "id": "13400809",
                    "key": "ARROW-13984",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400809",
                    "fields": {
                        "summary": "[Go][Parquet] Add File Package - readers",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                            "id": "7",
                            "description": "The sub-task of the issue",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                            "name": "Sub-task",
                            "subtask": true,
                            "avatarId": 21146
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
            "name": "zeroshade",
            "key": "zeroshade",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
            },
            "displayName": "Matthew Topol",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12333772",
                "id": "12333772",
                "name": "Go"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12337837",
                "id": "12337837",
                "name": "Parquet"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
            "name": "zeroshade",
            "key": "zeroshade",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
            },
            "displayName": "Matthew Topol",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
            "name": "zeroshade",
            "key": "zeroshade",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
            },
            "displayName": "Matthew Topol",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 47400,
            "total": 47400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 47400,
            "total": 47400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-13986/votes",
            "votes": 1,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 79,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/669692",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#issuecomment-951180626\n\n\n   https://issues.apache.org/jira/browse/ARROW-13986\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-25T18:15:31.602+0000",
                    "updated": "2021-10-25T18:15:31.602+0000",
                    "started": "2021-10-25T18:15:31.602+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "669692",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/670953",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "WilliamWhispell commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r737746900\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n\nReview comment:\n       I assume testing 2 cols is arbitrary? Why not test with 3?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T18:39:10.951+0000",
                    "updated": "2021-10-27T18:39:10.951+0000",
                    "started": "2021-10-27T18:39:10.951+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670953",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/670961",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "WilliamWhispell commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r737753679\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key. Supplies\n+ *                                  aad_prefix to verify file identity.\n+ *  - Decryption configuration 3:   Decrypt using explicit column and footer keys\n+ *                                  (instead of key retrieval callback).\n+ *  - Decryption Configuration 4:   PlainText Footer mode - test legacy reads,\n+ *                                  read the footer + all non-encrypted columns.\n+ *                                  (pairs with encryption configuration 3)\n+ *\n+ * The encrypted parquet files that is read was encrypted using one of the configurations\n+ * below:\n+ *\n+ *  - Encryption configuration 1:   Encrypt all columns and the footer with the same key.\n+ *                                  (uniform encryption)\n+ *  - Encryption configuration 2:   Encrypt two columns and the footer, with different\n+ *                                  keys.\n+ *  - Encryption configuration 3:   Encrypt two columns, with different keys.\n+ *                                  Don\u2019t encrypt footer (to enable legacy readers)\n+ *                                  - plaintext footer mode.\n+ *  - Encryption configuration 4:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix for file identity\n+ *                                  verification.\n+ *  - Encryption configuration 5:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix, and call\n+ *                                  disable_aad_prefix_storage to prevent file\n+ *                                  identity storage in file metadata.\n+ *  - Encryption configuration 6:   Encrypt two columns and the footer, with different\n+ *                                  keys. Use the alternative (AES_GCM_CTR_V1) algorithm.\n+ */\n+\n+func getDataDir() string {\n+\tdatadir := os.Getenv(\"PARQUET_TEST_DATA\")\n+\tif datadir == \"\" {\n+\t\tpanic(\"please point the PARQUET_TEST_DATA environment variable to the test data dir\")\n+\t}\n+\treturn datadir\n+}\n+\n+type TestDecryptionSuite struct {\n+\tsuite.Suite\n+\n+\tpathToDouble        string\n+\tpathToFloat         string\n+\tdecryptionConfigs   []*parquet.FileDecryptionProperties\n+\tfooterEncryptionKey string\n+\tcolEncryptionKey1   string\n+\tcolEncryptionKey2   string\n+\tfileName            string\n+}\n+\n+func (d *TestDecryptionSuite) TearDownSuite() {\n+\tos.Remove(tempdir)\n+}\n+\n+func TestFileEncryptionDecryption(t *testing.T) {\n+\tsuite.Run(t, new(EncryptionConfigTestSuite))\n+\tsuite.Run(t, new(TestDecryptionSuite))\n+}\n+\n+func (d *TestDecryptionSuite) SetupSuite() {\n+\td.pathToDouble = \"double_field\"\n+\td.pathToFloat = \"float_field\"\n+\td.footerEncryptionKey = FooterEncryptionKey\n+\td.colEncryptionKey1 = ColumnEncryptionKey1\n+\td.colEncryptionKey2 = ColumnEncryptionKey2\n+\td.fileName = FileName\n+\n+\td.createDecryptionConfigs()\n+}\n+\n+func (d *TestDecryptionSuite) createDecryptionConfigs() {\n+\t// Decryption configuration 1: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key.\n+\tstringKr1 := make(encryption.StringKeyIDRetriever)\n+\tstringKr1.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr1.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr1.PutKey(\"kc2\", d.colEncryptionKey2)\n+\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr1)))\n+\n+\t// Decryption configuration 2: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key. Supply aad_prefix.\n+\tstringKr2 := make(encryption.StringKeyIDRetriever)\n+\tstringKr2.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr2.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr2.PutKey(\"kc2\", d.colEncryptionKey2)\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr2), parquet.WithDecryptAadPrefix(d.fileName)))\n+\n+\t// Decryption configuration 3: Decrypt using explicit column and footer keys. Supply\n+\t// aad_prefix.\n+\tdecryptCols := make(parquet.ColumnPathToDecryptionPropsMap)\n+\tdecryptCols[d.pathToFloat] = parquet.NewColumnDecryptionProperties(d.pathToFloat, parquet.WithDecryptKey(d.colEncryptionKey2))\n+\tdecryptCols[d.pathToDouble] = parquet.NewColumnDecryptionProperties(d.pathToDouble, parquet.WithDecryptKey(d.colEncryptionKey1))\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithFooterKey(d.footerEncryptionKey), parquet.WithColumnKeys(decryptCols)))\n+\n+\t// Decryption Configuration 4: use plaintext footer mode, read only footer + plaintext\n+\t// columns.\n+\td.decryptionConfigs = append(d.decryptionConfigs, nil)\n+}\n+\n+func (d *TestDecryptionSuite) decryptFile(filename string, decryptConfigNum int) {\n+\t// if we get decryption_config_num = x then it means the actual number is x+1\n+\t// and since we want decryption_config_num=4 we set the condition to 3\n+\tprops := parquet.NewReaderProperties(memory.DefaultAllocator)\n+\tif decryptConfigNum != 3 {\n\nReview comment:\n       It would be nice if we had labels for the type of configuration, 1-6 doesn't tell us much about the chosen configuration\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T18:49:05.610+0000",
                    "updated": "2021-10-27T18:49:05.610+0000",
                    "started": "2021-10-27T18:49:05.609+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670961",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/670964",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "WilliamWhispell commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r737758398\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key. Supplies\n+ *                                  aad_prefix to verify file identity.\n+ *  - Decryption configuration 3:   Decrypt using explicit column and footer keys\n+ *                                  (instead of key retrieval callback).\n+ *  - Decryption Configuration 4:   PlainText Footer mode - test legacy reads,\n+ *                                  read the footer + all non-encrypted columns.\n+ *                                  (pairs with encryption configuration 3)\n+ *\n+ * The encrypted parquet files that is read was encrypted using one of the configurations\n+ * below:\n+ *\n+ *  - Encryption configuration 1:   Encrypt all columns and the footer with the same key.\n+ *                                  (uniform encryption)\n+ *  - Encryption configuration 2:   Encrypt two columns and the footer, with different\n+ *                                  keys.\n+ *  - Encryption configuration 3:   Encrypt two columns, with different keys.\n+ *                                  Don\u2019t encrypt footer (to enable legacy readers)\n+ *                                  - plaintext footer mode.\n+ *  - Encryption configuration 4:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix for file identity\n+ *                                  verification.\n+ *  - Encryption configuration 5:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix, and call\n+ *                                  disable_aad_prefix_storage to prevent file\n+ *                                  identity storage in file metadata.\n+ *  - Encryption configuration 6:   Encrypt two columns and the footer, with different\n+ *                                  keys. Use the alternative (AES_GCM_CTR_V1) algorithm.\n+ */\n+\n+func getDataDir() string {\n+\tdatadir := os.Getenv(\"PARQUET_TEST_DATA\")\n+\tif datadir == \"\" {\n+\t\tpanic(\"please point the PARQUET_TEST_DATA environment variable to the test data dir\")\n+\t}\n+\treturn datadir\n+}\n+\n+type TestDecryptionSuite struct {\n+\tsuite.Suite\n+\n+\tpathToDouble        string\n+\tpathToFloat         string\n+\tdecryptionConfigs   []*parquet.FileDecryptionProperties\n+\tfooterEncryptionKey string\n+\tcolEncryptionKey1   string\n+\tcolEncryptionKey2   string\n+\tfileName            string\n+}\n+\n+func (d *TestDecryptionSuite) TearDownSuite() {\n+\tos.Remove(tempdir)\n+}\n+\n+func TestFileEncryptionDecryption(t *testing.T) {\n+\tsuite.Run(t, new(EncryptionConfigTestSuite))\n+\tsuite.Run(t, new(TestDecryptionSuite))\n+}\n+\n+func (d *TestDecryptionSuite) SetupSuite() {\n+\td.pathToDouble = \"double_field\"\n+\td.pathToFloat = \"float_field\"\n+\td.footerEncryptionKey = FooterEncryptionKey\n+\td.colEncryptionKey1 = ColumnEncryptionKey1\n+\td.colEncryptionKey2 = ColumnEncryptionKey2\n+\td.fileName = FileName\n+\n+\td.createDecryptionConfigs()\n+}\n+\n+func (d *TestDecryptionSuite) createDecryptionConfigs() {\n+\t// Decryption configuration 1: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key.\n+\tstringKr1 := make(encryption.StringKeyIDRetriever)\n+\tstringKr1.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr1.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr1.PutKey(\"kc2\", d.colEncryptionKey2)\n+\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr1)))\n+\n+\t// Decryption configuration 2: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key. Supply aad_prefix.\n+\tstringKr2 := make(encryption.StringKeyIDRetriever)\n+\tstringKr2.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr2.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr2.PutKey(\"kc2\", d.colEncryptionKey2)\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr2), parquet.WithDecryptAadPrefix(d.fileName)))\n+\n+\t// Decryption configuration 3: Decrypt using explicit column and footer keys. Supply\n+\t// aad_prefix.\n+\tdecryptCols := make(parquet.ColumnPathToDecryptionPropsMap)\n+\tdecryptCols[d.pathToFloat] = parquet.NewColumnDecryptionProperties(d.pathToFloat, parquet.WithDecryptKey(d.colEncryptionKey2))\n+\tdecryptCols[d.pathToDouble] = parquet.NewColumnDecryptionProperties(d.pathToDouble, parquet.WithDecryptKey(d.colEncryptionKey1))\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithFooterKey(d.footerEncryptionKey), parquet.WithColumnKeys(decryptCols)))\n+\n+\t// Decryption Configuration 4: use plaintext footer mode, read only footer + plaintext\n+\t// columns.\n+\td.decryptionConfigs = append(d.decryptionConfigs, nil)\n+}\n+\n+func (d *TestDecryptionSuite) decryptFile(filename string, decryptConfigNum int) {\n+\t// if we get decryption_config_num = x then it means the actual number is x+1\n+\t// and since we want decryption_config_num=4 we set the condition to 3\n+\tprops := parquet.NewReaderProperties(memory.DefaultAllocator)\n+\tif decryptConfigNum != 3 {\n+\t\tprops.FileDecryptProps = d.decryptionConfigs[decryptConfigNum].Clone(\"\")\n+\t}\n+\n+\tfileReader, err := file.OpenParquetFile(filename, false, file.WithReadProps(props))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\tdefer fileReader.Close()\n+\t// get metadata\n+\tfileMetadata := fileReader.MetaData()\n+\t// get number of rowgroups\n+\tnumRowGroups := len(fileMetadata.RowGroups)\n+\t// number of columns\n+\tnumColumns := fileMetadata.Schema.NumColumns()\n+\td.Equal(8, numColumns)\n+\n+\tfor r := 0; r < numRowGroups; r++ {\n+\t\trowGroupReader := fileReader.RowGroup(r)\n+\n+\t\t// get rowgroup meta\n+\t\trgMeta := fileMetadata.RowGroup(r)\n+\n+\t\tvaluesRead := 0\n+\t\trowsRead := int64(0)\n+\n+\t\t// get col reader for boolean column\n+\t\tcolReader := rowGroupReader.Column(0)\n+\t\tboolReader := colReader.(*file.BooleanColumnChunkReader)\n+\n+\t\t// get column chunk metadata for boolean column\n+\t\tboolMd, _ := rgMeta.ColumnChunk(0)\n+\n+\t\t// Read all rows in column\n+\t\ti := 0\n+\t\tfor boolReader.HasNext() {\n+\t\t\tvar val [1]bool\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = boolReader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\texpected := i%2 == 0\n+\t\t\td.Equal(expected, val[0], \"i: \", i)\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, boolMd.NumValues())\n+\n+\t\t// Get column reader for int32 column\n+\t\tcolReader = rowGroupReader.Column(1)\n+\t\tint32reader := colReader.(*file.Int32ColumnChunkReader)\n+\n+\t\tint32md, _ := rgMeta.ColumnChunk(1)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int32reader.HasNext() {\n+\t\t\tvar val [1]int32\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int32reader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\td.EqualValues(i, val[0])\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int32md.NumValues())\n+\n+\t\t// Get column reader for int64 column\n+\t\tcolReader = rowGroupReader.Column(2)\n+\t\tint64reader := colReader.(*file.Int64ColumnChunkReader)\n+\n+\t\tint64md, _ := rgMeta.ColumnChunk(2)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int64reader.HasNext() {\n+\t\t\tvar (\n+\t\t\t\tval [1]int64\n+\t\t\t\tdef [1]int16\n+\t\t\t\trep [1]int16\n+\t\t\t)\n+\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int64reader.ReadBatch(1, val[:], def[:], rep[:])\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\texpectedValue := int64(i) * 1000 * 1000 * 1000 * 1000\n+\t\t\td.Equal(expectedValue, val[0])\n+\t\t\tif i%2 == 0 {\n+\t\t\t\td.EqualValues(1, rep[0])\n+\t\t\t} else {\n+\t\t\t\td.Zero(rep[0])\n+\t\t\t}\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int64md.NumValues())\n+\n+\t\t// Get column reader for int96 column\n+\t\tcolReader = rowGroupReader.Column(3)\n+\t\tint96reader := colReader.(*file.Int96ColumnChunkReader)\n+\n+\t\tint96md, _ := rgMeta.ColumnChunk(3)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int96reader.HasNext() {\n+\t\t\tvar (\n+\t\t\t\tval [1]parquet.Int96\n+\t\t\t)\n+\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int96reader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\tvar expectedValue parquet.Int96\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[:4], uint32(i))\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[4:], uint32(i+1))\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[8:], uint32(i+2))\n+\t\t\td.Equal(expectedValue, val[0])\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int96md.NumValues())\n+\n+\t\tif decryptConfigNum != 3 {\n+\t\t\t// Get column reader for the float column\n+\t\t\tcolReader = rowGroupReader.Column(4)\n+\t\t\tfloatReader := colReader.(*file.Float32ColumnChunkReader)\n+\n+\t\t\tfloatmd, _ := rgMeta.ColumnChunk(4)\n+\n+\t\t\ti = 0\n+\t\t\tfor floatReader.HasNext() {\n+\t\t\t\tvar value [1]float32\n+\t\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t\t// read contains the number of non-null rows\n+\t\t\t\trowsRead, valuesRead, _ = floatReader.ReadBatch(1, value[:], nil, nil)\n+\t\t\t\t// ensure only 1 value is read\n+\t\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t\t// there are no null values\n+\t\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t\t// verify the value\n+\t\t\t\texpectedValue := float32(i) * 1.1\n+\t\t\t\td.Equal(expectedValue, value[0])\n+\t\t\t\ti++\n+\t\t\t}\n+\t\t\td.EqualValues(i, floatmd.NumValues())\n+\n+\t\t\t// Get column reader for the double column\n+\t\t\tcolReader = rowGroupReader.Column(5)\n+\t\t\tdblReader := colReader.(*file.Float64ColumnChunkReader)\n+\n+\t\t\tdblmd, _ := rgMeta.ColumnChunk(5)\n+\n+\t\t\ti = 0\n+\t\t\tfor dblReader.HasNext() {\n+\t\t\t\tvar value [1]float64\n+\t\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t\t// read contains the number of non-null rows\n+\t\t\t\trowsRead, valuesRead, _ = dblReader.ReadBatch(1, value[:], nil, nil)\n+\t\t\t\t// ensure only 1 value is read\n+\t\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t\t// there are no null values\n+\t\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t\t// verify the value\n+\t\t\t\texpectedValue := float64(i) * 1.1111111\n+\t\t\t\td.Equal(expectedValue, value[0])\n+\t\t\t\ti++\n+\t\t\t}\n+\t\t\td.EqualValues(i, dblmd.NumValues())\n+\t\t}\n+\n+\t\tcolReader = rowGroupReader.Column(6)\n+\t\tbareader := colReader.(*file.ByteArrayColumnChunkReader)\n+\n+\t\tbamd, _ := rgMeta.ColumnChunk(6)\n+\n+\t\ti = 0\n+\t\tfor bareader.HasNext() {\n+\t\t\tvar value [1]parquet.ByteArray\n+\t\t\tvar def [1]int16\n+\n+\t\t\trowsRead, valuesRead, _ := bareader.ReadBatch(1, value[:], def[:], nil)\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\texpected := [10]byte{'p', 'a', 'r', 'q', 'u', 'e', 't', 0, 0, 0}\n+\t\t\texpected[7] = byte('0') + byte(i/100)\n+\t\t\texpected[8] = byte('0') + byte(i/10)%10\n+\t\t\texpected[9] = byte('0') + byte(i%10)\n+\t\t\tif i%2 == 0 {\n+\t\t\t\td.Equal(1, valuesRead)\n+\t\t\t\td.Len(value[0], 10)\n+\t\t\t\td.EqualValues(expected[:], value[0])\n+\t\t\t\td.EqualValues(1, def[0])\n+\t\t\t} else {\n+\t\t\t\td.Zero(valuesRead)\n+\t\t\t\td.Zero(def[0])\n+\t\t\t}\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, bamd.NumValues())\n+\t}\n+}\n+\n+func (d *TestDecryptionSuite) checkResults(fileName string, decryptionConfig, encryptionConfig uint) {\n+\tdecFn := func() { d.decryptFile(fileName, int(decryptionConfig-1)) }\n+\n+\t// Encryption configuration number 5 contains aad_prefix and disable_aad_prefix_storage\n+\t// an exception is expected to be thrown if the file is not decrypted with aad_prefix\n+\tif encryptionConfig == 5 {\n+\t\tif decryptionConfig == 1 || decryptionConfig == 3 {\n+\t\t\td.Panics(decFn)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// decryption config number two contains aad_prefix. an exception\n+\t// is expected to be thrown if the file was not encrypted with the same aad_prefix\n+\tif decryptionConfig == 2 {\n+\t\tif encryptionConfig != 5 && encryptionConfig != 4 {\n+\t\t\td.Panics(decFn)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// decryption config 4 can only work when the encryption config is 3\n+\tif decryptionConfig == 4 && encryptionConfig != 3 {\n+\t\treturn\n+\t}\n+\td.NotPanics(decFn)\n+}\n+\n+// Read encrypted parquet file.\n+// the test reads two parquet files that were encrypted using the same encryption config\n+// one was generated in encryption_write_configurations_test.go tests and is deleted\n+// once the file is read and the second exists in parquet-testing/data folder\n+func (d *TestDecryptionSuite) TestDecryption() {\n+\ttests := []struct {\n+\t\tfile   string\n+\t\tconfig uint\n+\t}{\n+\t\t{\"uniform_encryption.parquet.encrypted\", 1},\n\nReview comment:\n       Could be moved to enum string type maybe and exported\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T18:56:07.929+0000",
                    "updated": "2021-10-27T18:56:07.929+0000",
                    "started": "2021-10-27T18:56:07.929+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670964",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/670972",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r737769470\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n\nReview comment:\n       i'm testing with what is in the pre-generated encrypted files in the parquet-testing-data. If you wanna know why not 3 columns, you'd need to ask whomever originally set these tests up for the C++ :smile:\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T19:12:57.989+0000",
                    "updated": "2021-10-27T19:12:57.989+0000",
                    "started": "2021-10-27T19:12:57.988+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670972",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/670973",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r737769689\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key. Supplies\n+ *                                  aad_prefix to verify file identity.\n+ *  - Decryption configuration 3:   Decrypt using explicit column and footer keys\n+ *                                  (instead of key retrieval callback).\n+ *  - Decryption Configuration 4:   PlainText Footer mode - test legacy reads,\n+ *                                  read the footer + all non-encrypted columns.\n+ *                                  (pairs with encryption configuration 3)\n+ *\n+ * The encrypted parquet files that is read was encrypted using one of the configurations\n+ * below:\n+ *\n+ *  - Encryption configuration 1:   Encrypt all columns and the footer with the same key.\n+ *                                  (uniform encryption)\n+ *  - Encryption configuration 2:   Encrypt two columns and the footer, with different\n+ *                                  keys.\n+ *  - Encryption configuration 3:   Encrypt two columns, with different keys.\n+ *                                  Don\u2019t encrypt footer (to enable legacy readers)\n+ *                                  - plaintext footer mode.\n+ *  - Encryption configuration 4:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix for file identity\n+ *                                  verification.\n+ *  - Encryption configuration 5:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix, and call\n+ *                                  disable_aad_prefix_storage to prevent file\n+ *                                  identity storage in file metadata.\n+ *  - Encryption configuration 6:   Encrypt two columns and the footer, with different\n+ *                                  keys. Use the alternative (AES_GCM_CTR_V1) algorithm.\n+ */\n+\n+func getDataDir() string {\n+\tdatadir := os.Getenv(\"PARQUET_TEST_DATA\")\n+\tif datadir == \"\" {\n+\t\tpanic(\"please point the PARQUET_TEST_DATA environment variable to the test data dir\")\n+\t}\n+\treturn datadir\n+}\n+\n+type TestDecryptionSuite struct {\n+\tsuite.Suite\n+\n+\tpathToDouble        string\n+\tpathToFloat         string\n+\tdecryptionConfigs   []*parquet.FileDecryptionProperties\n+\tfooterEncryptionKey string\n+\tcolEncryptionKey1   string\n+\tcolEncryptionKey2   string\n+\tfileName            string\n+}\n+\n+func (d *TestDecryptionSuite) TearDownSuite() {\n+\tos.Remove(tempdir)\n+}\n+\n+func TestFileEncryptionDecryption(t *testing.T) {\n+\tsuite.Run(t, new(EncryptionConfigTestSuite))\n+\tsuite.Run(t, new(TestDecryptionSuite))\n+}\n+\n+func (d *TestDecryptionSuite) SetupSuite() {\n+\td.pathToDouble = \"double_field\"\n+\td.pathToFloat = \"float_field\"\n+\td.footerEncryptionKey = FooterEncryptionKey\n+\td.colEncryptionKey1 = ColumnEncryptionKey1\n+\td.colEncryptionKey2 = ColumnEncryptionKey2\n+\td.fileName = FileName\n+\n+\td.createDecryptionConfigs()\n+}\n+\n+func (d *TestDecryptionSuite) createDecryptionConfigs() {\n+\t// Decryption configuration 1: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key.\n+\tstringKr1 := make(encryption.StringKeyIDRetriever)\n+\tstringKr1.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr1.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr1.PutKey(\"kc2\", d.colEncryptionKey2)\n+\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr1)))\n+\n+\t// Decryption configuration 2: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key. Supply aad_prefix.\n+\tstringKr2 := make(encryption.StringKeyIDRetriever)\n+\tstringKr2.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr2.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr2.PutKey(\"kc2\", d.colEncryptionKey2)\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr2), parquet.WithDecryptAadPrefix(d.fileName)))\n+\n+\t// Decryption configuration 3: Decrypt using explicit column and footer keys. Supply\n+\t// aad_prefix.\n+\tdecryptCols := make(parquet.ColumnPathToDecryptionPropsMap)\n+\tdecryptCols[d.pathToFloat] = parquet.NewColumnDecryptionProperties(d.pathToFloat, parquet.WithDecryptKey(d.colEncryptionKey2))\n+\tdecryptCols[d.pathToDouble] = parquet.NewColumnDecryptionProperties(d.pathToDouble, parquet.WithDecryptKey(d.colEncryptionKey1))\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithFooterKey(d.footerEncryptionKey), parquet.WithColumnKeys(decryptCols)))\n+\n+\t// Decryption Configuration 4: use plaintext footer mode, read only footer + plaintext\n+\t// columns.\n+\td.decryptionConfigs = append(d.decryptionConfigs, nil)\n+}\n+\n+func (d *TestDecryptionSuite) decryptFile(filename string, decryptConfigNum int) {\n+\t// if we get decryption_config_num = x then it means the actual number is x+1\n+\t// and since we want decryption_config_num=4 we set the condition to 3\n+\tprops := parquet.NewReaderProperties(memory.DefaultAllocator)\n+\tif decryptConfigNum != 3 {\n\nReview comment:\n       that's why the comment at the top explains the configurations\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T19:13:13.336+0000",
                    "updated": "2021-10-27T19:13:13.336+0000",
                    "started": "2021-10-27T19:13:13.335+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670973",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/670974",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "zeroshade commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r737769904\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key. Supplies\n+ *                                  aad_prefix to verify file identity.\n+ *  - Decryption configuration 3:   Decrypt using explicit column and footer keys\n+ *                                  (instead of key retrieval callback).\n+ *  - Decryption Configuration 4:   PlainText Footer mode - test legacy reads,\n+ *                                  read the footer + all non-encrypted columns.\n+ *                                  (pairs with encryption configuration 3)\n+ *\n+ * The encrypted parquet files that is read was encrypted using one of the configurations\n+ * below:\n+ *\n+ *  - Encryption configuration 1:   Encrypt all columns and the footer with the same key.\n+ *                                  (uniform encryption)\n+ *  - Encryption configuration 2:   Encrypt two columns and the footer, with different\n+ *                                  keys.\n+ *  - Encryption configuration 3:   Encrypt two columns, with different keys.\n+ *                                  Don\u2019t encrypt footer (to enable legacy readers)\n+ *                                  - plaintext footer mode.\n+ *  - Encryption configuration 4:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix for file identity\n+ *                                  verification.\n+ *  - Encryption configuration 5:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix, and call\n+ *                                  disable_aad_prefix_storage to prevent file\n+ *                                  identity storage in file metadata.\n+ *  - Encryption configuration 6:   Encrypt two columns and the footer, with different\n+ *                                  keys. Use the alternative (AES_GCM_CTR_V1) algorithm.\n+ */\n+\n+func getDataDir() string {\n+\tdatadir := os.Getenv(\"PARQUET_TEST_DATA\")\n+\tif datadir == \"\" {\n+\t\tpanic(\"please point the PARQUET_TEST_DATA environment variable to the test data dir\")\n+\t}\n+\treturn datadir\n+}\n+\n+type TestDecryptionSuite struct {\n+\tsuite.Suite\n+\n+\tpathToDouble        string\n+\tpathToFloat         string\n+\tdecryptionConfigs   []*parquet.FileDecryptionProperties\n+\tfooterEncryptionKey string\n+\tcolEncryptionKey1   string\n+\tcolEncryptionKey2   string\n+\tfileName            string\n+}\n+\n+func (d *TestDecryptionSuite) TearDownSuite() {\n+\tos.Remove(tempdir)\n+}\n+\n+func TestFileEncryptionDecryption(t *testing.T) {\n+\tsuite.Run(t, new(EncryptionConfigTestSuite))\n+\tsuite.Run(t, new(TestDecryptionSuite))\n+}\n+\n+func (d *TestDecryptionSuite) SetupSuite() {\n+\td.pathToDouble = \"double_field\"\n+\td.pathToFloat = \"float_field\"\n+\td.footerEncryptionKey = FooterEncryptionKey\n+\td.colEncryptionKey1 = ColumnEncryptionKey1\n+\td.colEncryptionKey2 = ColumnEncryptionKey2\n+\td.fileName = FileName\n+\n+\td.createDecryptionConfigs()\n+}\n+\n+func (d *TestDecryptionSuite) createDecryptionConfigs() {\n+\t// Decryption configuration 1: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key.\n+\tstringKr1 := make(encryption.StringKeyIDRetriever)\n+\tstringKr1.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr1.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr1.PutKey(\"kc2\", d.colEncryptionKey2)\n+\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr1)))\n+\n+\t// Decryption configuration 2: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key. Supply aad_prefix.\n+\tstringKr2 := make(encryption.StringKeyIDRetriever)\n+\tstringKr2.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr2.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr2.PutKey(\"kc2\", d.colEncryptionKey2)\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr2), parquet.WithDecryptAadPrefix(d.fileName)))\n+\n+\t// Decryption configuration 3: Decrypt using explicit column and footer keys. Supply\n+\t// aad_prefix.\n+\tdecryptCols := make(parquet.ColumnPathToDecryptionPropsMap)\n+\tdecryptCols[d.pathToFloat] = parquet.NewColumnDecryptionProperties(d.pathToFloat, parquet.WithDecryptKey(d.colEncryptionKey2))\n+\tdecryptCols[d.pathToDouble] = parquet.NewColumnDecryptionProperties(d.pathToDouble, parquet.WithDecryptKey(d.colEncryptionKey1))\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithFooterKey(d.footerEncryptionKey), parquet.WithColumnKeys(decryptCols)))\n+\n+\t// Decryption Configuration 4: use plaintext footer mode, read only footer + plaintext\n+\t// columns.\n+\td.decryptionConfigs = append(d.decryptionConfigs, nil)\n+}\n+\n+func (d *TestDecryptionSuite) decryptFile(filename string, decryptConfigNum int) {\n+\t// if we get decryption_config_num = x then it means the actual number is x+1\n+\t// and since we want decryption_config_num=4 we set the condition to 3\n+\tprops := parquet.NewReaderProperties(memory.DefaultAllocator)\n+\tif decryptConfigNum != 3 {\n+\t\tprops.FileDecryptProps = d.decryptionConfigs[decryptConfigNum].Clone(\"\")\n+\t}\n+\n+\tfileReader, err := file.OpenParquetFile(filename, false, file.WithReadProps(props))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\tdefer fileReader.Close()\n+\t// get metadata\n+\tfileMetadata := fileReader.MetaData()\n+\t// get number of rowgroups\n+\tnumRowGroups := len(fileMetadata.RowGroups)\n+\t// number of columns\n+\tnumColumns := fileMetadata.Schema.NumColumns()\n+\td.Equal(8, numColumns)\n+\n+\tfor r := 0; r < numRowGroups; r++ {\n+\t\trowGroupReader := fileReader.RowGroup(r)\n+\n+\t\t// get rowgroup meta\n+\t\trgMeta := fileMetadata.RowGroup(r)\n+\n+\t\tvaluesRead := 0\n+\t\trowsRead := int64(0)\n+\n+\t\t// get col reader for boolean column\n+\t\tcolReader := rowGroupReader.Column(0)\n+\t\tboolReader := colReader.(*file.BooleanColumnChunkReader)\n+\n+\t\t// get column chunk metadata for boolean column\n+\t\tboolMd, _ := rgMeta.ColumnChunk(0)\n+\n+\t\t// Read all rows in column\n+\t\ti := 0\n+\t\tfor boolReader.HasNext() {\n+\t\t\tvar val [1]bool\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = boolReader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\texpected := i%2 == 0\n+\t\t\td.Equal(expected, val[0], \"i: \", i)\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, boolMd.NumValues())\n+\n+\t\t// Get column reader for int32 column\n+\t\tcolReader = rowGroupReader.Column(1)\n+\t\tint32reader := colReader.(*file.Int32ColumnChunkReader)\n+\n+\t\tint32md, _ := rgMeta.ColumnChunk(1)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int32reader.HasNext() {\n+\t\t\tvar val [1]int32\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int32reader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\td.EqualValues(i, val[0])\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int32md.NumValues())\n+\n+\t\t// Get column reader for int64 column\n+\t\tcolReader = rowGroupReader.Column(2)\n+\t\tint64reader := colReader.(*file.Int64ColumnChunkReader)\n+\n+\t\tint64md, _ := rgMeta.ColumnChunk(2)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int64reader.HasNext() {\n+\t\t\tvar (\n+\t\t\t\tval [1]int64\n+\t\t\t\tdef [1]int16\n+\t\t\t\trep [1]int16\n+\t\t\t)\n+\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int64reader.ReadBatch(1, val[:], def[:], rep[:])\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\texpectedValue := int64(i) * 1000 * 1000 * 1000 * 1000\n+\t\t\td.Equal(expectedValue, val[0])\n+\t\t\tif i%2 == 0 {\n+\t\t\t\td.EqualValues(1, rep[0])\n+\t\t\t} else {\n+\t\t\t\td.Zero(rep[0])\n+\t\t\t}\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int64md.NumValues())\n+\n+\t\t// Get column reader for int96 column\n+\t\tcolReader = rowGroupReader.Column(3)\n+\t\tint96reader := colReader.(*file.Int96ColumnChunkReader)\n+\n+\t\tint96md, _ := rgMeta.ColumnChunk(3)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int96reader.HasNext() {\n+\t\t\tvar (\n+\t\t\t\tval [1]parquet.Int96\n+\t\t\t)\n+\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int96reader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\tvar expectedValue parquet.Int96\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[:4], uint32(i))\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[4:], uint32(i+1))\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[8:], uint32(i+2))\n+\t\t\td.Equal(expectedValue, val[0])\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int96md.NumValues())\n+\n+\t\tif decryptConfigNum != 3 {\n+\t\t\t// Get column reader for the float column\n+\t\t\tcolReader = rowGroupReader.Column(4)\n+\t\t\tfloatReader := colReader.(*file.Float32ColumnChunkReader)\n+\n+\t\t\tfloatmd, _ := rgMeta.ColumnChunk(4)\n+\n+\t\t\ti = 0\n+\t\t\tfor floatReader.HasNext() {\n+\t\t\t\tvar value [1]float32\n+\t\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t\t// read contains the number of non-null rows\n+\t\t\t\trowsRead, valuesRead, _ = floatReader.ReadBatch(1, value[:], nil, nil)\n+\t\t\t\t// ensure only 1 value is read\n+\t\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t\t// there are no null values\n+\t\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t\t// verify the value\n+\t\t\t\texpectedValue := float32(i) * 1.1\n+\t\t\t\td.Equal(expectedValue, value[0])\n+\t\t\t\ti++\n+\t\t\t}\n+\t\t\td.EqualValues(i, floatmd.NumValues())\n+\n+\t\t\t// Get column reader for the double column\n+\t\t\tcolReader = rowGroupReader.Column(5)\n+\t\t\tdblReader := colReader.(*file.Float64ColumnChunkReader)\n+\n+\t\t\tdblmd, _ := rgMeta.ColumnChunk(5)\n+\n+\t\t\ti = 0\n+\t\t\tfor dblReader.HasNext() {\n+\t\t\t\tvar value [1]float64\n+\t\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t\t// read contains the number of non-null rows\n+\t\t\t\trowsRead, valuesRead, _ = dblReader.ReadBatch(1, value[:], nil, nil)\n+\t\t\t\t// ensure only 1 value is read\n+\t\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t\t// there are no null values\n+\t\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t\t// verify the value\n+\t\t\t\texpectedValue := float64(i) * 1.1111111\n+\t\t\t\td.Equal(expectedValue, value[0])\n+\t\t\t\ti++\n+\t\t\t}\n+\t\t\td.EqualValues(i, dblmd.NumValues())\n+\t\t}\n+\n+\t\tcolReader = rowGroupReader.Column(6)\n+\t\tbareader := colReader.(*file.ByteArrayColumnChunkReader)\n+\n+\t\tbamd, _ := rgMeta.ColumnChunk(6)\n+\n+\t\ti = 0\n+\t\tfor bareader.HasNext() {\n+\t\t\tvar value [1]parquet.ByteArray\n+\t\t\tvar def [1]int16\n+\n+\t\t\trowsRead, valuesRead, _ := bareader.ReadBatch(1, value[:], def[:], nil)\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\texpected := [10]byte{'p', 'a', 'r', 'q', 'u', 'e', 't', 0, 0, 0}\n+\t\t\texpected[7] = byte('0') + byte(i/100)\n+\t\t\texpected[8] = byte('0') + byte(i/10)%10\n+\t\t\texpected[9] = byte('0') + byte(i%10)\n+\t\t\tif i%2 == 0 {\n+\t\t\t\td.Equal(1, valuesRead)\n+\t\t\t\td.Len(value[0], 10)\n+\t\t\t\td.EqualValues(expected[:], value[0])\n+\t\t\t\td.EqualValues(1, def[0])\n+\t\t\t} else {\n+\t\t\t\td.Zero(valuesRead)\n+\t\t\t\td.Zero(def[0])\n+\t\t\t}\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, bamd.NumValues())\n+\t}\n+}\n+\n+func (d *TestDecryptionSuite) checkResults(fileName string, decryptionConfig, encryptionConfig uint) {\n+\tdecFn := func() { d.decryptFile(fileName, int(decryptionConfig-1)) }\n+\n+\t// Encryption configuration number 5 contains aad_prefix and disable_aad_prefix_storage\n+\t// an exception is expected to be thrown if the file is not decrypted with aad_prefix\n+\tif encryptionConfig == 5 {\n+\t\tif decryptionConfig == 1 || decryptionConfig == 3 {\n+\t\t\td.Panics(decFn)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// decryption config number two contains aad_prefix. an exception\n+\t// is expected to be thrown if the file was not encrypted with the same aad_prefix\n+\tif decryptionConfig == 2 {\n+\t\tif encryptionConfig != 5 && encryptionConfig != 4 {\n+\t\t\td.Panics(decFn)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// decryption config 4 can only work when the encryption config is 3\n+\tif decryptionConfig == 4 && encryptionConfig != 3 {\n+\t\treturn\n+\t}\n+\td.NotPanics(decFn)\n+}\n+\n+// Read encrypted parquet file.\n+// the test reads two parquet files that were encrypted using the same encryption config\n+// one was generated in encryption_write_configurations_test.go tests and is deleted\n+// once the file is read and the second exists in parquet-testing/data folder\n+func (d *TestDecryptionSuite) TestDecryption() {\n+\ttests := []struct {\n+\t\tfile   string\n+\t\tconfig uint\n+\t}{\n+\t\t{\"uniform_encryption.parquet.encrypted\", 1},\n\nReview comment:\n       I don't think it's particularly useful to do that\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T19:13:28.821+0000",
                    "updated": "2021-10-27T19:13:28.821+0000",
                    "started": "2021-10-27T19:13:28.821+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "670974",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/671084",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "NightTrain commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r737889077\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n+\n+\t// the total number of stored values. for repeated or optional values. this\n+\t// number may be lower than numBuffered\n+\tnumBufferedEncoded int64\n+\n+\trowsWritten       int\n+\ttotalBytesWritten int64\n+\t// records the current number of compressed bytes in a column\n+\ttotalCompressedBytes int64\n+\tclosed               bool\n+\tfallback             bool\n+\n+\tpages []DataPage\n+\n+\tdefLevelSink *encoding.PooledBufferWriter\n+\trepLevelSink *encoding.PooledBufferWriter\n+\n+\tuncompressedData bytes.Buffer\n+\tcompressedTemp   *bytes.Buffer\n+\n+\tcurrentEncoder encoding.TypedEncoder\n+}\n+\n+func newColumnWriterBase(metaData *metadata.ColumnChunkMetaDataBuilder, pager PageWriter, useDict bool, enc parquet.Encoding, props *parquet.WriterProperties) columnWriter {\n+\tret := columnWriter{\n+\t\tmetaData:     metaData,\n+\t\tdescr:        metaData.Descr(),\n+\t\tlevelInfo:    computeLevelInfo(metaData.Descr()),\n+\t\tpager:        pager,\n+\t\thasDict:      useDict,\n+\t\tencoding:     enc,\n+\t\tprops:        props,\n+\t\tmem:          props.Allocator(),\n+\t\tdefLevelSink: encoding.NewPooledBufferWriter(0),\n+\t\trepLevelSink: encoding.NewPooledBufferWriter(0),\n+\t}\n+\tif pager.HasCompressor() {\n+\t\tret.compressedTemp = new(bytes.Buffer)\n+\t}\n+\tif props.StatisticsEnabledFor(ret.descr.Path()) && ret.descr.SortOrder() != schema.SortUNKNOWN {\n+\t\tret.pageStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t\tret.chunkStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t}\n+\n+\tif ret.props.DataPageVersion() == parquet.DataPageV1 {\n+\t\tif ret.descr.MaxDefinitionLevel() > 0 {\n+\t\t\tret.defLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t\tif ret.descr.MaxRepetitionLevel() > 0 {\n+\t\t\tret.repLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t}\n+\n+\tret.defEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxDefinitionLevel(), ret.defLevelSink)\n+\tret.repEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxRepetitionLevel(), ret.repLevelSink)\n+\treturn ret\n+}\n+\n+func (w *columnWriter) SetBitsBuffer(buf *memory.Buffer) { w.bitsBuffer = buf }\n+\n+func (w *columnWriter) LevelInfo() LevelInfo { return w.levelInfo }\n+\n+func (w *columnWriter) Type() parquet.Type {\n+\treturn w.descr.PhysicalType()\n+}\n+\n+func (w *columnWriter) Descr() *schema.Column {\n+\treturn w.descr\n+}\n+\n+func (w *columnWriter) Properties() *parquet.WriterProperties {\n+\treturn w.props\n+}\n+\n+func (w *columnWriter) TotalCompressedBytes() int64 {\n+\treturn w.totalCompressedBytes\n+}\n+\n+func (w *columnWriter) TotalBytesWritten() int64 {\n+\treturn w.totalBytesWritten\n+}\n+\n+func (w *columnWriter) RowsWritten() int64 {\n+\treturn int64(w.rowsWritten)\n+}\n+\n+func (w *columnWriter) WriteDataPage(page DataPage) error {\n+\twritten, err := w.pager.WriteDataPage(page)\n+\tw.totalBytesWritten += written\n+\treturn err\n+}\n+\n+func (w *columnWriter) WriteDefinitionLevels(levels []int16) {\n+\tw.defEncoder.EncodeNoFlush(levels)\n+}\n+\n+func (w *columnWriter) WriteRepetitionLevels(levels []int16) {\n+\tw.repEncoder.EncodeNoFlush(levels)\n+}\n+\n+func (w *columnWriter) init() {\n+\tw.defLevelSink.Reset(0)\n+\tw.repLevelSink.Reset(0)\n+\n+\tif w.props.DataPageVersion() == parquet.DataPageV1 {\n+\t\tif w.descr.MaxDefinitionLevel() > 0 {\n+\t\t\tw.defLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t\tif w.descr.MaxRepetitionLevel() > 0 {\n+\t\t\tw.repLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t}\n+\n+\tw.defEncoder.Reset(w.descr.MaxDefinitionLevel())\n+\tw.repEncoder.Reset(w.descr.MaxRepetitionLevel())\n+}\n+\n+func (w *columnWriter) concatBuffers(defLevelsSize, repLevelsSize int64, values []byte, wr io.Writer) {\n+\twr.Write(w.repLevelSink.Bytes()[:repLevelsSize])\n+\twr.Write(w.defLevelSink.Bytes()[:defLevelsSize])\n+\twr.Write(values)\n+}\n+\n+func (w *columnWriter) EstimatedBufferedValueBytes() int64 {\n+\treturn w.currentEncoder.EstimatedDataEncodedSize()\n+}\n+\n+func (w *columnWriter) commitWriteAndCheckPageLimit(numLevels, numValues int64) error {\n+\tw.numBuffered += numLevels\n+\tw.numBufferedEncoded += numValues\n+\n+\tif w.currentEncoder.EstimatedDataEncodedSize() >= w.props.DataPageSize() {\n+\t\treturn w.AddDataPage()\n+\t}\n+\treturn nil\n+}\n+\n+func (w *columnWriter) AddDataPage() error {\n+\tvar (\n+\t\tdefLevelsRLESize int64 = 0\n+\t\trepLevelsRLESize int64 = 0\n+\t)\n+\n+\tvalues, err := w.currentEncoder.FlushValues()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tdefer values.Release()\n+\n+\tisV1DataPage := w.props.DataPageVersion() == parquet.DataPageV1\n+\tif w.descr.MaxDefinitionLevel() > 0 {\n+\t\tw.defEncoder.Flush()\n+\t\tw.defLevelSink.SetOffset(0)\n+\t\tsz := w.defEncoder.Len()\n+\t\tif isV1DataPage {\n+\t\t\tsz += arrow.Uint32SizeBytes\n+\t\t\tbinary.LittleEndian.PutUint32(w.defLevelSink.Bytes(), uint32(w.defEncoder.Len()))\n+\t\t}\n+\t\tdefLevelsRLESize = int64(sz)\n+\t}\n+\n+\tif w.descr.MaxRepetitionLevel() > 0 {\n+\t\tw.repEncoder.Flush()\n+\t\tw.repLevelSink.SetOffset(0)\n+\t\tif isV1DataPage {\n+\t\t\tbinary.LittleEndian.PutUint32(w.repLevelSink.Bytes(), uint32(w.repEncoder.Len()))\n+\t\t}\n+\t\trepLevelsRLESize = int64(w.repLevelSink.Len())\n+\t}\n+\n+\tuncompressed := defLevelsRLESize + repLevelsRLESize + int64(values.Len())\n+\tif isV1DataPage {\n+\t\tw.buildDataPageV1(defLevelsRLESize, repLevelsRLESize, uncompressed, values.Bytes())\n+\t} else {\n+\t\tw.buildDataPageV2(defLevelsRLESize, repLevelsRLESize, uncompressed, values.Bytes())\n+\t}\n+\n+\tw.init()\n+\tw.numBuffered, w.numBufferedEncoded = 0, 0\n+\treturn nil\n+}\n+\n+func (w *columnWriter) buildDataPageV1(defLevelsRLESize, repLevelsRLESize, uncompressed int64, values []byte) error {\n+\tw.uncompressedData.Reset()\n+\tw.uncompressedData.Grow(int(uncompressed))\n+\tw.concatBuffers(defLevelsRLESize, repLevelsRLESize, values, &w.uncompressedData)\n+\n+\tpageStats, err := w.getPageStatistics()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tpageStats.ApplyStatSizeLimits(int(w.props.MaxStatsSizeFor(w.descr.Path())))\n+\tpageStats.Signed = schema.SortSIGNED == w.descr.SortOrder()\n+\tw.resetPageStatistics()\n+\n+\tvar data []byte\n+\tif w.pager.HasCompressor() {\n+\t\tw.compressedTemp.Reset()\n+\t\tdata = w.pager.Compress(w.compressedTemp, w.uncompressedData.Bytes())\n+\t} else {\n+\t\tdata = w.uncompressedData.Bytes()\n+\t}\n+\n+\t// write the page to sink eagerly if there's no dictionary or if dictionary encoding has fallen back\n+\tif w.hasDict && !w.fallback {\n+\t\tpageSlice := make([]byte, len(data))\n+\t\tcopy(pageSlice, data)\n+\t\tpage := NewDataPageV1WithStats(memory.NewBufferBytes(pageSlice), int32(w.numBuffered), w.encoding, parquet.Encodings.RLE, parquet.Encodings.RLE, uncompressed, pageStats)\n+\t\tw.totalCompressedBytes += int64(page.buf.Len()) // + size of Pageheader\n+\t\tw.pages = append(w.pages, page)\n+\t} else {\n+\t\tw.totalCompressedBytes += int64(len(data))\n+\t\tdp := NewDataPageV1WithStats(memory.NewBufferBytes(data), int32(w.numBuffered), w.encoding, parquet.Encodings.RLE, parquet.Encodings.RLE, uncompressed, pageStats)\n+\t\tdefer dp.Release()\n+\t\tw.WriteDataPage(dp)\n+\t}\n+\treturn nil\n+}\n+\n+func (w *columnWriter) buildDataPageV2(defLevelsRLESize, repLevelsRLESize, uncompressed int64, values []byte) error {\n+\tvar data []byte\n+\tif w.pager.HasCompressor() {\n+\t\tw.compressedTemp.Reset()\n+\t\tdata = w.pager.Compress(w.compressedTemp, values)\n+\t\t// data = w.compressedTemp.Bytes()\n+\t} else {\n+\t\tdata = values\n+\t}\n+\n+\t// concatenate uncompressed levels and the possibly compressed values\n+\tcombined := make([]byte, int(defLevelsRLESize+repLevelsRLESize+int64(len(data))))\n+\tcopy(combined, w.repLevelSink.Bytes()[:repLevelsRLESize])\n+\tcopy(combined[repLevelsRLESize:], w.defLevelSink.Bytes()[:defLevelsRLESize])\n+\tcopy(combined[repLevelsRLESize+defLevelsRLESize:], data)\n+\n+\tpageStats, err := w.getPageStatistics()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tpageStats.ApplyStatSizeLimits(int(w.props.MaxStatsSizeFor(w.descr.Path())))\n+\tpageStats.Signed = schema.SortSIGNED == w.descr.SortOrder()\n+\tw.resetPageStatistics()\n+\n+\tnumValues := int32(w.numBuffered)\n+\tnullCount := int32(pageStats.NullCount)\n+\tdefLevelsByteLen := int32(defLevelsRLESize)\n+\trepLevelsByteLen := int32(repLevelsRLESize)\n+\n+\tpage := NewDataPageV2WithStats(memory.NewBufferBytes(combined), numValues, nullCount, numValues, w.encoding,\n+\t\tdefLevelsByteLen, repLevelsByteLen, uncompressed, w.pager.HasCompressor(), pageStats)\n+\tif w.hasDict && !w.fallback {\n+\t\tw.totalCompressedBytes += int64(page.buf.Len()) // + sizeof pageheader\n+\t\tw.pages = append(w.pages, page)\n+\t} else {\n+\t\tw.totalCompressedBytes += int64(len(combined))\n+\t\tdefer page.Release()\n+\t\tw.WriteDataPage(page)\n+\t}\n+\treturn nil\n+}\n+\n+func (w *columnWriter) FlushBufferedDataPages() {\n+\tif w.numBuffered > 0 {\n+\t\tw.AddDataPage()\n+\t}\n+\n+\tfor _, p := range w.pages {\n+\t\tdefer p.Release()\n+\t\tw.WriteDataPage(p)\n+\t}\n+\tw.pages = w.pages[:0]\n+\tw.totalCompressedBytes = 0\n+}\n+\n+func (w *columnWriter) writeLevels(numValues int64, defLevels, repLevels []int16) int64 {\n+\ttoWrite := int64(0)\n+\t// if the field is required and non-repeated, no definition levels\n+\tif defLevels != nil && w.descr.MaxDefinitionLevel() > 0 {\n+\t\tfor _, v := range defLevels {\n+\t\t\tif v == w.descr.MaxDefinitionLevel() {\n+\t\t\t\ttoWrite++\n+\t\t\t}\n+\t\t}\n+\t\tw.WriteDefinitionLevels(defLevels[:numValues])\n+\t} else {\n+\t\ttoWrite = numValues\n+\t}\n+\n+\tif repLevels != nil && w.descr.MaxRepetitionLevel() > 0 {\n+\t\t// a row could include more than one value\n+\t\t//count the occasions where we start a new row\n+\t\tfor _, v := range repLevels {\n+\t\t\tif v == 0 {\n+\t\t\t\tw.rowsWritten++\n+\t\t\t}\n+\t\t}\n+\n+\t\tw.WriteRepetitionLevels(repLevels[:numValues])\n+\t} else {\n+\t\t// each value is exactly 1 row\n+\t\tw.rowsWritten += int(numValues)\n+\t}\n+\treturn toWrite\n+}\n+\n+func (w *columnWriter) writeLevelsSpaced(numLevels int64, defLevels, repLevels []int16) {\n+\tif w.descr.MaxDefinitionLevel() > 0 {\n+\t\tw.WriteDefinitionLevels(defLevels[:numLevels])\n+\t}\n+\n+\tif w.descr.MaxRepetitionLevel() > 0 {\n+\t\tfor _, v := range repLevels {\n+\t\t\tif v == 0 {\n+\t\t\t\tw.rowsWritten++\n+\t\t\t}\n+\t\t}\n+\t\tw.WriteRepetitionLevels(repLevels[:numLevels])\n+\t} else {\n+\t\tw.rowsWritten += int(numLevels)\n+\t}\n+}\n+\n+func (w *columnWriter) WriteDictionaryPage() error {\n+\tdictEncoder := w.currentEncoder.(encoding.DictEncoder)\n+\tbuffer := memory.NewResizableBuffer(w.mem)\n+\tbuffer.Resize(dictEncoder.DictEncodedSize())\n+\tdictEncoder.WriteDict(buffer.Bytes())\n+\tdefer buffer.Release()\n+\n+\tpage := NewDictionaryPage(buffer, int32(dictEncoder.NumEntries()), w.props.DictionaryPageEncoding())\n+\twritten, err := w.pager.WriteDictionaryPage(page)\n+\tw.totalBytesWritten += written\n+\treturn err\n+}\n+\n+// this will always update the three otuput params\n+// outValsToWrite, outSpacedValsToWrite, and NullCount. Additionally\n+// it will update the validity bitmap if required (i.e. if at least one\n+// level of nullable structs directly preced the leaf node)\n\nReview comment:\n       Spelling for \"preced\"\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-27T22:23:26.571+0000",
                    "updated": "2021-10-27T22:23:26.571+0000",
                    "started": "2021-10-27T22:23:26.570+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "671084",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673771",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741406958\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key. Supplies\n+ *                                  aad_prefix to verify file identity.\n+ *  - Decryption configuration 3:   Decrypt using explicit column and footer keys\n+ *                                  (instead of key retrieval callback).\n+ *  - Decryption Configuration 4:   PlainText Footer mode - test legacy reads,\n+ *                                  read the footer + all non-encrypted columns.\n+ *                                  (pairs with encryption configuration 3)\n+ *\n+ * The encrypted parquet files that is read was encrypted using one of the configurations\n+ * below:\n+ *\n+ *  - Encryption configuration 1:   Encrypt all columns and the footer with the same key.\n+ *                                  (uniform encryption)\n+ *  - Encryption configuration 2:   Encrypt two columns and the footer, with different\n+ *                                  keys.\n+ *  - Encryption configuration 3:   Encrypt two columns, with different keys.\n+ *                                  Don\u2019t encrypt footer (to enable legacy readers)\n+ *                                  - plaintext footer mode.\n+ *  - Encryption configuration 4:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix for file identity\n+ *                                  verification.\n+ *  - Encryption configuration 5:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix, and call\n+ *                                  disable_aad_prefix_storage to prevent file\n+ *                                  identity storage in file metadata.\n+ *  - Encryption configuration 6:   Encrypt two columns and the footer, with different\n+ *                                  keys. Use the alternative (AES_GCM_CTR_V1) algorithm.\n+ */\n+\n+func getDataDir() string {\n+\tdatadir := os.Getenv(\"PARQUET_TEST_DATA\")\n+\tif datadir == \"\" {\n+\t\tpanic(\"please point the PARQUET_TEST_DATA environment variable to the test data dir\")\n+\t}\n+\treturn datadir\n+}\n+\n+type TestDecryptionSuite struct {\n+\tsuite.Suite\n+\n+\tpathToDouble        string\n+\tpathToFloat         string\n+\tdecryptionConfigs   []*parquet.FileDecryptionProperties\n+\tfooterEncryptionKey string\n+\tcolEncryptionKey1   string\n+\tcolEncryptionKey2   string\n+\tfileName            string\n+}\n+\n+func (d *TestDecryptionSuite) TearDownSuite() {\n+\tos.Remove(tempdir)\n+}\n+\n+func TestFileEncryptionDecryption(t *testing.T) {\n+\tsuite.Run(t, new(EncryptionConfigTestSuite))\n+\tsuite.Run(t, new(TestDecryptionSuite))\n+}\n+\n+func (d *TestDecryptionSuite) SetupSuite() {\n+\td.pathToDouble = \"double_field\"\n+\td.pathToFloat = \"float_field\"\n+\td.footerEncryptionKey = FooterEncryptionKey\n+\td.colEncryptionKey1 = ColumnEncryptionKey1\n+\td.colEncryptionKey2 = ColumnEncryptionKey2\n+\td.fileName = FileName\n+\n+\td.createDecryptionConfigs()\n+}\n+\n+func (d *TestDecryptionSuite) createDecryptionConfigs() {\n+\t// Decryption configuration 1: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key.\n+\tstringKr1 := make(encryption.StringKeyIDRetriever)\n+\tstringKr1.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr1.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr1.PutKey(\"kc2\", d.colEncryptionKey2)\n+\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr1)))\n+\n+\t// Decryption configuration 2: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key. Supply aad_prefix.\n+\tstringKr2 := make(encryption.StringKeyIDRetriever)\n+\tstringKr2.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr2.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr2.PutKey(\"kc2\", d.colEncryptionKey2)\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr2), parquet.WithDecryptAadPrefix(d.fileName)))\n+\n+\t// Decryption configuration 3: Decrypt using explicit column and footer keys. Supply\n+\t// aad_prefix.\n+\tdecryptCols := make(parquet.ColumnPathToDecryptionPropsMap)\n+\tdecryptCols[d.pathToFloat] = parquet.NewColumnDecryptionProperties(d.pathToFloat, parquet.WithDecryptKey(d.colEncryptionKey2))\n+\tdecryptCols[d.pathToDouble] = parquet.NewColumnDecryptionProperties(d.pathToDouble, parquet.WithDecryptKey(d.colEncryptionKey1))\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithFooterKey(d.footerEncryptionKey), parquet.WithColumnKeys(decryptCols)))\n+\n+\t// Decryption Configuration 4: use plaintext footer mode, read only footer + plaintext\n+\t// columns.\n+\td.decryptionConfigs = append(d.decryptionConfigs, nil)\n+}\n+\n+func (d *TestDecryptionSuite) decryptFile(filename string, decryptConfigNum int) {\n+\t// if we get decryption_config_num = x then it means the actual number is x+1\n+\t// and since we want decryption_config_num=4 we set the condition to 3\n+\tprops := parquet.NewReaderProperties(memory.DefaultAllocator)\n+\tif decryptConfigNum != 3 {\n\nReview comment:\n       it might be more cleaner to encapsulate the test case into its own class/struct instead of the if/then here.  At the end of the day that might be just a slightly different form of obfuscation.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:37:21.447+0000",
                    "updated": "2021-11-02T19:37:21.447+0000",
                    "started": "2021-11-02T19:37:21.447+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673771",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673773",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741408200\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key. Supplies\n+ *                                  aad_prefix to verify file identity.\n+ *  - Decryption configuration 3:   Decrypt using explicit column and footer keys\n+ *                                  (instead of key retrieval callback).\n+ *  - Decryption Configuration 4:   PlainText Footer mode - test legacy reads,\n+ *                                  read the footer + all non-encrypted columns.\n+ *                                  (pairs with encryption configuration 3)\n+ *\n+ * The encrypted parquet files that is read was encrypted using one of the configurations\n+ * below:\n+ *\n+ *  - Encryption configuration 1:   Encrypt all columns and the footer with the same key.\n+ *                                  (uniform encryption)\n+ *  - Encryption configuration 2:   Encrypt two columns and the footer, with different\n+ *                                  keys.\n+ *  - Encryption configuration 3:   Encrypt two columns, with different keys.\n+ *                                  Don\u2019t encrypt footer (to enable legacy readers)\n+ *                                  - plaintext footer mode.\n+ *  - Encryption configuration 4:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix for file identity\n+ *                                  verification.\n+ *  - Encryption configuration 5:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix, and call\n+ *                                  disable_aad_prefix_storage to prevent file\n+ *                                  identity storage in file metadata.\n+ *  - Encryption configuration 6:   Encrypt two columns and the footer, with different\n+ *                                  keys. Use the alternative (AES_GCM_CTR_V1) algorithm.\n+ */\n+\n+func getDataDir() string {\n+\tdatadir := os.Getenv(\"PARQUET_TEST_DATA\")\n+\tif datadir == \"\" {\n+\t\tpanic(\"please point the PARQUET_TEST_DATA environment variable to the test data dir\")\n+\t}\n+\treturn datadir\n+}\n+\n+type TestDecryptionSuite struct {\n+\tsuite.Suite\n+\n+\tpathToDouble        string\n+\tpathToFloat         string\n+\tdecryptionConfigs   []*parquet.FileDecryptionProperties\n+\tfooterEncryptionKey string\n+\tcolEncryptionKey1   string\n+\tcolEncryptionKey2   string\n+\tfileName            string\n+}\n+\n+func (d *TestDecryptionSuite) TearDownSuite() {\n+\tos.Remove(tempdir)\n+}\n+\n+func TestFileEncryptionDecryption(t *testing.T) {\n+\tsuite.Run(t, new(EncryptionConfigTestSuite))\n+\tsuite.Run(t, new(TestDecryptionSuite))\n+}\n+\n+func (d *TestDecryptionSuite) SetupSuite() {\n+\td.pathToDouble = \"double_field\"\n+\td.pathToFloat = \"float_field\"\n+\td.footerEncryptionKey = FooterEncryptionKey\n+\td.colEncryptionKey1 = ColumnEncryptionKey1\n+\td.colEncryptionKey2 = ColumnEncryptionKey2\n+\td.fileName = FileName\n+\n+\td.createDecryptionConfigs()\n+}\n+\n+func (d *TestDecryptionSuite) createDecryptionConfigs() {\n+\t// Decryption configuration 1: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key.\n+\tstringKr1 := make(encryption.StringKeyIDRetriever)\n+\tstringKr1.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr1.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr1.PutKey(\"kc2\", d.colEncryptionKey2)\n+\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr1)))\n+\n+\t// Decryption configuration 2: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key. Supply aad_prefix.\n+\tstringKr2 := make(encryption.StringKeyIDRetriever)\n+\tstringKr2.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr2.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr2.PutKey(\"kc2\", d.colEncryptionKey2)\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr2), parquet.WithDecryptAadPrefix(d.fileName)))\n+\n+\t// Decryption configuration 3: Decrypt using explicit column and footer keys. Supply\n+\t// aad_prefix.\n+\tdecryptCols := make(parquet.ColumnPathToDecryptionPropsMap)\n+\tdecryptCols[d.pathToFloat] = parquet.NewColumnDecryptionProperties(d.pathToFloat, parquet.WithDecryptKey(d.colEncryptionKey2))\n+\tdecryptCols[d.pathToDouble] = parquet.NewColumnDecryptionProperties(d.pathToDouble, parquet.WithDecryptKey(d.colEncryptionKey1))\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithFooterKey(d.footerEncryptionKey), parquet.WithColumnKeys(decryptCols)))\n+\n+\t// Decryption Configuration 4: use plaintext footer mode, read only footer + plaintext\n+\t// columns.\n+\td.decryptionConfigs = append(d.decryptionConfigs, nil)\n+}\n+\n+func (d *TestDecryptionSuite) decryptFile(filename string, decryptConfigNum int) {\n+\t// if we get decryption_config_num = x then it means the actual number is x+1\n+\t// and since we want decryption_config_num=4 we set the condition to 3\n+\tprops := parquet.NewReaderProperties(memory.DefaultAllocator)\n+\tif decryptConfigNum != 3 {\n+\t\tprops.FileDecryptProps = d.decryptionConfigs[decryptConfigNum].Clone(\"\")\n+\t}\n+\n+\tfileReader, err := file.OpenParquetFile(filename, false, file.WithReadProps(props))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\tdefer fileReader.Close()\n+\t// get metadata\n+\tfileMetadata := fileReader.MetaData()\n+\t// get number of rowgroups\n+\tnumRowGroups := len(fileMetadata.RowGroups)\n+\t// number of columns\n+\tnumColumns := fileMetadata.Schema.NumColumns()\n+\td.Equal(8, numColumns)\n+\n+\tfor r := 0; r < numRowGroups; r++ {\n+\t\trowGroupReader := fileReader.RowGroup(r)\n+\n+\t\t// get rowgroup meta\n+\t\trgMeta := fileMetadata.RowGroup(r)\n+\n+\t\tvaluesRead := 0\n+\t\trowsRead := int64(0)\n+\n+\t\t// get col reader for boolean column\n+\t\tcolReader := rowGroupReader.Column(0)\n+\t\tboolReader := colReader.(*file.BooleanColumnChunkReader)\n+\n+\t\t// get column chunk metadata for boolean column\n+\t\tboolMd, _ := rgMeta.ColumnChunk(0)\n+\n+\t\t// Read all rows in column\n+\t\ti := 0\n+\t\tfor boolReader.HasNext() {\n+\t\t\tvar val [1]bool\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = boolReader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\texpected := i%2 == 0\n+\t\t\td.Equal(expected, val[0], \"i: \", i)\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, boolMd.NumValues())\n+\n+\t\t// Get column reader for int32 column\n+\t\tcolReader = rowGroupReader.Column(1)\n+\t\tint32reader := colReader.(*file.Int32ColumnChunkReader)\n+\n+\t\tint32md, _ := rgMeta.ColumnChunk(1)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int32reader.HasNext() {\n+\t\t\tvar val [1]int32\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int32reader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\td.EqualValues(i, val[0])\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int32md.NumValues())\n+\n+\t\t// Get column reader for int64 column\n+\t\tcolReader = rowGroupReader.Column(2)\n+\t\tint64reader := colReader.(*file.Int64ColumnChunkReader)\n+\n+\t\tint64md, _ := rgMeta.ColumnChunk(2)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int64reader.HasNext() {\n+\t\t\tvar (\n+\t\t\t\tval [1]int64\n+\t\t\t\tdef [1]int16\n+\t\t\t\trep [1]int16\n+\t\t\t)\n+\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int64reader.ReadBatch(1, val[:], def[:], rep[:])\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\texpectedValue := int64(i) * 1000 * 1000 * 1000 * 1000\n+\t\t\td.Equal(expectedValue, val[0])\n+\t\t\tif i%2 == 0 {\n+\t\t\t\td.EqualValues(1, rep[0])\n+\t\t\t} else {\n+\t\t\t\td.Zero(rep[0])\n+\t\t\t}\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int64md.NumValues())\n+\n+\t\t// Get column reader for int96 column\n+\t\tcolReader = rowGroupReader.Column(3)\n+\t\tint96reader := colReader.(*file.Int96ColumnChunkReader)\n+\n+\t\tint96md, _ := rgMeta.ColumnChunk(3)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int96reader.HasNext() {\n+\t\t\tvar (\n+\t\t\t\tval [1]parquet.Int96\n+\t\t\t)\n+\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int96reader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\tvar expectedValue parquet.Int96\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[:4], uint32(i))\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[4:], uint32(i+1))\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[8:], uint32(i+2))\n+\t\t\td.Equal(expectedValue, val[0])\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int96md.NumValues())\n+\n+\t\tif decryptConfigNum != 3 {\n\nReview comment:\n       i think it is worth commenting here and above why this logic doesn't apply to case number 3\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:39:06.238+0000",
                    "updated": "2021-11-02T19:39:06.238+0000",
                    "started": "2021-11-02T19:39:06.237+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673773",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673775",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741409147\n\n\n\n##########\nFile path: go/parquet/encryption_read_config_test.go\n##########\n@@ -0,0 +1,443 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package parquet_test\n+\n+import (\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"path\"\n+\t\"testing\"\n+\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/file\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encryption\"\n+\t\"github.com/stretchr/testify/suite\"\n+)\n+\n+/*\n+ * This file contains a unit-test for reading encrypted Parquet files with\n+ * different decryption configurations.\n+ *\n+ * The unit-test is called multiple times, each time to decrypt parquet files using\n+ * different decryption configuration as described below.\n+ * In each call two encrypted files are read: one temporary file that was generated using\n+ * encryption_write_config_test.go test and will be deleted upon\n+ * reading it, while the second resides in\n+ * parquet-testing/data repository. Those two encrypted files were encrypted using the\n+ * same encryption configuration.\n+ * The encrypted parquet file names are passed as parameter to the unit-test.\n+ *\n+ * A detailed description of the Parquet Modular Encryption specification can be found\n+ * here:\n+ * https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n+ *\n+ * The following decryption configurations are used to decrypt each parquet file:\n+ *\n+ *  - Decryption configuration 1:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key.\n+ *  - Decryption configuration 2:   Decrypt using key retriever that holds the keys of\n+ *                                  two encrypted columns and the footer key. Supplies\n+ *                                  aad_prefix to verify file identity.\n+ *  - Decryption configuration 3:   Decrypt using explicit column and footer keys\n+ *                                  (instead of key retrieval callback).\n+ *  - Decryption Configuration 4:   PlainText Footer mode - test legacy reads,\n+ *                                  read the footer + all non-encrypted columns.\n+ *                                  (pairs with encryption configuration 3)\n+ *\n+ * The encrypted parquet files that is read was encrypted using one of the configurations\n+ * below:\n+ *\n+ *  - Encryption configuration 1:   Encrypt all columns and the footer with the same key.\n+ *                                  (uniform encryption)\n+ *  - Encryption configuration 2:   Encrypt two columns and the footer, with different\n+ *                                  keys.\n+ *  - Encryption configuration 3:   Encrypt two columns, with different keys.\n+ *                                  Don\u2019t encrypt footer (to enable legacy readers)\n+ *                                  - plaintext footer mode.\n+ *  - Encryption configuration 4:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix for file identity\n+ *                                  verification.\n+ *  - Encryption configuration 5:   Encrypt two columns and the footer, with different\n+ *                                  keys. Supply aad_prefix, and call\n+ *                                  disable_aad_prefix_storage to prevent file\n+ *                                  identity storage in file metadata.\n+ *  - Encryption configuration 6:   Encrypt two columns and the footer, with different\n+ *                                  keys. Use the alternative (AES_GCM_CTR_V1) algorithm.\n+ */\n+\n+func getDataDir() string {\n+\tdatadir := os.Getenv(\"PARQUET_TEST_DATA\")\n+\tif datadir == \"\" {\n+\t\tpanic(\"please point the PARQUET_TEST_DATA environment variable to the test data dir\")\n+\t}\n+\treturn datadir\n+}\n+\n+type TestDecryptionSuite struct {\n+\tsuite.Suite\n+\n+\tpathToDouble        string\n+\tpathToFloat         string\n+\tdecryptionConfigs   []*parquet.FileDecryptionProperties\n+\tfooterEncryptionKey string\n+\tcolEncryptionKey1   string\n+\tcolEncryptionKey2   string\n+\tfileName            string\n+}\n+\n+func (d *TestDecryptionSuite) TearDownSuite() {\n+\tos.Remove(tempdir)\n+}\n+\n+func TestFileEncryptionDecryption(t *testing.T) {\n+\tsuite.Run(t, new(EncryptionConfigTestSuite))\n+\tsuite.Run(t, new(TestDecryptionSuite))\n+}\n+\n+func (d *TestDecryptionSuite) SetupSuite() {\n+\td.pathToDouble = \"double_field\"\n+\td.pathToFloat = \"float_field\"\n+\td.footerEncryptionKey = FooterEncryptionKey\n+\td.colEncryptionKey1 = ColumnEncryptionKey1\n+\td.colEncryptionKey2 = ColumnEncryptionKey2\n+\td.fileName = FileName\n+\n+\td.createDecryptionConfigs()\n+}\n+\n+func (d *TestDecryptionSuite) createDecryptionConfigs() {\n+\t// Decryption configuration 1: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key.\n+\tstringKr1 := make(encryption.StringKeyIDRetriever)\n+\tstringKr1.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr1.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr1.PutKey(\"kc2\", d.colEncryptionKey2)\n+\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr1)))\n+\n+\t// Decryption configuration 2: Decrypt using key retriever callback that holds the\n+\t// keys of two encrypted columns and the footer key. Supply aad_prefix.\n+\tstringKr2 := make(encryption.StringKeyIDRetriever)\n+\tstringKr2.PutKey(\"kf\", d.footerEncryptionKey)\n+\tstringKr2.PutKey(\"kc1\", d.colEncryptionKey1)\n+\tstringKr2.PutKey(\"kc2\", d.colEncryptionKey2)\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithKeyRetriever(stringKr2), parquet.WithDecryptAadPrefix(d.fileName)))\n+\n+\t// Decryption configuration 3: Decrypt using explicit column and footer keys. Supply\n+\t// aad_prefix.\n+\tdecryptCols := make(parquet.ColumnPathToDecryptionPropsMap)\n+\tdecryptCols[d.pathToFloat] = parquet.NewColumnDecryptionProperties(d.pathToFloat, parquet.WithDecryptKey(d.colEncryptionKey2))\n+\tdecryptCols[d.pathToDouble] = parquet.NewColumnDecryptionProperties(d.pathToDouble, parquet.WithDecryptKey(d.colEncryptionKey1))\n+\td.decryptionConfigs = append(d.decryptionConfigs,\n+\t\tparquet.NewFileDecryptionProperties(parquet.WithFooterKey(d.footerEncryptionKey), parquet.WithColumnKeys(decryptCols)))\n+\n+\t// Decryption Configuration 4: use plaintext footer mode, read only footer + plaintext\n+\t// columns.\n+\td.decryptionConfigs = append(d.decryptionConfigs, nil)\n+}\n+\n+func (d *TestDecryptionSuite) decryptFile(filename string, decryptConfigNum int) {\n+\t// if we get decryption_config_num = x then it means the actual number is x+1\n+\t// and since we want decryption_config_num=4 we set the condition to 3\n+\tprops := parquet.NewReaderProperties(memory.DefaultAllocator)\n+\tif decryptConfigNum != 3 {\n+\t\tprops.FileDecryptProps = d.decryptionConfigs[decryptConfigNum].Clone(\"\")\n+\t}\n+\n+\tfileReader, err := file.OpenParquetFile(filename, false, file.WithReadProps(props))\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\tdefer fileReader.Close()\n+\t// get metadata\n+\tfileMetadata := fileReader.MetaData()\n+\t// get number of rowgroups\n+\tnumRowGroups := len(fileMetadata.RowGroups)\n+\t// number of columns\n+\tnumColumns := fileMetadata.Schema.NumColumns()\n+\td.Equal(8, numColumns)\n+\n+\tfor r := 0; r < numRowGroups; r++ {\n+\t\trowGroupReader := fileReader.RowGroup(r)\n+\n+\t\t// get rowgroup meta\n+\t\trgMeta := fileMetadata.RowGroup(r)\n+\n+\t\tvaluesRead := 0\n+\t\trowsRead := int64(0)\n+\n+\t\t// get col reader for boolean column\n+\t\tcolReader := rowGroupReader.Column(0)\n+\t\tboolReader := colReader.(*file.BooleanColumnChunkReader)\n+\n+\t\t// get column chunk metadata for boolean column\n+\t\tboolMd, _ := rgMeta.ColumnChunk(0)\n+\n+\t\t// Read all rows in column\n+\t\ti := 0\n+\t\tfor boolReader.HasNext() {\n+\t\t\tvar val [1]bool\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = boolReader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\texpected := i%2 == 0\n+\t\t\td.Equal(expected, val[0], \"i: \", i)\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, boolMd.NumValues())\n+\n+\t\t// Get column reader for int32 column\n+\t\tcolReader = rowGroupReader.Column(1)\n+\t\tint32reader := colReader.(*file.Int32ColumnChunkReader)\n+\n+\t\tint32md, _ := rgMeta.ColumnChunk(1)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int32reader.HasNext() {\n+\t\t\tvar val [1]int32\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int32reader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\td.EqualValues(i, val[0])\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int32md.NumValues())\n+\n+\t\t// Get column reader for int64 column\n+\t\tcolReader = rowGroupReader.Column(2)\n+\t\tint64reader := colReader.(*file.Int64ColumnChunkReader)\n+\n+\t\tint64md, _ := rgMeta.ColumnChunk(2)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int64reader.HasNext() {\n+\t\t\tvar (\n+\t\t\t\tval [1]int64\n+\t\t\t\tdef [1]int16\n+\t\t\t\trep [1]int16\n+\t\t\t)\n+\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int64reader.ReadBatch(1, val[:], def[:], rep[:])\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\texpectedValue := int64(i) * 1000 * 1000 * 1000 * 1000\n+\t\t\td.Equal(expectedValue, val[0])\n+\t\t\tif i%2 == 0 {\n+\t\t\t\td.EqualValues(1, rep[0])\n+\t\t\t} else {\n+\t\t\t\td.Zero(rep[0])\n+\t\t\t}\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int64md.NumValues())\n+\n+\t\t// Get column reader for int96 column\n+\t\tcolReader = rowGroupReader.Column(3)\n+\t\tint96reader := colReader.(*file.Int96ColumnChunkReader)\n+\n+\t\tint96md, _ := rgMeta.ColumnChunk(3)\n+\t\t// Read all rows in column\n+\t\ti = 0\n+\t\tfor int96reader.HasNext() {\n+\t\t\tvar (\n+\t\t\t\tval [1]parquet.Int96\n+\t\t\t)\n+\n+\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t// read contains the number of non-null rows\n+\t\t\trowsRead, valuesRead, _ = int96reader.ReadBatch(1, val[:], nil, nil)\n+\t\t\t// ensure only 1 value is read\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t// there are no null values\n+\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t// verify the value\n+\t\t\tvar expectedValue parquet.Int96\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[:4], uint32(i))\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[4:], uint32(i+1))\n+\t\t\tbinary.LittleEndian.PutUint32(expectedValue[8:], uint32(i+2))\n+\t\t\td.Equal(expectedValue, val[0])\n+\t\t\ti++\n+\t\t}\n+\t\td.EqualValues(i, int96md.NumValues())\n+\n+\t\tif decryptConfigNum != 3 {\n+\t\t\t// Get column reader for the float column\n+\t\t\tcolReader = rowGroupReader.Column(4)\n+\t\t\tfloatReader := colReader.(*file.Float32ColumnChunkReader)\n+\n+\t\t\tfloatmd, _ := rgMeta.ColumnChunk(4)\n+\n+\t\t\ti = 0\n+\t\t\tfor floatReader.HasNext() {\n+\t\t\t\tvar value [1]float32\n+\t\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t\t// read contains the number of non-null rows\n+\t\t\t\trowsRead, valuesRead, _ = floatReader.ReadBatch(1, value[:], nil, nil)\n+\t\t\t\t// ensure only 1 value is read\n+\t\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t\t// there are no null values\n+\t\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t\t// verify the value\n+\t\t\t\texpectedValue := float32(i) * 1.1\n+\t\t\t\td.Equal(expectedValue, value[0])\n+\t\t\t\ti++\n+\t\t\t}\n+\t\t\td.EqualValues(i, floatmd.NumValues())\n+\n+\t\t\t// Get column reader for the double column\n+\t\t\tcolReader = rowGroupReader.Column(5)\n+\t\t\tdblReader := colReader.(*file.Float64ColumnChunkReader)\n+\n+\t\t\tdblmd, _ := rgMeta.ColumnChunk(5)\n+\n+\t\t\ti = 0\n+\t\t\tfor dblReader.HasNext() {\n+\t\t\t\tvar value [1]float64\n+\t\t\t\t// read one value at a time. the number of rows read is returned. values\n+\t\t\t\t// read contains the number of non-null rows\n+\t\t\t\trowsRead, valuesRead, _ = dblReader.ReadBatch(1, value[:], nil, nil)\n+\t\t\t\t// ensure only 1 value is read\n+\t\t\t\td.EqualValues(1, rowsRead)\n+\t\t\t\t// there are no null values\n+\t\t\t\td.EqualValues(1, valuesRead)\n+\t\t\t\t// verify the value\n+\t\t\t\texpectedValue := float64(i) * 1.1111111\n+\t\t\t\td.Equal(expectedValue, value[0])\n+\t\t\t\ti++\n+\t\t\t}\n+\t\t\td.EqualValues(i, dblmd.NumValues())\n+\t\t}\n+\n+\t\tcolReader = rowGroupReader.Column(6)\n+\t\tbareader := colReader.(*file.ByteArrayColumnChunkReader)\n+\n+\t\tbamd, _ := rgMeta.ColumnChunk(6)\n+\n+\t\ti = 0\n+\t\tfor bareader.HasNext() {\n+\t\t\tvar value [1]parquet.ByteArray\n+\t\t\tvar def [1]int16\n+\n+\t\t\trowsRead, valuesRead, _ := bareader.ReadBatch(1, value[:], def[:], nil)\n+\t\t\td.EqualValues(1, rowsRead)\n+\t\t\texpected := [10]byte{'p', 'a', 'r', 'q', 'u', 'e', 't', 0, 0, 0}\n+\t\t\texpected[7] = byte('0') + byte(i/100)\n+\t\t\texpected[8] = byte('0') + byte(i/10)%10\n+\t\t\texpected[9] = byte('0') + byte(i%10)\n+\t\t\tif i%2 == 0 {\n+\t\t\t\td.Equal(1, valuesRead)\n\nReview comment:\n       what is this verifying exactly?  Should this be augmented with a test verifying errors/inequality when reading data from an encrypted file without keys?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:40:45.247+0000",
                    "updated": "2021-11-02T19:40:45.247+0000",
                    "started": "2021-11-02T19:40:45.247+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673775",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673777",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741413346\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n+\n+\t// the total number of stored values. for repeated or optional values. this\n+\t// number may be lower than numBuffered\n+\tnumBufferedEncoded int64\n\nReview comment:\n       numDataValues might be a better name?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:47:32.035+0000",
                    "updated": "2021-11-02T19:47:32.035+0000",
                    "started": "2021-11-02T19:47:32.035+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673777",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673778",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741413847\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n\nReview comment:\n       numBufferedValues or numBufferedLevels might be better?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:48:15.515+0000",
                    "updated": "2021-11-02T19:48:15.515+0000",
                    "started": "2021-11-02T19:48:15.515+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673778",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673780",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741415194\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n\nReview comment:\n       ```suggestion\r\n   \t// total number of values stored in the data current page. this is the maximum\r\n   ```\r\n   Does this make it more accurate. I wonder if adding comment dividing sections between current Page values and column writer values would make this clearer? \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:50:14.359+0000",
                    "updated": "2021-11-02T19:50:14.359+0000",
                    "started": "2021-11-02T19:50:14.359+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673780",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673781",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741416127\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n+\n+\t// the total number of stored values. for repeated or optional values. this\n+\t// number may be lower than numBuffered\n+\tnumBufferedEncoded int64\n+\n+\trowsWritten       int\n+\ttotalBytesWritten int64\n+\t// records the current number of compressed bytes in a column\n+\ttotalCompressedBytes int64\n+\tclosed               bool\n+\tfallback             bool\n\nReview comment:\n       is this fallback to non dictionary?  could the name be improved?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:51:38.204+0000",
                    "updated": "2021-11-02T19:51:38.204+0000",
                    "started": "2021-11-02T19:51:38.204+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673781",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673784",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741417518\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n+\n+\t// the total number of stored values. for repeated or optional values. this\n+\t// number may be lower than numBuffered\n+\tnumBufferedEncoded int64\n+\n+\trowsWritten       int\n+\ttotalBytesWritten int64\n+\t// records the current number of compressed bytes in a column\n+\ttotalCompressedBytes int64\n+\tclosed               bool\n+\tfallback             bool\n+\n+\tpages []DataPage\n+\n+\tdefLevelSink *encoding.PooledBufferWriter\n+\trepLevelSink *encoding.PooledBufferWriter\n+\n+\tuncompressedData bytes.Buffer\n+\tcompressedTemp   *bytes.Buffer\n+\n+\tcurrentEncoder encoding.TypedEncoder\n+}\n+\n+func newColumnWriterBase(metaData *metadata.ColumnChunkMetaDataBuilder, pager PageWriter, useDict bool, enc parquet.Encoding, props *parquet.WriterProperties) columnWriter {\n+\tret := columnWriter{\n+\t\tmetaData:     metaData,\n+\t\tdescr:        metaData.Descr(),\n+\t\tlevelInfo:    computeLevelInfo(metaData.Descr()),\n+\t\tpager:        pager,\n+\t\thasDict:      useDict,\n+\t\tencoding:     enc,\n+\t\tprops:        props,\n+\t\tmem:          props.Allocator(),\n+\t\tdefLevelSink: encoding.NewPooledBufferWriter(0),\n+\t\trepLevelSink: encoding.NewPooledBufferWriter(0),\n+\t}\n+\tif pager.HasCompressor() {\n+\t\tret.compressedTemp = new(bytes.Buffer)\n+\t}\n+\tif props.StatisticsEnabledFor(ret.descr.Path()) && ret.descr.SortOrder() != schema.SortUNKNOWN {\n+\t\tret.pageStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t\tret.chunkStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t}\n+\n+\tif ret.props.DataPageVersion() == parquet.DataPageV1 {\n+\t\tif ret.descr.MaxDefinitionLevel() > 0 {\n\nReview comment:\n       maybe add a comment on why the offset is being added (this is so the number of levels can be recorded at the beginning of the buffer?)\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:53:55.161+0000",
                    "updated": "2021-11-02T19:53:55.161+0000",
                    "started": "2021-11-02T19:53:55.161+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673784",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673786",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741418805\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n+\n+\t// the total number of stored values. for repeated or optional values. this\n+\t// number may be lower than numBuffered\n+\tnumBufferedEncoded int64\n+\n+\trowsWritten       int\n+\ttotalBytesWritten int64\n+\t// records the current number of compressed bytes in a column\n+\ttotalCompressedBytes int64\n+\tclosed               bool\n+\tfallback             bool\n+\n+\tpages []DataPage\n+\n+\tdefLevelSink *encoding.PooledBufferWriter\n+\trepLevelSink *encoding.PooledBufferWriter\n+\n+\tuncompressedData bytes.Buffer\n+\tcompressedTemp   *bytes.Buffer\n+\n+\tcurrentEncoder encoding.TypedEncoder\n+}\n+\n+func newColumnWriterBase(metaData *metadata.ColumnChunkMetaDataBuilder, pager PageWriter, useDict bool, enc parquet.Encoding, props *parquet.WriterProperties) columnWriter {\n+\tret := columnWriter{\n+\t\tmetaData:     metaData,\n+\t\tdescr:        metaData.Descr(),\n+\t\tlevelInfo:    computeLevelInfo(metaData.Descr()),\n+\t\tpager:        pager,\n+\t\thasDict:      useDict,\n+\t\tencoding:     enc,\n+\t\tprops:        props,\n+\t\tmem:          props.Allocator(),\n+\t\tdefLevelSink: encoding.NewPooledBufferWriter(0),\n+\t\trepLevelSink: encoding.NewPooledBufferWriter(0),\n+\t}\n+\tif pager.HasCompressor() {\n+\t\tret.compressedTemp = new(bytes.Buffer)\n+\t}\n+\tif props.StatisticsEnabledFor(ret.descr.Path()) && ret.descr.SortOrder() != schema.SortUNKNOWN {\n+\t\tret.pageStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t\tret.chunkStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t}\n+\n+\tif ret.props.DataPageVersion() == parquet.DataPageV1 {\n+\t\tif ret.descr.MaxDefinitionLevel() > 0 {\n+\t\t\tret.defLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t\tif ret.descr.MaxRepetitionLevel() > 0 {\n+\t\t\tret.repLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t}\n+\n+\tret.defEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxDefinitionLevel(), ret.defLevelSink)\n+\tret.repEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxRepetitionLevel(), ret.repLevelSink)\n+\treturn ret\n+}\n+\n+func (w *columnWriter) SetBitsBuffer(buf *memory.Buffer) { w.bitsBuffer = buf }\n+\n+func (w *columnWriter) LevelInfo() LevelInfo { return w.levelInfo }\n+\n+func (w *columnWriter) Type() parquet.Type {\n+\treturn w.descr.PhysicalType()\n+}\n+\n+func (w *columnWriter) Descr() *schema.Column {\n+\treturn w.descr\n+}\n+\n+func (w *columnWriter) Properties() *parquet.WriterProperties {\n+\treturn w.props\n+}\n+\n+func (w *columnWriter) TotalCompressedBytes() int64 {\n+\treturn w.totalCompressedBytes\n+}\n+\n+func (w *columnWriter) TotalBytesWritten() int64 {\n+\treturn w.totalBytesWritten\n+}\n+\n+func (w *columnWriter) RowsWritten() int64 {\n\nReview comment:\n       is there a strong reason to up-cast this to int64?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:56:03.793+0000",
                    "updated": "2021-11-02T19:56:03.793+0000",
                    "started": "2021-11-02T19:56:03.792+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673786",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673787",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741419723\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n+\n+\t// the total number of stored values. for repeated or optional values. this\n+\t// number may be lower than numBuffered\n+\tnumBufferedEncoded int64\n+\n+\trowsWritten       int\n+\ttotalBytesWritten int64\n+\t// records the current number of compressed bytes in a column\n+\ttotalCompressedBytes int64\n+\tclosed               bool\n+\tfallback             bool\n+\n+\tpages []DataPage\n+\n+\tdefLevelSink *encoding.PooledBufferWriter\n+\trepLevelSink *encoding.PooledBufferWriter\n+\n+\tuncompressedData bytes.Buffer\n+\tcompressedTemp   *bytes.Buffer\n+\n+\tcurrentEncoder encoding.TypedEncoder\n+}\n+\n+func newColumnWriterBase(metaData *metadata.ColumnChunkMetaDataBuilder, pager PageWriter, useDict bool, enc parquet.Encoding, props *parquet.WriterProperties) columnWriter {\n+\tret := columnWriter{\n+\t\tmetaData:     metaData,\n+\t\tdescr:        metaData.Descr(),\n+\t\tlevelInfo:    computeLevelInfo(metaData.Descr()),\n+\t\tpager:        pager,\n+\t\thasDict:      useDict,\n+\t\tencoding:     enc,\n+\t\tprops:        props,\n+\t\tmem:          props.Allocator(),\n+\t\tdefLevelSink: encoding.NewPooledBufferWriter(0),\n+\t\trepLevelSink: encoding.NewPooledBufferWriter(0),\n+\t}\n+\tif pager.HasCompressor() {\n+\t\tret.compressedTemp = new(bytes.Buffer)\n+\t}\n+\tif props.StatisticsEnabledFor(ret.descr.Path()) && ret.descr.SortOrder() != schema.SortUNKNOWN {\n+\t\tret.pageStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t\tret.chunkStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t}\n+\n+\tif ret.props.DataPageVersion() == parquet.DataPageV1 {\n+\t\tif ret.descr.MaxDefinitionLevel() > 0 {\n+\t\t\tret.defLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t\tif ret.descr.MaxRepetitionLevel() > 0 {\n+\t\t\tret.repLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t}\n+\n+\tret.defEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxDefinitionLevel(), ret.defLevelSink)\n+\tret.repEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxRepetitionLevel(), ret.repLevelSink)\n+\treturn ret\n+}\n+\n+func (w *columnWriter) SetBitsBuffer(buf *memory.Buffer) { w.bitsBuffer = buf }\n+\n+func (w *columnWriter) LevelInfo() LevelInfo { return w.levelInfo }\n+\n+func (w *columnWriter) Type() parquet.Type {\n+\treturn w.descr.PhysicalType()\n+}\n+\n+func (w *columnWriter) Descr() *schema.Column {\n+\treturn w.descr\n+}\n+\n+func (w *columnWriter) Properties() *parquet.WriterProperties {\n+\treturn w.props\n+}\n+\n+func (w *columnWriter) TotalCompressedBytes() int64 {\n+\treturn w.totalCompressedBytes\n+}\n+\n+func (w *columnWriter) TotalBytesWritten() int64 {\n+\treturn w.totalBytesWritten\n+}\n+\n+func (w *columnWriter) RowsWritten() int64 {\n+\treturn int64(w.rowsWritten)\n+}\n+\n+func (w *columnWriter) WriteDataPage(page DataPage) error {\n+\twritten, err := w.pager.WriteDataPage(page)\n+\tw.totalBytesWritten += written\n+\treturn err\n+}\n+\n+func (w *columnWriter) WriteDefinitionLevels(levels []int16) {\n+\tw.defEncoder.EncodeNoFlush(levels)\n+}\n+\n+func (w *columnWriter) WriteRepetitionLevels(levels []int16) {\n+\tw.repEncoder.EncodeNoFlush(levels)\n+}\n+\n+func (w *columnWriter) init() {\n+\tw.defLevelSink.Reset(0)\n+\tw.repLevelSink.Reset(0)\n+\n+\tif w.props.DataPageVersion() == parquet.DataPageV1 {\n\nReview comment:\n       this seems duplicative with the code above?  Can they be consolidated?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:57:28.089+0000",
                    "updated": "2021-11-02T19:57:28.089+0000",
                    "started": "2021-11-02T19:57:28.088+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673787",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673788",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741420463\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n+\n+\t// the total number of stored values. for repeated or optional values. this\n+\t// number may be lower than numBuffered\n+\tnumBufferedEncoded int64\n+\n+\trowsWritten       int\n+\ttotalBytesWritten int64\n+\t// records the current number of compressed bytes in a column\n+\ttotalCompressedBytes int64\n+\tclosed               bool\n+\tfallback             bool\n+\n+\tpages []DataPage\n+\n+\tdefLevelSink *encoding.PooledBufferWriter\n+\trepLevelSink *encoding.PooledBufferWriter\n+\n+\tuncompressedData bytes.Buffer\n+\tcompressedTemp   *bytes.Buffer\n+\n+\tcurrentEncoder encoding.TypedEncoder\n+}\n+\n+func newColumnWriterBase(metaData *metadata.ColumnChunkMetaDataBuilder, pager PageWriter, useDict bool, enc parquet.Encoding, props *parquet.WriterProperties) columnWriter {\n+\tret := columnWriter{\n+\t\tmetaData:     metaData,\n+\t\tdescr:        metaData.Descr(),\n+\t\tlevelInfo:    computeLevelInfo(metaData.Descr()),\n+\t\tpager:        pager,\n+\t\thasDict:      useDict,\n+\t\tencoding:     enc,\n+\t\tprops:        props,\n+\t\tmem:          props.Allocator(),\n+\t\tdefLevelSink: encoding.NewPooledBufferWriter(0),\n+\t\trepLevelSink: encoding.NewPooledBufferWriter(0),\n+\t}\n+\tif pager.HasCompressor() {\n+\t\tret.compressedTemp = new(bytes.Buffer)\n+\t}\n+\tif props.StatisticsEnabledFor(ret.descr.Path()) && ret.descr.SortOrder() != schema.SortUNKNOWN {\n+\t\tret.pageStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t\tret.chunkStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t}\n+\n+\tif ret.props.DataPageVersion() == parquet.DataPageV1 {\n+\t\tif ret.descr.MaxDefinitionLevel() > 0 {\n+\t\t\tret.defLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t\tif ret.descr.MaxRepetitionLevel() > 0 {\n+\t\t\tret.repLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t}\n+\n+\tret.defEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxDefinitionLevel(), ret.defLevelSink)\n+\tret.repEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxRepetitionLevel(), ret.repLevelSink)\n+\treturn ret\n+}\n+\n+func (w *columnWriter) SetBitsBuffer(buf *memory.Buffer) { w.bitsBuffer = buf }\n+\n+func (w *columnWriter) LevelInfo() LevelInfo { return w.levelInfo }\n+\n+func (w *columnWriter) Type() parquet.Type {\n+\treturn w.descr.PhysicalType()\n+}\n+\n+func (w *columnWriter) Descr() *schema.Column {\n+\treturn w.descr\n+}\n+\n+func (w *columnWriter) Properties() *parquet.WriterProperties {\n+\treturn w.props\n+}\n+\n+func (w *columnWriter) TotalCompressedBytes() int64 {\n+\treturn w.totalCompressedBytes\n+}\n+\n+func (w *columnWriter) TotalBytesWritten() int64 {\n+\treturn w.totalBytesWritten\n+}\n+\n+func (w *columnWriter) RowsWritten() int64 {\n+\treturn int64(w.rowsWritten)\n+}\n+\n+func (w *columnWriter) WriteDataPage(page DataPage) error {\n+\twritten, err := w.pager.WriteDataPage(page)\n+\tw.totalBytesWritten += written\n+\treturn err\n+}\n+\n+func (w *columnWriter) WriteDefinitionLevels(levels []int16) {\n+\tw.defEncoder.EncodeNoFlush(levels)\n+}\n+\n+func (w *columnWriter) WriteRepetitionLevels(levels []int16) {\n+\tw.repEncoder.EncodeNoFlush(levels)\n+}\n+\n+func (w *columnWriter) init() {\n\nReview comment:\n       would reset be a better name for this function?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:58:41.790+0000",
                    "updated": "2021-11-02T19:58:41.790+0000",
                    "started": "2021-11-02T19:58:41.790+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673788",
                    "issueId": "13400825"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/worklog/673790",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #11538:\nURL: https://github.com/apache/arrow/pull/11538#discussion_r741421297\n\n\n\n##########\nFile path: go/parquet/file/column_writer.go\n##########\n@@ -0,0 +1,567 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+// http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package file\n+\n+import (\n+\t\"bytes\"\n+\t\"encoding/binary\"\n+\t\"io\"\n+\n+\t\"github.com/apache/arrow/go/arrow\"\n+\t\"github.com/apache/arrow/go/arrow/array\"\n+\t\"github.com/apache/arrow/go/arrow/bitutil\"\n+\t\"github.com/apache/arrow/go/arrow/memory\"\n+\t\"github.com/apache/arrow/go/parquet\"\n+\t\"github.com/apache/arrow/go/parquet/internal/encoding\"\n+\t\"github.com/apache/arrow/go/parquet/metadata\"\n+\t\"github.com/apache/arrow/go/parquet/schema\"\n+)\n+\n+//go:generate go run ../../arrow/_tools/tmpl/main.go -i -data=../internal/encoding/physical_types.tmpldata column_writer_types.gen.go.tmpl\n+\n+// ColumnChunkWriter is the base interface for all columnwriters. To directly write\n+// data to the column, you need to assert it to the correctly typed ColumnChunkWriter\n+// instance, such as Int32ColumnWriter.\n+type ColumnChunkWriter interface {\n+\t// Close ends this column and returns the number of bytes written\n+\tClose() error\n+\t// Type returns the underlying physical parquet type for this column\n+\tType() parquet.Type\n+\t// Descr returns the column information for this writer\n+\tDescr() *schema.Column\n+\t// RowsWritten returns the number of rows that have so far been written with this writer\n+\tRowsWritten() int64\n+\t// TotalCompressedBytes returns the number of bytes, after compression, that have been written so far\n+\tTotalCompressedBytes() int64\n+\t// TotalBytesWritten includes the bytes for writing dictionary pages, while TotalCompressedBytes is\n+\t// just the data and page headers\n+\tTotalBytesWritten() int64\n+\t// Properties returns the current WriterProperties in use for this writer\n+\tProperties() *parquet.WriterProperties\n+\n+\tLevelInfo() LevelInfo\n+\tSetBitsBuffer(*memory.Buffer)\n+}\n+\n+func computeLevelInfo(descr *schema.Column) (info LevelInfo) {\n+\tinfo.DefLevel = descr.MaxDefinitionLevel()\n+\tinfo.RepLevel = descr.MaxRepetitionLevel()\n+\n+\tminSpacedDefLevel := descr.MaxDefinitionLevel()\n+\tn := descr.SchemaNode()\n+\tfor n != nil && n.RepetitionType() != parquet.Repetitions.Repeated {\n+\t\tif n.RepetitionType() == parquet.Repetitions.Optional {\n+\t\t\tminSpacedDefLevel--\n+\t\t}\n+\t\tn = n.Parent()\n+\t}\n+\tinfo.RepeatedAncestorDefLevel = minSpacedDefLevel\n+\treturn\n+}\n+\n+type columnWriter struct {\n+\tmetaData *metadata.ColumnChunkMetaDataBuilder\n+\tdescr    *schema.Column\n+\n+\t// scratch buffer if validity bits need to be recalculated\n+\tbitsBuffer *memory.Buffer\n+\tlevelInfo  LevelInfo\n+\tpager      PageWriter\n+\thasDict    bool\n+\tencoding   parquet.Encoding\n+\tprops      *parquet.WriterProperties\n+\tdefEncoder encoding.LevelEncoder\n+\trepEncoder encoding.LevelEncoder\n+\tmem        memory.Allocator\n+\n+\tpageStatistics  metadata.TypedStatistics\n+\tchunkStatistics metadata.TypedStatistics\n+\n+\t// total number of values stored in the data page. this is the maximum\n+\t// of the number of encoded def levels or encoded values. for\n+\t// non-repeated, required columns, this is equal to the number of encoded\n+\t// values. For repeated or optional values, there may be fewer data values\n+\t// than levels, and this tells you how many encoded levels there are in that case\n+\tnumBuffered int64\n+\n+\t// the total number of stored values. for repeated or optional values. this\n+\t// number may be lower than numBuffered\n+\tnumBufferedEncoded int64\n+\n+\trowsWritten       int\n+\ttotalBytesWritten int64\n+\t// records the current number of compressed bytes in a column\n+\ttotalCompressedBytes int64\n+\tclosed               bool\n+\tfallback             bool\n+\n+\tpages []DataPage\n+\n+\tdefLevelSink *encoding.PooledBufferWriter\n+\trepLevelSink *encoding.PooledBufferWriter\n+\n+\tuncompressedData bytes.Buffer\n+\tcompressedTemp   *bytes.Buffer\n+\n+\tcurrentEncoder encoding.TypedEncoder\n+}\n+\n+func newColumnWriterBase(metaData *metadata.ColumnChunkMetaDataBuilder, pager PageWriter, useDict bool, enc parquet.Encoding, props *parquet.WriterProperties) columnWriter {\n+\tret := columnWriter{\n+\t\tmetaData:     metaData,\n+\t\tdescr:        metaData.Descr(),\n+\t\tlevelInfo:    computeLevelInfo(metaData.Descr()),\n+\t\tpager:        pager,\n+\t\thasDict:      useDict,\n+\t\tencoding:     enc,\n+\t\tprops:        props,\n+\t\tmem:          props.Allocator(),\n+\t\tdefLevelSink: encoding.NewPooledBufferWriter(0),\n+\t\trepLevelSink: encoding.NewPooledBufferWriter(0),\n+\t}\n+\tif pager.HasCompressor() {\n+\t\tret.compressedTemp = new(bytes.Buffer)\n+\t}\n+\tif props.StatisticsEnabledFor(ret.descr.Path()) && ret.descr.SortOrder() != schema.SortUNKNOWN {\n+\t\tret.pageStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t\tret.chunkStatistics = metadata.NewStatistics(ret.descr, props.Allocator())\n+\t}\n+\n+\tif ret.props.DataPageVersion() == parquet.DataPageV1 {\n+\t\tif ret.descr.MaxDefinitionLevel() > 0 {\n+\t\t\tret.defLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t\tif ret.descr.MaxRepetitionLevel() > 0 {\n+\t\t\tret.repLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t}\n+\n+\tret.defEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxDefinitionLevel(), ret.defLevelSink)\n+\tret.repEncoder.Init(parquet.Encodings.RLE, ret.descr.MaxRepetitionLevel(), ret.repLevelSink)\n+\treturn ret\n+}\n+\n+func (w *columnWriter) SetBitsBuffer(buf *memory.Buffer) { w.bitsBuffer = buf }\n+\n+func (w *columnWriter) LevelInfo() LevelInfo { return w.levelInfo }\n+\n+func (w *columnWriter) Type() parquet.Type {\n+\treturn w.descr.PhysicalType()\n+}\n+\n+func (w *columnWriter) Descr() *schema.Column {\n+\treturn w.descr\n+}\n+\n+func (w *columnWriter) Properties() *parquet.WriterProperties {\n+\treturn w.props\n+}\n+\n+func (w *columnWriter) TotalCompressedBytes() int64 {\n+\treturn w.totalCompressedBytes\n+}\n+\n+func (w *columnWriter) TotalBytesWritten() int64 {\n+\treturn w.totalBytesWritten\n+}\n+\n+func (w *columnWriter) RowsWritten() int64 {\n+\treturn int64(w.rowsWritten)\n+}\n+\n+func (w *columnWriter) WriteDataPage(page DataPage) error {\n+\twritten, err := w.pager.WriteDataPage(page)\n+\tw.totalBytesWritten += written\n+\treturn err\n+}\n+\n+func (w *columnWriter) WriteDefinitionLevels(levels []int16) {\n+\tw.defEncoder.EncodeNoFlush(levels)\n+}\n+\n+func (w *columnWriter) WriteRepetitionLevels(levels []int16) {\n+\tw.repEncoder.EncodeNoFlush(levels)\n+}\n+\n+func (w *columnWriter) init() {\n+\tw.defLevelSink.Reset(0)\n+\tw.repLevelSink.Reset(0)\n+\n+\tif w.props.DataPageVersion() == parquet.DataPageV1 {\n+\t\tif w.descr.MaxDefinitionLevel() > 0 {\n+\t\t\tw.defLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t\tif w.descr.MaxRepetitionLevel() > 0 {\n+\t\t\tw.repLevelSink.SetOffset(arrow.Uint32SizeBytes)\n+\t\t}\n+\t}\n+\n+\tw.defEncoder.Reset(w.descr.MaxDefinitionLevel())\n+\tw.repEncoder.Reset(w.descr.MaxRepetitionLevel())\n+}\n+\n+func (w *columnWriter) concatBuffers(defLevelsSize, repLevelsSize int64, values []byte, wr io.Writer) {\n\nReview comment:\n       nit: would flushBuffers be a better name for this method?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T19:59:49.473+0000",
                    "updated": "2021-11-02T19:59:49.473+0000",
                    "started": "2021-11-02T19:59:49.473+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673790",
                    "issueId": "13400825"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
            "id": "7",
            "description": "The sub-task of the issue",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
            "name": "Sub-task",
            "subtask": true,
            "avatarId": 21146
        },
        "timespent": 47400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@75bc04a[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@61c5ab43[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@9923ccc[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@45c1010e[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@629aba7b[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@253783e3[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3d5bbfd5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@70df1001[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@47a12503[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@5e908e8f[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3e55279f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@3602774c[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 47400,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Jan 12 18:26:51 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-01-12T18:26:51.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-13986/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-09-13T16:11:30.000+0000",
        "updated": "2022-01-13T15:27:23.000+0000",
        "timeoriginalestimate": null,
        "description": "Add the package for manipulating files directly, column reader/writer, file reader/writer",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "13h 10m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 47400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Go][Parquet] Add File Package - writers",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13400825/comment/17474792",
                    "id": "17474792",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
                        "name": "zeroshade",
                        "key": "zeroshade",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
                        },
                        "displayName": "Matthew Topol",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 11538\n[https://github.com/apache/arrow/pull/11538]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=zeroshade",
                        "name": "zeroshade",
                        "key": "zeroshade",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=zeroshade&avatarId=31230",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zeroshade&avatarId=31230",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zeroshade&avatarId=31230",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zeroshade&avatarId=31230"
                        },
                        "displayName": "Matthew Topol",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2022-01-12T18:26:51.162+0000",
                    "updated": "2022-01-12T18:26:51.162+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0uv34:",
        "customfield_12314139": null
    }
}