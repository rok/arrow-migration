{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13359118",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118",
    "key": "ARROW-11677",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12349493",
                "id": "12349493",
                "description": "",
                "name": "4.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-04-26"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
            "name": "lidavidm",
            "key": "lidavidm",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "David Li",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332532",
                "id": "12332532",
                "name": "Documentation"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
            "name": "apitrou",
            "key": "pitrou",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
            },
            "displayName": "Antoine Pitrou",
            "active": true,
            "timeZone": "Europe/Paris"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
            "name": "apitrou",
            "key": "pitrou",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
            },
            "displayName": "Antoine Pitrou",
            "active": true,
            "timeZone": "Europe/Paris"
        },
        "aggregateprogress": {
            "progress": 19800,
            "total": 19800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 19800,
            "total": 19800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11677/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 33,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/572713",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm opened a new pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810\n\n\n   This is mostly based on the Python documentation. This also adds a new C++ example to accompany the documentation.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-26T15:28:41.361+0000",
                    "updated": "2021-03-26T15:28:41.361+0000",
                    "started": "2021-03-26T15:28:41.361+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "572713",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/572714",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#issuecomment-808311116\n\n\n   https://issues.apache.org/jira/browse/ARROW-11677\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-26T15:28:54.858+0000",
                    "updated": "2021-03-26T15:28:54.858+0000",
                    "started": "2021-03-26T15:28:54.858+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "572714",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/572715",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#issuecomment-808311064\n\n\n   Rendered (though Github is acting up):\r\n   - https://lidavidm.github.io/arrow-docs-next/cpp/dataset.html#tabular-datasets\r\n   - https://lidavidm.github.io/arrow-docs-next/cpp/api/dataset.html#dataset\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-26T15:28:57.953+0000",
                    "updated": "2021-03-26T15:28:57.953+0000",
                    "started": "2021-03-26T15:28:57.953+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "572715",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/574191",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r604173489\n\n\n\n##########\nFile path: docs/source/cpp/api/dataset.rst\n##########\n@@ -0,0 +1,165 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+=======\n+Dataset\n+=======\n+\n+.. sidebar:: Contents\n+\n+   .. contents:: :local:\n+\n+Interface\n+=========\n+\n+.. doxygenclass:: arrow::dataset::Fragment\n\nReview comment:\n       Note that you may use `doxygengroup`s to reduce the burden of listing and maintaining individual APIs here.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-30T14:53:56.433+0000",
                    "updated": "2021-03-30T14:53:56.433+0000",
                    "started": "2021-03-30T14:53:56.433+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "574191",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/574192",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#issuecomment-810329678\n\n\n   I'll let @bkietz comment on the correctness and idiomaticity of the example. I find the code length a bit intimidating, but I guess C++ doesn't allow for a very concise example :-)\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-30T14:55:43.206+0000",
                    "updated": "2021-03-30T14:55:43.206+0000",
                    "started": "2021-03-30T14:55:43.206+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "574192",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/574193",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#issuecomment-810330783\n\n\n   The example is a bit long because it does the same thing several times with minor variations, with most things duplicated each time so snippets can be used as standalone examples in the docs. However the example could certainly be better structured/documented so it's readable on its own as well.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-30T14:57:06.926+0000",
                    "updated": "2021-03-30T14:57:06.926+0000",
                    "started": "2021-03-30T14:57:06.926+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "574193",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/574435",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#issuecomment-810607005\n\n\n   @lidavidm I'm curious on your thoughts about other things that `ScannerBuilder`s can be constructed from and whether this is something we would want to add to these commits, or if we should have a separate commit?\r\n   \r\n   As an example, one of the things that was difficult to figure out for our research group from existing documentation was how to apply a scan to something already in memory:\r\n   \r\n   ```cpp\r\n       using InMemDatasetPtr = std::shared_ptr<arrow::dataset::InMemoryDataset>;\r\n       using ScanBuilderPtr  = std::shared_ptr<arrow::dataset::ScannerBuilder>;\r\n   \r\n       ...\r\n   \r\n       // Create an in-memory dataset from the parsed record batch\r\n       InMemDatasetPtr batch_as_dataset = std::make_shared<arrow::dataset::InMemoryDataset>(\r\n            table_batch->schema()\r\n           ,RecordBatchVec({ table_batch })\r\n       );\r\n   \r\n       // Create a scanner to pass the expression\r\n       ARROW_ASSIGN_OR_RAISE(\r\n            ScanBuilderPtr scanbuilder\r\n           ,batch_as_dataset->NewScan()\r\n       );\r\n   \r\n       ...\r\n   ```\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-30T22:01:55.814+0000",
                    "updated": "2021-03-30T22:01:55.814+0000",
                    "started": "2021-03-30T22:01:55.814+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "574435",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/574436",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#issuecomment-810608422\n\n\n   @drin thanks for the feedback. I can add a snippet about that as well. If there are other things that were confusing or difficult to figure out, I'd like to tackle them too.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-30T22:04:22.371+0000",
                    "updated": "2021-03-30T22:04:22.371+0000",
                    "started": "2021-03-30T22:04:22.371+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "574436",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/574442",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r604466376\n\n\n\n##########\nFile path: cpp/examples/arrow/dataset-documentation-example.cc\n##########\n@@ -217,24 +229,29 @@ std::shared_ptr<arrow::Table> SelectAndProjectDataset(\n   auto scan_builder = dataset->NewScan().ValueOrDie();\n   std::vector<std::string> names;\n   std::vector<ds::Expression> exprs;\n+  // Read all the original columns.\n   for (const auto& field : dataset->schema()->fields()) {\n     names.push_back(field->name());\n     exprs.push_back(ds::field_ref(field->name()));\n   }\n+  // Also derive a new column.\n   names.push_back(\"b_large\");\n   exprs.push_back(ds::greater(ds::field_ref(\"b\"), ds::literal(1)));\n   ABORT_ON_FAILURE(scan_builder->Project(exprs, names));\n\nReview comment:\n       From what I've seen, we've tried to do this directly on the ScannerBuilder:\r\n   \r\n   ```cpp\r\n       std::vector<std::string>   projection_attrs;\r\n       arrow::dataset::Expression selection_expr;\r\n   \r\n       ...\r\n   \r\n       ARROW_RETURN_NOT_OK(scanbuilder->Project(projection_attrs));\r\n       ARROW_RETURN_NOT_OK(scanbuilder->Filter(selection_expr));\r\n   \r\n       ...\r\n   ```\r\n   \r\n   I am looking at the code first, so I may just need to read the actual documentation included in this PR, but it may be nice to include a comment that references the documentation of why to prefer one approach to another or if they're equivalent mechanisms.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-30T22:14:37.935+0000",
                    "updated": "2021-03-30T22:14:37.935+0000",
                    "started": "2021-03-30T22:14:37.934+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "574442",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/574444",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#issuecomment-810615985\n\n\n   > @drin thanks for the feedback. I can add a snippet about that as well. If there are other things that were confusing or difficult to figure out, I'd like to tackle them too.\r\n   \r\n   Sounds great! I appreciate your work. I have only recently gathered enough experience to be able to express what is/was difficult and why, so I'll try to keep adding my thoughts here (and directly in JIRA).\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-30T22:18:43.489+0000",
                    "updated": "2021-03-30T22:18:43.489+0000",
                    "started": "2021-03-30T22:18:43.489+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "574444",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/574827",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r604880136\n\n\n\n##########\nFile path: cpp/examples/arrow/dataset-documentation-example.cc\n##########\n@@ -217,24 +229,29 @@ std::shared_ptr<arrow::Table> SelectAndProjectDataset(\n   auto scan_builder = dataset->NewScan().ValueOrDie();\n   std::vector<std::string> names;\n   std::vector<ds::Expression> exprs;\n+  // Read all the original columns.\n   for (const auto& field : dataset->schema()->fields()) {\n     names.push_back(field->name());\n     exprs.push_back(ds::field_ref(field->name()));\n   }\n+  // Also derive a new column.\n   names.push_back(\"b_large\");\n   exprs.push_back(ds::greater(ds::field_ref(\"b\"), ds::literal(1)));\n   ABORT_ON_FAILURE(scan_builder->Project(exprs, names));\n\nReview comment:\n       Ah, the example here is using Project to define new virtual columns from the physical columns, while your snippet is simply selecting a subset of the physical columns to read. The former API is a superset of the latter. There's an example of what you currently do a little bit prior to this in the file. I've added explanatory comments for these examples, though in the prose I think it should also be clear.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-03-31T13:07:30.659+0000",
                    "updated": "2021-03-31T13:07:30.659+0000",
                    "started": "2021-03-31T13:07:30.659+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "574827",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579644",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r610177257\n\n\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n\nReview comment:\n       Maybe get rid of `fine-grained managing of tasks`?  You could change `parallel reading` to `potentially parallel reading`.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n\nReview comment:\n       `and the file format to read` -> `and the file format to use for reading`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n\nReview comment:\n       It seems a bit odd that here we iterate through and print `dataset->files()` when in an earlier example we were iterating `dataset->GetFragments()`.  Although I suppose I understand it is because we are working with the more specialized `FileSystemDataset` and not a plain old `Dataset`.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n\nReview comment:\n       Maybe change:\r\n   \r\n   > A unified interface for different sources: supporting different sources and\r\n   \r\n   to...\r\n   \r\n   `A unified interface for supporting different sources and`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n\nReview comment:\n       `may look like on disk` -> `may have the following layout`.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n\nReview comment:\n       ```\r\n   (e.g.  database connections)\r\n   ``` \r\n   to \r\n   ```\r\n   (e.g. database connections)\"\r\n   ```\r\n   [extra space]\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n\nReview comment:\n       Drop `in combination with expressions.`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n\nReview comment:\n       `Here, we're` -> `We're`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n\nReview comment:\n       \"given the base directory path\" -> \"given a base directory path\"\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n\nReview comment:\n       Take all these grammar / style nits with a grain of salt.\r\n   \r\n   > tabular, potentially larger than memory and multi-file datasets:\r\n   \r\n   to...\r\n   \r\n   `tabular, potentially larger than memory and multi-file datasets.  For example:`\r\n   \r\n   or...\r\n   \r\n   `tabular, potentially larger than memory and multi-file datasets.  This includes:`\r\n   \n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n\nReview comment:\n       Maybe stop at line 165 in this snippet?\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n+partitioning scheme:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 256-279\n+   :emphasize-lines: 7,9-11\n+   :linenos:\n+   :lineno-match:\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. code-block:: text\n+\n+   $ ./debug/dataset_documentation_example file:///tmp parquet_hive partitioned\n+   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet\n+   Partition expression: (part == \"a\")\n+   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet\n+   Partition expression: (part == \"b\")\n+   Read 20 rows\n+   a: int64\n+     -- field metadata --\n+     PARQUET:field_id: '1'\n+   b: double\n+     -- field metadata --\n+     PARQUET:field_id: '2'\n+   c: int64\n+     -- field metadata --\n+     PARQUET:field_id: '3'\n+   part: string\n+   ----\n+   # snip...\n+\n+We can now filter on the partition keys, which avoids loading files\n+altogether if they do not match the predicate:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 281-301\n+   :emphasize-lines: 15-18\n+   :linenos:\n+   :lineno-match:\n+\n+Different partitioning schemes\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above example uses a hive-like directory scheme, such as \"/year=2009/month=11/day=15\".\n+We specified this by passing the partition *factory*. In this case, the types of\n\nReview comment:\n       `the partition *factory*` -> `the hive partitioning factory`\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -38,9 +38,9 @@ tabular, potentially larger than memory and multi-file datasets:\n * Optimized reading with predicate pushdown (filtering rows), projection\n   (selecting columns), parallel reading or fine-grained managing of tasks.\n \n-Currently, only Parquet and Feather / Arrow IPC files are supported. The goal\n-is to expand this in the future to other file formats and data sources (e.g.\n-database connections).\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n\nReview comment:\n       ```\r\n   (e.g.  database\r\n   ```\r\n   ```\r\n   (e.g. database\r\n   ```\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n\nReview comment:\n       `a base directory path` -> `searching a base directory`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n\nReview comment:\n       `choose between things like reading` -> `choose between reading`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n+partitioning scheme:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 256-279\n+   :emphasize-lines: 7,9-11\n+   :linenos:\n+   :lineno-match:\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. code-block:: text\n+\n+   $ ./debug/dataset_documentation_example file:///tmp parquet_hive partitioned\n+   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet\n+   Partition expression: (part == \"a\")\n+   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet\n+   Partition expression: (part == \"b\")\n+   Read 20 rows\n+   a: int64\n+     -- field metadata --\n+     PARQUET:field_id: '1'\n+   b: double\n+     -- field metadata --\n+     PARQUET:field_id: '2'\n+   c: int64\n+     -- field metadata --\n+     PARQUET:field_id: '3'\n+   part: string\n+   ----\n+   # snip...\n+\n+We can now filter on the partition keys, which avoids loading files\n+altogether if they do not match the predicate:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 281-301\n+   :emphasize-lines: 15-18\n+   :linenos:\n+   :lineno-match:\n+\n+Different partitioning schemes\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above example uses a hive-like directory scheme, such as \"/year=2009/month=11/day=15\".\n+We specified this by passing the partition *factory*. In this case, the types of\n+the partition keys are inferred from the file paths.\n+\n+It is also possible directly construct the partitioning and explicitly define\n+the schema of the partition keys. For example:\n+\n+.. code-block:: cpp\n+\n+    auto part = std::make_shared<ds::HivePartitioning>(arrow::schema({\n+        arrow::field(\"year\", arrow::int16()),\n+        arrow::field(\"month\", arrow::int8()),\n+        arrow::field(\"day\", arrow::int32())\n+    }));\n+\n+\"Directory partitioning\" is also supported, where the segments in the file path\n+represent the values of the partition keys without including the name (the\n+field name are implicit in the segment's index). For example, given field names\n+\"year\", \"month\", and \"day\", one path might be \"/2019/11/15\".\n+\n+Since the names are not included in the file paths, these must be specified\n+when constructing a directory partitioning:\n+\n+.. code-block:: cpp\n+\n+    auto part = ds::DirectoryPartitioning::MakeFactory({\"year\", \"month\", \"day\"});\n+\n+Directory partitioning also supports providing a full schema rather than inferring\n+types from file paths.\n+\n+Reading from other data sources\n+-------------------------------\n+\n+Reading in-memory data\n+~~~~~~~~~~~~~~~~~~~~~~\n+\n+If you already have data in memory that you'd like to use with the Datasets\n+interface (e.g. to filter/project data, or to write it out to a filesystem),\n+you can wrap it in a :class:`arrow::dataset::InMemoryDataset`:\n+\n+.. code-block:: cpp\n+\n+   auto table = arrow::Table::FromRecordBatches(...);\n+   auto dataset = std::make_shared<arrow::dataset::InMemoryDataset>(std::move(table));\n+   // Scan the dataset, filter, it, etc.\n+   auto scanner_builder = dataset->NewScan();\n+\n+In the example, we used the InMemoryDataset to write our example data to local\n+disk to be used in the rest of the example:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 24-28\n+   :linenos:\n+   :lineno-match:\n+\n+Reading from cloud storage\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+In addition to local files, Arrow Datasets also supports reading from cloud\n\nReview comment:\n       `supports` -> `support`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n\nReview comment:\n       \"loads nothing into memory\" seems a little misleading.  Wouldn't it be more accurate to say \"does not begin reading the data\"?  Also what is `in this way` referring to?  Isn't this statement true whether you are creating a dataset from a directory or from a list of paths?  In fact, you might want to explain why it is crawling the directory to find all the files given we just presumably listed the paths manually.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n\nReview comment:\n       I don't believe the `,` belongs here.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n\nReview comment:\n       Also, you use a `::` here but elsewhere you use a `:`.  But this may just be an `rst` thing I don't understand.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n\nReview comment:\n       `multiple file formats and sources` -> `multiple file formats and filesystems`?  Not sure on this one.  Either way, you don't really expand on this point.  There are no examples of using a \"different source\" whatever that might be.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n\nReview comment:\n       `as a Feather file` -> `as Feather files`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n\nReview comment:\n       You're inconsistent about whether you start a continuing sentence with `...` or not.  I'm not sure which is more correct but you do it differently on line 93.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n\nReview comment:\n       `we've read the entire dataset` -> `we've been reading the entire dataset`\r\n   \r\n   Instead of `inefficient` can you be more specific?  Maybe `use too much memory or waste time reading data we don't need.`?\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n\nReview comment:\n       Technically `Project` does more than just that but I think it's fine to omit that complexity at the moment.  Maybe change to `In this snippet we use Project to select which columns to read`.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n\nReview comment:\n       `how they are read` -> `how files are read`.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n\nReview comment:\n       `Feather file using the same functions, but passing a` -> `Feather file by passing a`.\r\n   \r\n   Also, I'm pretty sure this is the wrong snippet to be referencing here.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n\nReview comment:\n       Maybe add a sentence...\r\n   \r\n   `  Some formats (such as Parquet) can use this filter to reduce the amount of I/O needed.`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n\nReview comment:\n       \"pushdown\" is maybe not a term known by the audience here.  Also, I don't think column selection is technically \"pushdown\".  Maybe `Some formats (such as Parquet) can reduce I/O costs by reading specific columns from the filesystem.`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n\nReview comment:\n       Second comma on this line not needed.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n\nReview comment:\n       `uses` -> `should use`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n\nReview comment:\n       The whole sentence starting with `However` is a little awkward to me.  Maybe...\r\n   \r\n   ```\r\n   However, a \"partitioned\" dataset relies on nested directory structures where sub-directory names hold describe the data stored in that directory\r\n   ```\r\n   \r\n   ...but I'm not really sold on that either.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n+partitioning scheme:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 256-279\n+   :emphasize-lines: 7,9-11\n+   :linenos:\n+   :lineno-match:\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. code-block:: text\n+\n+   $ ./debug/dataset_documentation_example file:///tmp parquet_hive partitioned\n+   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet\n+   Partition expression: (part == \"a\")\n+   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet\n+   Partition expression: (part == \"b\")\n+   Read 20 rows\n+   a: int64\n+     -- field metadata --\n+     PARQUET:field_id: '1'\n+   b: double\n+     -- field metadata --\n+     PARQUET:field_id: '2'\n+   c: int64\n+     -- field metadata --\n+     PARQUET:field_id: '3'\n+   part: string\n+   ----\n+   # snip...\n+\n+We can now filter on the partition keys, which avoids loading files\n+altogether if they do not match the predicate:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 281-301\n+   :emphasize-lines: 15-18\n+   :linenos:\n+   :lineno-match:\n+\n+Different partitioning schemes\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above example uses a hive-like directory scheme, such as \"/year=2009/month=11/day=15\".\n+We specified this by passing the partition *factory*. In this case, the types of\n+the partition keys are inferred from the file paths.\n+\n+It is also possible directly construct the partitioning and explicitly define\n+the schema of the partition keys. For example:\n+\n+.. code-block:: cpp\n+\n+    auto part = std::make_shared<ds::HivePartitioning>(arrow::schema({\n+        arrow::field(\"year\", arrow::int16()),\n+        arrow::field(\"month\", arrow::int8()),\n+        arrow::field(\"day\", arrow::int32())\n+    }));\n+\n+\"Directory partitioning\" is also supported, where the segments in the file path\n\nReview comment:\n       \"Directory partitioning\" is kind of a confusing name when \"partitioning\" means \"using directories\".  Maybe to help with that we could rephrase this to...\r\n   \r\n   ```\r\n   Arrow supports another form of partitioning, \"Directory partitioning\", where the segments...\r\n   ```\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n\nReview comment:\n       It feels a bit weird to have \"Dataset\" with a capital D and no backticks or link.  Could maybe phrase it as \"Arrow's dataset writing functionality.\"\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n+partitioning scheme:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 256-279\n+   :emphasize-lines: 7,9-11\n+   :linenos:\n+   :lineno-match:\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. code-block:: text\n+\n+   $ ./debug/dataset_documentation_example file:///tmp parquet_hive partitioned\n+   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet\n+   Partition expression: (part == \"a\")\n+   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet\n+   Partition expression: (part == \"b\")\n+   Read 20 rows\n+   a: int64\n+     -- field metadata --\n+     PARQUET:field_id: '1'\n+   b: double\n+     -- field metadata --\n+     PARQUET:field_id: '2'\n+   c: int64\n+     -- field metadata --\n+     PARQUET:field_id: '3'\n+   part: string\n+   ----\n+   # snip...\n+\n+We can now filter on the partition keys, which avoids loading files\n+altogether if they do not match the predicate:\n\nReview comment:\n       `predicate` -> `filter`.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n+partitioning scheme:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 256-279\n+   :emphasize-lines: 7,9-11\n+   :linenos:\n+   :lineno-match:\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. code-block:: text\n+\n+   $ ./debug/dataset_documentation_example file:///tmp parquet_hive partitioned\n+   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet\n+   Partition expression: (part == \"a\")\n+   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet\n+   Partition expression: (part == \"b\")\n+   Read 20 rows\n+   a: int64\n+     -- field metadata --\n+     PARQUET:field_id: '1'\n+   b: double\n+     -- field metadata --\n+     PARQUET:field_id: '2'\n+   c: int64\n+     -- field metadata --\n+     PARQUET:field_id: '3'\n+   part: string\n+   ----\n+   # snip...\n+\n+We can now filter on the partition keys, which avoids loading files\n+altogether if they do not match the predicate:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 281-301\n+   :emphasize-lines: 15-18\n+   :linenos:\n+   :lineno-match:\n+\n+Different partitioning schemes\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above example uses a hive-like directory scheme, such as \"/year=2009/month=11/day=15\".\n+We specified this by passing the partition *factory*. In this case, the types of\n+the partition keys are inferred from the file paths.\n+\n+It is also possible directly construct the partitioning and explicitly define\n+the schema of the partition keys. For example:\n+\n+.. code-block:: cpp\n+\n+    auto part = std::make_shared<ds::HivePartitioning>(arrow::schema({\n+        arrow::field(\"year\", arrow::int16()),\n+        arrow::field(\"month\", arrow::int8()),\n+        arrow::field(\"day\", arrow::int32())\n+    }));\n+\n+\"Directory partitioning\" is also supported, where the segments in the file path\n+represent the values of the partition keys without including the name (the\n+field name are implicit in the segment's index). For example, given field names\n+\"year\", \"month\", and \"day\", one path might be \"/2019/11/15\".\n+\n+Since the names are not included in the file paths, these must be specified\n+when constructing a directory partitioning:\n+\n+.. code-block:: cpp\n+\n+    auto part = ds::DirectoryPartitioning::MakeFactory({\"year\", \"month\", \"day\"});\n+\n+Directory partitioning also supports providing a full schema rather than inferring\n+types from file paths.\n+\n+Reading from other data sources\n+-------------------------------\n+\n+Reading in-memory data\n+~~~~~~~~~~~~~~~~~~~~~~\n+\n+If you already have data in memory that you'd like to use with the Datasets\n+interface (e.g. to filter/project data, or to write it out to a filesystem),\n+you can wrap it in a :class:`arrow::dataset::InMemoryDataset`:\n\nReview comment:\n       `a` -> `an`\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n+partitioning scheme:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 256-279\n+   :emphasize-lines: 7,9-11\n+   :linenos:\n+   :lineno-match:\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. code-block:: text\n+\n+   $ ./debug/dataset_documentation_example file:///tmp parquet_hive partitioned\n+   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet\n+   Partition expression: (part == \"a\")\n+   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet\n+   Partition expression: (part == \"b\")\n+   Read 20 rows\n+   a: int64\n+     -- field metadata --\n+     PARQUET:field_id: '1'\n+   b: double\n+     -- field metadata --\n+     PARQUET:field_id: '2'\n+   c: int64\n+     -- field metadata --\n+     PARQUET:field_id: '3'\n+   part: string\n+   ----\n+   # snip...\n+\n+We can now filter on the partition keys, which avoids loading files\n+altogether if they do not match the predicate:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 281-301\n+   :emphasize-lines: 15-18\n+   :linenos:\n+   :lineno-match:\n+\n+Different partitioning schemes\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above example uses a hive-like directory scheme, such as \"/year=2009/month=11/day=15\".\n+We specified this by passing the partition *factory*. In this case, the types of\n+the partition keys are inferred from the file paths.\n+\n+It is also possible directly construct the partitioning and explicitly define\n+the schema of the partition keys. For example:\n+\n+.. code-block:: cpp\n+\n+    auto part = std::make_shared<ds::HivePartitioning>(arrow::schema({\n+        arrow::field(\"year\", arrow::int16()),\n+        arrow::field(\"month\", arrow::int8()),\n+        arrow::field(\"day\", arrow::int32())\n+    }));\n+\n+\"Directory partitioning\" is also supported, where the segments in the file path\n+represent the values of the partition keys without including the name (the\n+field name are implicit in the segment's index). For example, given field names\n+\"year\", \"month\", and \"day\", one path might be \"/2019/11/15\".\n+\n+Since the names are not included in the file paths, these must be specified\n+when constructing a directory partitioning:\n+\n+.. code-block:: cpp\n+\n+    auto part = ds::DirectoryPartitioning::MakeFactory({\"year\", \"month\", \"day\"});\n+\n+Directory partitioning also supports providing a full schema rather than inferring\n+types from file paths.\n+\n+Reading from other data sources\n+-------------------------------\n+\n+Reading in-memory data\n+~~~~~~~~~~~~~~~~~~~~~~\n+\n+If you already have data in memory that you'd like to use with the Datasets\n+interface (e.g. to filter/project data, or to write it out to a filesystem),\n\nReview comment:\n       Not sure but `interface` -> `API`?\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n+partitioning scheme:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 256-279\n+   :emphasize-lines: 7,9-11\n+   :linenos:\n+   :lineno-match:\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. code-block:: text\n+\n+   $ ./debug/dataset_documentation_example file:///tmp parquet_hive partitioned\n+   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet\n+   Partition expression: (part == \"a\")\n+   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet\n+   Partition expression: (part == \"b\")\n+   Read 20 rows\n+   a: int64\n+     -- field metadata --\n+     PARQUET:field_id: '1'\n+   b: double\n+     -- field metadata --\n+     PARQUET:field_id: '2'\n+   c: int64\n+     -- field metadata --\n+     PARQUET:field_id: '3'\n+   part: string\n+   ----\n+   # snip...\n+\n+We can now filter on the partition keys, which avoids loading files\n+altogether if they do not match the predicate:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 281-301\n+   :emphasize-lines: 15-18\n+   :linenos:\n+   :lineno-match:\n+\n+Different partitioning schemes\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above example uses a hive-like directory scheme, such as \"/year=2009/month=11/day=15\".\n+We specified this by passing the partition *factory*. In this case, the types of\n+the partition keys are inferred from the file paths.\n+\n+It is also possible directly construct the partitioning and explicitly define\n+the schema of the partition keys. For example:\n+\n+.. code-block:: cpp\n+\n+    auto part = std::make_shared<ds::HivePartitioning>(arrow::schema({\n+        arrow::field(\"year\", arrow::int16()),\n+        arrow::field(\"month\", arrow::int8()),\n+        arrow::field(\"day\", arrow::int32())\n+    }));\n+\n+\"Directory partitioning\" is also supported, where the segments in the file path\n+represent the values of the partition keys without including the name (the\n+field name are implicit in the segment's index). For example, given field names\n\nReview comment:\n       `name` -> `names` or `name are` -> `name is`.  I think both work.\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n+\n+  auto format = std::make_shared<ds::ParquetFileFormat>();\n+  format->reader_options.dict_columns.insert(\"a\");\n+\n+Will configure column ``\"a\"`` to be dictionary-encoded when read.\n+\n+.. _cpp-dataset-filtering-data:\n+\n+Filtering data\n+--------------\n+\n+So far, we've read the entire dataset, but this can be inefficient or use too\n+much memory. The :class:`arrow::dataset::Scanner` offers control over what data\n+to read.\n+\n+:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified\n+columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 16\n+   :linenos:\n+   :lineno-match:\n+\n+For formats which support pushdown (such as Parquet), this means that only the\n+specified columns will be read from the filesystem, saving I/O costs.\n+\n+A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,\n+so that rows which do not match the filter predicate will not be included in\n+the returned table.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 172-191\n+   :emphasize-lines: 17\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO Expressions not documented pending renamespacing\n+\n+Projecting columns\n+------------------\n+\n+In addition to selecting columns,\n+:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more\n+complex projections in combination with expressions.\n+\n+In this case, we pass a vector of expressions used to construct column values,\n+and a vector of names for the columns:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 193-223\n+   :emphasize-lines: 18-28\n+   :linenos:\n+   :lineno-match:\n+\n+This also determines the column selection; only the given columns will be\n+present in the resulting table. If you want to include a derived column in\n+*addition* to the existing columns, you can build up the expressions from the\n+dataset schema:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 225-254\n+   :emphasize-lines: 17-27\n+   :linenos:\n+   :lineno-match:\n+\n+Reading and writing partitioned data\n+------------------------------------\n+\n+So far, we've been working with datasets consisting of flat directories with\n+files. However, a dataset can exploit nested directory structures defining a\n+partitioned dataset, where sub-directory names hold information about which\n+subset of the data is stored in that directory.\n+\n+For example, a dataset partitioned by year and month may look like on disk:\n+\n+.. code-block:: text\n+\n+   dataset_name/\n+     year=2007/\n+       month=01/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=02/\n+          data0.parquet\n+          data1.parquet\n+          ...\n+       month=03/\n+       ...\n+     year=2008/\n+       month=01/\n+       ...\n+     ...\n+\n+The above partitioning scheme is using \"/key=value/\" directory names, as found\n+in Apache Hive.\n+\n+Let's create a small partitioned dataset. For this, we'll use Dataset's writing\n+functionality.\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 106-149\n+   :emphasize-lines: 25-42\n+   :linenos:\n+   :lineno-match:\n+\n+The above created a directory with two subdirectories (\"part=a\" and \"part=b\"),\n+and the Parquet files written in those directories no longer include the \"part\"\n+column.\n+\n+Reading this dataset, we now specify that the dataset uses a Hive-like\n+partitioning scheme:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 256-279\n+   :emphasize-lines: 7,9-11\n+   :linenos:\n+   :lineno-match:\n+\n+Although the partition fields are not included in the actual Parquet files,\n+they will be added back to the resulting table when scanning this dataset:\n+\n+.. code-block:: text\n+\n+   $ ./debug/dataset_documentation_example file:///tmp parquet_hive partitioned\n+   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet\n+   Partition expression: (part == \"a\")\n+   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet\n+   Partition expression: (part == \"b\")\n+   Read 20 rows\n+   a: int64\n+     -- field metadata --\n+     PARQUET:field_id: '1'\n+   b: double\n+     -- field metadata --\n+     PARQUET:field_id: '2'\n+   c: int64\n+     -- field metadata --\n+     PARQUET:field_id: '3'\n+   part: string\n+   ----\n+   # snip...\n+\n+We can now filter on the partition keys, which avoids loading files\n+altogether if they do not match the predicate:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 281-301\n+   :emphasize-lines: 15-18\n+   :linenos:\n+   :lineno-match:\n+\n+Different partitioning schemes\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above example uses a hive-like directory scheme, such as \"/year=2009/month=11/day=15\".\n+We specified this by passing the partition *factory*. In this case, the types of\n+the partition keys are inferred from the file paths.\n+\n+It is also possible directly construct the partitioning and explicitly define\n+the schema of the partition keys. For example:\n+\n+.. code-block:: cpp\n+\n+    auto part = std::make_shared<ds::HivePartitioning>(arrow::schema({\n+        arrow::field(\"year\", arrow::int16()),\n+        arrow::field(\"month\", arrow::int8()),\n+        arrow::field(\"day\", arrow::int32())\n+    }));\n+\n+\"Directory partitioning\" is also supported, where the segments in the file path\n+represent the values of the partition keys without including the name (the\n+field name are implicit in the segment's index). For example, given field names\n+\"year\", \"month\", and \"day\", one path might be \"/2019/11/15\".\n+\n+Since the names are not included in the file paths, these must be specified\n+when constructing a directory partitioning:\n+\n+.. code-block:: cpp\n+\n+    auto part = ds::DirectoryPartitioning::MakeFactory({\"year\", \"month\", \"day\"});\n+\n+Directory partitioning also supports providing a full schema rather than inferring\n+types from file paths.\n+\n+Reading from other data sources\n+-------------------------------\n+\n+Reading in-memory data\n+~~~~~~~~~~~~~~~~~~~~~~\n+\n+If you already have data in memory that you'd like to use with the Datasets\n+interface (e.g. to filter/project data, or to write it out to a filesystem),\n+you can wrap it in a :class:`arrow::dataset::InMemoryDataset`:\n+\n+.. code-block:: cpp\n+\n+   auto table = arrow::Table::FromRecordBatches(...);\n+   auto dataset = std::make_shared<arrow::dataset::InMemoryDataset>(std::move(table));\n+   // Scan the dataset, filter, it, etc.\n+   auto scanner_builder = dataset->NewScan();\n+\n+In the example, we used the InMemoryDataset to write our example data to local\n+disk to be used in the rest of the example:\n\nReview comment:\n       `to be used` -> `which was used`\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-08T23:50:17.889+0000",
                    "updated": "2021-04-08T23:50:17.889+0000",
                    "started": "2021-04-08T23:50:17.889+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579644",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579651",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#issuecomment-816312147\n\n\n   Thanks for the comments. I'll go through and make sure changes are consistent between Python/C++ (the C++ side of the docs was based on the Python one).\r\n   \r\n   FWIW, the double-colon is an reST thing: https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html#literal-blocks\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-09T00:07:32.026+0000",
                    "updated": "2021-04-09T00:07:32.026+0000",
                    "started": "2021-04-09T00:07:32.026+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579651",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579700",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r610307942\n\n\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n\nReview comment:\n       I don't think this is necessary\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-09T02:55:53.883+0000",
                    "updated": "2021-04-09T02:55:53.883+0000",
                    "started": "2021-04-09T02:55:53.883+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579700",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579712",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r610317754\n\n\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n\nReview comment:\n       Actually, looking at the below section, maybe lift the first bullet point so that the datasets library isn't defined as a library for datasets? This suggestion includes @westonpace 's next few comments and the following bullet points:\r\n   \r\n   ```\r\n   The Arrow Datasets library provides a unified interface for operations on tabular data from various sources, in various formats. The data may be from buffers, local filesystems, and cloud filesystems; and may be larger than memory or span multiple units (e.g. multiple files). Some supported data formats include parquet, feather, and IPC. Some supported operations include discovery, predicate pushdowns, and read parallelism.\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-09T03:24:40.752+0000",
                    "updated": "2021-04-09T03:24:40.752+0000",
                    "started": "2021-04-09T03:24:40.752+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579712",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579713",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r610317754\n\n\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n\nReview comment:\n       Actually, looking at the below section, maybe lift the first bullet point so that the datasets library isn't defined as a library for datasets? This suggestion includes @westonpace 's next few comments and the following bullet points:\r\n   \r\n   ```\r\n   The Arrow Datasets library provides a unified interface for\r\n   operations on tabular data from various sources, in various\r\n   formats. The data may be from buffers, local filesystems,\r\n   and cloud filesystems; and may be larger than memory or span\r\n   multiple units (e.g. multiple files). Some supported data formats\r\n   include parquet, feather, and IPC. Some supported operations\r\n   include discovery, predicate pushdowns, and read parallelism.\r\n   ```\r\n   \r\n   (newlines added based on formatting for this comment)\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-09T03:26:34.217+0000",
                    "updated": "2021-04-09T03:26:34.217+0000",
                    "started": "2021-04-09T03:26:34.216+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579713",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579714",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r610318529\n\n\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n\nReview comment:\n       Change `supported` -> `implemented`\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-09T03:27:27.766+0000",
                    "updated": "2021-04-09T03:27:27.766+0000",
                    "started": "2021-04-09T03:27:27.766+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579714",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579715",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r610318957\n\n\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n\nReview comment:\n       also, if this is kept, then maybe the reference to data formats above should just be removed to reduce unnecessary repetition\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-09T03:28:43.169+0000",
                    "updated": "2021-04-09T03:28:43.169+0000",
                    "started": "2021-04-09T03:28:43.168+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579715",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579722",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r610322157\n\n\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n\nReview comment:\n       I think it is actually correct here (https://www.grammarly.com/blog/comma-before-but/)\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-09T03:40:52.951+0000",
                    "updated": "2021-04-09T03:40:52.951+0000",
                    "started": "2021-04-09T03:40:52.951+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579722",
                    "issueId": "13359118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/worklog/579728",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drin commented on a change in pull request #9810:\nURL: https://github.com/apache/arrow/pull/9810#discussion_r610324994\n\n\n\n##########\nFile path: docs/source/cpp/dataset.rst\n##########\n@@ -0,0 +1,381 @@\n+.. Licensed to the Apache Software Foundation (ASF) under one\n+.. or more contributor license agreements.  See the NOTICE file\n+.. distributed with this work for additional information\n+.. regarding copyright ownership.  The ASF licenses this file\n+.. to you under the Apache License, Version 2.0 (the\n+.. \"License\"); you may not use this file except in compliance\n+.. with the License.  You may obtain a copy of the License at\n+\n+..   http://www.apache.org/licenses/LICENSE-2.0\n+\n+.. Unless required by applicable law or agreed to in writing,\n+.. software distributed under the License is distributed on an\n+.. \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+.. KIND, either express or implied.  See the License for the\n+.. specific language governing permissions and limitations\n+.. under the License.\n+\n+.. default-domain:: cpp\n+.. highlight:: cpp\n+\n+================\n+Tabular Datasets\n+================\n+\n+.. seealso::\n+   :doc:`Dataset API reference <api/dataset>`\n+\n+.. warning::\n+\n+    The ``arrow::dataset`` namespace is experimental, and a stable API\n+    is not yet guaranteed.\n+\n+The Arrow Datasets library provides functionality to efficiently work with\n+tabular, potentially larger than memory and multi-file datasets:\n+\n+* A unified interface for different sources: supporting different sources and\n+  file formats (Parquet, Feather files) and different file systems (local,\n+  cloud).\n+* Discovery of sources (crawling directories, handle directory-based partitioned\n+  datasets, basic schema normalization, ..)\n+* Optimized reading with predicate pushdown (filtering rows), projection\n+  (selecting columns), parallel reading, or fine-grained managing of tasks.\n+\n+Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The\n+goal is to expand this in the future to other file formats and data sources\n+(e.g.  database connections).\n+\n+Reading Datasets\n+----------------\n+\n+For the examples below, let's create a small dataset consisting\n+of a directory with two parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 50-85\n+   :linenos:\n+   :lineno-match:\n+\n+(See the full example at bottom: :ref:`cpp-dataset-full-example`.)\n+\n+Dataset discovery\n+~~~~~~~~~~~~~~~~~\n+\n+A :class:`arrow::dataset::Dataset` object can be created using the various\n+:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the\n+:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset\n+given the base directory path:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 6-11\n+   :linenos:\n+   :lineno-match:\n+\n+Here, we're also passing the filesystem to use and the file format to\n+read. This lets us choose between things like reading local files or files in\n+Amazon S3, or between Parquet and CSV.\n+\n+In addition to a base directory path, we can list file paths manually.\n+\n+Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into\n+memory; it only crawls the directory to find all the files\n+(:func:`arrow::dataset::FileSystemDataset::files`):\n+\n+.. code-block:: cpp\n+\n+   for (const auto& filename : dataset->files()) {\n+     std::cout << filename << std::endl;\n+   }\n+\n+\u2026and infers the dataset's schema (by default from the first file):\n+\n+.. code-block:: cpp\n+\n+   std::cout << dataset->schema()->ToString() << std::endl;\n+\n+Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a\n+:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into\n+a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`\n+method:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 151-170\n+   :emphasize-lines: 16-19\n+   :linenos:\n+   :lineno-match:\n+\n+.. TODO: iterative loading not documented pending API changes\n+.. note:: Depending on the size of your dataset, this can require a lot of\n+          memory; see :ref:`cpp-dataset-filtering-data` below on\n+          filtering/projecting.\n+\n+Reading different file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples use Parquet files on local disk, but the Dataset API\n+provides a consistent interface across multiple file formats and sources.\n+Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;\n+more formats are planned in the future.\n+\n+If we save the table as a Feather file instead of Parquet files:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 87-104\n+   :linenos:\n+   :lineno-match:\n+\n+then we can read the Feather file using the same functions, but passing a\n+:class:`arrow::dataset::IpcFileFormat`:\n+\n+.. literalinclude:: ../../../cpp/examples/arrow/dataset_documentation_example.cc\n+   :language: cpp\n+   :lines: 310,323\n+   :linenos:\n+\n+Customizing file formats\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+:class:`arrow::dataset::FileFormat` objects have properties that control how\n+they are read. For example::\n\nReview comment:\n       I also think that since the member variable is `reader_options`, I'd prefer to see:\r\n   \r\n   `FileFormat objects have options that control read behavior`\r\n   \r\n   ...but only ParquetFileFormat has `reader_options`? Is this called something different in each `FileFormat` class?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-04-09T03:51:26.706+0000",
                    "updated": "2021-04-09T03:51:26.706+0000",
                    "started": "2021-04-09T03:51:26.706+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "579728",
                    "issueId": "13359118"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 19800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@2b8dfc24[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7839a801[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5b630278[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@6e2fc0b5[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1986c338[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@5b84f908[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@183740c2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@88d1119[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@273d8c33[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@5b0d7d65[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@791e1796[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@197bc475[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 19800,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Apr 14 13:30:46 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-04-14T13:30:45.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11677/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2021-02-17T17:42:30.000+0000",
        "updated": "2021-04-14T16:07:22.000+0000",
        "timeoriginalestimate": null,
        "description": "The dataset component is currently undocumented. Documentation should be added in two parts:\r\n* a page in the User Guide\r\n* a page in the API reference\r\n",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "5.5h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 19800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Dataset] Write documentation",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/comment/17286021",
                    "id": "17286021",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "cc [~jonkeane]\u00a0[~octalene]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2021-02-17T17:42:56.709+0000",
                    "updated": "2021-02-17T17:42:56.709+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/comment/17286826",
                    "id": "17286826",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=octalene",
                        "name": "octalene",
                        "key": "octalene",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=octalene&avatarId=51083",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=octalene&avatarId=51083",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=octalene&avatarId=51083",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=octalene&avatarId=51083"
                        },
                        "displayName": "Aldrin Montana",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Thank you :)",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=octalene",
                        "name": "octalene",
                        "key": "octalene",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=octalene&avatarId=51083",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=octalene&avatarId=51083",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=octalene&avatarId=51083",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=octalene&avatarId=51083"
                        },
                        "displayName": "Aldrin Montana",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2021-02-19T02:44:47.030+0000",
                    "updated": "2021-02-19T02:44:47.030+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/comment/17299614",
                    "id": "17299614",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "cc [~bkietz]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2021-03-11T14:58:57.409+0000",
                    "updated": "2021-03-11T14:58:57.409+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13359118/comment/17321003",
                    "id": "17321003",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 9810\n[https://github.com/apache/arrow/pull/9810]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2021-04-14T13:30:46.017+0000",
                    "updated": "2021-04-14T13:30:46.017+0000"
                }
            ],
            "maxResults": 4,
            "total": 4,
            "startAt": 0
        },
        "customfield_12311820": "0|z0nrgw:",
        "customfield_12314139": null
    }
}