{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13170656",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13170656",
    "key": "ARROW-2807",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12343858",
                "id": "12343858",
                "description": "",
                "name": "0.12.0",
                "archived": false,
                "released": true,
                "releaseDate": "2019-01-20"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "parquet",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 2400,
            "total": 2400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 2400,
            "total": 2400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-2807/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 4,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13170656/worklog/165634",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm opened a new pull request #2954: ARROW-2807: [Python] [Parquet] Add memory_map= option to parquet.read_table, read_pandas, read_schema\nURL: https://github.com/apache/arrow/pull/2954\n \n \n   The default for `memory_map` remains True for now. \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-11-13T20:40:47.131+0000",
                    "updated": "2018-11-13T20:40:47.131+0000",
                    "started": "2018-11-13T20:40:47.130+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "165634",
                    "issueId": "13170656"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13170656/worklog/165678",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io commented on issue #2954: ARROW-2807: [Python] [Parquet] Add memory_map= option to parquet.read_table, read_pandas, read_schema\nURL: https://github.com/apache/arrow/pull/2954#issuecomment-438468932\n \n \n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/2954?src=pr&el=h1) Report\n   > Merging [#2954](https://codecov.io/gh/apache/arrow/pull/2954?src=pr&el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/1ef6c2644b654fa77c49cc20bb9d8fc66d3f0c4f?src=pr&el=desc) will **increase** coverage by `0.86%`.\n   > The diff coverage is `92.85%`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/2954/graphs/tree.svg?width=650&token=LpTCFbqVT1&height=150&src=pr)](https://codecov.io/gh/apache/arrow/pull/2954?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #2954      +/-   ##\n   ==========================================\n   + Coverage   86.51%   87.38%   +0.86%     \n   ==========================================\n     Files         490      422      -68     \n     Lines       68879    63679    -5200     \n   ==========================================\n   - Hits        59590    55643    -3947     \n   + Misses       9201     8036    -1165     \n   + Partials       88        0      -88\n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/2954?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [python/pyarrow/parquet.py](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvcGFycXVldC5weQ==) | `93.69% <100%> (\u00f8)` | :arrow_up: |\n   | [python/pyarrow/tests/test\\_parquet.py](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvdGVzdHMvdGVzdF9wYXJxdWV0LnB5) | `97.36% <90.9%> (-0.15%)` | :arrow_down: |\n   | [cpp/src/arrow/csv/column-builder.cc](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9jc3YvY29sdW1uLWJ1aWxkZXIuY2M=) | `95.1% <0%> (-2.1%)` | :arrow_down: |\n   | [rust/src/record\\_batch.rs](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-cnVzdC9zcmMvcmVjb3JkX2JhdGNoLnJz) | | |\n   | [go/arrow/array/table.go](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-Z28vYXJyb3cvYXJyYXkvdGFibGUuZ28=) | | |\n   | [rust/src/array.rs](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-cnVzdC9zcmMvYXJyYXkucnM=) | | |\n   | [go/arrow/math/uint64\\_amd64.go](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-Z28vYXJyb3cvbWF0aC91aW50NjRfYW1kNjQuZ28=) | | |\n   | [go/arrow/internal/testing/tools/bool.go](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-Z28vYXJyb3cvaW50ZXJuYWwvdGVzdGluZy90b29scy9ib29sLmdv) | | |\n   | [go/arrow/array/bufferbuilder.go](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-Z28vYXJyb3cvYXJyYXkvYnVmZmVyYnVpbGRlci5nbw==) | | |\n   | [go/arrow/internal/bitutil/bitutil.go](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree#diff-Z28vYXJyb3cvaW50ZXJuYWwvYml0dXRpbC9iaXR1dGlsLmdv) | | |\n   | ... and [61 more](https://codecov.io/gh/apache/arrow/pull/2954/diff?src=pr&el=tree-more) | |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2954?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/2954?src=pr&el=footer). Last update [1ef6c26...7ec33db](https://codecov.io/gh/apache/arrow/pull/2954?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-11-13T22:51:28.183+0000",
                    "updated": "2018-11-13T22:51:28.183+0000",
                    "started": "2018-11-13T22:51:28.182+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "165678",
                    "issueId": "13170656"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13170656/worklog/165931",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2954: ARROW-2807: [Python] [Parquet] Add memory_map= option to parquet.read_table, read_pandas, read_schema\nURL: https://github.com/apache/arrow/pull/2954#issuecomment-438710295\n \n \n   +1\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-11-14T15:48:45.823+0000",
                    "updated": "2018-11-14T15:48:45.823+0000",
                    "started": "2018-11-14T15:48:45.823+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "165931",
                    "issueId": "13170656"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13170656/worklog/165933",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm closed pull request #2954: ARROW-2807: [Python] [Parquet] Add memory_map= option to parquet.read_table, read_pandas, read_schema\nURL: https://github.com/apache/arrow/pull/2954\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/python/pyarrow/parquet.py b/python/pyarrow/parquet.py\nindex bb11511a16..a9e498d3ff 100644\n--- a/python/pyarrow/parquet.py\n+++ b/python/pyarrow/parquet.py\n@@ -95,11 +95,14 @@ class ParquetFile(object):\n     common_metadata : ParquetFileMetadata, default None\n         Will be used in reads for pandas schema metadata if not found in the\n         main file's metadata, no other uses at the moment\n+    memory_map : boolean, default True\n+        If the source is a file path, use a memory map to read file, which can\n+        improve performance in some environments\n     \"\"\"\n-\n-    def __init__(self, source, metadata=None, common_metadata=None):\n+    def __init__(self, source, metadata=None, common_metadata=None,\n+                 memory_map=True):\n         self.reader = ParquetReader()\n-        self.reader.open(source, metadata=metadata)\n+        self.reader.open(source, use_memory_map=memory_map, metadata=metadata)\n         self.common_metadata = common_metadata\n         self._nested_paths_by_prefix = self._build_nested_paths()\n \n@@ -1056,6 +1059,9 @@ def _make_manifest(path_or_paths, fs, pathsep='/', metadata_nthreads=1):\n     Perform multi-threaded column reads\n metadata : FileMetaData\n     If separately computed\n+memory_map : boolean, default True\n+    If the source is a file path, use a memory map to read file, which can\n+    improve performance in some environments\n {1}\n \n Returns\n@@ -1065,7 +1071,8 @@ def _make_manifest(path_or_paths, fs, pathsep='/', metadata_nthreads=1):\n \n \n def read_table(source, columns=None, use_threads=True, metadata=None,\n-               use_pandas_metadata=False, nthreads=None):\n+               use_pandas_metadata=False, memory_map=True,\n+               nthreads=None):\n     use_threads = _deprecate_nthreads(use_threads, nthreads)\n     if _is_path_like(source):\n         fs = _get_fs_from_path(source)\n@@ -1088,10 +1095,11 @@ def read_table(source, columns=None, use_threads=True, metadata=None,\n \n \n def read_pandas(source, columns=None, use_threads=True,\n-                nthreads=None, metadata=None):\n+                memory_map=True, nthreads=None, metadata=None):\n     return read_table(source, columns=columns,\n                       use_threads=use_threads,\n-                      metadata=metadata, use_pandas_metadata=True)\n+                      metadata=metadata, memory_map=True,\n+                      use_pandas_metadata=True)\n \n \n read_pandas.__doc__ = _read_table_docstring.format(\n@@ -1258,31 +1266,35 @@ def write_metadata(schema, where, version='1.0',\n     writer.close()\n \n \n-def read_metadata(where):\n+def read_metadata(where, memory_map=False):\n     \"\"\"\n     Read FileMetadata from footer of a single Parquet file\n \n     Parameters\n     ----------\n     where : string (filepath) or file-like object\n+    memory_map : boolean, default False\n+        Create memory map when the source is a file path\n \n     Returns\n     -------\n     metadata : FileMetadata\n     \"\"\"\n-    return ParquetFile(where).metadata\n+    return ParquetFile(where, memory_map=memory_map).metadata\n \n \n-def read_schema(where):\n+def read_schema(where, memory_map=False):\n     \"\"\"\n     Read effective Arrow schema from Parquet file metadata\n \n     Parameters\n     ----------\n     where : string (filepath) or file-like object\n+    memory_map : boolean, default False\n+        Create memory map when the source is a file path\n \n     Returns\n     -------\n     schema : pyarrow.Schema\n     \"\"\"\n-    return ParquetFile(where).schema.to_arrow_schema()\n+    return ParquetFile(where, memory_map=memory_map).schema.to_arrow_schema()\ndiff --git a/python/pyarrow/tests/test_parquet.py b/python/pyarrow/tests/test_parquet.py\nindex 78677a0a56..bacffdfbd2 100644\n--- a/python/pyarrow/tests/test_parquet.py\n+++ b/python/pyarrow/tests/test_parquet.py\n@@ -32,6 +32,11 @@\n from pyarrow.filesystem import LocalFileSystem\n from .pandas_examples import dataframe_with_arrays, dataframe_with_lists\n \n+try:\n+    import pyarrow.parquet as pq\n+except ImportError:\n+    pq = None\n+\n \n # Marks all of the tests in this module\n # Ignore these with pytest ... -m 'not parquet'\n@@ -44,6 +49,7 @@ def datadir(datadir):\n \n \n def _write_table(table, path, **kwargs):\n+    # So we see the ImportError somewhere\n     import pyarrow.parquet as pq\n \n     if isinstance(table, pd.DataFrame):\n@@ -54,27 +60,32 @@ def _write_table(table, path, **kwargs):\n \n \n def _read_table(*args, **kwargs):\n-    import pyarrow.parquet as pq\n-\n     return pq.read_table(*args, **kwargs)\n \n \n-def _roundtrip_table(table, **params):\n+def _roundtrip_table(table, read_table_kwargs=None,\n+                     **params):\n+    read_table_kwargs = read_table_kwargs or {}\n+\n     buf = io.BytesIO()\n     _write_table(table, buf, **params)\n     buf.seek(0)\n-\n-    return _read_table(buf)\n+    return _read_table(buf, **read_table_kwargs)\n \n \n-def _check_roundtrip(table, expected=None, **params):\n+def _check_roundtrip(table, expected=None, read_table_kwargs=None,\n+                     **params):\n     if expected is None:\n         expected = table\n \n+    read_table_kwargs = read_table_kwargs or {}\n+\n     # intentionally check twice\n-    result = _roundtrip_table(table, **params)\n+    result = _roundtrip_table(table, read_table_kwargs=read_table_kwargs,\n+                              **params)\n     assert result.equals(expected)\n-    result = _roundtrip_table(result, **params)\n+    result = _roundtrip_table(result, read_table_kwargs=read_table_kwargs,\n+                              **params)\n     assert result.equals(expected)\n \n \n@@ -136,8 +147,6 @@ def alltypes_sample(size=10000, seed=0, categorical=False):\n \n @pytest.mark.parametrize('chunk_size', [None, 1000])\n def test_pandas_parquet_2_0_rountrip(tempdir, chunk_size):\n-    import pyarrow.parquet as pq\n-\n     df = alltypes_sample(size=10000, categorical=True)\n \n     filename = tempdir / 'pandas_rountrip.parquet'\n@@ -172,6 +181,22 @@ def test_chunked_table_write():\n     _check_roundtrip(table, version='2.0')\n \n \n+def test_no_memory_map(tempdir):\n+    df = alltypes_sample(size=10)\n+    # The nanosecond->us conversion is a nuisance, so we just avoid it here\n+    del df['datetime']\n+\n+    table = pa.Table.from_pandas(df)\n+    _check_roundtrip(table, read_table_kwargs={'memory_map': False},\n+                     version='2.0')\n+\n+    filename = str(tempdir / 'tmp_file')\n+    with open(filename, 'wb') as f:\n+        _write_table(table, f, version='2.0')\n+    table_read = pq.read_pandas(filename, memory_map=False)\n+    assert table_read.equals(table)\n+\n+\n def test_empty_table_roundtrip():\n     df = alltypes_sample(size=10)\n     # The nanosecond->us conversion is a nuisance, so we just avoid it here\n@@ -196,8 +221,6 @@ def test_empty_lists_table_roundtrip():\n \n \n def test_pandas_parquet_datetime_tz():\n-    import pyarrow.parquet as pq\n-\n     s = pd.Series([datetime.datetime(2017, 9, 6)])\n     s = s.dt.tz_localize('utc')\n \n@@ -222,8 +245,6 @@ def test_pandas_parquet_datetime_tz():\n \n \n def test_pandas_parquet_custom_metadata(tempdir):\n-    import pyarrow.parquet as pq\n-\n     df = alltypes_sample(size=10000)\n \n     filename = tempdir / 'pandas_rountrip.parquet'\n@@ -240,8 +261,6 @@ def test_pandas_parquet_custom_metadata(tempdir):\n \n \n def test_pandas_parquet_column_multiindex(tempdir):\n-    import pyarrow.parquet as pq\n-\n     df = alltypes_sample(size=10)\n     df.columns = pd.MultiIndex.from_tuples(\n         list(zip(df.columns, df.columns[::-1])),\n@@ -260,8 +279,6 @@ def test_pandas_parquet_column_multiindex(tempdir):\n \n \n def test_pandas_parquet_2_0_rountrip_read_pandas_no_index_written(tempdir):\n-    import pyarrow.parquet as pq\n-\n     df = alltypes_sample(size=10000)\n \n     filename = tempdir / 'pandas_rountrip.parquet'\n@@ -393,8 +410,6 @@ def test_pandas_parquet_native_file_roundtrip(tempdir):\n \n \n def test_parquet_incremental_file_build(tempdir):\n-    import pyarrow.parquet as pq\n-\n     df = _test_dataframe(100)\n     df['unique_id'] = 0\n \n@@ -421,8 +436,6 @@ def test_parquet_incremental_file_build(tempdir):\n \n \n def test_read_pandas_column_subset(tempdir):\n-    import pyarrow.parquet as pq\n-\n     df = _test_dataframe(10000)\n     arrow_table = pa.Table.from_pandas(df)\n     imos = pa.BufferOutputStream()\n@@ -502,8 +515,6 @@ def test_pandas_parquet_configuration_options(tempdir):\n \n \n def make_sample_file(table_or_df):\n-    import pyarrow.parquet as pq\n-\n     if isinstance(table_or_df, pa.Table):\n         a_table = table_or_df\n     else:\n@@ -518,8 +529,6 @@ def make_sample_file(table_or_df):\n \n \n def test_parquet_metadata_api():\n-    import pyarrow.parquet as pq\n-\n     df = alltypes_sample(size=10000)\n     df = df.reindex(columns=sorted(df.columns))\n \n@@ -664,8 +673,6 @@ def test_parquet_column_statistics_api(data, type, physical_type, min_value,\n \n \n def test_compare_schemas():\n-    import pyarrow.parquet as pq\n-\n     df = alltypes_sample(size=10000)\n \n     fileh = make_sample_file(df)\n@@ -693,8 +700,6 @@ def test_compare_schemas():\n \n def test_validate_schema_write_table(tempdir):\n     # ARROW-2926\n-    import pyarrow.parquet as pq\n-\n     simple_fields = [\n         pa.field('POS', pa.uint32()),\n         pa.field('desc', pa.string())\n@@ -984,8 +989,6 @@ def test_min_chunksize():\n \n \n def test_pass_separate_metadata():\n-    import pyarrow.parquet as pq\n-\n     # ARROW-471\n     df = alltypes_sample(size=10000)\n \n@@ -1005,8 +1008,6 @@ def test_pass_separate_metadata():\n \n \n def test_read_single_row_group():\n-    import pyarrow.parquet as pq\n-\n     # ARROW-471\n     N, K = 10000, 4\n     df = alltypes_sample(size=N)\n@@ -1029,8 +1030,6 @@ def test_read_single_row_group():\n \n \n def test_read_single_row_group_with_column_subset():\n-    import pyarrow.parquet as pq\n-\n     N, K = 10000, 4\n     df = alltypes_sample(size=N)\n     a_table = pa.Table.from_pandas(df)\n@@ -1049,8 +1048,6 @@ def test_read_single_row_group_with_column_subset():\n \n \n def test_scan_contents():\n-    import pyarrow.parquet as pq\n-\n     N, K = 10000, 4\n     df = alltypes_sample(size=N)\n     a_table = pa.Table.from_pandas(df)\n@@ -1067,8 +1064,6 @@ def test_scan_contents():\n \n \n def test_parquet_piece_read(tempdir):\n-    import pyarrow.parquet as pq\n-\n     df = _test_dataframe(1000)\n     table = pa.Table.from_pandas(df)\n \n@@ -1082,8 +1077,6 @@ def test_parquet_piece_read(tempdir):\n \n \n def test_parquet_piece_basics():\n-    import pyarrow.parquet as pq\n-\n     path = '/baz.parq'\n \n     piece1 = pq.ParquetDatasetPiece(path)\n@@ -1102,8 +1095,6 @@ def test_parquet_piece_basics():\n \n \n def test_partition_set_dictionary_type():\n-    import pyarrow.parquet as pq\n-\n     set1 = pq.PartitionSet('key1', [u('foo'), u('bar'), u('baz')])\n     set2 = pq.PartitionSet('key2', [2007, 2008, 2009])\n \n@@ -1121,8 +1112,6 @@ def test_read_partitioned_directory(tempdir):\n \n \n def test_create_parquet_dataset_multi_threaded(tempdir):\n-    import pyarrow.parquet as pq\n-\n     fs = LocalFileSystem.get_instance()\n     base_path = tempdir\n \n@@ -1139,8 +1128,6 @@ def test_create_parquet_dataset_multi_threaded(tempdir):\n \n \n def test_equivalency(tempdir):\n-    import pyarrow.parquet as pq\n-\n     fs = LocalFileSystem.get_instance()\n     base_path = tempdir\n \n@@ -1216,8 +1203,6 @@ def test_equivalency(tempdir):\n \n \n def test_cutoff_exclusive_integer(tempdir):\n-    import pyarrow.parquet as pq\n-\n     fs = LocalFileSystem.get_instance()\n     base_path = tempdir\n \n@@ -1255,8 +1240,6 @@ def test_cutoff_exclusive_integer(tempdir):\n     reason='Loss of type information in creation of categoricals.'\n )\n def test_cutoff_exclusive_datetime(tempdir):\n-    import pyarrow.parquet as pq\n-\n     fs = LocalFileSystem.get_instance()\n     base_path = tempdir\n \n@@ -1299,8 +1282,6 @@ def test_cutoff_exclusive_datetime(tempdir):\n \n \n def test_inclusive_integer(tempdir):\n-    import pyarrow.parquet as pq\n-\n     fs = LocalFileSystem.get_instance()\n     base_path = tempdir\n \n@@ -1334,8 +1315,6 @@ def test_inclusive_integer(tempdir):\n \n \n def test_inclusive_set(tempdir):\n-    import pyarrow.parquet as pq\n-\n     fs = LocalFileSystem.get_instance()\n     base_path = tempdir\n \n@@ -1371,8 +1350,6 @@ def test_inclusive_set(tempdir):\n \n \n def test_invalid_pred_op(tempdir):\n-    import pyarrow.parquet as pq\n-\n     fs = LocalFileSystem.get_instance()\n     base_path = tempdir\n \n@@ -1430,7 +1407,6 @@ def s3_example():\n \n @pytest.mark.s3\n def test_read_partitioned_directory_s3fs(s3_example):\n-    import pyarrow.parquet as pq\n     from pyarrow.filesystem import S3FSWrapper\n \n     fs, bucket_uri = s3_example\n@@ -1443,8 +1419,6 @@ def test_read_partitioned_directory_s3fs(s3_example):\n \n \n def _partition_test_for_filesystem(fs, base_path):\n-    import pyarrow.parquet as pq\n-\n     foo_keys = [0, 1]\n     bar_keys = ['a', 'b', 'c']\n     partition_spec = [\n@@ -1512,8 +1486,6 @@ def _visit_level(base_dir, level, part_keys):\n \n \n def _test_read_common_metadata_files(fs, base_path):\n-    import pyarrow.parquet as pq\n-\n     N = 100\n     df = pd.DataFrame({\n         'index': np.arange(N),\n@@ -1550,8 +1522,6 @@ def test_read_common_metadata_files(tempdir):\n \n \n def test_read_metadata_files(tempdir):\n-    import pyarrow.parquet as pq\n-\n     fs = LocalFileSystem.get_instance()\n \n     N = 100\n@@ -1580,8 +1550,6 @@ def test_read_metadata_files(tempdir):\n \n \n def test_read_schema(tempdir):\n-    import pyarrow.parquet as pq\n-\n     N = 100\n     df = pd.DataFrame({\n         'index': np.arange(N),\n@@ -1594,6 +1562,7 @@ def test_read_schema(tempdir):\n     _write_table(table, data_path)\n \n     assert table.schema.equals(pq.read_schema(data_path))\n+    assert table.schema.equals(pq.read_schema(data_path, memory_map=True))\n \n \n def _filter_partition(df, part_keys):\n@@ -1613,8 +1582,6 @@ def _filter_partition(df, part_keys):\n \n \n def test_read_multiple_files(tempdir):\n-    import pyarrow.parquet as pq\n-\n     nfiles = 10\n     size = 5\n \n@@ -1694,8 +1661,6 @@ def read_multiple_files(paths, columns=None, use_threads=True, **kwargs):\n \n \n def test_dataset_read_pandas(tempdir):\n-    import pyarrow.parquet as pq\n-\n     nfiles = 5\n     size = 5\n \n@@ -1728,8 +1693,6 @@ def test_dataset_read_pandas(tempdir):\n \n @pytest.mark.parametrize('preserve_index', [True, False])\n def test_dataset_read_pandas_common_metadata(tempdir, preserve_index):\n-    import pyarrow.parquet as pq\n-\n     # ARROW-1103\n     nfiles = 5\n     size = 5\n@@ -1784,8 +1747,6 @@ def _make_example_multifile_dataset(base_path, nfiles=10, file_nrows=5):\n \n \n def test_ignore_private_directories(tempdir):\n-    import pyarrow.parquet as pq\n-\n     dirpath = tempdir / guid()\n     dirpath.mkdir()\n \n@@ -1800,8 +1761,6 @@ def test_ignore_private_directories(tempdir):\n \n \n def test_ignore_hidden_files(tempdir):\n-    import pyarrow.parquet as pq\n-\n     dirpath = tempdir / guid()\n     dirpath.mkdir()\n \n@@ -1864,8 +1823,6 @@ def test_write_error_deletes_incomplete_file(tempdir):\n \n \n def test_read_non_existent_file(tempdir):\n-    import pyarrow.parquet as pq\n-\n     path = 'non-existent-file.parquet'\n     try:\n         pq.read_table(path)\n@@ -1874,8 +1831,6 @@ def test_read_non_existent_file(tempdir):\n \n \n def test_read_table_doesnt_warn(datadir):\n-    import pyarrow.parquet as pq\n-\n     with pytest.warns(None) as record:\n         pq.read_table(datadir / 'v0.7.1.parquet')\n \n@@ -1885,8 +1840,6 @@ def test_read_table_doesnt_warn(datadir):\n def _test_write_to_dataset_with_partitions(base_path,\n                                            filesystem=None,\n                                            schema=None):\n-    import pyarrow.parquet as pq\n-\n     # ARROW-1400\n     output_df = pd.DataFrame({'group1': list('aaabbbbccc'),\n                               'group2': list('eefeffgeee'),\n@@ -1933,8 +1886,6 @@ def _test_write_to_dataset_with_partitions(base_path,\n \n \n def _test_write_to_dataset_no_partitions(base_path, filesystem=None):\n-    import pyarrow.parquet as pq\n-\n     # ARROW-1400\n     output_df = pd.DataFrame({'group1': list('aaabbbbccc'),\n                               'group2': list('eefeffgeee'),\n@@ -2021,8 +1972,6 @@ def test_index_column_name_duplicate(tempdir):\n \n \n def test_parquet_nested_convenience(tempdir):\n-    import pyarrow.parquet as pq\n-\n     # ARROW-1684\n     df = pd.DataFrame({\n         'a': [[1, 2, 3], None, [4, 5], []],\n@@ -2169,8 +2118,6 @@ def test_decimal_roundtrip_negative_scale(tempdir):\n \n \n def test_parquet_writer_context_obj(tempdir):\n-    import pyarrow.parquet as pq\n-\n     df = _test_dataframe(100)\n     df['unique_id'] = 0\n \n@@ -2195,8 +2142,6 @@ def test_parquet_writer_context_obj(tempdir):\n \n \n def test_parquet_writer_context_obj_with_exception(tempdir):\n-    import pyarrow.parquet as pq\n-\n     df = _test_dataframe(100)\n     df['unique_id'] = 0\n \n@@ -2229,8 +2174,6 @@ def test_parquet_writer_context_obj_with_exception(tempdir):\n \n def test_zlib_compression_bug():\n     # ARROW-3514: \"zlib deflate failed, output buffer too small\"\n-    import pyarrow.parquet as pq\n-\n     table = pa.Table.from_arrays([pa.array(['abc', 'def'])], ['some_col'])\n     f = io.BytesIO()\n     pq.write_table(table, f, compression='gzip')\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-11-14T15:49:28.115+0000",
                    "updated": "2018-11-14T15:49:28.115+0000",
                    "started": "2018-11-14T15:49:28.114+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "165933",
                    "issueId": "13170656"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 2400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@4bb64f20[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@55a804a4[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@14411c9c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@1d9a794[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7bace936[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@2a00e2e7[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3639aec7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@7a904a93[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@66e9f9ae[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@4b53284e[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@30e7f875[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@48dbf828[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 2400,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Nov 14 15:55:04 UTC 2018",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2018-11-14T15:55:04.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-2807/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2018-07-07T20:37:13.000+0000",
        "updated": "2018-11-14T15:55:04.000+0000",
        "timeoriginalestimate": null,
        "description": "See relevant discussion in ARROW-2654",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "40m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 2400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python] Enable memory-mapping to be toggled in get_reader when reading Parquet files",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13170656/comment/16686762",
                    "id": "16686762",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 2954\n[https://github.com/apache/arrow/pull/2954]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-11-14T15:55:04.381+0000",
                    "updated": "2018-11-14T15:55:04.381+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|i3vn4n:",
        "customfield_12314139": null
    }
}