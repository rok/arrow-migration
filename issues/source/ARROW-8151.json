{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13292546",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546",
    "key": "ARROW-8151",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12346687",
                "id": "12346687",
                "description": "",
                "name": "0.17.0",
                "archived": false,
                "released": true,
                "releaseDate": "2020-04-20"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "dataset",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12583489",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12583489",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13289295",
                    "key": "ARROW-7995",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13289295",
                    "fields": {
                        "summary": "[C++] IO: coalescing and caching read ranges",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/5",
                            "id": "5",
                            "description": "General wishlist item.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Wish",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
            "name": "lidavidm",
            "key": "lidavidm",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "David Li",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12334978",
                "id": "12334978",
                "name": "Benchmarking"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
            "name": "lidavidm",
            "key": "lidavidm",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "David Li",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
            "name": "lidavidm",
            "key": "lidavidm",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "David Li",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 13800,
            "total": 13800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 13800,
            "total": 13800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-8151/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 25,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/407120",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675\n \n \n   This does some basic refactoring of the S3 filesystem test to share some code, but I'd appreciate any feedback on how to better organize things.\r\n   \r\n   This currently has 4 sets of benchmarks:\r\n   - A test of reading an entire file of various sizes from S3\r\n   - A test of reading sequential small chunks of those files\r\n   - A test of using coalescing to read those sequential small chunks\r\n   - A test of reading a Parquet file\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-20T16:43:10.985+0000",
                    "updated": "2020-03-20T16:43:10.985+0000",
                    "started": "2020-03-20T16:43:10.984+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "407120",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/407121",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on issue #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#issuecomment-601799620\n \n \n   I ran the benchmark both with and without connection latency, introduced via [toxiproxy](https://github.com/Shopify/toxiproxy/). Latency added was 20ms +/- 2ms jitter (what I saw to Amazon S3 from my laptop).\r\n   \r\n   Toxiproxy commands:\r\n   \r\n   ```\r\n   ./toxiproxy-cli-linux-amd64 create minio --listen 127.0.0.1:50052 --upstream 127.0.0.1:50051\r\n   ./toxiproxy-cli-linux-amd64 toxic add minio -n latency -t latency -a latency=20 -a jitter=2\r\n   ```\r\n   \r\n   Reading the entire file in one shot (`ReadAll`):\r\n   \r\n   ```\r\n   --------------------------------------------------------------------------------------------\r\n   Benchmark                                                     Time           CPU Iterations\r\n   --------------------------------------------------------------------------------------------\r\n   MinioFixture/ReadAll1Mib/min_time:15.000/real_time      4524154 ns    2257122 ns       4682   221.036MB/s    221.036 items/s\r\n   MinioFixture/ReadAll100Mib/min_time:15.000/real_time  191434898 ns  181662703 ns        109   522.371MB/s    5.22371 items/s\r\n   MinioFixture/ReadAll500Mib/min_time:15.000/real_time  885562963 ns  860429294 ns         24   564.613MB/s    1.12923 items/s\r\n   \r\n   With Latency\r\n   --------------------------------------------------------------------------------------------\r\n   MinioFixture/ReadAll1Mib/min_time:15.000/real_time     46570687 ns    2750293 ns        447   21.4727MB/s    21.4727 items/s\r\n   MinioFixture/ReadAll100Mib/min_time:15.000/real_time  276031415 ns  226440164 ns         75   362.278MB/s    3.62278 items/s\r\n   MinioFixture/ReadAll500Mib/min_time:15.000/real_time 1185247456 ns 1114922703 ns         18   421.853MB/s   0.843706 items/s\r\n   ```\r\n   \r\n   Reading the file in chunks:\r\n   \r\n   ```\r\n   ------------------------------------------------------------------------------------------------\r\n   Benchmark                                                         Time           CPU Iterations\r\n   ------------------------------------------------------------------------------------------------\r\n   MinioFixture/ReadChunked100Mib/min_time:15.000/real_time  196150922 ns  152319302 ns        106   509.812MB/s    5.09812 items/s\r\n   MinioFixture/ReadChunked500Mib/min_time:15.000/real_time  942929100 ns  762863911 ns         22   530.263MB/s    1.06053 items/s\r\n   \r\n   With latency\r\n   ------------------------------------------------------------------------------------------------\r\n   MinioFixture/ReadChunked100Mib/min_time:15.000/real_time  701457618 ns  201526399 ns         29    142.56MB/s     1.4256 items/s\r\n   MinioFixture/ReadChunked500Mib/min_time:15.000/real_time 3397605245 ns  986615721 ns          6   147.162MB/s   0.294325 items/s\r\n   ```\r\n   \r\n   Reading the file in chunks, with coalescing:\r\n   \r\n   ```\r\n   --------------------------------------------------------------------------------------------------\r\n   Benchmark                                                           Time           CPU Iterations\r\n   --------------------------------------------------------------------------------------------------\r\n   MinioFixture/ReadCoalesced100Mib/min_time:15.000/real_time  175538760 ns   17630239 ns        113   569.675MB/s    5.69675 items/s\r\n   MinioFixture/ReadCoalesced500Mib/min_time:15.000/real_time  746773493 ns   83969652 ns         28   669.547MB/s    1.33909 items/s\r\n   \r\n   With Latency\r\n   --------------------------------------------------------------------------------------------------\r\n   MinioFixture/ReadCoalesced100Mib/min_time:15.000/real_time  249571387 ns   18055469 ns         83   400.687MB/s    4.00687 items/s\r\n   MinioFixture/ReadCoalesced500Mib/min_time:15.000/real_time 1073518943 ns   87166607 ns         20   465.758MB/s   0.931516 items/s\r\n   ```\r\n   \r\n   Parquet:\r\n   \r\n   ```\r\n   ----------------------------------------------------------------------------------------------\r\n   Benchmark                                                       Time           CPU Iterations\r\n   ----------------------------------------------------------------------------------------------\r\n   MinioFixture/ReadParquet250K/min_time:15.000/real_time 1339233367 ns 1156336875 ns         16   162.304MB/s   0.746696 items/s\r\n   \r\n   With throttling\r\n   ----------------------------------------------------------------------------------------------\r\n   MinioFixture/ReadParquet250K/min_time:15.000/real_time 3816421753 ns 1417826696 ns          5   56.9546MB/s   0.262026 items/s\r\n   ```\r\n   \r\n   Read coalescing helps here, and could potentially do more - I didn't tune the parameters.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-20T16:44:12.470+0000",
                    "updated": "2020-03-20T16:44:12.470+0000",
                    "started": "2020-03-20T16:44:12.469+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "407121",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/407123",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on issue #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#issuecomment-601800778\n \n \n   https://issues.apache.org/jira/browse/ARROW-8151\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-20T16:46:34.575+0000",
                    "updated": "2020-03-20T16:46:34.575+0000",
                    "started": "2020-03-20T16:46:34.575+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "407123",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/407792",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396306186\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,317 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+    ASSERT_OK(MakeBucket());\n+    ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+    ASSERT_OK(MakeParquetObject(\"bucket/pq_c100_r250k\", 100, 250000));\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(\"bucket\"));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      std::shared_ptr<Array> values;\n+      std::vector<double> c_values;\n+      arrow::random_real(num_rows, static_cast<uint32_t>(i), -1.e10, 1e10, &c_values);\n+      arrow::NumericBuilder<DoubleType> builder(std::make_shared<DoubleType>(),\n \n Review comment:\n   Can use `arrow/testing/random.h` instead.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T09:29:21.295+0000",
                    "updated": "2020-03-23T09:29:21.295+0000",
                    "started": "2020-03-23T09:29:21.295+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "407792",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/407793",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396306619\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3_test_util.h\n ##########\n @@ -0,0 +1,158 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <utility>\n+\n+// boost/process/detail/windows/handle_workaround.hpp doesn't work\n+// without BOOST_USE_WINDOWS_H with MinGW because MinGW doesn't\n+// provide __kernel_entry without winternl.h.\n+//\n+// See also:\n+// https://github.com/boostorg/process/blob/develop/include/boost/process/detail/windows/handle_workaround.hpp\n+#include <boost/process.hpp>\n+\n+#include <gtest/gtest.h>\n+\n+#include <aws/core/Aws.h>\n+\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/util.h\"\n \n Review comment:\n   Is this include actually used here ?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T09:29:21.517+0000",
                    "updated": "2020-03-23T09:29:21.517+0000",
                    "started": "2020-03-23T09:29:21.516+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "407793",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/407794",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396312504\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,317 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+    ASSERT_OK(MakeBucket());\n+    ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+    ASSERT_OK(MakeParquetObject(\"bucket/pq_c100_r250k\", 100, 250000));\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(\"bucket\"));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      std::shared_ptr<Array> values;\n+      std::vector<double> c_values;\n+      arrow::random_real(num_rows, static_cast<uint32_t>(i), -1.e10, 1e10, &c_values);\n+      arrow::NumericBuilder<DoubleType> builder(std::make_shared<DoubleType>(),\n+                                                arrow::default_memory_pool());\n+      std::vector<uint8_t> valid_bytes(num_rows, 1);\n+      RETURN_NOT_OK(\n+          builder.AppendValues(c_values.data(), c_values.size(), valid_bytes.data()));\n+      RETURN_NOT_OK(builder.Finish(&values));\n+\n+      std::stringstream ss;\n+      ss << \"col\" << i;\n+      columns[i] = std::make_shared<ChunkedArray>(values);\n+      fields[i] = ::arrow::field(ss.str(), values->type());\n+    }\n+    auto schema = std::make_shared<::arrow::Schema>(fields);\n+\n+    std::shared_ptr<Table> table = Table::Make(schema, columns);\n+\n+    std::shared_ptr<io::OutputStream> sink;\n+    ARROW_ASSIGN_OR_RAISE(sink, fs_->OpenOutputStream(path));\n+    auto writer_properties =\n+        parquet::WriterProperties::Builder().write_batch_size(128)->build();\n+    auto arrow_properties = parquet::default_arrow_writer_properties();\n+    RETURN_NOT_OK(parquet::arrow::WriteTable(*table, arrow::default_memory_pool(), sink,\n+                                             num_rows, writer_properties,\n+                                             arrow_properties));\n+\n+    return Status::OK();\n+  }\n+\n+  void TearDown(const ::benchmark::State& state) override { ASSERT_OK(minio_.Stop()); }\n+\n+ protected:\n+  MinioTestServer minio_;\n+  Aws::Client::ClientConfiguration client_config_;\n+  Aws::Auth::AWSCredentials credentials_;\n+  std::unique_ptr<Aws::S3::S3Client> client_;\n+  S3Options options_;\n+  std::shared_ptr<S3FileSystem> fs_;\n+};\n+\n+/// Set up/tear down the AWS SDK globally.\n+/// (GBenchmark doesn't run GTest environments.)\n+class S3BenchmarkEnvironment {\n+ public:\n+  S3BenchmarkEnvironment() { s3_env->SetUp(); }\n+  ~S3BenchmarkEnvironment() { s3_env->TearDown(); }\n+};\n+\n+S3BenchmarkEnvironment env{};\n+\n+/// Read the entire file into memory in one go to measure bandwidth.\n+static void NaiveRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(0, size));\n+    total_bytes += buf->size();\n+    total_items += 1;\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+constexpr int64_t kChunkSize = 5 * 1024 * 1024;\n+\n+/// Mimic the Parquet reader, reading the file in small chunks.\n+static void ChunkedRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size = 0;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    total_items += 1;\n+\n+    int64_t offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(offset, read));\n+      total_bytes += buf->size();\n+      offset += buf->size();\n+    }\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+/// Read the file in small chunks, but using read coalescing.\n+static void CoalescedRead(benchmark::State& st, S3FileSystem* fs,\n+                          const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size = 0;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    total_items += 1;\n+\n+    io::internal::ReadRangeCache cache(file, 8192, 64 * 1024 * 1024);\n+    std::vector<io::ReadRange> ranges;\n+\n+    int64_t offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ranges.push_back(io::ReadRange{offset, read});\n+      offset += read;\n+    }\n+    ASSERT_OK(cache.Cache(ranges));\n+\n+    offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ASSERT_OK_AND_ASSIGN(buf, cache.Read({offset, read}));\n+      total_bytes += buf->size();\n+      offset += read;\n+    }\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+/// Read a Parquet file from S3.\n+static void ParquetRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    int64_t size = 0;\n+    // N.B. this is an extra call to S3 and influences the\n+    // benchmark. We should compute or cache the size.\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+\n+    parquet::ArrowReaderProperties properties;\n+    properties.set_use_threads(true);\n+    std::unique_ptr<parquet::arrow::FileReader> reader;\n+    parquet::arrow::FileReaderBuilder builder;\n+    ASSERT_OK(builder.Open(file));\n+    ASSERT_OK(builder.properties(properties)->Build(&reader));\n+    std::shared_ptr<RecordBatchReader> rb_reader;\n+    ASSERT_OK(reader->GetRecordBatchReader({0}, &rb_reader));\n+    std::shared_ptr<Table> table;\n+    ASSERT_OK(rb_reader->ReadAll(&table));\n+\n+    // TODO: actually measure table memory usage\n+    total_bytes += size;\n+    total_items += 1;\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+BENCHMARK_DEFINE_F(MinioFixture, ReadAll1Mib)(benchmark::State& st) {\n+  NaiveRead(st, fs_.get(), \"bucket/bytes_1mib\");\n+}\n+BENCHMARK_REGISTER_F(MinioFixture, ReadAll1Mib)->MinTime(15)->UseRealTime();\n \n Review comment:\n   Are 15 seconds necessary for benchmark stability? Usually we don't specify min time in benchmark source and instead let the user pass `--benchmark_min_time` on the command line.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T09:29:21.688+0000",
                    "updated": "2020-03-23T09:29:21.688+0000",
                    "started": "2020-03-23T09:29:21.688+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "407794",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/407795",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396307551\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/CMakeLists.txt\n ##########\n @@ -36,6 +36,15 @@ if(ARROW_S3)\n                           ${GFLAGS_LIBRARIES} GTest::gtest)\n     add_dependencies(arrow-tests arrow-s3fs-narrative-test)\n   endif()\n+\n+  if(ARROW_BUILD_BENCHMARKS AND ARROW_PARQUET)\n+    add_arrow_benchmark(s3fs_benchmark PREFIX \"arrow-filesystem\")\n+    if(ARG_STATIC_LINK_LIBS)\n \n Review comment:\n   I don't think `ARG_STATIC_LINK_LIBS` exists at the top level?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T09:29:21.692+0000",
                    "updated": "2020-03-23T09:29:21.692+0000",
                    "started": "2020-03-23T09:29:21.692+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "407795",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/407796",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396309306\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,317 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+    ASSERT_OK(MakeBucket());\n+    ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+    ASSERT_OK(MakeParquetObject(\"bucket/pq_c100_r250k\", 100, 250000));\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(\"bucket\"));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      std::shared_ptr<Array> values;\n+      std::vector<double> c_values;\n+      arrow::random_real(num_rows, static_cast<uint32_t>(i), -1.e10, 1e10, &c_values);\n+      arrow::NumericBuilder<DoubleType> builder(std::make_shared<DoubleType>(),\n+                                                arrow::default_memory_pool());\n+      std::vector<uint8_t> valid_bytes(num_rows, 1);\n+      RETURN_NOT_OK(\n+          builder.AppendValues(c_values.data(), c_values.size(), valid_bytes.data()));\n+      RETURN_NOT_OK(builder.Finish(&values));\n+\n+      std::stringstream ss;\n+      ss << \"col\" << i;\n+      columns[i] = std::make_shared<ChunkedArray>(values);\n+      fields[i] = ::arrow::field(ss.str(), values->type());\n+    }\n+    auto schema = std::make_shared<::arrow::Schema>(fields);\n+\n+    std::shared_ptr<Table> table = Table::Make(schema, columns);\n+\n+    std::shared_ptr<io::OutputStream> sink;\n+    ARROW_ASSIGN_OR_RAISE(sink, fs_->OpenOutputStream(path));\n+    auto writer_properties =\n+        parquet::WriterProperties::Builder().write_batch_size(128)->build();\n+    auto arrow_properties = parquet::default_arrow_writer_properties();\n+    RETURN_NOT_OK(parquet::arrow::WriteTable(*table, arrow::default_memory_pool(), sink,\n+                                             num_rows, writer_properties,\n+                                             arrow_properties));\n+\n+    return Status::OK();\n+  }\n+\n+  void TearDown(const ::benchmark::State& state) override { ASSERT_OK(minio_.Stop()); }\n+\n+ protected:\n+  MinioTestServer minio_;\n+  Aws::Client::ClientConfiguration client_config_;\n+  Aws::Auth::AWSCredentials credentials_;\n+  std::unique_ptr<Aws::S3::S3Client> client_;\n+  S3Options options_;\n+  std::shared_ptr<S3FileSystem> fs_;\n+};\n+\n+/// Set up/tear down the AWS SDK globally.\n+/// (GBenchmark doesn't run GTest environments.)\n+class S3BenchmarkEnvironment {\n+ public:\n+  S3BenchmarkEnvironment() { s3_env->SetUp(); }\n+  ~S3BenchmarkEnvironment() { s3_env->TearDown(); }\n+};\n+\n+S3BenchmarkEnvironment env{};\n+\n+/// Read the entire file into memory in one go to measure bandwidth.\n+static void NaiveRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(0, size));\n+    total_bytes += buf->size();\n+    total_items += 1;\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+constexpr int64_t kChunkSize = 5 * 1024 * 1024;\n+\n+/// Mimic the Parquet reader, reading the file in small chunks.\n+static void ChunkedRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size = 0;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    total_items += 1;\n+\n+    int64_t offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(offset, read));\n+      total_bytes += buf->size();\n+      offset += buf->size();\n+    }\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+/// Read the file in small chunks, but using read coalescing.\n+static void CoalescedRead(benchmark::State& st, S3FileSystem* fs,\n+                          const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size = 0;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    total_items += 1;\n+\n+    io::internal::ReadRangeCache cache(file, 8192, 64 * 1024 * 1024);\n+    std::vector<io::ReadRange> ranges;\n+\n+    int64_t offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ranges.push_back(io::ReadRange{offset, read});\n+      offset += read;\n+    }\n+    ASSERT_OK(cache.Cache(ranges));\n+\n+    offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ASSERT_OK_AND_ASSIGN(buf, cache.Read({offset, read}));\n+      total_bytes += buf->size();\n+      offset += read;\n+    }\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+/// Read a Parquet file from S3.\n+static void ParquetRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    int64_t size = 0;\n+    // N.B. this is an extra call to S3 and influences the\n \n Review comment:\n   Do you mean the GetSize call? It doesn't call into S3.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T09:29:21.715+0000",
                    "updated": "2020-03-23T09:29:21.715+0000",
                    "started": "2020-03-23T09:29:21.715+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "407796",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408102",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396651491\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,317 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+    ASSERT_OK(MakeBucket());\n+    ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+    ASSERT_OK(MakeParquetObject(\"bucket/pq_c100_r250k\", 100, 250000));\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(\"bucket\"));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      std::shared_ptr<Array> values;\n+      std::vector<double> c_values;\n+      arrow::random_real(num_rows, static_cast<uint32_t>(i), -1.e10, 1e10, &c_values);\n+      arrow::NumericBuilder<DoubleType> builder(std::make_shared<DoubleType>(),\n \n Review comment:\n   Good call, thanks.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T18:04:44.983+0000",
                    "updated": "2020-03-23T18:04:44.983+0000",
                    "started": "2020-03-23T18:04:44.983+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408102",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408103",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396651570\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3_test_util.h\n ##########\n @@ -0,0 +1,158 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <utility>\n+\n+// boost/process/detail/windows/handle_workaround.hpp doesn't work\n+// without BOOST_USE_WINDOWS_H with MinGW because MinGW doesn't\n+// provide __kernel_entry without winternl.h.\n+//\n+// See also:\n+// https://github.com/boostorg/process/blob/develop/include/boost/process/detail/windows/handle_workaround.hpp\n+#include <boost/process.hpp>\n+\n+#include <gtest/gtest.h>\n+\n+#include <aws/core/Aws.h>\n+\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/util.h\"\n \n Review comment:\n   It's not, I've removed it.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T18:04:51.965+0000",
                    "updated": "2020-03-23T18:04:51.965+0000",
                    "started": "2020-03-23T18:04:51.965+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408103",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408104",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396651818\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/CMakeLists.txt\n ##########\n @@ -36,6 +36,15 @@ if(ARROW_S3)\n                           ${GFLAGS_LIBRARIES} GTest::gtest)\n     add_dependencies(arrow-tests arrow-s3fs-narrative-test)\n   endif()\n+\n+  if(ARROW_BUILD_BENCHMARKS AND ARROW_PARQUET)\n+    add_arrow_benchmark(s3fs_benchmark PREFIX \"arrow-filesystem\")\n+    if(ARG_STATIC_LINK_LIBS)\n \n Review comment:\n   I've changed this to look at `ARROW_TEST_LINKAGE` instead.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T18:05:11.682+0000",
                    "updated": "2020-03-23T18:05:11.682+0000",
                    "started": "2020-03-23T18:05:11.682+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408104",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408105",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396652030\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,317 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+    ASSERT_OK(MakeBucket());\n+    ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+    ASSERT_OK(MakeParquetObject(\"bucket/pq_c100_r250k\", 100, 250000));\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(\"bucket\"));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      std::shared_ptr<Array> values;\n+      std::vector<double> c_values;\n+      arrow::random_real(num_rows, static_cast<uint32_t>(i), -1.e10, 1e10, &c_values);\n+      arrow::NumericBuilder<DoubleType> builder(std::make_shared<DoubleType>(),\n+                                                arrow::default_memory_pool());\n+      std::vector<uint8_t> valid_bytes(num_rows, 1);\n+      RETURN_NOT_OK(\n+          builder.AppendValues(c_values.data(), c_values.size(), valid_bytes.data()));\n+      RETURN_NOT_OK(builder.Finish(&values));\n+\n+      std::stringstream ss;\n+      ss << \"col\" << i;\n+      columns[i] = std::make_shared<ChunkedArray>(values);\n+      fields[i] = ::arrow::field(ss.str(), values->type());\n+    }\n+    auto schema = std::make_shared<::arrow::Schema>(fields);\n+\n+    std::shared_ptr<Table> table = Table::Make(schema, columns);\n+\n+    std::shared_ptr<io::OutputStream> sink;\n+    ARROW_ASSIGN_OR_RAISE(sink, fs_->OpenOutputStream(path));\n+    auto writer_properties =\n+        parquet::WriterProperties::Builder().write_batch_size(128)->build();\n+    auto arrow_properties = parquet::default_arrow_writer_properties();\n+    RETURN_NOT_OK(parquet::arrow::WriteTable(*table, arrow::default_memory_pool(), sink,\n+                                             num_rows, writer_properties,\n+                                             arrow_properties));\n+\n+    return Status::OK();\n+  }\n+\n+  void TearDown(const ::benchmark::State& state) override { ASSERT_OK(minio_.Stop()); }\n+\n+ protected:\n+  MinioTestServer minio_;\n+  Aws::Client::ClientConfiguration client_config_;\n+  Aws::Auth::AWSCredentials credentials_;\n+  std::unique_ptr<Aws::S3::S3Client> client_;\n+  S3Options options_;\n+  std::shared_ptr<S3FileSystem> fs_;\n+};\n+\n+/// Set up/tear down the AWS SDK globally.\n+/// (GBenchmark doesn't run GTest environments.)\n+class S3BenchmarkEnvironment {\n+ public:\n+  S3BenchmarkEnvironment() { s3_env->SetUp(); }\n+  ~S3BenchmarkEnvironment() { s3_env->TearDown(); }\n+};\n+\n+S3BenchmarkEnvironment env{};\n+\n+/// Read the entire file into memory in one go to measure bandwidth.\n+static void NaiveRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(0, size));\n+    total_bytes += buf->size();\n+    total_items += 1;\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+constexpr int64_t kChunkSize = 5 * 1024 * 1024;\n+\n+/// Mimic the Parquet reader, reading the file in small chunks.\n+static void ChunkedRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size = 0;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    total_items += 1;\n+\n+    int64_t offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(offset, read));\n+      total_bytes += buf->size();\n+      offset += buf->size();\n+    }\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+/// Read the file in small chunks, but using read coalescing.\n+static void CoalescedRead(benchmark::State& st, S3FileSystem* fs,\n+                          const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size = 0;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    total_items += 1;\n+\n+    io::internal::ReadRangeCache cache(file, 8192, 64 * 1024 * 1024);\n+    std::vector<io::ReadRange> ranges;\n+\n+    int64_t offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ranges.push_back(io::ReadRange{offset, read});\n+      offset += read;\n+    }\n+    ASSERT_OK(cache.Cache(ranges));\n+\n+    offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ASSERT_OK_AND_ASSIGN(buf, cache.Read({offset, read}));\n+      total_bytes += buf->size();\n+      offset += read;\n+    }\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+/// Read a Parquet file from S3.\n+static void ParquetRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    int64_t size = 0;\n+    // N.B. this is an extra call to S3 and influences the\n \n Review comment:\n   Opening the file makes a HEAD request, but I've removed the comment since it is irrelevant.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T18:05:30.108+0000",
                    "updated": "2020-03-23T18:05:30.108+0000",
                    "started": "2020-03-23T18:05:30.107+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408105",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408107",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r396652373\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,317 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+    ASSERT_OK(MakeBucket());\n+    ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+    ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+    ASSERT_OK(MakeParquetObject(\"bucket/pq_c100_r250k\", 100, 250000));\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(\"bucket\"));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(\"bucket\"));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      std::shared_ptr<Array> values;\n+      std::vector<double> c_values;\n+      arrow::random_real(num_rows, static_cast<uint32_t>(i), -1.e10, 1e10, &c_values);\n+      arrow::NumericBuilder<DoubleType> builder(std::make_shared<DoubleType>(),\n+                                                arrow::default_memory_pool());\n+      std::vector<uint8_t> valid_bytes(num_rows, 1);\n+      RETURN_NOT_OK(\n+          builder.AppendValues(c_values.data(), c_values.size(), valid_bytes.data()));\n+      RETURN_NOT_OK(builder.Finish(&values));\n+\n+      std::stringstream ss;\n+      ss << \"col\" << i;\n+      columns[i] = std::make_shared<ChunkedArray>(values);\n+      fields[i] = ::arrow::field(ss.str(), values->type());\n+    }\n+    auto schema = std::make_shared<::arrow::Schema>(fields);\n+\n+    std::shared_ptr<Table> table = Table::Make(schema, columns);\n+\n+    std::shared_ptr<io::OutputStream> sink;\n+    ARROW_ASSIGN_OR_RAISE(sink, fs_->OpenOutputStream(path));\n+    auto writer_properties =\n+        parquet::WriterProperties::Builder().write_batch_size(128)->build();\n+    auto arrow_properties = parquet::default_arrow_writer_properties();\n+    RETURN_NOT_OK(parquet::arrow::WriteTable(*table, arrow::default_memory_pool(), sink,\n+                                             num_rows, writer_properties,\n+                                             arrow_properties));\n+\n+    return Status::OK();\n+  }\n+\n+  void TearDown(const ::benchmark::State& state) override { ASSERT_OK(minio_.Stop()); }\n+\n+ protected:\n+  MinioTestServer minio_;\n+  Aws::Client::ClientConfiguration client_config_;\n+  Aws::Auth::AWSCredentials credentials_;\n+  std::unique_ptr<Aws::S3::S3Client> client_;\n+  S3Options options_;\n+  std::shared_ptr<S3FileSystem> fs_;\n+};\n+\n+/// Set up/tear down the AWS SDK globally.\n+/// (GBenchmark doesn't run GTest environments.)\n+class S3BenchmarkEnvironment {\n+ public:\n+  S3BenchmarkEnvironment() { s3_env->SetUp(); }\n+  ~S3BenchmarkEnvironment() { s3_env->TearDown(); }\n+};\n+\n+S3BenchmarkEnvironment env{};\n+\n+/// Read the entire file into memory in one go to measure bandwidth.\n+static void NaiveRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(0, size));\n+    total_bytes += buf->size();\n+    total_items += 1;\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+constexpr int64_t kChunkSize = 5 * 1024 * 1024;\n+\n+/// Mimic the Parquet reader, reading the file in small chunks.\n+static void ChunkedRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size = 0;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    total_items += 1;\n+\n+    int64_t offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(offset, read));\n+      total_bytes += buf->size();\n+      offset += buf->size();\n+    }\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+/// Read the file in small chunks, but using read coalescing.\n+static void CoalescedRead(benchmark::State& st, S3FileSystem* fs,\n+                          const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size = 0;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    total_items += 1;\n+\n+    io::internal::ReadRangeCache cache(file, 8192, 64 * 1024 * 1024);\n+    std::vector<io::ReadRange> ranges;\n+\n+    int64_t offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ranges.push_back(io::ReadRange{offset, read});\n+      offset += read;\n+    }\n+    ASSERT_OK(cache.Cache(ranges));\n+\n+    offset = 0;\n+    while (offset < size) {\n+      const int64_t read = std::min(size, kChunkSize);\n+      ASSERT_OK_AND_ASSIGN(buf, cache.Read({offset, read}));\n+      total_bytes += buf->size();\n+      offset += read;\n+    }\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+/// Read a Parquet file from S3.\n+static void ParquetRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    int64_t size = 0;\n+    // N.B. this is an extra call to S3 and influences the\n+    // benchmark. We should compute or cache the size.\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+\n+    parquet::ArrowReaderProperties properties;\n+    properties.set_use_threads(true);\n+    std::unique_ptr<parquet::arrow::FileReader> reader;\n+    parquet::arrow::FileReaderBuilder builder;\n+    ASSERT_OK(builder.Open(file));\n+    ASSERT_OK(builder.properties(properties)->Build(&reader));\n+    std::shared_ptr<RecordBatchReader> rb_reader;\n+    ASSERT_OK(reader->GetRecordBatchReader({0}, &rb_reader));\n+    std::shared_ptr<Table> table;\n+    ASSERT_OK(rb_reader->ReadAll(&table));\n+\n+    // TODO: actually measure table memory usage\n+    total_bytes += size;\n+    total_items += 1;\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n+}\n+\n+BENCHMARK_DEFINE_F(MinioFixture, ReadAll1Mib)(benchmark::State& st) {\n+  NaiveRead(st, fs_.get(), \"bucket/bytes_1mib\");\n+}\n+BENCHMARK_REGISTER_F(MinioFixture, ReadAll1Mib)->MinTime(15)->UseRealTime();\n \n Review comment:\n   In general yes, we need a much higher min time in order to run more than 1 iteration of these benchmarks, but I've removed it in favor of the CLI switch.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T18:06:00.100+0000",
                    "updated": "2020-03-23T18:06:00.100+0000",
                    "started": "2020-03-23T18:06:00.100+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408107",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408108",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on issue #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#issuecomment-602766774\n \n \n   Thanks for the review. I've addressed the feedback and also added a few flags so that you can choose the bucket name and skip file creation. I will run the tests against S3 and report on the mailing list discussion once I get my AWS account activated.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-23T18:07:13.628+0000",
                    "updated": "2020-03-23T18:07:13.628+0000",
                    "started": "2020-03-23T18:07:13.628+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408108",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408645",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r397043357\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3_test_util.h\n ##########\n @@ -0,0 +1,157 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <utility>\n+\n+// boost/process/detail/windows/handle_workaround.hpp doesn't work\n \n Review comment:\n   Why this comment, by the way?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-24T10:29:08.867+0000",
                    "updated": "2020-03-24T10:29:08.867+0000",
                    "started": "2020-03-24T10:29:08.867+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408645",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408646",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r397046200\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,344 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Environment variables to configure the S3 test environment\n+static const char* kEnvBucketName = \"ARROW_TEST_S3_BUCKET\";\n+static const char* kEnvSkipSetup = \"ARROW_TEST_S3_SKIP_SETUP\";\n+static const char* kEnvAwsRegion = \"ARROW_TEST_S3_REGION\";\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    const char* region_str = std::getenv(kEnvAwsRegion);\n+    if (region_str) {\n+      region_ = region_str;\n+      std::cerr << \"Using region from environment: \" << region_ << std::endl;\n+    } else {\n+      std::cerr << \"Using default region\" << std::endl;\n+    }\n+\n+    const char* bucket_str = std::getenv(kEnvBucketName);\n+    if (bucket_str) {\n+      bucket_ = bucket_str;\n+      std::cerr << \"Using bucket from environment: \" << bucket_ << std::endl;\n+    } else {\n+      bucket_ = \"bucket\";\n+      std::cerr << \"Using default bucket: \" << bucket_ << std::endl;\n+    }\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    if (!region_.empty()) {\n+      client_config_.region = ToAwsString(region_);\n+    }\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+\n+    const char* skip_str = std::getenv(kEnvSkipSetup);\n+    const std::string skip = skip_str ? std::string(skip_str) : \"\";\n+    if (skip == \"true\") {\n+      std::cerr << \"Skipping creation of bucket/objects as requested\" << std::endl;\n+    } else {\n+      ASSERT_OK(MakeBucket());\n+      ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+      ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+      ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+      ASSERT_OK(MakeParquetObject(bucket_ + \"/pq_c100_r250k\", 100, 250000));\n+    }\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    if (!region_.empty()) {\n+      options_.region = region_;\n+    }\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(bucket_));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(bucket_));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(bucket_));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      arrow::random::RandomArrayGenerator generator(i);\n+      std::shared_ptr<Array> values = generator.Float64(num_rows, -1.e10, 1e10, 0);\n+      std::stringstream ss;\n+      ss << \"col\" << i;\n+      columns[i] = std::make_shared<ChunkedArray>(values);\n+      fields[i] = ::arrow::field(ss.str(), values->type());\n+    }\n+    auto schema = std::make_shared<::arrow::Schema>(fields);\n+\n+    std::shared_ptr<Table> table = Table::Make(schema, columns);\n+\n+    std::shared_ptr<io::OutputStream> sink;\n+    ARROW_ASSIGN_OR_RAISE(sink, fs_->OpenOutputStream(path));\n+    auto writer_properties =\n+        parquet::WriterProperties::Builder().write_batch_size(128)->build();\n \n Review comment:\n   Does `write_batch_size` influence the row group size or something? 128 rows at once sounds tiny.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-24T10:29:09.003+0000",
                    "updated": "2020-03-24T10:29:09.003+0000",
                    "started": "2020-03-24T10:29:09.002+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408646",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408647",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r397045104\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,344 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Environment variables to configure the S3 test environment\n+static const char* kEnvBucketName = \"ARROW_TEST_S3_BUCKET\";\n+static const char* kEnvSkipSetup = \"ARROW_TEST_S3_SKIP_SETUP\";\n+static const char* kEnvAwsRegion = \"ARROW_TEST_S3_REGION\";\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    const char* region_str = std::getenv(kEnvAwsRegion);\n+    if (region_str) {\n+      region_ = region_str;\n+      std::cerr << \"Using region from environment: \" << region_ << std::endl;\n+    } else {\n+      std::cerr << \"Using default region\" << std::endl;\n+    }\n+\n+    const char* bucket_str = std::getenv(kEnvBucketName);\n+    if (bucket_str) {\n+      bucket_ = bucket_str;\n+      std::cerr << \"Using bucket from environment: \" << bucket_ << std::endl;\n+    } else {\n+      bucket_ = \"bucket\";\n+      std::cerr << \"Using default bucket: \" << bucket_ << std::endl;\n+    }\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    if (!region_.empty()) {\n+      client_config_.region = ToAwsString(region_);\n+    }\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+\n+    const char* skip_str = std::getenv(kEnvSkipSetup);\n+    const std::string skip = skip_str ? std::string(skip_str) : \"\";\n+    if (skip == \"true\") {\n \n Review comment:\n   Shouldn't it be skipped if the variable has any non-empty value?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-24T10:29:09.006+0000",
                    "updated": "2020-03-24T10:29:09.006+0000",
                    "started": "2020-03-24T10:29:09.006+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408647",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408648",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r397046828\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,344 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Environment variables to configure the S3 test environment\n+static const char* kEnvBucketName = \"ARROW_TEST_S3_BUCKET\";\n+static const char* kEnvSkipSetup = \"ARROW_TEST_S3_SKIP_SETUP\";\n+static const char* kEnvAwsRegion = \"ARROW_TEST_S3_REGION\";\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    const char* region_str = std::getenv(kEnvAwsRegion);\n+    if (region_str) {\n+      region_ = region_str;\n+      std::cerr << \"Using region from environment: \" << region_ << std::endl;\n+    } else {\n+      std::cerr << \"Using default region\" << std::endl;\n+    }\n+\n+    const char* bucket_str = std::getenv(kEnvBucketName);\n+    if (bucket_str) {\n+      bucket_ = bucket_str;\n+      std::cerr << \"Using bucket from environment: \" << bucket_ << std::endl;\n+    } else {\n+      bucket_ = \"bucket\";\n+      std::cerr << \"Using default bucket: \" << bucket_ << std::endl;\n+    }\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    if (!region_.empty()) {\n+      client_config_.region = ToAwsString(region_);\n+    }\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+\n+    const char* skip_str = std::getenv(kEnvSkipSetup);\n+    const std::string skip = skip_str ? std::string(skip_str) : \"\";\n+    if (skip == \"true\") {\n+      std::cerr << \"Skipping creation of bucket/objects as requested\" << std::endl;\n+    } else {\n+      ASSERT_OK(MakeBucket());\n+      ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+      ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+      ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+      ASSERT_OK(MakeParquetObject(bucket_ + \"/pq_c100_r250k\", 100, 250000));\n+    }\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    if (!region_.empty()) {\n+      options_.region = region_;\n+    }\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(bucket_));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(bucket_));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(bucket_));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      arrow::random::RandomArrayGenerator generator(i);\n+      std::shared_ptr<Array> values = generator.Float64(num_rows, -1.e10, 1e10, 0);\n+      std::stringstream ss;\n+      ss << \"col\" << i;\n+      columns[i] = std::make_shared<ChunkedArray>(values);\n+      fields[i] = ::arrow::field(ss.str(), values->type());\n+    }\n+    auto schema = std::make_shared<::arrow::Schema>(fields);\n+\n+    std::shared_ptr<Table> table = Table::Make(schema, columns);\n+\n+    std::shared_ptr<io::OutputStream> sink;\n+    ARROW_ASSIGN_OR_RAISE(sink, fs_->OpenOutputStream(path));\n+    auto writer_properties =\n+        parquet::WriterProperties::Builder().write_batch_size(128)->build();\n+    auto arrow_properties = parquet::default_arrow_writer_properties();\n+    RETURN_NOT_OK(parquet::arrow::WriteTable(*table, arrow::default_memory_pool(), sink,\n+                                             num_rows, writer_properties,\n+                                             arrow_properties));\n+\n+    return Status::OK();\n+  }\n+\n+  void TearDown(const ::benchmark::State& state) override { ASSERT_OK(minio_.Stop()); }\n+\n+ protected:\n+  MinioTestServer minio_;\n+  std::string region_;\n+  std::string bucket_;\n+  Aws::Client::ClientConfiguration client_config_;\n+  Aws::Auth::AWSCredentials credentials_;\n+  std::unique_ptr<Aws::S3::S3Client> client_;\n+  S3Options options_;\n+  std::shared_ptr<S3FileSystem> fs_;\n+};\n+\n+/// Set up/tear down the AWS SDK globally.\n+/// (GBenchmark doesn't run GTest environments.)\n+class S3BenchmarkEnvironment {\n+ public:\n+  S3BenchmarkEnvironment() { s3_env->SetUp(); }\n+  ~S3BenchmarkEnvironment() { s3_env->TearDown(); }\n+};\n+\n+S3BenchmarkEnvironment env{};\n+\n+/// Read the entire file into memory in one go to measure bandwidth.\n+static void NaiveRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(0, size));\n+    total_bytes += buf->size();\n+    total_items += 1;\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n \n Review comment:\n   `std::cerr` is used in other parts. Is there a reason for the discrepancy?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-24T10:29:09.080+0000",
                    "updated": "2020-03-24T10:29:09.080+0000",
                    "started": "2020-03-24T10:29:09.080+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408648",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408695",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r397105029\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3_test_util.h\n ##########\n @@ -0,0 +1,157 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <utility>\n+\n+// boost/process/detail/windows/handle_workaround.hpp doesn't work\n \n Review comment:\n   It originates in f8868686bb; I copied some of the includes and associated comments out of `s3fs_test.cc` and put them here when I refactored out the minio utilities.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-24T12:11:35.700+0000",
                    "updated": "2020-03-24T12:11:35.700+0000",
                    "started": "2020-03-24T12:11:35.700+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408695",
                    "issueId": "13292546"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/worklog/408696",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #6675: ARROW-8151: [Dataset][Benchmarking] benchmark S3File performance\nURL: https://github.com/apache/arrow/pull/6675#discussion_r397105161\n \n \n\n ##########\n File path: cpp/src/arrow/filesystem/s3fs_benchmark.cc\n ##########\n @@ -0,0 +1,344 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <memory>\n+#include <sstream>\n+#include <utility>\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include <aws/core/auth/AWSCredentials.h>\n+#include <aws/s3/S3Client.h>\n+#include <aws/s3/model/CreateBucketRequest.h>\n+#include <aws/s3/model/HeadBucketRequest.h>\n+#include <aws/s3/model/PutObjectRequest.h>\n+\n+#include \"arrow/filesystem/s3_internal.h\"\n+#include \"arrow/filesystem/s3_test_util.h\"\n+#include \"arrow/filesystem/s3fs.h\"\n+#include \"arrow/io/caching.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/properties.h\"\n+\n+namespace arrow {\n+namespace fs {\n+\n+using ::arrow::fs::internal::ConnectRetryStrategy;\n+using ::arrow::fs::internal::OutcomeToStatus;\n+using ::arrow::fs::internal::ToAwsString;\n+\n+// Environment variables to configure the S3 test environment\n+static const char* kEnvBucketName = \"ARROW_TEST_S3_BUCKET\";\n+static const char* kEnvSkipSetup = \"ARROW_TEST_S3_SKIP_SETUP\";\n+static const char* kEnvAwsRegion = \"ARROW_TEST_S3_REGION\";\n+\n+// Set up Minio and create the test bucket and files.\n+class MinioFixture : public benchmark::Fixture {\n+ public:\n+  void SetUp(const ::benchmark::State& state) override {\n+    ASSERT_OK(minio_.Start());\n+\n+    const char* region_str = std::getenv(kEnvAwsRegion);\n+    if (region_str) {\n+      region_ = region_str;\n+      std::cerr << \"Using region from environment: \" << region_ << std::endl;\n+    } else {\n+      std::cerr << \"Using default region\" << std::endl;\n+    }\n+\n+    const char* bucket_str = std::getenv(kEnvBucketName);\n+    if (bucket_str) {\n+      bucket_ = bucket_str;\n+      std::cerr << \"Using bucket from environment: \" << bucket_ << std::endl;\n+    } else {\n+      bucket_ = \"bucket\";\n+      std::cerr << \"Using default bucket: \" << bucket_ << std::endl;\n+    }\n+\n+    client_config_.endpointOverride = ToAwsString(minio_.connect_string());\n+    client_config_.scheme = Aws::Http::Scheme::HTTP;\n+    if (!region_.empty()) {\n+      client_config_.region = ToAwsString(region_);\n+    }\n+    client_config_.retryStrategy = std::make_shared<ConnectRetryStrategy>();\n+    credentials_ = {ToAwsString(minio_.access_key()), ToAwsString(minio_.secret_key())};\n+    bool use_virtual_addressing = false;\n+    client_.reset(\n+        new Aws::S3::S3Client(credentials_, client_config_,\n+                              Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n+                              use_virtual_addressing));\n+\n+    MakeFileSystem();\n+\n+    const char* skip_str = std::getenv(kEnvSkipSetup);\n+    const std::string skip = skip_str ? std::string(skip_str) : \"\";\n+    if (skip == \"true\") {\n+      std::cerr << \"Skipping creation of bucket/objects as requested\" << std::endl;\n+    } else {\n+      ASSERT_OK(MakeBucket());\n+      ASSERT_OK(MakeObject(\"bytes_1mib\", 1024 * 1024));\n+      ASSERT_OK(MakeObject(\"bytes_100mib\", 100 * 1024 * 1024));\n+      ASSERT_OK(MakeObject(\"bytes_500mib\", 500 * 1024 * 1024));\n+      ASSERT_OK(MakeParquetObject(bucket_ + \"/pq_c100_r250k\", 100, 250000));\n+    }\n+  }\n+\n+  void MakeFileSystem() {\n+    options_.ConfigureAccessKey(minio_.access_key(), minio_.secret_key());\n+    options_.scheme = \"http\";\n+    if (!region_.empty()) {\n+      options_.region = region_;\n+    }\n+    options_.endpoint_override = minio_.connect_string();\n+    ASSERT_OK_AND_ASSIGN(fs_, S3FileSystem::Make(options_));\n+  }\n+\n+  /// Set up bucket if it doesn't exist.\n+  ///\n+  /// When using Minio we'll have a fresh setup each time, but\n+  /// otherwise we may have a leftover bucket.\n+  Status MakeBucket() {\n+    Aws::S3::Model::HeadBucketRequest head;\n+    head.SetBucket(ToAwsString(bucket_));\n+    const Status st = OutcomeToStatus(client_->HeadBucket(head));\n+    if (st.ok()) {\n+      // Bucket exists already\n+      return st;\n+    }\n+    Aws::S3::Model::CreateBucketRequest req;\n+    req.SetBucket(ToAwsString(bucket_));\n+    return OutcomeToStatus(client_->CreateBucket(req));\n+  }\n+\n+  /// Make an object with dummy data.\n+  Status MakeObject(const std::string& name, int size) {\n+    Aws::S3::Model::PutObjectRequest req;\n+    req.SetBucket(ToAwsString(bucket_));\n+    req.SetKey(ToAwsString(name));\n+    req.SetBody(std::make_shared<std::stringstream>(std::string(size, 'a')));\n+    return OutcomeToStatus(client_->PutObject(req));\n+  }\n+\n+  /// Make an object with Parquet data.\n+  Status MakeParquetObject(const std::string& path, int num_columns, int num_rows) {\n+    std::vector<std::shared_ptr<ChunkedArray>> columns(num_columns);\n+    std::vector<std::shared_ptr<Field>> fields(num_columns);\n+    for (int i = 0; i < num_columns; ++i) {\n+      arrow::random::RandomArrayGenerator generator(i);\n+      std::shared_ptr<Array> values = generator.Float64(num_rows, -1.e10, 1e10, 0);\n+      std::stringstream ss;\n+      ss << \"col\" << i;\n+      columns[i] = std::make_shared<ChunkedArray>(values);\n+      fields[i] = ::arrow::field(ss.str(), values->type());\n+    }\n+    auto schema = std::make_shared<::arrow::Schema>(fields);\n+\n+    std::shared_ptr<Table> table = Table::Make(schema, columns);\n+\n+    std::shared_ptr<io::OutputStream> sink;\n+    ARROW_ASSIGN_OR_RAISE(sink, fs_->OpenOutputStream(path));\n+    auto writer_properties =\n+        parquet::WriterProperties::Builder().write_batch_size(128)->build();\n+    auto arrow_properties = parquet::default_arrow_writer_properties();\n+    RETURN_NOT_OK(parquet::arrow::WriteTable(*table, arrow::default_memory_pool(), sink,\n+                                             num_rows, writer_properties,\n+                                             arrow_properties));\n+\n+    return Status::OK();\n+  }\n+\n+  void TearDown(const ::benchmark::State& state) override { ASSERT_OK(minio_.Stop()); }\n+\n+ protected:\n+  MinioTestServer minio_;\n+  std::string region_;\n+  std::string bucket_;\n+  Aws::Client::ClientConfiguration client_config_;\n+  Aws::Auth::AWSCredentials credentials_;\n+  std::unique_ptr<Aws::S3::S3Client> client_;\n+  S3Options options_;\n+  std::shared_ptr<S3FileSystem> fs_;\n+};\n+\n+/// Set up/tear down the AWS SDK globally.\n+/// (GBenchmark doesn't run GTest environments.)\n+class S3BenchmarkEnvironment {\n+ public:\n+  S3BenchmarkEnvironment() { s3_env->SetUp(); }\n+  ~S3BenchmarkEnvironment() { s3_env->TearDown(); }\n+};\n+\n+S3BenchmarkEnvironment env{};\n+\n+/// Read the entire file into memory in one go to measure bandwidth.\n+static void NaiveRead(benchmark::State& st, S3FileSystem* fs, const std::string& path) {\n+  int64_t total_bytes = 0;\n+  int total_items = 0;\n+  for (auto _ : st) {\n+    std::shared_ptr<io::RandomAccessFile> file;\n+    std::shared_ptr<Buffer> buf;\n+    int64_t size;\n+    ASSERT_OK_AND_ASSIGN(file, fs->OpenInputFile(path));\n+    ASSERT_OK_AND_ASSIGN(size, file->GetSize());\n+    ASSERT_OK_AND_ASSIGN(buf, file->ReadAt(0, size));\n+    total_bytes += buf->size();\n+    total_items += 1;\n+  }\n+  st.SetBytesProcessed(total_bytes);\n+  st.SetItemsProcessed(total_items);\n+  std::cout << \"Read the file \" << total_items << \" times\" << std::endl;\n \n Review comment:\n   An oversight on my part; I've made these all cerr.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-03-24T12:11:51.040+0000",
                    "updated": "2020-03-24T12:11:51.040+0000",
                    "started": "2020-03-24T12:11:51.040+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "408696",
                    "issueId": "13292546"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 13800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@23e2f2c[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3fadceb2[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3da2d088[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@6c2b51d0[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@582404f6[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@20c319d2[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@bceabd2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@3d5ddeaa[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@45a66e43[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@7d93f2ac[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@37d46f7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@500670bc[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 13800,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Mar 26 10:19:09 UTC 2020",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2020-03-26T10:19:09.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-8151/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2020-03-18T18:29:42.000+0000",
        "updated": "2020-04-10T15:31:33.000+0000",
        "timeoriginalestimate": null,
        "description": "We should establish a performance baseline with the current S3File implementation and Parquet reader before proceeding with work like PARQUET-1698.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "3h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 13800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Benchmarking][Dataset] Benchmark Parquet read performance with S3File",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13292546/comment/17067573",
                    "id": "17067573",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "Issue resolved by pull request 6675\n[https://github.com/apache/arrow/pull/6675]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2020-03-26T10:19:09.088+0000",
                    "updated": "2020-03-26T10:19:09.088+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0co8w:",
        "customfield_12314139": null
    }
}