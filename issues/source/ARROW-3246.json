{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13185395",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395",
    "key": "ARROW-3246",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12345978",
                "id": "12345978",
                "description": "",
                "name": "0.15.0",
                "archived": false,
                "released": true,
                "releaseDate": "2019-10-05"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
            "name": "Minor",
            "id": "4"
        },
        "labels": [
            "parquet",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12563572",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12563572",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13165847",
                    "key": "ARROW-3772",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13165847",
                    "fields": {
                        "summary": "[C++] Read Parquet dictionary encoded ColumnChunks directly into an Arrow DictionaryArray",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12566927",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12566927",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13195080",
                    "key": "ARROW-3652",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13195080",
                    "fields": {
                        "summary": "[Python] CategoricalIndex is lost after reading back",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "id": "1",
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "name": "Bug",
                            "subtask": false,
                            "avatarId": 21133
                        }
                    }
                }
            },
            {
                "id": "12567904",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12567904",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13225448",
                    "key": "ARROW-5089",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13225448",
                    "fields": {
                        "summary": "[C++/Python] Writing dictionary encoded columns to parquet is extremely slow when using chunk size",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "id": "1",
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "name": "Bug",
                            "subtask": false,
                            "avatarId": 21133
                        }
                    }
                }
            },
            {
                "id": "12567838",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12567838",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13028176",
                    "key": "PARQUET-800",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13028176",
                    "fields": {
                        "summary": "[C++] Provide public API to access dictionary-encoded indices and values",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            },
            {
                "id": "12567839",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12567839",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13059181",
                    "key": "PARQUET-924",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13059181",
                    "fields": {
                        "summary": "[C++] Persist original type metadata from Arrow schemas",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            },
            {
                "id": "12563573",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12563573",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13187263",
                    "key": "ARROW-3325",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13187263",
                    "fields": {
                        "summary": "[Python] Support reading Parquet binary/string columns directly as DictionaryArray",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12562184",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12562184",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13237110",
                    "key": "ARROW-5480",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13237110",
                    "fields": {
                        "summary": "[Python] Pandas categorical type doesn't survive a round-trip through parquet",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                            "name": "Minor",
                            "id": "4"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12608711",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12608711",
                "type": {
                    "id": "12310051",
                    "name": "Supercedes",
                    "inward": "is superceded by",
                    "outward": "supercedes",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310051"
                },
                "outwardIssue": {
                    "id": "13211535",
                    "key": "ARROW-4359",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13211535",
                    "fields": {
                        "summary": "[Python] Column metadata is not saved or loaded in parquet",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "id": "1",
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "name": "Bug",
                            "subtask": false,
                            "avatarId": 21133
                        }
                    }
                }
            },
            {
                "id": "12567182",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12567182",
                "type": {
                    "id": "10001",
                    "name": "dependent",
                    "inward": "is depended upon by",
                    "outward": "depends upon",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10001"
                },
                "outwardIssue": {
                    "id": "13249219",
                    "key": "ARROW-6152",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13249219",
                    "fields": {
                        "summary": "[C++][Parquet] Write arrow::Array directly into parquet::TypedColumnWriter<T>",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "id": "1",
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "name": "Bug",
                            "subtask": false,
                            "avatarId": 21133
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
            "name": "mdurant",
            "key": "mdurant",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Martin Durant",
            "active": true,
            "timeZone": "America/Montreal"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
            "name": "mdurant",
            "key": "mdurant",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Martin Durant",
            "active": true,
            "timeZone": "America/Montreal"
        },
        "aggregateprogress": {
            "progress": 34800,
            "total": 34800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 34800,
            "total": 34800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-3246/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 64,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/294134",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077\n \n \n   There's a lot going of interconnected pieces in this patch, so let me try to unpack:\r\n   \r\n   * Refactor TypedColumnWriterImpl::WriteBatch/WriteBatchSpaced to utilize more common code and be more readable\r\n   * Add `TypedEncoder<T>::Put(const arrow::Array&)` and implement for BYTE_ARRAY so avoid having to first create `ByteArray*` as before. This should improve write performance for regular binary data -- I will do some benchmarks to measure by how much\r\n   * Add `TypedStatistics<T>::Update(const arrow::Array&)` and implement for BYTE_ARRAY. This is necessary to be able to update the statistics given directly-inserted Arrow data without serialization\r\n   * Implement `PutDictionary` and `PutIndices` methods on `DictEncoder`. `PutDictionary` is only implemented for BYTE_ARRAY but can be easily generalized to more types (we should open a follow up JIRA for this)\r\n   * Implement internal `TypedColumnWriterImpl::WriteArrowDictionary` that writes dictionary values and indices directly into a DictEncoder. This circumvents the dictionary page size checks so that we will continue to call `PutIndices` until a new dictionary is encountered or some non-dictionary data is written\r\n   * Add `ArrowWriterProperties::store_schema()` option which stores the Arrow schema used to create a Parquet file in a special `ARROW:schema` key in the metadata, so that we can detect that a column was originally DictionaryArray. This option is off by default, but enabled in the Python bindings. We can always make it the default in the future\r\n   \r\n   I think that's most things. One end result of this is that `arrow::DictionaryArray` types from C++ and `pandas.Categorical` types coerced from pandas with string dictionary values will be accurately preserved end-to-end. With a little more work (which I think can be done in a follow up PR) we can support the rest of the Parquet physical types.\r\n   \r\n   This was a fairly ugly project and I've doubtlessly left some messes around that we should clean up, but perhaps in follow up patches. \r\n   \r\n   I'll post some benchmarks later to assess improvements in read and write performance. In the case of writing dictionary-encoded strings the boost should be significant. \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-13T19:56:34.893+0000",
                    "updated": "2019-08-13T19:56:34.893+0000",
                    "started": "2019-08-13T19:56:34.893+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "294134",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/294434",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#issuecomment-521099675\n \n \n   Looks like I have an ASAN failure and some compiler warnings on Windows. Will investigate tomorrow\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-14T04:45:25.958+0000",
                    "updated": "2019-08-14T04:45:25.958+0000",
                    "started": "2019-08-14T04:45:25.958+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "294434",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/294961",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#issuecomment-521384708\n \n \n   OK I think I've fixed the issues that popped up. I know there's a lot to review here, so we may have to leave many of the refinements to follow up work\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-14T19:27:31.568+0000",
                    "updated": "2019-08-14T19:27:31.568+0000",
                    "started": "2019-08-14T19:27:31.567+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "294961",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295388",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314186432\n \n \n\n ##########\n File path: cpp/src/parquet/arrow/reader_internal.cc\n ##########\n @@ -533,9 +536,56 @@ Status NodeToSchemaField(const Node& node, int16_t max_def_level, int16_t max_re\n   }\n }\n \n+Status GetOriginSchema(const std::shared_ptr<const KeyValueMetadata>& metadata,\n+                       std::shared_ptr<const KeyValueMetadata>* clean_metadata,\n+                       std::shared_ptr<::arrow::Schema>* out) {\n+  if (metadata == nullptr) {\n+    *out = nullptr;\n+    *clean_metadata = nullptr;\n+    return Status::OK();\n+  }\n+\n+  static const std::string kArrowSchemaKey = \"ARROW:schema\";\n+  int schema_index = metadata->FindKey(kArrowSchemaKey);\n+  if (schema_index == -1) {\n+    *out = nullptr;\n+    *clean_metadata = metadata;\n+    return Status::OK();\n+  }\n+\n+  // The original Arrow schema was serialized using the store_schema option. We\n+  // deserialize it here and use it to inform read options such as\n+  // dictionary-encoded fields\n+  auto schema_buf = std::make_shared<Buffer>(metadata->value(schema_index));\n+\n+  ::arrow::ipc::DictionaryMemo dict_memo;\n+  ::arrow::io::BufferReader input(schema_buf);\n+  RETURN_NOT_OK(::arrow::ipc::ReadSchema(&input, &dict_memo, out));\n \n Review comment:\n   It would be worth adding a check for `ARROW_IPC` as a cmake option dependency of `ARROW_PARQUET`.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:19.815+0000",
                    "updated": "2019-08-15T11:23:19.815+0000",
                    "started": "2019-08-15T11:23:19.814+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295388",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295389",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314187577\n \n \n\n ##########\n File path: cpp/src/parquet/arrow/writer.cc\n ##########\n @@ -444,28 +456,6 @@ class FileWriterImpl : public FileWriter {\n \n   Status WriteColumnChunk(const std::shared_ptr<ChunkedArray>& data, int64_t offset,\n                           int64_t size) override {\n-    // DictionaryArrays are not yet handled with a fast path. To still support\n \n Review comment:\n   Nice to see this go.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:20.388+0000",
                    "updated": "2019-08-15T11:23:20.388+0000",
                    "started": "2019-08-15T11:23:20.387+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295389",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295390",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314187205\n \n \n\n ##########\n File path: cpp/src/parquet/arrow/reader_internal.cc\n ##########\n @@ -544,8 +594,24 @@ Status BuildSchemaManifest(const SchemaDescriptor* schema,\n   manifest->descr = schema;\n   manifest->schema_fields.resize(schema_node.field_count());\n   for (int i = 0; i < static_cast<int>(schema_node.field_count()); ++i) {\n+    SchemaField* out_field = &manifest->schema_fields[i];\n     RETURN_NOT_OK(NodeToSchemaField(*schema_node.field(i), 0, 0, &ctx,\n-                                    /*parent=*/nullptr, &manifest->schema_fields[i]));\n+                                    /*parent=*/nullptr, out_field));\n+\n+    // TODO(wesm): as follow up to ARROW-3246, we should really pass the origin\n+    // schema (if any) through all functions in the schema reconstruction, but\n+    // I'm being lazy and just setting dictionary fields at the top level for\n+    // now\n+    if (manifest->origin_schema) {\n+      auto origin_field = manifest->origin_schema->field(i);\n+      auto current_type = out_field->field->type();\n+      if (origin_field->type()->id() == ::arrow::Type::DICTIONARY) {\n+        if (current_type->id() != ::arrow::Type::DICTIONARY) {\n+          out_field->field = out_field->field->WithType(\n+              ::arrow::dictionary(::arrow::int32(), current_type));\n \n Review comment:\n   This appears to fix the indices to `arrow::int32` types.  Is that a short-term limitation or is there something more basic that requires this?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:20.591+0000",
                    "updated": "2019-08-15T11:23:20.591+0000",
                    "started": "2019-08-15T11:23:20.590+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295390",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295391",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314259027\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -768,194 +925,124 @@ void TypedColumnWriterImpl<DType>::CheckDictionarySizeLimit() {\n                                    properties_->memory_pool());\n     encoding_ = Encoding::PLAIN;\n   }\n-}\n \n-// ----------------------------------------------------------------------\n-// Instantiate templated classes\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatch(int64_t num_values,\n-                                                     const int16_t* def_levels,\n-                                                     const int16_t* rep_levels,\n-                                                     const T* values) {\n-  int64_t values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n+  // Checks if the Dictionary Page size limit is reached\n+  // If the limit is reached, the Dictionary and Data Pages are serialized\n+  // The encoding is switched to PLAIN\n+  //\n+  // Only one Dictionary Page is written.\n+  // Fallback to PLAIN if dictionary page limit is reached.\n+  void CheckDictionarySizeLimit() {\n+    if (!has_dictionary_ || fallback_) {\n+      // Either not using dictionary encoding, or we have already fallen back\n+      // to PLAIN encoding because the size threshold was reached\n+      return;\n     }\n \n-    WriteDefinitionLevels(num_values, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_values;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    // We have to dynamic cast here because TypedEncoder<Type> as some compilers\n+    // don't want to cast through virtual inheritance\n+    auto dict_encoder = dynamic_cast<DictEncoder<DType>*>(current_encoder_.get());\n+    if (dict_encoder->dict_encoded_size() >= properties_->dictionary_pagesize_limit()) {\n+      FallbackToPlainEncoding();\n     }\n-\n-    WriteRepetitionLevels(num_values, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_values);\n   }\n \n-  // PARQUET-780\n-  if (values_to_write > 0) {\n-    DCHECK(nullptr != values) << \"Values ptr cannot be NULL\";\n-  }\n-\n-  WriteValues(values_to_write, values);\n-\n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->Update(values, values_to_write, num_values - values_to_write);\n-  }\n-\n-  num_buffered_values_ += num_values;\n-  num_buffered_encoded_values_ += values_to_write;\n-\n-  if (current_encoder_->EstimatedDataEncodedSize() >= properties_->data_pagesize()) {\n-    AddDataPage();\n-  }\n-  if (has_dictionary_ && !fallback_) {\n-    CheckDictionarySizeLimit();\n+  void WriteValues(const T* values, int64_t num_values, int64_t num_nulls) {\n+    dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+        ->Put(values, static_cast<int>(num_values));\n+    if (page_statistics_ != nullptr) {\n+      page_statistics_->Update(values, num_values, num_nulls);\n+    }\n   }\n \n-  return values_to_write;\n-}\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatchSpaced(\n-    int64_t num_levels, const int16_t* def_levels, const int16_t* rep_levels,\n-    const uint8_t* valid_bits, int64_t valid_bits_offset, const T* values,\n-    int64_t* num_spaced_written) {\n-  int64_t values_to_write = 0;\n-  int64_t spaced_values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    // Minimal definition level for which spaced values are written\n-    int16_t min_spaced_def_level = descr_->max_definition_level();\n+  void WriteValuesSpaced(const T* values, int64_t num_values, int64_t num_spaced_values,\n+                         const uint8_t* valid_bits, int64_t valid_bits_offset) {\n     if (descr_->schema_node()->is_optional()) {\n-      min_spaced_def_level--;\n-    }\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n-      if (def_levels[i] >= min_spaced_def_level) {\n-        ++spaced_values_to_write;\n-      }\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->PutSpaced(values, static_cast<int>(num_spaced_values), valid_bits,\n+                      valid_bits_offset);\n+    } else {\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->Put(values, static_cast<int>(num_values));\n     }\n-\n-    WriteDefinitionLevels(num_levels, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_levels;\n-    spaced_values_to_write = num_levels;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    if (page_statistics_ != nullptr) {\n+      const int64_t num_nulls = num_spaced_values - num_values;\n+      page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, num_values,\n+                                     num_nulls);\n     }\n-\n-    WriteRepetitionLevels(num_levels, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_levels);\n   }\n+};\n \n-  if (descr_->schema_node()->is_optional()) {\n-    WriteValuesSpaced(spaced_values_to_write, valid_bits, valid_bits_offset, values);\n-  } else {\n-    WriteValues(values_to_write, values);\n-  }\n-  *num_spaced_written = spaced_values_to_write;\n+template <typename DType>\n+Status TypedColumnWriterImpl<DType>::WriteArrowDictionary(const int16_t* def_levels,\n+                                                          const int16_t* rep_levels,\n+                                                          int64_t num_levels,\n+                                                          const arrow::Array& array,\n+                                                          ArrowWriteContext* ctx) {\n+  // If this is the first time writing a DictionaryArray, then there's\n+  // a few possible paths to take:\n+  //\n+  // - If dictionary encoding is not enabled, convert to densely\n+  //   encoded and call WriteArrow\n+  // - Dictionary encoding enabled\n+  //   - If this is the first time this is called, then we call\n+  //     PutDictionary into the encoder and then PutIndices on each\n+  //     chunk. We store the dictionary that was written in\n+  //     preserved_dictionary_ so that subsequent calls to this method\n+  //     can make sure the dictionary has not changed\n+  //   - On subsequent calls, we have to check whether the dictionary\n+  //     has changed. If it has, then we trigger the varying\n+  //     dictionary path and materialize each chunk and then call\n+  //     WriteArrow with that\n+  auto WriteDense = [&] {\n+    std::shared_ptr<arrow::Array> dense_array;\n+    RETURN_NOT_OK(MaterializeDictionary(array, properties_->memory_pool(), &dense_array));\n+    return WriteArrowDense(def_levels, rep_levels, num_levels, *dense_array, ctx);\n+  };\n \n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, values_to_write,\n-                                   spaced_values_to_write - values_to_write);\n+  if (current_encoder_->encoding() != Encoding::PLAIN_DICTIONARY ||\n \n Review comment:\n   I think this should also check for `Encoding::RLE_DICTIONARY` used in v2 or possibly reuse this logic from [`column_reader.cc`](https://github.com/apache/arrow/blob/091b25d91cc710d6b56033cc52f0886509be2945/cpp/src/parquet/column_reader.cc#L277):\r\n   \r\n   ```cpp\r\n   // PLAIN_DICTIONARY is deprecated but used to be used as a dictionary index\r\n   // encoding.\r\n   static bool IsDictionaryIndexEncoding(const Encoding::type& e) {\r\n     return e == Encoding::RLE_DICTIONARY || e == Encoding::PLAIN_DICTIONARY;\r\n   }\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:20.635+0000",
                    "updated": "2019-08-15T11:23:20.635+0000",
                    "started": "2019-08-15T11:23:20.634+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295391",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295392",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314256477\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -630,6 +632,45 @@ void ColumnWriterImpl::FlushBufferedDataPages() {\n // ----------------------------------------------------------------------\n // TypedColumnWriter\n \n+template <typename Action>\n+inline void DoInBatches(int64_t total, int64_t batch_size, Action&& action) {\n+  int64_t num_batches = static_cast<int>(total / batch_size);\n+  for (int round = 0; round < num_batches; round++) {\n+    action(round * batch_size, batch_size);\n+  }\n+  // Write the remaining values\n+  if (total % batch_size > 0) {\n+    action(num_batches * batch_size, total % batch_size);\n+  }\n+}\n+\n+bool DictionaryDirectWriteSupported(const arrow::Array& array) {\n+  const arrow::DictionaryType& dict_type =\n+      static_cast<const arrow::DictionaryType&>(*array.type());\n+  auto id = dict_type.value_type()->id();\n+  return id == arrow::Type::BINARY || id == arrow::Type::STRING;\n+}\n+\n+Status MaterializeDictionary(const arrow::Array& array, MemoryPool* pool,\n \n Review comment:\n   I had to look at where this is called to understand what it did.  Perhaps rename to `MaterializeDenseFromDictionary` ? \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:20.654+0000",
                    "updated": "2019-08-15T11:23:20.654+0000",
                    "started": "2019-08-15T11:23:20.654+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295392",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295393",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314261047\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -768,194 +925,124 @@ void TypedColumnWriterImpl<DType>::CheckDictionarySizeLimit() {\n                                    properties_->memory_pool());\n     encoding_ = Encoding::PLAIN;\n   }\n-}\n \n-// ----------------------------------------------------------------------\n-// Instantiate templated classes\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatch(int64_t num_values,\n-                                                     const int16_t* def_levels,\n-                                                     const int16_t* rep_levels,\n-                                                     const T* values) {\n-  int64_t values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n+  // Checks if the Dictionary Page size limit is reached\n+  // If the limit is reached, the Dictionary and Data Pages are serialized\n+  // The encoding is switched to PLAIN\n+  //\n+  // Only one Dictionary Page is written.\n+  // Fallback to PLAIN if dictionary page limit is reached.\n+  void CheckDictionarySizeLimit() {\n+    if (!has_dictionary_ || fallback_) {\n+      // Either not using dictionary encoding, or we have already fallen back\n+      // to PLAIN encoding because the size threshold was reached\n+      return;\n     }\n \n-    WriteDefinitionLevels(num_values, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_values;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    // We have to dynamic cast here because TypedEncoder<Type> as some compilers\n+    // don't want to cast through virtual inheritance\n+    auto dict_encoder = dynamic_cast<DictEncoder<DType>*>(current_encoder_.get());\n+    if (dict_encoder->dict_encoded_size() >= properties_->dictionary_pagesize_limit()) {\n+      FallbackToPlainEncoding();\n     }\n-\n-    WriteRepetitionLevels(num_values, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_values);\n   }\n \n-  // PARQUET-780\n-  if (values_to_write > 0) {\n-    DCHECK(nullptr != values) << \"Values ptr cannot be NULL\";\n-  }\n-\n-  WriteValues(values_to_write, values);\n-\n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->Update(values, values_to_write, num_values - values_to_write);\n-  }\n-\n-  num_buffered_values_ += num_values;\n-  num_buffered_encoded_values_ += values_to_write;\n-\n-  if (current_encoder_->EstimatedDataEncodedSize() >= properties_->data_pagesize()) {\n-    AddDataPage();\n-  }\n-  if (has_dictionary_ && !fallback_) {\n-    CheckDictionarySizeLimit();\n+  void WriteValues(const T* values, int64_t num_values, int64_t num_nulls) {\n+    dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+        ->Put(values, static_cast<int>(num_values));\n+    if (page_statistics_ != nullptr) {\n+      page_statistics_->Update(values, num_values, num_nulls);\n+    }\n   }\n \n-  return values_to_write;\n-}\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatchSpaced(\n-    int64_t num_levels, const int16_t* def_levels, const int16_t* rep_levels,\n-    const uint8_t* valid_bits, int64_t valid_bits_offset, const T* values,\n-    int64_t* num_spaced_written) {\n-  int64_t values_to_write = 0;\n-  int64_t spaced_values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    // Minimal definition level for which spaced values are written\n-    int16_t min_spaced_def_level = descr_->max_definition_level();\n+  void WriteValuesSpaced(const T* values, int64_t num_values, int64_t num_spaced_values,\n+                         const uint8_t* valid_bits, int64_t valid_bits_offset) {\n     if (descr_->schema_node()->is_optional()) {\n-      min_spaced_def_level--;\n-    }\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n-      if (def_levels[i] >= min_spaced_def_level) {\n-        ++spaced_values_to_write;\n-      }\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->PutSpaced(values, static_cast<int>(num_spaced_values), valid_bits,\n+                      valid_bits_offset);\n+    } else {\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->Put(values, static_cast<int>(num_values));\n     }\n-\n-    WriteDefinitionLevels(num_levels, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_levels;\n-    spaced_values_to_write = num_levels;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    if (page_statistics_ != nullptr) {\n+      const int64_t num_nulls = num_spaced_values - num_values;\n+      page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, num_values,\n+                                     num_nulls);\n     }\n-\n-    WriteRepetitionLevels(num_levels, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_levels);\n   }\n+};\n \n-  if (descr_->schema_node()->is_optional()) {\n-    WriteValuesSpaced(spaced_values_to_write, valid_bits, valid_bits_offset, values);\n-  } else {\n-    WriteValues(values_to_write, values);\n-  }\n-  *num_spaced_written = spaced_values_to_write;\n+template <typename DType>\n+Status TypedColumnWriterImpl<DType>::WriteArrowDictionary(const int16_t* def_levels,\n+                                                          const int16_t* rep_levels,\n+                                                          int64_t num_levels,\n+                                                          const arrow::Array& array,\n+                                                          ArrowWriteContext* ctx) {\n+  // If this is the first time writing a DictionaryArray, then there's\n+  // a few possible paths to take:\n+  //\n+  // - If dictionary encoding is not enabled, convert to densely\n+  //   encoded and call WriteArrow\n+  // - Dictionary encoding enabled\n+  //   - If this is the first time this is called, then we call\n+  //     PutDictionary into the encoder and then PutIndices on each\n+  //     chunk. We store the dictionary that was written in\n+  //     preserved_dictionary_ so that subsequent calls to this method\n+  //     can make sure the dictionary has not changed\n+  //   - On subsequent calls, we have to check whether the dictionary\n+  //     has changed. If it has, then we trigger the varying\n+  //     dictionary path and materialize each chunk and then call\n+  //     WriteArrow with that\n+  auto WriteDense = [&] {\n+    std::shared_ptr<arrow::Array> dense_array;\n+    RETURN_NOT_OK(MaterializeDictionary(array, properties_->memory_pool(), &dense_array));\n+    return WriteArrowDense(def_levels, rep_levels, num_levels, *dense_array, ctx);\n+  };\n \n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, values_to_write,\n-                                   spaced_values_to_write - values_to_write);\n+  if (current_encoder_->encoding() != Encoding::PLAIN_DICTIONARY ||\n+      !DictionaryDirectWriteSupported(array)) {\n+    // No longer dictionary-encoding for whatever reason, maybe we\n+    // never were or we decided to stop\n+    return WriteDense();\n   }\n \n-  num_buffered_values_ += num_levels;\n-  num_buffered_encoded_values_ += values_to_write;\n+  auto dict_encoder = dynamic_cast<DictEncoder<DType>*>(current_encoder_.get());\n+  const auto& data = checked_cast<const arrow::DictionaryArray&>(array);\n+  std::shared_ptr<arrow::Array> dictionary = data.dictionary();\n+  std::shared_ptr<arrow::Array> indices = data.indices();\n \n-  if (current_encoder_->EstimatedDataEncodedSize() >= properties_->data_pagesize()) {\n-    AddDataPage();\n-  }\n-  if (has_dictionary_ && !fallback_) {\n-    CheckDictionarySizeLimit();\n-  }\n+  int64_t value_offset = 0;\n+  auto WriteIndicesChunk = [&](int64_t offset, int64_t batch_size) {\n+    int64_t batch_num_values = 0;\n+    int64_t batch_num_spaced_values = 0;\n+    WriteLevelsSpaced(batch_size, def_levels + offset, rep_levels + offset,\n+                      &batch_num_values, &batch_num_spaced_values);\n+    dict_encoder->PutIndices(*indices->Slice(value_offset, batch_num_spaced_values));\n+    CommitWriteAndCheckPageLimit(batch_size, batch_num_values);\n+    value_offset += batch_num_spaced_values;\n+  };\n \n-  return values_to_write;\n-}\n+  // Handle seeing dictionary for the first time\n+  if (!preserved_dictionary_) {\n+    // It's a new dictionary. Call PutDictionary and keep track of it\n+    PARQUET_CATCH_NOT_OK(dict_encoder->PutDictionary(*dictionary));\n \n-template <typename DType>\n-void TypedColumnWriterImpl<DType>::WriteBatch(int64_t num_values,\n-                                              const int16_t* def_levels,\n-                                              const int16_t* rep_levels,\n-                                              const T* values) {\n-  // We check for DataPage limits only after we have inserted the values. If a user\n-  // writes a large number of values, the DataPage size can be much above the limit.\n-  // The purpose of this chunking is to bound this. Even if a user writes large number\n-  // of values, the chunking will ensure the AddDataPage() is called at a reasonable\n-  // pagesize limit\n-  int64_t write_batch_size = properties_->write_batch_size();\n-  int num_batches = static_cast<int>(num_values / write_batch_size);\n-  int64_t num_remaining = num_values % write_batch_size;\n-  int64_t value_offset = 0;\n-  for (int round = 0; round < num_batches; round++) {\n-    int64_t offset = round * write_batch_size;\n-    int64_t num_values = WriteMiniBatch(write_batch_size, &def_levels[offset],\n-                                        &rep_levels[offset], &values[value_offset]);\n-    value_offset += num_values;\n+    // TODO(wesm): If some dictionary values are unobserved, then the\n+    // statistics will be inaccurate. Do we care enough to fix it?\n+    if (page_statistics_ != nullptr) {\n+      PARQUET_CATCH_NOT_OK(page_statistics_->Update(*dictionary));\n+    }\n+    preserved_dictionary_ = dictionary;\n+  } else if (!dictionary->Equals(*preserved_dictionary_)) {\n+    // Dictionary has changed\n+    PARQUET_CATCH_NOT_OK(FallbackToPlainEncoding());\n \n Review comment:\n   The earlier comment makes sense now. \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:20.687+0000",
                    "updated": "2019-08-15T11:23:20.687+0000",
                    "started": "2019-08-15T11:23:20.686+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295393",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295394",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314257873\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -645,23 +686,64 @@ class TypedColumnWriterImpl : public ColumnWriterImpl, public TypedColumnWriter<\n \n     if (properties->statistics_enabled(descr_->path()) &&\n         (SortOrder::UNKNOWN != descr_->sort_order())) {\n-      page_statistics_ = TypedStats::Make(descr_, allocator_);\n-      chunk_statistics_ = TypedStats::Make(descr_, allocator_);\n+      page_statistics_ = MakeStatistics<DType>(descr_, allocator_);\n+      chunk_statistics_ = MakeStatistics<DType>(descr_, allocator_);\n     }\n   }\n \n   int64_t Close() override { return ColumnWriterImpl::Close(); }\n \n   void WriteBatch(int64_t num_values, const int16_t* def_levels,\n-                  const int16_t* rep_levels, const T* values) override;\n+                  const int16_t* rep_levels, const T* values) override {\n+    // We check for DataPage limits only after we have inserted the values. If a user\n+    // writes a large number of values, the DataPage size can be much above the limit.\n+    // The purpose of this chunking is to bound this. Even if a user writes large number\n+    // of values, the chunking will ensure the AddDataPage() is called at a reasonable\n+    // pagesize limit\n+    int64_t value_offset = 0;\n+    auto WriteChunk = [&](int64_t offset, int64_t batch_size) {\n+      int64_t values_to_write =\n+          WriteLevels(batch_size, def_levels + offset, rep_levels + offset);\n+      // PARQUET-780\n+      if (values_to_write > 0) {\n+        DCHECK_NE(nullptr, values);\n+      }\n+      WriteValues(values + value_offset, values_to_write, batch_size - values_to_write);\n+      CommitWriteAndCheckPageLimit(batch_size, values_to_write);\n+      CheckDictionarySizeLimit();\n \n Review comment:\n   I think it makes sense to move the call to `CheckDictionarySizeLimit` into the `CommitWriteAndCheckPageLimit` function.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:20.696+0000",
                    "updated": "2019-08-15T11:23:20.696+0000",
                    "started": "2019-08-15T11:23:20.695+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295394",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295396",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314264911\n \n \n\n ##########\n File path: cpp/src/parquet/encoding.cc\n ##########\n @@ -479,25 +557,61 @@ inline void DictEncoderImpl<FLBAType>::Put(const FixedLenByteArray& v) {\n   buffered_indices_.push_back(memo_index);\n }\n \n-class DictByteArrayEncoder : public DictEncoderImpl<ByteArrayType>,\n-                             virtual public ByteArrayEncoder {\n- public:\n-  using BASE = DictEncoderImpl<ByteArrayType>;\n-  using BASE::DictEncoderImpl;\n-};\n+template <typename DType>\n+void DictEncoderImpl<DType>::Put(const arrow::Array& values) {\n+  ParquetException::NYI(values.type()->ToString());\n+}\n \n-class DictFLBAEncoder : public DictEncoderImpl<FLBAType>, virtual public FLBAEncoder {\n- public:\n-  using BASE = DictEncoderImpl<FLBAType>;\n-  using BASE::DictEncoderImpl;\n-};\n+template <>\n+void DictEncoderImpl<ByteArrayType>::Put(const arrow::Array& values) {\n+  AssertBinary(values);\n \n Review comment:\n   The body of this implementation appears to be a duplicate of `PlainEncoder<ByteArrayType>::Put`, could you refactor this into a shared impl?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:20.729+0000",
                    "updated": "2019-08-15T11:23:20.729+0000",
                    "started": "2019-08-15T11:23:20.728+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295396",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295395",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314260242\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -768,194 +925,124 @@ void TypedColumnWriterImpl<DType>::CheckDictionarySizeLimit() {\n                                    properties_->memory_pool());\n     encoding_ = Encoding::PLAIN;\n   }\n-}\n \n-// ----------------------------------------------------------------------\n-// Instantiate templated classes\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatch(int64_t num_values,\n-                                                     const int16_t* def_levels,\n-                                                     const int16_t* rep_levels,\n-                                                     const T* values) {\n-  int64_t values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n+  // Checks if the Dictionary Page size limit is reached\n+  // If the limit is reached, the Dictionary and Data Pages are serialized\n+  // The encoding is switched to PLAIN\n+  //\n+  // Only one Dictionary Page is written.\n+  // Fallback to PLAIN if dictionary page limit is reached.\n+  void CheckDictionarySizeLimit() {\n+    if (!has_dictionary_ || fallback_) {\n+      // Either not using dictionary encoding, or we have already fallen back\n+      // to PLAIN encoding because the size threshold was reached\n+      return;\n     }\n \n-    WriteDefinitionLevels(num_values, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_values;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    // We have to dynamic cast here because TypedEncoder<Type> as some compilers\n+    // don't want to cast through virtual inheritance\n+    auto dict_encoder = dynamic_cast<DictEncoder<DType>*>(current_encoder_.get());\n+    if (dict_encoder->dict_encoded_size() >= properties_->dictionary_pagesize_limit()) {\n+      FallbackToPlainEncoding();\n     }\n-\n-    WriteRepetitionLevels(num_values, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_values);\n   }\n \n-  // PARQUET-780\n-  if (values_to_write > 0) {\n-    DCHECK(nullptr != values) << \"Values ptr cannot be NULL\";\n-  }\n-\n-  WriteValues(values_to_write, values);\n-\n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->Update(values, values_to_write, num_values - values_to_write);\n-  }\n-\n-  num_buffered_values_ += num_values;\n-  num_buffered_encoded_values_ += values_to_write;\n-\n-  if (current_encoder_->EstimatedDataEncodedSize() >= properties_->data_pagesize()) {\n-    AddDataPage();\n-  }\n-  if (has_dictionary_ && !fallback_) {\n-    CheckDictionarySizeLimit();\n+  void WriteValues(const T* values, int64_t num_values, int64_t num_nulls) {\n+    dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+        ->Put(values, static_cast<int>(num_values));\n+    if (page_statistics_ != nullptr) {\n+      page_statistics_->Update(values, num_values, num_nulls);\n+    }\n   }\n \n-  return values_to_write;\n-}\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatchSpaced(\n-    int64_t num_levels, const int16_t* def_levels, const int16_t* rep_levels,\n-    const uint8_t* valid_bits, int64_t valid_bits_offset, const T* values,\n-    int64_t* num_spaced_written) {\n-  int64_t values_to_write = 0;\n-  int64_t spaced_values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    // Minimal definition level for which spaced values are written\n-    int16_t min_spaced_def_level = descr_->max_definition_level();\n+  void WriteValuesSpaced(const T* values, int64_t num_values, int64_t num_spaced_values,\n+                         const uint8_t* valid_bits, int64_t valid_bits_offset) {\n     if (descr_->schema_node()->is_optional()) {\n-      min_spaced_def_level--;\n-    }\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n-      if (def_levels[i] >= min_spaced_def_level) {\n-        ++spaced_values_to_write;\n-      }\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->PutSpaced(values, static_cast<int>(num_spaced_values), valid_bits,\n+                      valid_bits_offset);\n+    } else {\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->Put(values, static_cast<int>(num_values));\n     }\n-\n-    WriteDefinitionLevels(num_levels, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_levels;\n-    spaced_values_to_write = num_levels;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    if (page_statistics_ != nullptr) {\n+      const int64_t num_nulls = num_spaced_values - num_values;\n+      page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, num_values,\n+                                     num_nulls);\n     }\n-\n-    WriteRepetitionLevels(num_levels, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_levels);\n   }\n+};\n \n-  if (descr_->schema_node()->is_optional()) {\n-    WriteValuesSpaced(spaced_values_to_write, valid_bits, valid_bits_offset, values);\n-  } else {\n-    WriteValues(values_to_write, values);\n-  }\n-  *num_spaced_written = spaced_values_to_write;\n+template <typename DType>\n+Status TypedColumnWriterImpl<DType>::WriteArrowDictionary(const int16_t* def_levels,\n+                                                          const int16_t* rep_levels,\n+                                                          int64_t num_levels,\n+                                                          const arrow::Array& array,\n+                                                          ArrowWriteContext* ctx) {\n+  // If this is the first time writing a DictionaryArray, then there's\n+  // a few possible paths to take:\n+  //\n+  // - If dictionary encoding is not enabled, convert to densely\n+  //   encoded and call WriteArrow\n+  // - Dictionary encoding enabled\n+  //   - If this is the first time this is called, then we call\n+  //     PutDictionary into the encoder and then PutIndices on each\n+  //     chunk. We store the dictionary that was written in\n+  //     preserved_dictionary_ so that subsequent calls to this method\n+  //     can make sure the dictionary has not changed\n+  //   - On subsequent calls, we have to check whether the dictionary\n+  //     has changed. If it has, then we trigger the varying\n+  //     dictionary path and materialize each chunk and then call\n+  //     WriteArrow with that\n+  auto WriteDense = [&] {\n+    std::shared_ptr<arrow::Array> dense_array;\n+    RETURN_NOT_OK(MaterializeDictionary(array, properties_->memory_pool(), &dense_array));\n+    return WriteArrowDense(def_levels, rep_levels, num_levels, *dense_array, ctx);\n+  };\n \n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, values_to_write,\n-                                   spaced_values_to_write - values_to_write);\n+  if (current_encoder_->encoding() != Encoding::PLAIN_DICTIONARY ||\n+      !DictionaryDirectWriteSupported(array)) {\n+    // No longer dictionary-encoding for whatever reason, maybe we\n+    // never were or we decided to stop\n \n Review comment:\n   What are the conditions that we might stop?  I thought this would by-pass the fallback logic.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T11:23:20.729+0000",
                    "updated": "2019-08-15T11:23:20.729+0000",
                    "started": "2019-08-15T11:23:20.728+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295395",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295439",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314313560\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -645,23 +686,64 @@ class TypedColumnWriterImpl : public ColumnWriterImpl, public TypedColumnWriter<\n \n     if (properties->statistics_enabled(descr_->path()) &&\n         (SortOrder::UNKNOWN != descr_->sort_order())) {\n-      page_statistics_ = TypedStats::Make(descr_, allocator_);\n-      chunk_statistics_ = TypedStats::Make(descr_, allocator_);\n+      page_statistics_ = MakeStatistics<DType>(descr_, allocator_);\n+      chunk_statistics_ = MakeStatistics<DType>(descr_, allocator_);\n     }\n   }\n \n   int64_t Close() override { return ColumnWriterImpl::Close(); }\n \n   void WriteBatch(int64_t num_values, const int16_t* def_levels,\n-                  const int16_t* rep_levels, const T* values) override;\n+                  const int16_t* rep_levels, const T* values) override {\n+    // We check for DataPage limits only after we have inserted the values. If a user\n+    // writes a large number of values, the DataPage size can be much above the limit.\n+    // The purpose of this chunking is to bound this. Even if a user writes large number\n+    // of values, the chunking will ensure the AddDataPage() is called at a reasonable\n+    // pagesize limit\n+    int64_t value_offset = 0;\n+    auto WriteChunk = [&](int64_t offset, int64_t batch_size) {\n+      int64_t values_to_write =\n+          WriteLevels(batch_size, def_levels + offset, rep_levels + offset);\n+      // PARQUET-780\n+      if (values_to_write > 0) {\n+        DCHECK_NE(nullptr, values);\n+      }\n+      WriteValues(values + value_offset, values_to_write, batch_size - values_to_write);\n+      CommitWriteAndCheckPageLimit(batch_size, values_to_write);\n+      CheckDictionarySizeLimit();\n \n Review comment:\n   We can't have it there because the data page limit logic is needed in the direct dictionary write path, where we don't want to fall back to plain encoding. \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T13:45:49.017+0000",
                    "updated": "2019-08-15T13:45:49.017+0000",
                    "started": "2019-08-15T13:45:49.016+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295439",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295440",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314314115\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -768,194 +925,124 @@ void TypedColumnWriterImpl<DType>::CheckDictionarySizeLimit() {\n                                    properties_->memory_pool());\n     encoding_ = Encoding::PLAIN;\n   }\n-}\n \n-// ----------------------------------------------------------------------\n-// Instantiate templated classes\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatch(int64_t num_values,\n-                                                     const int16_t* def_levels,\n-                                                     const int16_t* rep_levels,\n-                                                     const T* values) {\n-  int64_t values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n+  // Checks if the Dictionary Page size limit is reached\n+  // If the limit is reached, the Dictionary and Data Pages are serialized\n+  // The encoding is switched to PLAIN\n+  //\n+  // Only one Dictionary Page is written.\n+  // Fallback to PLAIN if dictionary page limit is reached.\n+  void CheckDictionarySizeLimit() {\n+    if (!has_dictionary_ || fallback_) {\n+      // Either not using dictionary encoding, or we have already fallen back\n+      // to PLAIN encoding because the size threshold was reached\n+      return;\n     }\n \n-    WriteDefinitionLevels(num_values, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_values;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    // We have to dynamic cast here because TypedEncoder<Type> as some compilers\n+    // don't want to cast through virtual inheritance\n+    auto dict_encoder = dynamic_cast<DictEncoder<DType>*>(current_encoder_.get());\n+    if (dict_encoder->dict_encoded_size() >= properties_->dictionary_pagesize_limit()) {\n+      FallbackToPlainEncoding();\n     }\n-\n-    WriteRepetitionLevels(num_values, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_values);\n   }\n \n-  // PARQUET-780\n-  if (values_to_write > 0) {\n-    DCHECK(nullptr != values) << \"Values ptr cannot be NULL\";\n-  }\n-\n-  WriteValues(values_to_write, values);\n-\n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->Update(values, values_to_write, num_values - values_to_write);\n-  }\n-\n-  num_buffered_values_ += num_values;\n-  num_buffered_encoded_values_ += values_to_write;\n-\n-  if (current_encoder_->EstimatedDataEncodedSize() >= properties_->data_pagesize()) {\n-    AddDataPage();\n-  }\n-  if (has_dictionary_ && !fallback_) {\n-    CheckDictionarySizeLimit();\n+  void WriteValues(const T* values, int64_t num_values, int64_t num_nulls) {\n+    dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+        ->Put(values, static_cast<int>(num_values));\n+    if (page_statistics_ != nullptr) {\n+      page_statistics_->Update(values, num_values, num_nulls);\n+    }\n   }\n \n-  return values_to_write;\n-}\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatchSpaced(\n-    int64_t num_levels, const int16_t* def_levels, const int16_t* rep_levels,\n-    const uint8_t* valid_bits, int64_t valid_bits_offset, const T* values,\n-    int64_t* num_spaced_written) {\n-  int64_t values_to_write = 0;\n-  int64_t spaced_values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    // Minimal definition level for which spaced values are written\n-    int16_t min_spaced_def_level = descr_->max_definition_level();\n+  void WriteValuesSpaced(const T* values, int64_t num_values, int64_t num_spaced_values,\n+                         const uint8_t* valid_bits, int64_t valid_bits_offset) {\n     if (descr_->schema_node()->is_optional()) {\n-      min_spaced_def_level--;\n-    }\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n-      if (def_levels[i] >= min_spaced_def_level) {\n-        ++spaced_values_to_write;\n-      }\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->PutSpaced(values, static_cast<int>(num_spaced_values), valid_bits,\n+                      valid_bits_offset);\n+    } else {\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->Put(values, static_cast<int>(num_values));\n     }\n-\n-    WriteDefinitionLevels(num_levels, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_levels;\n-    spaced_values_to_write = num_levels;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    if (page_statistics_ != nullptr) {\n+      const int64_t num_nulls = num_spaced_values - num_values;\n+      page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, num_values,\n+                                     num_nulls);\n     }\n-\n-    WriteRepetitionLevels(num_levels, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_levels);\n   }\n+};\n \n-  if (descr_->schema_node()->is_optional()) {\n-    WriteValuesSpaced(spaced_values_to_write, valid_bits, valid_bits_offset, values);\n-  } else {\n-    WriteValues(values_to_write, values);\n-  }\n-  *num_spaced_written = spaced_values_to_write;\n+template <typename DType>\n+Status TypedColumnWriterImpl<DType>::WriteArrowDictionary(const int16_t* def_levels,\n+                                                          const int16_t* rep_levels,\n+                                                          int64_t num_levels,\n+                                                          const arrow::Array& array,\n+                                                          ArrowWriteContext* ctx) {\n+  // If this is the first time writing a DictionaryArray, then there's\n+  // a few possible paths to take:\n+  //\n+  // - If dictionary encoding is not enabled, convert to densely\n+  //   encoded and call WriteArrow\n+  // - Dictionary encoding enabled\n+  //   - If this is the first time this is called, then we call\n+  //     PutDictionary into the encoder and then PutIndices on each\n+  //     chunk. We store the dictionary that was written in\n+  //     preserved_dictionary_ so that subsequent calls to this method\n+  //     can make sure the dictionary has not changed\n+  //   - On subsequent calls, we have to check whether the dictionary\n+  //     has changed. If it has, then we trigger the varying\n+  //     dictionary path and materialize each chunk and then call\n+  //     WriteArrow with that\n+  auto WriteDense = [&] {\n+    std::shared_ptr<arrow::Array> dense_array;\n+    RETURN_NOT_OK(MaterializeDictionary(array, properties_->memory_pool(), &dense_array));\n+    return WriteArrowDense(def_levels, rep_levels, num_levels, *dense_array, ctx);\n+  };\n \n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, values_to_write,\n-                                   spaced_values_to_write - values_to_write);\n+  if (current_encoder_->encoding() != Encoding::PLAIN_DICTIONARY ||\n+      !DictionaryDirectWriteSupported(array)) {\n+    // No longer dictionary-encoding for whatever reason, maybe we\n+    // never were or we decided to stop\n \n Review comment:\n   Technically you can feed this function a mix of DictionaryArray and non-DictionaryArray and it will encode them happily. I can add a comment clarifying\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T13:46:53.592+0000",
                    "updated": "2019-08-15T13:46:53.592+0000",
                    "started": "2019-08-15T13:46:53.592+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295440",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295441",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314314855\n \n \n\n ##########\n File path: cpp/src/parquet/encoding.cc\n ##########\n @@ -479,25 +557,61 @@ inline void DictEncoderImpl<FLBAType>::Put(const FixedLenByteArray& v) {\n   buffered_indices_.push_back(memo_index);\n }\n \n-class DictByteArrayEncoder : public DictEncoderImpl<ByteArrayType>,\n-                             virtual public ByteArrayEncoder {\n- public:\n-  using BASE = DictEncoderImpl<ByteArrayType>;\n-  using BASE::DictEncoderImpl;\n-};\n+template <typename DType>\n+void DictEncoderImpl<DType>::Put(const arrow::Array& values) {\n+  ParquetException::NYI(values.type()->ToString());\n+}\n \n-class DictFLBAEncoder : public DictEncoderImpl<FLBAType>, virtual public FLBAEncoder {\n- public:\n-  using BASE = DictEncoderImpl<FLBAType>;\n-  using BASE::DictEncoderImpl;\n-};\n+template <>\n+void DictEncoderImpl<ByteArrayType>::Put(const arrow::Array& values) {\n+  AssertBinary(values);\n \n Review comment:\n   I'll see what I can do\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T13:48:25.362+0000",
                    "updated": "2019-08-15T13:48:25.362+0000",
                    "started": "2019-08-15T13:48:25.361+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295441",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295442",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314316736\n \n \n\n ##########\n File path: cpp/src/parquet/arrow/reader_internal.cc\n ##########\n @@ -544,8 +594,24 @@ Status BuildSchemaManifest(const SchemaDescriptor* schema,\n   manifest->descr = schema;\n   manifest->schema_fields.resize(schema_node.field_count());\n   for (int i = 0; i < static_cast<int>(schema_node.field_count()); ++i) {\n+    SchemaField* out_field = &manifest->schema_fields[i];\n     RETURN_NOT_OK(NodeToSchemaField(*schema_node.field(i), 0, 0, &ctx,\n-                                    /*parent=*/nullptr, &manifest->schema_fields[i]));\n+                                    /*parent=*/nullptr, out_field));\n+\n+    // TODO(wesm): as follow up to ARROW-3246, we should really pass the origin\n+    // schema (if any) through all functions in the schema reconstruction, but\n+    // I'm being lazy and just setting dictionary fields at the top level for\n+    // now\n+    if (manifest->origin_schema) {\n+      auto origin_field = manifest->origin_schema->field(i);\n+      auto current_type = out_field->field->type();\n+      if (origin_field->type()->id() == ::arrow::Type::DICTIONARY) {\n+        if (current_type->id() != ::arrow::Type::DICTIONARY) {\n+          out_field->field = out_field->field->WithType(\n+              ::arrow::dictionary(::arrow::int32(), current_type));\n \n Review comment:\n   We don't (can't, in general) know the cardinality of the dictionary up front. Further, the cardinality could be different from row group to row group or file to file, so if you allow the indices to be the smallest type possible, you may end up with a bunch of arrays with different index types. That was the rationale for adding a DictionaryBuilder variant that always returns int32 indices https://github.com/apache/arrow/commit/089e3db8859f48ad32657165788bb50373ddef75#diff-e15ddea2c1937474e62615fb906f6d97\r\n   \r\n   There might be some mechanism we could explore in the future to allow DictionaryArrays in a ChunkedArray to each have different index types\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T13:52:39.163+0000",
                    "updated": "2019-08-15T13:52:39.163+0000",
                    "started": "2019-08-15T13:52:39.163+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295442",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295446",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314321271\n \n \n\n ##########\n File path: cpp/src/parquet/arrow/reader_internal.cc\n ##########\n @@ -544,8 +594,24 @@ Status BuildSchemaManifest(const SchemaDescriptor* schema,\n   manifest->descr = schema;\n   manifest->schema_fields.resize(schema_node.field_count());\n   for (int i = 0; i < static_cast<int>(schema_node.field_count()); ++i) {\n+    SchemaField* out_field = &manifest->schema_fields[i];\n     RETURN_NOT_OK(NodeToSchemaField(*schema_node.field(i), 0, 0, &ctx,\n-                                    /*parent=*/nullptr, &manifest->schema_fields[i]));\n+                                    /*parent=*/nullptr, out_field));\n+\n+    // TODO(wesm): as follow up to ARROW-3246, we should really pass the origin\n+    // schema (if any) through all functions in the schema reconstruction, but\n+    // I'm being lazy and just setting dictionary fields at the top level for\n+    // now\n+    if (manifest->origin_schema) {\n+      auto origin_field = manifest->origin_schema->field(i);\n+      auto current_type = out_field->field->type();\n+      if (origin_field->type()->id() == ::arrow::Type::DICTIONARY) {\n+        if (current_type->id() != ::arrow::Type::DICTIONARY) {\n+          out_field->field = out_field->field->WithType(\n+              ::arrow::dictionary(::arrow::int32(), current_type));\n \n Review comment:\n   Thanks, that makes more sense to me now.  \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T14:02:36.491+0000",
                    "updated": "2019-08-15T14:02:36.491+0000",
                    "started": "2019-08-15T14:02:36.490+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295446",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295448",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314323862\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -630,6 +632,45 @@ void ColumnWriterImpl::FlushBufferedDataPages() {\n // ----------------------------------------------------------------------\n // TypedColumnWriter\n \n+template <typename Action>\n+inline void DoInBatches(int64_t total, int64_t batch_size, Action&& action) {\n+  int64_t num_batches = static_cast<int>(total / batch_size);\n+  for (int round = 0; round < num_batches; round++) {\n+    action(round * batch_size, batch_size);\n+  }\n+  // Write the remaining values\n+  if (total % batch_size > 0) {\n+    action(num_batches * batch_size, total % batch_size);\n+  }\n+}\n+\n+bool DictionaryDirectWriteSupported(const arrow::Array& array) {\n+  const arrow::DictionaryType& dict_type =\n+      static_cast<const arrow::DictionaryType&>(*array.type());\n+  auto id = dict_type.value_type()->id();\n+  return id == arrow::Type::BINARY || id == arrow::Type::STRING;\n+}\n+\n+Status MaterializeDictionary(const arrow::Array& array, MemoryPool* pool,\n \n Review comment:\n   Renaming `ConvertDictionaryToDense`\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T14:08:10.700+0000",
                    "updated": "2019-08-15T14:08:10.700+0000",
                    "started": "2019-08-15T14:08:10.699+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295448",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295450",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314324270\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -645,23 +686,64 @@ class TypedColumnWriterImpl : public ColumnWriterImpl, public TypedColumnWriter<\n \n     if (properties->statistics_enabled(descr_->path()) &&\n         (SortOrder::UNKNOWN != descr_->sort_order())) {\n-      page_statistics_ = TypedStats::Make(descr_, allocator_);\n-      chunk_statistics_ = TypedStats::Make(descr_, allocator_);\n+      page_statistics_ = MakeStatistics<DType>(descr_, allocator_);\n+      chunk_statistics_ = MakeStatistics<DType>(descr_, allocator_);\n     }\n   }\n \n   int64_t Close() override { return ColumnWriterImpl::Close(); }\n \n   void WriteBatch(int64_t num_values, const int16_t* def_levels,\n-                  const int16_t* rep_levels, const T* values) override;\n+                  const int16_t* rep_levels, const T* values) override {\n+    // We check for DataPage limits only after we have inserted the values. If a user\n+    // writes a large number of values, the DataPage size can be much above the limit.\n+    // The purpose of this chunking is to bound this. Even if a user writes large number\n+    // of values, the chunking will ensure the AddDataPage() is called at a reasonable\n+    // pagesize limit\n+    int64_t value_offset = 0;\n+    auto WriteChunk = [&](int64_t offset, int64_t batch_size) {\n+      int64_t values_to_write =\n+          WriteLevels(batch_size, def_levels + offset, rep_levels + offset);\n+      // PARQUET-780\n+      if (values_to_write > 0) {\n+        DCHECK_NE(nullptr, values);\n+      }\n+      WriteValues(values + value_offset, values_to_write, batch_size - values_to_write);\n+      CommitWriteAndCheckPageLimit(batch_size, values_to_write);\n+      CheckDictionarySizeLimit();\n \n Review comment:\n   Added a comment. \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T14:09:06.485+0000",
                    "updated": "2019-08-15T14:09:06.485+0000",
                    "started": "2019-08-15T14:09:06.484+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295450",
                    "issueId": "13185395"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/worklog/295452",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #5077: ARROW-3246: [C++][Python][Parquet] Direct writing of DictionaryArray to Parquet columns, automatic decoding to Arrow\nURL: https://github.com/apache/arrow/pull/5077#discussion_r314325074\n \n \n\n ##########\n File path: cpp/src/parquet/column_writer.cc\n ##########\n @@ -768,194 +925,124 @@ void TypedColumnWriterImpl<DType>::CheckDictionarySizeLimit() {\n                                    properties_->memory_pool());\n     encoding_ = Encoding::PLAIN;\n   }\n-}\n \n-// ----------------------------------------------------------------------\n-// Instantiate templated classes\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatch(int64_t num_values,\n-                                                     const int16_t* def_levels,\n-                                                     const int16_t* rep_levels,\n-                                                     const T* values) {\n-  int64_t values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n+  // Checks if the Dictionary Page size limit is reached\n+  // If the limit is reached, the Dictionary and Data Pages are serialized\n+  // The encoding is switched to PLAIN\n+  //\n+  // Only one Dictionary Page is written.\n+  // Fallback to PLAIN if dictionary page limit is reached.\n+  void CheckDictionarySizeLimit() {\n+    if (!has_dictionary_ || fallback_) {\n+      // Either not using dictionary encoding, or we have already fallen back\n+      // to PLAIN encoding because the size threshold was reached\n+      return;\n     }\n \n-    WriteDefinitionLevels(num_values, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_values;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_values; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    // We have to dynamic cast here because TypedEncoder<Type> as some compilers\n+    // don't want to cast through virtual inheritance\n+    auto dict_encoder = dynamic_cast<DictEncoder<DType>*>(current_encoder_.get());\n+    if (dict_encoder->dict_encoded_size() >= properties_->dictionary_pagesize_limit()) {\n+      FallbackToPlainEncoding();\n     }\n-\n-    WriteRepetitionLevels(num_values, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_values);\n   }\n \n-  // PARQUET-780\n-  if (values_to_write > 0) {\n-    DCHECK(nullptr != values) << \"Values ptr cannot be NULL\";\n-  }\n-\n-  WriteValues(values_to_write, values);\n-\n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->Update(values, values_to_write, num_values - values_to_write);\n-  }\n-\n-  num_buffered_values_ += num_values;\n-  num_buffered_encoded_values_ += values_to_write;\n-\n-  if (current_encoder_->EstimatedDataEncodedSize() >= properties_->data_pagesize()) {\n-    AddDataPage();\n-  }\n-  if (has_dictionary_ && !fallback_) {\n-    CheckDictionarySizeLimit();\n+  void WriteValues(const T* values, int64_t num_values, int64_t num_nulls) {\n+    dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+        ->Put(values, static_cast<int>(num_values));\n+    if (page_statistics_ != nullptr) {\n+      page_statistics_->Update(values, num_values, num_nulls);\n+    }\n   }\n \n-  return values_to_write;\n-}\n-\n-template <typename DType>\n-int64_t TypedColumnWriterImpl<DType>::WriteMiniBatchSpaced(\n-    int64_t num_levels, const int16_t* def_levels, const int16_t* rep_levels,\n-    const uint8_t* valid_bits, int64_t valid_bits_offset, const T* values,\n-    int64_t* num_spaced_written) {\n-  int64_t values_to_write = 0;\n-  int64_t spaced_values_to_write = 0;\n-  // If the field is required and non-repeated, there are no definition levels\n-  if (descr_->max_definition_level() > 0) {\n-    // Minimal definition level for which spaced values are written\n-    int16_t min_spaced_def_level = descr_->max_definition_level();\n+  void WriteValuesSpaced(const T* values, int64_t num_values, int64_t num_spaced_values,\n+                         const uint8_t* valid_bits, int64_t valid_bits_offset) {\n     if (descr_->schema_node()->is_optional()) {\n-      min_spaced_def_level--;\n-    }\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (def_levels[i] == descr_->max_definition_level()) {\n-        ++values_to_write;\n-      }\n-      if (def_levels[i] >= min_spaced_def_level) {\n-        ++spaced_values_to_write;\n-      }\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->PutSpaced(values, static_cast<int>(num_spaced_values), valid_bits,\n+                      valid_bits_offset);\n+    } else {\n+      dynamic_cast<ValueEncoderType*>(current_encoder_.get())\n+          ->Put(values, static_cast<int>(num_values));\n     }\n-\n-    WriteDefinitionLevels(num_levels, def_levels);\n-  } else {\n-    // Required field, write all values\n-    values_to_write = num_levels;\n-    spaced_values_to_write = num_levels;\n-  }\n-\n-  // Not present for non-repeated fields\n-  if (descr_->max_repetition_level() > 0) {\n-    // A row could include more than one value\n-    // Count the occasions where we start a new row\n-    for (int64_t i = 0; i < num_levels; ++i) {\n-      if (rep_levels[i] == 0) {\n-        rows_written_++;\n-      }\n+    if (page_statistics_ != nullptr) {\n+      const int64_t num_nulls = num_spaced_values - num_values;\n+      page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, num_values,\n+                                     num_nulls);\n     }\n-\n-    WriteRepetitionLevels(num_levels, rep_levels);\n-  } else {\n-    // Each value is exactly one row\n-    rows_written_ += static_cast<int>(num_levels);\n   }\n+};\n \n-  if (descr_->schema_node()->is_optional()) {\n-    WriteValuesSpaced(spaced_values_to_write, valid_bits, valid_bits_offset, values);\n-  } else {\n-    WriteValues(values_to_write, values);\n-  }\n-  *num_spaced_written = spaced_values_to_write;\n+template <typename DType>\n+Status TypedColumnWriterImpl<DType>::WriteArrowDictionary(const int16_t* def_levels,\n+                                                          const int16_t* rep_levels,\n+                                                          int64_t num_levels,\n+                                                          const arrow::Array& array,\n+                                                          ArrowWriteContext* ctx) {\n+  // If this is the first time writing a DictionaryArray, then there's\n+  // a few possible paths to take:\n+  //\n+  // - If dictionary encoding is not enabled, convert to densely\n+  //   encoded and call WriteArrow\n+  // - Dictionary encoding enabled\n+  //   - If this is the first time this is called, then we call\n+  //     PutDictionary into the encoder and then PutIndices on each\n+  //     chunk. We store the dictionary that was written in\n+  //     preserved_dictionary_ so that subsequent calls to this method\n+  //     can make sure the dictionary has not changed\n+  //   - On subsequent calls, we have to check whether the dictionary\n+  //     has changed. If it has, then we trigger the varying\n+  //     dictionary path and materialize each chunk and then call\n+  //     WriteArrow with that\n+  auto WriteDense = [&] {\n+    std::shared_ptr<arrow::Array> dense_array;\n+    RETURN_NOT_OK(MaterializeDictionary(array, properties_->memory_pool(), &dense_array));\n+    return WriteArrowDense(def_levels, rep_levels, num_levels, *dense_array, ctx);\n+  };\n \n-  if (page_statistics_ != nullptr) {\n-    page_statistics_->UpdateSpaced(values, valid_bits, valid_bits_offset, values_to_write,\n-                                   spaced_values_to_write - values_to_write);\n+  if (current_encoder_->encoding() != Encoding::PLAIN_DICTIONARY ||\n \n Review comment:\n   When we create the encoder it's always `PLAIN_DICTIONARY`\r\n   \r\n   https://github.com/apache/arrow/blob/apache-arrow-0.14.1/cpp/src/parquet/encoding.cc#L355\r\n   \r\n   I'll factor out this check into a function anyway for readability\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-15T14:10:47.557+0000",
                    "updated": "2019-08-15T14:10:47.557+0000",
                    "started": "2019-08-15T14:10:47.557+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "295452",
                    "issueId": "13185395"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 34800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@27eafc84[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@251fd3f2[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3f8ac02c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@6451046c[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2f1840e5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@40df26c7[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@293d1550[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@45a3167f[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@28516946[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@11818b95[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@50579b60[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@d1a7ffb[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 34800,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Aug 16 13:54:07 UTC 2019",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2019-08-16T13:54:07.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-3246/watchers",
            "watchCount": 4,
            "isWatching": false
        },
        "created": "2018-09-17T00:00:18.000+0000",
        "updated": "2021-02-17T18:19:35.000+0000",
        "timeoriginalestimate": null,
        "description": "Parquet supports \"dictionary encoding\" of column data in a manner very similar to the concept of Categoricals in pandas. It is natural to use this encoding for a column which originated as a categorical. Conversely, when loading, if the file metadata says that a given column came from a pandas (or arrow) categorical, then we can trust that the whole of the column is dictionary-encoded and load the data directly into a categorical column, rather than expanding the labels upon load and recategorising later.\r\n\r\nIf the data does not have the pandas metadata, then the guarantee cannot hold, and we cannot assume either that the whole column is dictionary encoded or that the labels are the same throughout. In this case, the current behaviour is fine.\r\n\r\n\u00a0\r\n\r\n(please forgive that some of this has already been mentioned elsewhere; this is one of the entries in the list at\u00a0[https://github.com/dask/fastparquet/issues/374]\u00a0as a feature that is useful in fastparquet)",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "9h 40m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 34800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python][Parquet] direct reading/writing of pandas categoricals in parquet",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16616987",
                    "id": "16616987",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "This can only be implemented in the narrow case where there is metadata indicating that the dictionary in each row group is expected to be the same (as a result of having been written by pandas). Otherwise, in general, the observed dictionaries may not be the same from row group to row group",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-09-17T01:59:36.756+0000",
                    "updated": "2018-09-17T01:59:36.756+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16617003",
                    "id": "16617003",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "body": "> can only be implemented in the narrow case\r\n\r\n\u00a0\r\n\r\nYes, exactly what I was trying to say. However, a great optimisation in that specific case.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=mdurant",
                        "name": "mdurant",
                        "key": "mdurant",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Martin Durant",
                        "active": true,
                        "timeZone": "America/Montreal"
                    },
                    "created": "2018-09-17T02:25:58.387+0000",
                    "updated": "2018-09-17T02:25:58.387+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16791008",
                    "id": "16791008",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "I moved this to 0.14. A bit of work will be needed in order to be able to sidestep hashing to categorical. If we can read BYTE_ARRAY columns directly back as Categorical (but have to hash) that is a good first step. ",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-03-12T21:04:32.870+0000",
                    "updated": "2019-03-12T21:04:32.870+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16901506",
                    "id": "16901506",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "I've been looking at what's required to write {{arrow::DictionaryArray}} directly into the appropriate lower-level ColumnWriter class. The trouble with the way the software is layered right now is that there is a \"Chinese wall\" between {{TypedColumnWriter<T>}} and the Arrow write layer. We can only communicate with this class using the Parquet C types such as {{ByteArray}} and {{FixedLenByteArray}}. This is also a performance issue since we cannot write directly into the writer from {{arrow::BinaryArray}} or similar cases where it might make sense. \r\n\r\nI think the only way to fix the current situation is to add a {{TypedColumnWriter<T>::WriteArrow(const ::arrow::Array&)}} method and \"push down\" a lot of the logic that's currently in parquet/arrow/writer.cc into the {{TypedColumnWriter<T>}} implementation. This will enable us to do various write performance optimizations and also address the direct dictionary write issue. This is not a small project, but I would say that it's overdue and will put us on a better footing going forward\r\n\r\ncc [~xhochy] [~hatem] for any thoughts",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-06T21:57:40.878+0000",
                    "updated": "2019-08-06T21:57:40.878+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16901508",
                    "id": "16901508",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "I created ARROW-6152 to cover the initial feature-preserving refactoring. I estimate about a day of effort for that, will report in once I make a little progress",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-06T22:02:30.998+0000",
                    "updated": "2019-08-06T22:02:30.998+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16901988",
                    "id": "16901988",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=hatem",
                        "name": "hatem",
                        "key": "hatem",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Hatem Helal",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Adding {{TypedColumnWriter<T>::WriteArrow(const ::arrow::Array&)}} makes a lot of sense to me. [~wesmckinn] do you have a list of cases that you know can be optimized? The main one I'm aware of is the [dictionary array|https://github.com/apache/arrow/blob/master/cpp/src/parquet/arrow/writer.cc#L1079] case, but but I'm curious if there are others arrow types that could be handled more efficiently.\r\n\r\nAs an aside, has it ever been considered to automatically tune the size of the dictionary page? I think for the limited case where of writing {{arrow::DictionaryArray}} we might want to ensure that the encoder doesn't fallback to plain encoding. That could be handled as a separate feature.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=hatem",
                        "name": "hatem",
                        "key": "hatem",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Hatem Helal",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2019-08-07T11:19:24.261+0000",
                    "updated": "2019-08-07T11:19:24.261+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16902341",
                    "id": "16902341",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Writing BYTE_ARRAY can also definitely be made more efficient. See logic at\r\n\r\nhttps://github.com/apache/arrow/blob/master/cpp/src/parquet/arrow/writer.cc#L858\r\n\r\nThe dictionary page size issue is usually handled through the WriterProperties\r\n\r\nhttps://github.com/apache/arrow/blob/master/cpp/src/parquet/properties.h#L178\r\n\r\nIf the dictionary is written all at once then this property can be circumvented, that would be my plan.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-07T17:41:10.021+0000",
                    "updated": "2019-08-07T17:41:10.021+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16902652",
                    "id": "16902652",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "OK, I was able to get the initial refactor done today. Now we need the plumbing to be able to write dictionary values and indices separately to {{DictEncoder<T>}}",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-08T04:03:21.321+0000",
                    "updated": "2019-08-08T04:03:21.321+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16902896",
                    "id": "16902896",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=hatem",
                        "name": "hatem",
                        "key": "hatem",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Hatem Helal",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": ">\u00a0 If the dictionary is written all at once then this property can be circumvented, that would be my plan.\r\n\r\nI like that plan.\r\n\u00a0\r\n\u00a0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=hatem",
                        "name": "hatem",
                        "key": "hatem",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Hatem Helal",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2019-08-08T11:39:55.959+0000",
                    "updated": "2019-08-08T11:39:55.959+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16904020",
                    "id": "16904020",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Making some progress on this. It's a can of worms because of the interplay between the ColumnWriter, Encoder, and Statistics types. ",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-09T16:34:30.812+0000",
                    "updated": "2019-08-09T16:34:30.812+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16905735",
                    "id": "16905735",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "This has been quite the saga, but I should be able to get a patch up for this tomorrow. I have to decide how to get the dictionary types to be automatically read correctly without setting the {{read_dictionary}} property",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-13T02:08:35.556+0000",
                    "updated": "2019-08-13T02:08:35.556+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13185395/comment/16909065",
                    "id": "16909065",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 5077\n[https://github.com/apache/arrow/pull/5077]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-08-16T13:54:07.250+0000",
                    "updated": "2019-08-16T13:54:07.250+0000"
                }
            ],
            "maxResults": 12,
            "total": 12,
            "startAt": 0
        },
        "customfield_12311820": "0|i3y54v:",
        "customfield_12314139": null
    }
}