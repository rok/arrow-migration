{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13320540",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540",
    "key": "ARROW-9619",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12345977",
                "id": "12345977",
                "description": "",
                "name": "2.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2020-10-19"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
            "name": "jorgecarleitao",
            "key": "jorgecarleitao",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
            },
            "displayName": "Jorge Leit\u00e3o",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12335005",
                "id": "12335005",
                "name": "Rust - DataFusion"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
            "name": "jorgecarleitao",
            "key": "jorgecarleitao",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
            },
            "displayName": "Jorge Leit\u00e3o",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
            "name": "jorgecarleitao",
            "key": "jorgecarleitao",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
            },
            "displayName": "Jorge Leit\u00e3o",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 16800,
            "total": 16800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 16800,
            "total": 16800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-9619/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 28,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/465392",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao opened a new pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880\n\n\n   This PR adds a new optimizer to push filters down. For example, a plan of the form \r\n   \r\n   ```\r\n   Selection: #SUM(c) Gt Int64(10)\\\r\n     Selection: #b Gt Int64(10)\\\r\n       Aggregate: groupBy=[[#b]], aggr=[[SUM(#c)]]\\\r\n         Projection: #a AS b, #c\\\r\n           TableScan: test projection=None\"\r\n   ```\r\n   \r\n   is converted to \r\n   \r\n   ```\r\n   Selection: #SUM(c) Gt Int64(10)\\\r\n     Aggregate: groupBy=[[#b]], aggr=[[SUM(#c)]]\\\r\n       Projection: #a AS b, #c\\\r\n         Selection: #a Gt Int64(10)\\\r\n           TableScan: test projection=None\";\r\n   ```\r\n   \r\n   (note how the filter expression changed, and how only the filter on the key of the aggregate was pushed)\r\n   \r\n   This works by performing two passes on the plan. On the first pass (analyze), it identifies:\r\n   \r\n   1. all filters are on the plan (selections)\r\n   2. all projections are on the plan (projections)\r\n   3. all places where a filter on a column cannot be pushed down (break_points)\r\n   \r\n   After this pass, it computes the maximum depth that a filter can be pushed down as well as the new expression that the filter should have, given all the projections that exist.\r\n   \r\n   On the second pass (optimize), it:\r\n   \r\n   * removes all old filters\r\n   * adds all new filters\r\n   \r\n   See comments on the code for details.\r\n   \r\n   This PR is built on top of #7879 (first two commits).\r\n   \r\n   FYI @andygrove @sunchao \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-02T12:23:18.433+0000",
                    "updated": "2020-08-02T12:23:18.433+0000",
                    "started": "2020-08-02T12:23:18.432+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "465392",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/465393",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-667668376\n\n\n   https://issues.apache.org/jira/browse/ARROW-9619\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-02T12:31:54.625+0000",
                    "updated": "2020-08-02T12:31:54.625+0000",
                    "started": "2020-08-02T12:31:54.625+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "465393",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/469911",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-673076257\n\n\n   Any of you @alamb @houqp @nevi-me @paddyhoran could help out here? I think that this does significantly speeds querying for anything more complex, as we run aggregations and projections on much less data.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-12T19:50:59.055+0000",
                    "updated": "2020-08-12T19:50:59.055+0000",
                    "started": "2020-08-12T19:50:59.055+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "469911",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/469945",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-673117865\n\n\n   I'll try and check it out carefully tomorrow morning (US EST time)\n   \n   On Wed, Aug 12, 2020 at 3:51 PM Jorge Leitao <notifications@github.com>\n   wrote:\n   \n   > Any of you @alamb <https://github.com/alamb> @houqp\n   > <https://github.com/houqp> @nevi-me <https://github.com/nevi-me>\n   > @paddyhoran <https://github.com/paddyhoran> could help out here? I think\n   > that this does significantly speeds querying for anything more complex, as\n   > we run aggregations and projections on much less data.\n   >\n   > \u2014\n   > You are receiving this because you were mentioned.\n   > Reply to this email directly, view it on GitHub\n   > <https://github.com/apache/arrow/pull/7880#issuecomment-673076257>, or\n   > unsubscribe\n   > <https://github.com/notifications/unsubscribe-auth/AADXZMNMPNXVREY4G7Y5XWDSALXCVANCNFSM4PSQWBHA>\n   > .\n   >\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-12T21:22:53.217+0000",
                    "updated": "2020-08-12T21:22:53.217+0000",
                    "started": "2020-08-12T21:22:53.217+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "469945",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/470253",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r469914297\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/utils.rs\n##########\n@@ -183,6 +183,162 @@ fn _get_supertype(l: &DataType, r: &DataType) -> Option<DataType> {\n     }\n }\n \n+/// returns all expressions in the logical plan.\n+pub fn expressions(plan: &LogicalPlan) -> Vec<Expr> {\n+    match plan {\n+        LogicalPlan::Projection { expr, .. } => expr.clone(),\n+        LogicalPlan::Selection { expr, .. } => vec![expr.clone()],\n+        LogicalPlan::Aggregate {\n+            group_expr,\n+            aggr_expr,\n+            ..\n+        } => {\n+            let mut result = group_expr.clone();\n+            result.extend(aggr_expr.clone());\n+            result\n+        }\n+        LogicalPlan::Sort { expr, .. } => expr.clone(),\n+        // plans without expressions\n+        LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::Limit { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => vec![],\n+    }\n+}\n+\n+/// returns all inputs in the logical plan\n+pub fn inputs(plan: &LogicalPlan) -> Vec<&LogicalPlan> {\n+    match plan {\n+        LogicalPlan::Projection { input, .. } => vec![input],\n+        LogicalPlan::Selection { input, .. } => vec![input],\n+        LogicalPlan::Aggregate { input, .. } => vec![input],\n+        LogicalPlan::Sort { input, .. } => vec![input],\n+        LogicalPlan::Limit { input, .. } => vec![input],\n+        // plans without inputs\n+        LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => vec![],\n+    }\n+}\n+\n+/// Returns a new logical plan based on the original one with inputs and expressions replaced\n+pub fn from_plan(\n+    plan: &LogicalPlan,\n+    expr: &Vec<Expr>,\n+    inputs: &Vec<LogicalPlan>,\n+) -> Result<LogicalPlan> {\n+    match plan {\n+        LogicalPlan::Projection { schema, .. } => Ok(LogicalPlan::Projection {\n+            expr: expr.clone(),\n+            input: Box::new(inputs[0].clone()),\n+            schema: schema.clone(),\n+        }),\n+        LogicalPlan::Selection { .. } => Ok(LogicalPlan::Selection {\n+            expr: expr[0].clone(),\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::Aggregate {\n+            group_expr, schema, ..\n+        } => Ok(LogicalPlan::Aggregate {\n+            group_expr: expr[0..group_expr.len()].to_vec(),\n+            aggr_expr: expr[group_expr.len()..].to_vec(),\n+            input: Box::new(inputs[0].clone()),\n+            schema: schema.clone(),\n+        }),\n+        LogicalPlan::Sort { .. } => Ok(LogicalPlan::Sort {\n+            expr: expr.clone(),\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::Limit { n, .. } => Ok(LogicalPlan::Limit {\n+            n: *n,\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => Ok(plan.clone()),\n+    }\n+}\n+\n+/// Returns all expressions composing the expression.\n+/// E.g. if the expression is \"(a + 1) + 1\", it returns [\"a + 1\", \"1\"] (as Expr objects)\n+pub fn expr_expressions(expr: &Expr) -> Result<Vec<&Expr>> {\n+    match expr {\n+        Expr::BinaryExpr { left, right, .. } => Ok(vec![left, right]),\n+        Expr::IsNull(e) => Ok(vec![e]),\n+        Expr::IsNotNull(e) => Ok(vec![e]),\n+        Expr::ScalarFunction { args, .. } => Ok(args.iter().collect()),\n+        Expr::AggregateFunction { args, .. } => Ok(args.iter().collect()),\n+        Expr::Cast { expr, .. } => Ok(vec![expr]),\n+        Expr::Column(_) => Ok(vec![]),\n+        Expr::Alias(expr, ..) => Ok(vec![expr]),\n+        Expr::Literal(_) => Ok(vec![]),\n+        Expr::Not(expr) => Ok(vec![expr]),\n+        Expr::Sort { expr, .. } => Ok(vec![expr]),\n+        Expr::Wildcard { .. } => Err(ExecutionError::General(\n+            \"Wildcard expressions are not valid in a logical query plan\".to_owned(),\n+        )),\n+        Expr::Nested(expr) => Ok(vec![expr]),\n+    }\n+}\n+\n+/// returns a new expression where the expressions in expr are replaced by the ones in `expr`.\n+/// This is used in conjunction with ``expr_expressions`` to re-write expressions.\n+pub fn from_expression(expr: &Expr, expressions: &Vec<Expr>) -> Result<Expr> {\n\nReview comment:\n       Calling this `rewrite_expression` that might make it clearer what this function was doing.\n\n##########\nFile path: rust/datafusion/src/optimizer/utils.rs\n##########\n@@ -183,6 +183,162 @@ fn _get_supertype(l: &DataType, r: &DataType) -> Option<DataType> {\n     }\n }\n \n+/// returns all expressions in the logical plan.\n+pub fn expressions(plan: &LogicalPlan) -> Vec<Expr> {\n+    match plan {\n+        LogicalPlan::Projection { expr, .. } => expr.clone(),\n+        LogicalPlan::Selection { expr, .. } => vec![expr.clone()],\n+        LogicalPlan::Aggregate {\n+            group_expr,\n+            aggr_expr,\n+            ..\n+        } => {\n+            let mut result = group_expr.clone();\n+            result.extend(aggr_expr.clone());\n+            result\n+        }\n+        LogicalPlan::Sort { expr, .. } => expr.clone(),\n+        // plans without expressions\n+        LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::Limit { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => vec![],\n+    }\n+}\n+\n+/// returns all inputs in the logical plan\n+pub fn inputs(plan: &LogicalPlan) -> Vec<&LogicalPlan> {\n+    match plan {\n+        LogicalPlan::Projection { input, .. } => vec![input],\n+        LogicalPlan::Selection { input, .. } => vec![input],\n+        LogicalPlan::Aggregate { input, .. } => vec![input],\n+        LogicalPlan::Sort { input, .. } => vec![input],\n+        LogicalPlan::Limit { input, .. } => vec![input],\n+        // plans without inputs\n+        LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => vec![],\n+    }\n+}\n+\n+/// Returns a new logical plan based on the original one with inputs and expressions replaced\n+pub fn from_plan(\n+    plan: &LogicalPlan,\n+    expr: &Vec<Expr>,\n+    inputs: &Vec<LogicalPlan>,\n+) -> Result<LogicalPlan> {\n+    match plan {\n+        LogicalPlan::Projection { schema, .. } => Ok(LogicalPlan::Projection {\n+            expr: expr.clone(),\n+            input: Box::new(inputs[0].clone()),\n+            schema: schema.clone(),\n+        }),\n+        LogicalPlan::Selection { .. } => Ok(LogicalPlan::Selection {\n+            expr: expr[0].clone(),\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::Aggregate {\n+            group_expr, schema, ..\n+        } => Ok(LogicalPlan::Aggregate {\n+            group_expr: expr[0..group_expr.len()].to_vec(),\n+            aggr_expr: expr[group_expr.len()..].to_vec(),\n+            input: Box::new(inputs[0].clone()),\n+            schema: schema.clone(),\n+        }),\n+        LogicalPlan::Sort { .. } => Ok(LogicalPlan::Sort {\n+            expr: expr.clone(),\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::Limit { n, .. } => Ok(LogicalPlan::Limit {\n+            n: *n,\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => Ok(plan.clone()),\n+    }\n+}\n+\n+/// Returns all expressions composing the expression.\n+/// E.g. if the expression is \"(a + 1) + 1\", it returns [\"a + 1\", \"1\"] (as Expr objects)\n+pub fn expr_expressions(expr: &Expr) -> Result<Vec<&Expr>> {\n\nReview comment:\n       ```suggestion\r\n   pub fn expr_sub_expressions(expr: &Expr) -> Result<Vec<&Expr>> {\r\n   ```\n\n##########\nFile path: rust/datafusion/src/optimizer/utils.rs\n##########\n@@ -183,6 +183,162 @@ fn _get_supertype(l: &DataType, r: &DataType) -> Option<DataType> {\n     }\n }\n \n+/// returns all expressions in the logical plan.\n+pub fn expressions(plan: &LogicalPlan) -> Vec<Expr> {\n+    match plan {\n+        LogicalPlan::Projection { expr, .. } => expr.clone(),\n+        LogicalPlan::Selection { expr, .. } => vec![expr.clone()],\n+        LogicalPlan::Aggregate {\n+            group_expr,\n+            aggr_expr,\n+            ..\n+        } => {\n+            let mut result = group_expr.clone();\n+            result.extend(aggr_expr.clone());\n+            result\n+        }\n+        LogicalPlan::Sort { expr, .. } => expr.clone(),\n+        // plans without expressions\n+        LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::Limit { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => vec![],\n+    }\n+}\n+\n+/// returns all inputs in the logical plan\n+pub fn inputs(plan: &LogicalPlan) -> Vec<&LogicalPlan> {\n+    match plan {\n+        LogicalPlan::Projection { input, .. } => vec![input],\n+        LogicalPlan::Selection { input, .. } => vec![input],\n+        LogicalPlan::Aggregate { input, .. } => vec![input],\n+        LogicalPlan::Sort { input, .. } => vec![input],\n+        LogicalPlan::Limit { input, .. } => vec![input],\n+        // plans without inputs\n+        LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => vec![],\n+    }\n+}\n+\n+/// Returns a new logical plan based on the original one with inputs and expressions replaced\n+pub fn from_plan(\n+    plan: &LogicalPlan,\n+    expr: &Vec<Expr>,\n+    inputs: &Vec<LogicalPlan>,\n+) -> Result<LogicalPlan> {\n+    match plan {\n+        LogicalPlan::Projection { schema, .. } => Ok(LogicalPlan::Projection {\n+            expr: expr.clone(),\n+            input: Box::new(inputs[0].clone()),\n+            schema: schema.clone(),\n+        }),\n+        LogicalPlan::Selection { .. } => Ok(LogicalPlan::Selection {\n+            expr: expr[0].clone(),\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::Aggregate {\n+            group_expr, schema, ..\n+        } => Ok(LogicalPlan::Aggregate {\n+            group_expr: expr[0..group_expr.len()].to_vec(),\n+            aggr_expr: expr[group_expr.len()..].to_vec(),\n+            input: Box::new(inputs[0].clone()),\n+            schema: schema.clone(),\n+        }),\n+        LogicalPlan::Sort { .. } => Ok(LogicalPlan::Sort {\n+            expr: expr.clone(),\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::Limit { n, .. } => Ok(LogicalPlan::Limit {\n+            n: *n,\n+            input: Box::new(inputs[0].clone()),\n+        }),\n+        LogicalPlan::EmptyRelation { .. }\n+        | LogicalPlan::TableScan { .. }\n+        | LogicalPlan::InMemoryScan { .. }\n+        | LogicalPlan::ParquetScan { .. }\n+        | LogicalPlan::CsvScan { .. }\n+        | LogicalPlan::CreateExternalTable { .. } => Ok(plan.clone()),\n+    }\n+}\n+\n+/// Returns all expressions composing the expression.\n\nReview comment:\n       ```suggestion\r\n   /// Returns direct children `Expression`s of `expr`.\r\n   ```\n\n##########\nFile path: rust/datafusion/src/optimizer/utils.rs\n##########\n@@ -183,6 +183,162 @@ fn _get_supertype(l: &DataType, r: &DataType) -> Option<DataType> {\n     }\n }\n \n+/// returns all expressions in the logical plan.\n\nReview comment:\n       ```suggestion\r\n   /// returns all expressions (non-recursively) in the current logical plan node.\r\n   ```\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,467 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let (break_points, selections, projections) = analyze_plan(plan, 0)?;\n+\n+        // compute max depth for each of the columns\n+        let mut breaks: HashMap<String, usize> = HashMap::new();\n+        for (key, depth) in break_points {\n+            match breaks.get(&key) {\n+                Some(current_depth) => {\n+                    if depth > *current_depth {\n+                        breaks.insert(key, depth);\n+                    }\n+                }\n+                None => {\n+                    breaks.insert(key, depth);\n+                }\n+            }\n+        }\n+\n+        // construct optimized position of each of the new selections\n+        let mut new_selections: HashMap<usize, Expr> = HashMap::new();\n+        for (selection_depth, expr) in selections {\n+            let mut columns: HashSet<String> = HashSet::new();\n+            utils::expr_to_column_names(&expr, &mut columns)?;\n+\n+            // compute the depths of each of the observed columns and the respective maximum\n+            let depth = columns\n+                .iter()\n+                .filter_map(|column| breaks.get(column))\n+                .max_by_key(|depth| **depth);\n+\n+            let new_depth = match depth {\n+                None => selection_depth,\n+                Some(d) => *d,\n+            };\n+\n+            // re-write the new selections based on all projections that it crossed.\n+            // E.g. in `Selection: #b\\n  Projection: #a > 1 as b`, we can swap them, but the selection must be \"#a > 1\"\n+            let mut new_expression = expr.clone();\n+            for depth_i in selection_depth..new_depth {\n+                if let Some(projection) = projections.get(&depth_i) {\n+                    new_expression = rewrite(&new_expression, projection)?;\n+                }\n+            }\n+\n+            new_selections.insert(new_depth, new_expression);\n+        }\n+\n+        optimize_plan(plan, &new_selections, 0)\n+    }\n+}\n+\n+/// Recursively transverses the logical plan looking for depths that break filter pushdown\n+/// Returns a tuple:\n+/// 0: map \"column -> depth\" of the depth that each column is found up to.\n+/// 1: map \"depth -> filter expression\"\n+/// 2: map \"depth -> projection\"\n+fn analyze_plan(\n+    plan: &LogicalPlan,\n+    depth: usize,\n+) -> Result<(\n+    HashMap<String, usize>,\n+    HashMap<usize, Expr>,\n+    HashMap<usize, HashMap<String, Expr>>,\n+)> {\n+    match plan {\n+        LogicalPlan::Selection { input, expr } => {\n+            let mut result = analyze_plan(&input, depth + 1)?;\n+            result.1.insert(depth, expr.clone());\n+            Ok(result)\n+        }\n+        LogicalPlan::Projection {\n+            input,\n+            expr,\n+            schema,\n+        } => {\n+            let mut result = analyze_plan(&input, depth + 1)?;\n+\n+            // collect projection.\n+            let mut projection = HashMap::new();\n+            schema.fields().iter().enumerate().for_each(|(i, field)| {\n+                // strip alias, as they should not be part of selections\n+                let expr = match &expr[i] {\n+                    Expr::Alias(expr, _) => expr.as_ref().clone(),\n+                    expr => expr.clone(),\n+                };\n+\n+                projection.insert(field.name().clone(), expr);\n+            });\n+            result.2.insert(depth, projection);\n+            Ok(result)\n+        }\n+        LogicalPlan::Aggregate {\n+            input, aggr_expr, ..\n+        } => {\n+            let mut result = analyze_plan(&input, depth + 1)?;\n+\n+            let mut accum = HashSet::new();\n+            utils::exprlist_to_column_names(aggr_expr, &mut accum)?;\n+\n+            accum.iter().for_each(|x: &String| {\n+                result.0.insert(x.clone(), depth);\n+            });\n+\n+            Ok(result)\n+        }\n+        LogicalPlan::Sort { input, .. } => analyze_plan(&input, depth + 1),\n+        LogicalPlan::Limit { input, .. } => {\n+            let mut result = analyze_plan(&input, depth + 1)?;\n+\n+            // pick all fields in the input schema of this limit: none of them can be used after this limit.\n+            input.schema().fields().iter().for_each(|x| {\n+                result.0.insert(x.name().clone(), depth);\n+            });\n+            Ok(result)\n+        }\n+        // all other plans add breaks to all their columns to indicate that filters can't proceed further.\n+        _ => {\n+            let mut result = HashMap::new();\n+            plan.schema().fields().iter().for_each(|x| {\n+                result.insert(x.name().clone(), depth);\n+            });\n+            Ok((result, HashMap::new(), HashMap::new()))\n+        }\n+    }\n+}\n+\n+impl FilterPushDown {\n+    #[allow(missing_docs)]\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+}\n+\n+/// Returns a re-written logical plan where all old filters are removed and the new ones are added.\n+fn optimize_plan(\n+    plan: &LogicalPlan,\n+    new_selections: &HashMap<usize, Expr>,\n+    depth: usize,\n+) -> Result<LogicalPlan> {\n+    // optimize the plan recursively:\n+    let new_plan = match plan {\n+        LogicalPlan::Selection { input, .. } => {\n+            // ignore old selections\n+            Ok(optimize_plan(&input, new_selections, depth + 1)?)\n+        }\n+        _ => {\n+            // all other nodes are copied, optimizing recursively.\n+            let expr = utils::expressions(plan);\n+\n+            let inputs = utils::inputs(plan);\n+            let new_inputs = inputs\n+                .iter()\n+                .map(|plan| optimize_plan(plan, new_selections, depth + 1))\n+                .collect::<Result<Vec<_>>>()?;\n+\n+            utils::from_plan(plan, &expr, &new_inputs)\n+        }\n+    }?;\n+\n+    // if a new selection is to be applied, apply it\n+    if let Some(expr) = new_selections.get(&depth) {\n+        return Ok(LogicalPlan::Selection {\n+            expr: expr.clone(),\n+            input: Box::new(new_plan),\n+        });\n+    } else {\n+        Ok(new_plan)\n+    }\n+}\n+\n+/// replaces columns by its name on the projection.\n+fn rewrite(expr: &Expr, projection: &HashMap<String, Expr>) -> Result<Expr> {\n+    let expressions = utils::expr_expressions(&expr)?;\n+\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite(e, &projection))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    match expr {\n+        Expr::Column(name) => {\n+            if let Some(expr) = projection.get(name) {\n+                return Ok(expr.clone());\n+            }\n+        }\n+        _ => {}\n+    }\n+\n+    utils::from_expression(&expr, &expressions)\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+    use crate::logicalplan::col;\n+    use crate::logicalplan::ScalarValue;\n+    use crate::logicalplan::{aggregate_expr, lit, Expr, LogicalPlanBuilder, Operator};\n+    use crate::test::*;\n+    use arrow::datatypes::DataType;\n+\n+    fn assert_optimized_plan_eq(plan: &LogicalPlan, expected: &str) {\n+        let mut rule = FilterPushDown::new();\n+        let optimized_plan = rule.optimize(plan).expect(\"failed to optimize plan\");\n+        let formatted_plan = format!(\"{:?}\", optimized_plan);\n+        assert_eq!(formatted_plan, expected);\n+    }\n+\n+    #[test]\n+    fn filter_before_projection() -> Result<()> {\n+        let table_scan = test_table_scan()?;\n+        let plan = LogicalPlanBuilder::from(&table_scan)\n+            .project(vec![col(\"a\"), col(\"b\")])?\n+            .filter(col(\"a\").eq(&Expr::Literal(ScalarValue::Int64(1))))?\n+            .build()?;\n+        // selection is before projection\n+        let expected = \"\\\n+            Projection: #a, #b\\\n+            \\n  Selection: #a Eq Int64(1)\\\n+            \\n    TableScan: test projection=None\";\n+        assert_optimized_plan_eq(&plan, expected);\n+        Ok(())\n+    }\n+\n+    #[test]\n+    fn filter_after_limit() -> Result<()> {\n+        let table_scan = test_table_scan()?;\n+        let plan = LogicalPlanBuilder::from(&table_scan)\n+            .project(vec![col(\"a\"), col(\"b\")])?\n+            .limit(10)?\n+            .filter(col(\"a\").eq(&Expr::Literal(ScalarValue::Int64(1))))?\n+            .build()?;\n+        // selection is before single projection\n+        let expected = \"\\\n+            Selection: #a Eq Int64(1)\\\n+            \\n  Limit: 10\\\n+            \\n    Projection: #a, #b\\\n+            \\n      TableScan: test projection=None\";\n+        assert_optimized_plan_eq(&plan, expected);\n+        Ok(())\n+    }\n+\n+    #[test]\n+    fn filter_jump_2_plans() -> Result<()> {\n+        let table_scan = test_table_scan()?;\n+        let plan = LogicalPlanBuilder::from(&table_scan)\n+            .project(vec![col(\"a\"), col(\"b\"), col(\"c\")])?\n+            .project(vec![col(\"c\"), col(\"b\")])?\n+            .filter(col(\"a\").eq(&Expr::Literal(ScalarValue::Int64(1))))?\n+            .build()?;\n+        // selection is before double projection\n+        let expected = \"\\\n+            Projection: #c, #b\\\n+            \\n  Projection: #a, #b, #c\\\n+            \\n    Selection: #a Eq Int64(1)\\\n+            \\n      TableScan: test projection=None\";\n+        assert_optimized_plan_eq(&plan, expected);\n+        Ok(())\n+    }\n+\n+    #[test]\n+    fn filter_move_agg() -> Result<()> {\n+        let table_scan = test_table_scan()?;\n+        let plan = LogicalPlanBuilder::from(&table_scan)\n+            .aggregate(\n+                vec![col(\"a\")],\n+                vec![aggregate_expr(\"SUM\", col(\"b\"), DataType::Int32)\n+                    .alias(\"total_salary\")],\n+            )?\n+            .filter(col(\"a\").gt(&Expr::Literal(ScalarValue::Int64(10))))?\n+            .build()?;\n+        // selection of key aggregation is commutative\n+        let expected = \"\\\n+            Aggregate: groupBy=[[#a]], aggr=[[SUM(#b) AS total_salary]]\\\n+            \\n  Selection: #a Gt Int64(10)\\\n+            \\n    TableScan: test projection=None\";\n+        assert_optimized_plan_eq(&plan, expected);\n+        Ok(())\n+    }\n+\n+    #[test]\n+    fn filter_keep_agg() -> Result<()> {\n+        let table_scan = test_table_scan()?;\n+        let plan = LogicalPlanBuilder::from(&table_scan)\n+            .aggregate(\n+                vec![col(\"a\")],\n+                vec![aggregate_expr(\"SUM\", col(\"b\"), DataType::Int32).alias(\"b\")],\n+            )?\n+            .filter(col(\"b\").gt(&Expr::Literal(ScalarValue::Int64(10))))?\n+            .build()?;\n+        // selection of aggregate is after aggregation since they are non-commutative\n+        let expected = \"\\\n+            Selection: #b Gt Int64(10)\\\n+            \\n  Aggregate: groupBy=[[#a]], aggr=[[SUM(#b) AS b]]\\\n+            \\n    TableScan: test projection=None\";\n+        assert_optimized_plan_eq(&plan, expected);\n+        Ok(())\n+    }\n+\n+    /// verifies that a filter is pushed to before a projection, the filter expression is correctly re-written\n+    #[test]\n+    fn alias() -> Result<()> {\n+        let table_scan = test_table_scan()?;\n+        let plan = LogicalPlanBuilder::from(&table_scan)\n+            .project(vec![col(\"a\").alias(\"b\"), col(\"c\")])?\n+            .filter(col(\"b\").eq(&Expr::Literal(ScalarValue::Int64(1))))?\n+            .build()?;\n+        // selection is before projection\n+        let expected = \"\\\n+            Projection: #a AS b, #c\\\n+            \\n  Selection: #a Eq Int64(1)\\\n+            \\n    TableScan: test projection=None\";\n+        assert_optimized_plan_eq(&plan, expected);\n+        Ok(())\n+    }\n+\n+    fn add(left: Expr, right: Expr) -> Expr {\n+        Expr::BinaryExpr {\n+            left: Box::new(left),\n+            op: Operator::Plus,\n+            right: Box::new(right),\n+        }\n+    }\n+\n+    fn multiply(left: Expr, right: Expr) -> Expr {\n+        Expr::BinaryExpr {\n+            left: Box::new(left),\n+            op: Operator::Multiply,\n+            right: Box::new(right),\n+        }\n+    }\n+\n+    /// verifies that a filter is pushed to before a projection with a complex expression, the filter expression is correctly re-written\n+    #[test]\n+    fn complex_expression() -> Result<()> {\n+        let table_scan = test_table_scan()?;\n+        let plan = LogicalPlanBuilder::from(&table_scan)\n+            .project(vec![\n+                add(multiply(col(\"a\"), lit(2)), col(\"c\")).alias(\"b\"),\n+                col(\"c\"),\n+            ])?\n+            .filter(col(\"b\").eq(&Expr::Literal(ScalarValue::Int64(1))))?\n+            .build()?;\n+\n+        // not part of the test, just good to know:\n+        assert_eq!(\n+            format!(\"{:?}\", plan),\n+            \"\\\n+            Selection: #b Eq Int64(1)\\\n+            \\n  Projection: #a Multiply Int32(2) Plus #c AS b, #c\\\n+            \\n    TableScan: test projection=None\"\n+        );\n+\n+        // selection is before projection\n+        let expected = \"\\\n+            Projection: #a Multiply Int32(2) Plus #c AS b, #c\\\n+            \\n  Selection: #a Multiply Int32(2) Plus #c Eq Int64(1)\\\n+            \\n    TableScan: test projection=None\";\n+        assert_optimized_plan_eq(&plan, expected);\n+        Ok(())\n+    }\n+\n+    /// verifies that when a filter is pushed to after 2 projections, the filter expression is correctly re-written\n+    #[test]\n+    fn complex_plan() -> Result<()> {\n+        let table_scan = test_table_scan()?;\n+        let plan = LogicalPlanBuilder::from(&table_scan)\n+            .project(vec![\n+                add(multiply(col(\"a\"), lit(2)), col(\"c\")).alias(\"b\"),\n+                col(\"c\"),\n+            ])?\n+            // second projection where we rename columns, just to make it difficult\n+            .project(vec![multiply(col(\"b\"), lit(3)).alias(\"a\"), col(\"c\")])?\n+            .filter(col(\"a\").eq(&Expr::Literal(ScalarValue::Int64(1))))?\n+            .build()?;\n+\n+        // not part of the test, just good to know:\n\nReview comment:\n       \ud83d\udc4d \n\n##########\nFile path: rust/datafusion/src/optimizer/type_coercion.rs\n##########\n@@ -43,138 +45,77 @@ impl<'a> TypeCoercionRule<'a> {\n         Self { scalar_functions }\n     }\n \n-    /// Rewrite an expression list to include explicit CAST operations when required\n-    fn rewrite_expr_list(&self, expr: &[Expr], schema: &Schema) -> Result<Vec<Expr>> {\n-        Ok(expr\n+    /// Rewrite an expression to include explicit CAST operations when required\n+    fn rewrite_expr(&self, expr: &Expr, schema: &Schema) -> Result<Expr> {\n+        let expressions = utils::expr_expressions(expr)?;\n+\n+        // recurse of the re-write\n+        let mut expressions = expressions\n             .iter()\n             .map(|e| self.rewrite_expr(e, schema))\n-            .collect::<Result<Vec<_>>>()?)\n-    }\n+            .collect::<Result<Vec<_>>>()?;\n \n-    /// Rewrite an expression to include explicit CAST operations when required\n-    fn rewrite_expr(&self, expr: &Expr, schema: &Schema) -> Result<Expr> {\n+        // modify `expressions` by introducing casts when necessary\n         match expr {\n-            Expr::BinaryExpr { left, op, right } => {\n-                let left = self.rewrite_expr(left, schema)?;\n-                let right = self.rewrite_expr(right, schema)?;\n-                let left_type = left.get_type(schema)?;\n-                let right_type = right.get_type(schema)?;\n-                if left_type == right_type {\n-                    Ok(Expr::BinaryExpr {\n-                        left: Box::new(left),\n-                        op: op.clone(),\n-                        right: Box::new(right),\n-                    })\n-                } else {\n+            Expr::BinaryExpr { .. } => {\n+                let left_type = expressions[0].get_type(schema)?;\n+                let right_type = expressions[1].get_type(schema)?;\n+                if left_type != right_type {\n                     let super_type = utils::get_supertype(&left_type, &right_type)?;\n-                    Ok(Expr::BinaryExpr {\n-                        left: Box::new(left.cast_to(&super_type, schema)?),\n-                        op: op.clone(),\n-                        right: Box::new(right.cast_to(&super_type, schema)?),\n-                    })\n+\n+                    expressions[0] = expressions[0].cast_to(&super_type, schema)?;\n+                    expressions[1] = expressions[1].cast_to(&super_type, schema)?;\n                 }\n             }\n-            Expr::IsNull(e) => Ok(Expr::IsNull(Box::new(self.rewrite_expr(e, schema)?))),\n-            Expr::IsNotNull(e) => {\n-                Ok(Expr::IsNotNull(Box::new(self.rewrite_expr(e, schema)?)))\n-            }\n-            Expr::ScalarFunction {\n-                name,\n-                args,\n-                return_type,\n-            } => {\n+            Expr::ScalarFunction { name, .. } => {\n                 // cast the inputs of scalar functions to the appropriate type where possible\n                 match self.scalar_functions.get(name) {\n                     Some(func_meta) => {\n-                        let mut func_args = Vec::with_capacity(args.len());\n-                        for i in 0..args.len() {\n+                        for i in 0..expressions.len() {\n                             let field = &func_meta.args[i];\n-                            let expr = self.rewrite_expr(&args[i], schema)?;\n-                            let actual_type = expr.get_type(schema)?;\n+                            let actual_type = expressions[i].get_type(schema)?;\n                             let required_type = field.data_type();\n-                            if &actual_type == required_type {\n-                                func_args.push(expr)\n-                            } else {\n+                            if &actual_type != required_type {\n                                 let super_type =\n                                     utils::get_supertype(&actual_type, required_type)?;\n-                                func_args.push(expr.cast_to(&super_type, schema)?);\n-                            }\n+                                expressions[i] =\n+                                    expressions[i].cast_to(&super_type, schema)?\n+                            };\n                         }\n-\n-                        Ok(Expr::ScalarFunction {\n-                            name: name.clone(),\n-                            args: func_args,\n-                            return_type: return_type.clone(),\n-                        })\n                     }\n-                    _ => Err(ExecutionError::General(format!(\n-                        \"Invalid scalar function {}\",\n-                        name\n-                    ))),\n+                    _ => {\n+                        return Err(ExecutionError::General(format!(\n+                            \"Invalid scalar function {}\",\n+                            name\n+                        )))\n+                    }\n                 }\n             }\n-            Expr::AggregateFunction {\n-                name,\n-                args,\n-                return_type,\n-            } => Ok(Expr::AggregateFunction {\n-                name: name.clone(),\n-                args: args\n-                    .iter()\n-                    .map(|a| self.rewrite_expr(a, schema))\n-                    .collect::<Result<Vec<_>>>()?,\n-                return_type: return_type.clone(),\n-            }),\n-            Expr::Cast { .. } => Ok(expr.clone()),\n-            Expr::Column(_) => Ok(expr.clone()),\n-            Expr::Alias(expr, alias) => Ok(Expr::Alias(\n-                Box::new(self.rewrite_expr(expr, schema)?),\n-                alias.to_owned(),\n-            )),\n-            Expr::Literal(_) => Ok(expr.clone()),\n-            Expr::Not(_) => Ok(expr.clone()),\n-            Expr::Sort { .. } => Ok(expr.clone()),\n-            Expr::Wildcard { .. } => Err(ExecutionError::General(\n-                \"Wildcard expressions are not valid in a logical query plan\".to_owned(),\n-            )),\n-            Expr::Nested(e) => self.rewrite_expr(e, schema),\n-        }\n+            _ => {}\n+        };\n+        utils::from_expression(expr, &expressions)\n     }\n }\n \n impl<'a> OptimizerRule for TypeCoercionRule<'a> {\n     fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n-        match plan {\n-            LogicalPlan::Projection { expr, input, .. } => {\n-                LogicalPlanBuilder::from(&self.optimize(input)?)\n-                    .project(self.rewrite_expr_list(expr, input.schema())?)?\n-                    .build()\n-            }\n-            LogicalPlan::Selection { expr, input, .. } => {\n-                LogicalPlanBuilder::from(&self.optimize(input)?)\n-                    .filter(self.rewrite_expr(expr, input.schema())?)?\n-                    .build()\n-            }\n-            LogicalPlan::Aggregate {\n-                input,\n-                group_expr,\n-                aggr_expr,\n-                ..\n-            } => LogicalPlanBuilder::from(&self.optimize(input)?)\n-                .aggregate(\n-                    self.rewrite_expr_list(group_expr, input.schema())?,\n-                    self.rewrite_expr_list(aggr_expr, input.schema())?,\n-                )?\n-                .build(),\n-            LogicalPlan::TableScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::InMemoryScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::ParquetScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::CsvScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::EmptyRelation { .. } => Ok(plan.clone()),\n-            LogicalPlan::Limit { .. } => Ok(plan.clone()),\n-            LogicalPlan::Sort { .. } => Ok(plan.clone()),\n-            LogicalPlan::CreateExternalTable { .. } => Ok(plan.clone()),\n-        }\n+        let inputs = utils::inputs(plan);\n+        let expressions = utils::expressions(plan);\n+\n+        // apply the optimization to all inputs of the plan\n+        let new_inputs = inputs\n+            .iter()\n+            .map(|plan| self.optimize(*plan))\n+            .collect::<Result<Vec<_>>>()?;\n+        // re-write all expressions on this plan.\n+        // This assumes a single input, [0]. It wont work for join, subqueries and union operations with more than one input.\n+        // It is currently not an issue as we do not have any plan with more than one input.\n+        let new_expressions = expressions\n\nReview comment:\n       Do you have to check for the case here where the `LogicalPlan` node has no inputs?\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-13T14:20:29.041+0000",
                    "updated": "2020-08-13T14:20:29.041+0000",
                    "started": "2020-08-13T14:20:29.040+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "470253",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/470575",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r470414491\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/type_coercion.rs\n##########\n@@ -43,138 +45,77 @@ impl<'a> TypeCoercionRule<'a> {\n         Self { scalar_functions }\n     }\n \n-    /// Rewrite an expression list to include explicit CAST operations when required\n-    fn rewrite_expr_list(&self, expr: &[Expr], schema: &Schema) -> Result<Vec<Expr>> {\n-        Ok(expr\n+    /// Rewrite an expression to include explicit CAST operations when required\n+    fn rewrite_expr(&self, expr: &Expr, schema: &Schema) -> Result<Expr> {\n+        let expressions = utils::expr_expressions(expr)?;\n+\n+        // recurse of the re-write\n+        let mut expressions = expressions\n             .iter()\n             .map(|e| self.rewrite_expr(e, schema))\n-            .collect::<Result<Vec<_>>>()?)\n-    }\n+            .collect::<Result<Vec<_>>>()?;\n \n-    /// Rewrite an expression to include explicit CAST operations when required\n-    fn rewrite_expr(&self, expr: &Expr, schema: &Schema) -> Result<Expr> {\n+        // modify `expressions` by introducing casts when necessary\n         match expr {\n-            Expr::BinaryExpr { left, op, right } => {\n-                let left = self.rewrite_expr(left, schema)?;\n-                let right = self.rewrite_expr(right, schema)?;\n-                let left_type = left.get_type(schema)?;\n-                let right_type = right.get_type(schema)?;\n-                if left_type == right_type {\n-                    Ok(Expr::BinaryExpr {\n-                        left: Box::new(left),\n-                        op: op.clone(),\n-                        right: Box::new(right),\n-                    })\n-                } else {\n+            Expr::BinaryExpr { .. } => {\n+                let left_type = expressions[0].get_type(schema)?;\n+                let right_type = expressions[1].get_type(schema)?;\n+                if left_type != right_type {\n                     let super_type = utils::get_supertype(&left_type, &right_type)?;\n-                    Ok(Expr::BinaryExpr {\n-                        left: Box::new(left.cast_to(&super_type, schema)?),\n-                        op: op.clone(),\n-                        right: Box::new(right.cast_to(&super_type, schema)?),\n-                    })\n+\n+                    expressions[0] = expressions[0].cast_to(&super_type, schema)?;\n+                    expressions[1] = expressions[1].cast_to(&super_type, schema)?;\n                 }\n             }\n-            Expr::IsNull(e) => Ok(Expr::IsNull(Box::new(self.rewrite_expr(e, schema)?))),\n-            Expr::IsNotNull(e) => {\n-                Ok(Expr::IsNotNull(Box::new(self.rewrite_expr(e, schema)?)))\n-            }\n-            Expr::ScalarFunction {\n-                name,\n-                args,\n-                return_type,\n-            } => {\n+            Expr::ScalarFunction { name, .. } => {\n                 // cast the inputs of scalar functions to the appropriate type where possible\n                 match self.scalar_functions.get(name) {\n                     Some(func_meta) => {\n-                        let mut func_args = Vec::with_capacity(args.len());\n-                        for i in 0..args.len() {\n+                        for i in 0..expressions.len() {\n                             let field = &func_meta.args[i];\n-                            let expr = self.rewrite_expr(&args[i], schema)?;\n-                            let actual_type = expr.get_type(schema)?;\n+                            let actual_type = expressions[i].get_type(schema)?;\n                             let required_type = field.data_type();\n-                            if &actual_type == required_type {\n-                                func_args.push(expr)\n-                            } else {\n+                            if &actual_type != required_type {\n                                 let super_type =\n                                     utils::get_supertype(&actual_type, required_type)?;\n-                                func_args.push(expr.cast_to(&super_type, schema)?);\n-                            }\n+                                expressions[i] =\n+                                    expressions[i].cast_to(&super_type, schema)?\n+                            };\n                         }\n-\n-                        Ok(Expr::ScalarFunction {\n-                            name: name.clone(),\n-                            args: func_args,\n-                            return_type: return_type.clone(),\n-                        })\n                     }\n-                    _ => Err(ExecutionError::General(format!(\n-                        \"Invalid scalar function {}\",\n-                        name\n-                    ))),\n+                    _ => {\n+                        return Err(ExecutionError::General(format!(\n+                            \"Invalid scalar function {}\",\n+                            name\n+                        )))\n+                    }\n                 }\n             }\n-            Expr::AggregateFunction {\n-                name,\n-                args,\n-                return_type,\n-            } => Ok(Expr::AggregateFunction {\n-                name: name.clone(),\n-                args: args\n-                    .iter()\n-                    .map(|a| self.rewrite_expr(a, schema))\n-                    .collect::<Result<Vec<_>>>()?,\n-                return_type: return_type.clone(),\n-            }),\n-            Expr::Cast { .. } => Ok(expr.clone()),\n-            Expr::Column(_) => Ok(expr.clone()),\n-            Expr::Alias(expr, alias) => Ok(Expr::Alias(\n-                Box::new(self.rewrite_expr(expr, schema)?),\n-                alias.to_owned(),\n-            )),\n-            Expr::Literal(_) => Ok(expr.clone()),\n-            Expr::Not(_) => Ok(expr.clone()),\n-            Expr::Sort { .. } => Ok(expr.clone()),\n-            Expr::Wildcard { .. } => Err(ExecutionError::General(\n-                \"Wildcard expressions are not valid in a logical query plan\".to_owned(),\n-            )),\n-            Expr::Nested(e) => self.rewrite_expr(e, schema),\n-        }\n+            _ => {}\n+        };\n+        utils::from_expression(expr, &expressions)\n     }\n }\n \n impl<'a> OptimizerRule for TypeCoercionRule<'a> {\n     fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n-        match plan {\n-            LogicalPlan::Projection { expr, input, .. } => {\n-                LogicalPlanBuilder::from(&self.optimize(input)?)\n-                    .project(self.rewrite_expr_list(expr, input.schema())?)?\n-                    .build()\n-            }\n-            LogicalPlan::Selection { expr, input, .. } => {\n-                LogicalPlanBuilder::from(&self.optimize(input)?)\n-                    .filter(self.rewrite_expr(expr, input.schema())?)?\n-                    .build()\n-            }\n-            LogicalPlan::Aggregate {\n-                input,\n-                group_expr,\n-                aggr_expr,\n-                ..\n-            } => LogicalPlanBuilder::from(&self.optimize(input)?)\n-                .aggregate(\n-                    self.rewrite_expr_list(group_expr, input.schema())?,\n-                    self.rewrite_expr_list(aggr_expr, input.schema())?,\n-                )?\n-                .build(),\n-            LogicalPlan::TableScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::InMemoryScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::ParquetScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::CsvScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::EmptyRelation { .. } => Ok(plan.clone()),\n-            LogicalPlan::Limit { .. } => Ok(plan.clone()),\n-            LogicalPlan::Sort { .. } => Ok(plan.clone()),\n-            LogicalPlan::CreateExternalTable { .. } => Ok(plan.clone()),\n-        }\n+        let inputs = utils::inputs(plan);\n+        let expressions = utils::expressions(plan);\n+\n+        // apply the optimization to all inputs of the plan\n+        let new_inputs = inputs\n+            .iter()\n+            .map(|plan| self.optimize(*plan))\n+            .collect::<Result<Vec<_>>>()?;\n+        // re-write all expressions on this plan.\n+        // This assumes a single input, [0]. It wont work for join, subqueries and union operations with more than one input.\n+        // It is currently not an issue as we do not have any plan with more than one input.\n+        let new_expressions = expressions\n\nReview comment:\n       Good catch. No because I am unsure how that could be possible: if we have expressions on a plan, we need an `input` to convert them to physical expressions and evaluate them against. AFAIK an expression always requires an input to be evaluated against. \r\n   \r\n   Do you have an example in mind?\r\n   \r\n   AFAIK even a literal expression requires a schema to pass to `Expr::get_type` and `Expr::name`.\r\n   \r\n   \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-14T05:16:13.712+0000",
                    "updated": "2020-08-14T05:16:13.712+0000",
                    "started": "2020-08-14T05:16:13.712+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "470575",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/470576",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-673890773\n\n\n   Thank you very much @alamb for reviewing it!\r\n   \r\n   This optimizer is mostly useful in the `table` or `DataFrame` API, on which a view can be declared as a sequence of statements that are not optimized for execution, but optimized for a logical and code organization's point of view.\r\n   \r\n   One example is when we have a dataframe `df` that was constructed optimally, but we would like to only look at rows whose `'a' > 2`. Instead of having to go through the actual code that built that DataFrame and place the filter in the correct place after investigating where we should place it, we can just write `df.filter(df['a'] > 2).collect()`, and let the optimizer figure it out where to place it.\r\n   \r\n   I incorporated the comments above into #7879 , as IMO they are part of that PR, and rebased the whole thing. I will still address your comment about not full understanding the algorithm by adding a more extended comment and maybe try drawing some ASCII to better explain the idea, so that it is not only on my head.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-14T05:24:05.702+0000",
                    "updated": "2020-08-14T05:24:05.702+0000",
                    "started": "2020-08-14T05:24:05.702+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "470576",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/470772",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r470753855\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/type_coercion.rs\n##########\n@@ -43,138 +45,77 @@ impl<'a> TypeCoercionRule<'a> {\n         Self { scalar_functions }\n     }\n \n-    /// Rewrite an expression list to include explicit CAST operations when required\n-    fn rewrite_expr_list(&self, expr: &[Expr], schema: &Schema) -> Result<Vec<Expr>> {\n-        Ok(expr\n+    /// Rewrite an expression to include explicit CAST operations when required\n+    fn rewrite_expr(&self, expr: &Expr, schema: &Schema) -> Result<Expr> {\n+        let expressions = utils::expr_expressions(expr)?;\n+\n+        // recurse of the re-write\n+        let mut expressions = expressions\n             .iter()\n             .map(|e| self.rewrite_expr(e, schema))\n-            .collect::<Result<Vec<_>>>()?)\n-    }\n+            .collect::<Result<Vec<_>>>()?;\n \n-    /// Rewrite an expression to include explicit CAST operations when required\n-    fn rewrite_expr(&self, expr: &Expr, schema: &Schema) -> Result<Expr> {\n+        // modify `expressions` by introducing casts when necessary\n         match expr {\n-            Expr::BinaryExpr { left, op, right } => {\n-                let left = self.rewrite_expr(left, schema)?;\n-                let right = self.rewrite_expr(right, schema)?;\n-                let left_type = left.get_type(schema)?;\n-                let right_type = right.get_type(schema)?;\n-                if left_type == right_type {\n-                    Ok(Expr::BinaryExpr {\n-                        left: Box::new(left),\n-                        op: op.clone(),\n-                        right: Box::new(right),\n-                    })\n-                } else {\n+            Expr::BinaryExpr { .. } => {\n+                let left_type = expressions[0].get_type(schema)?;\n+                let right_type = expressions[1].get_type(schema)?;\n+                if left_type != right_type {\n                     let super_type = utils::get_supertype(&left_type, &right_type)?;\n-                    Ok(Expr::BinaryExpr {\n-                        left: Box::new(left.cast_to(&super_type, schema)?),\n-                        op: op.clone(),\n-                        right: Box::new(right.cast_to(&super_type, schema)?),\n-                    })\n+\n+                    expressions[0] = expressions[0].cast_to(&super_type, schema)?;\n+                    expressions[1] = expressions[1].cast_to(&super_type, schema)?;\n                 }\n             }\n-            Expr::IsNull(e) => Ok(Expr::IsNull(Box::new(self.rewrite_expr(e, schema)?))),\n-            Expr::IsNotNull(e) => {\n-                Ok(Expr::IsNotNull(Box::new(self.rewrite_expr(e, schema)?)))\n-            }\n-            Expr::ScalarFunction {\n-                name,\n-                args,\n-                return_type,\n-            } => {\n+            Expr::ScalarFunction { name, .. } => {\n                 // cast the inputs of scalar functions to the appropriate type where possible\n                 match self.scalar_functions.get(name) {\n                     Some(func_meta) => {\n-                        let mut func_args = Vec::with_capacity(args.len());\n-                        for i in 0..args.len() {\n+                        for i in 0..expressions.len() {\n                             let field = &func_meta.args[i];\n-                            let expr = self.rewrite_expr(&args[i], schema)?;\n-                            let actual_type = expr.get_type(schema)?;\n+                            let actual_type = expressions[i].get_type(schema)?;\n                             let required_type = field.data_type();\n-                            if &actual_type == required_type {\n-                                func_args.push(expr)\n-                            } else {\n+                            if &actual_type != required_type {\n                                 let super_type =\n                                     utils::get_supertype(&actual_type, required_type)?;\n-                                func_args.push(expr.cast_to(&super_type, schema)?);\n-                            }\n+                                expressions[i] =\n+                                    expressions[i].cast_to(&super_type, schema)?\n+                            };\n                         }\n-\n-                        Ok(Expr::ScalarFunction {\n-                            name: name.clone(),\n-                            args: func_args,\n-                            return_type: return_type.clone(),\n-                        })\n                     }\n-                    _ => Err(ExecutionError::General(format!(\n-                        \"Invalid scalar function {}\",\n-                        name\n-                    ))),\n+                    _ => {\n+                        return Err(ExecutionError::General(format!(\n+                            \"Invalid scalar function {}\",\n+                            name\n+                        )))\n+                    }\n                 }\n             }\n-            Expr::AggregateFunction {\n-                name,\n-                args,\n-                return_type,\n-            } => Ok(Expr::AggregateFunction {\n-                name: name.clone(),\n-                args: args\n-                    .iter()\n-                    .map(|a| self.rewrite_expr(a, schema))\n-                    .collect::<Result<Vec<_>>>()?,\n-                return_type: return_type.clone(),\n-            }),\n-            Expr::Cast { .. } => Ok(expr.clone()),\n-            Expr::Column(_) => Ok(expr.clone()),\n-            Expr::Alias(expr, alias) => Ok(Expr::Alias(\n-                Box::new(self.rewrite_expr(expr, schema)?),\n-                alias.to_owned(),\n-            )),\n-            Expr::Literal(_) => Ok(expr.clone()),\n-            Expr::Not(_) => Ok(expr.clone()),\n-            Expr::Sort { .. } => Ok(expr.clone()),\n-            Expr::Wildcard { .. } => Err(ExecutionError::General(\n-                \"Wildcard expressions are not valid in a logical query plan\".to_owned(),\n-            )),\n-            Expr::Nested(e) => self.rewrite_expr(e, schema),\n-        }\n+            _ => {}\n+        };\n+        utils::from_expression(expr, &expressions)\n     }\n }\n \n impl<'a> OptimizerRule for TypeCoercionRule<'a> {\n     fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n-        match plan {\n-            LogicalPlan::Projection { expr, input, .. } => {\n-                LogicalPlanBuilder::from(&self.optimize(input)?)\n-                    .project(self.rewrite_expr_list(expr, input.schema())?)?\n-                    .build()\n-            }\n-            LogicalPlan::Selection { expr, input, .. } => {\n-                LogicalPlanBuilder::from(&self.optimize(input)?)\n-                    .filter(self.rewrite_expr(expr, input.schema())?)?\n-                    .build()\n-            }\n-            LogicalPlan::Aggregate {\n-                input,\n-                group_expr,\n-                aggr_expr,\n-                ..\n-            } => LogicalPlanBuilder::from(&self.optimize(input)?)\n-                .aggregate(\n-                    self.rewrite_expr_list(group_expr, input.schema())?,\n-                    self.rewrite_expr_list(aggr_expr, input.schema())?,\n-                )?\n-                .build(),\n-            LogicalPlan::TableScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::InMemoryScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::ParquetScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::CsvScan { .. } => Ok(plan.clone()),\n-            LogicalPlan::EmptyRelation { .. } => Ok(plan.clone()),\n-            LogicalPlan::Limit { .. } => Ok(plan.clone()),\n-            LogicalPlan::Sort { .. } => Ok(plan.clone()),\n-            LogicalPlan::CreateExternalTable { .. } => Ok(plan.clone()),\n-        }\n+        let inputs = utils::inputs(plan);\n+        let expressions = utils::expressions(plan);\n+\n+        // apply the optimization to all inputs of the plan\n+        let new_inputs = inputs\n+            .iter()\n+            .map(|plan| self.optimize(*plan))\n+            .collect::<Result<Vec<_>>>()?;\n+        // re-write all expressions on this plan.\n+        // This assumes a single input, [0]. It wont work for join, subqueries and union operations with more than one input.\n+        // It is currently not an issue as we do not have any plan with more than one input.\n+        let new_expressions = expressions\n\nReview comment:\n       ```suggestion\r\n           assert!(expressions.len() == 0 || inputs.len() > 0, \"Assume that all plan nodes with expressions had inputs\");\r\n           let new_expressions = expressions\r\n   ```\r\n   \r\n   I think the `EmptyRelation`,  https://github.com/apache/arrow/blob/master/rust/datafusion/src/logicalplan.rs#L761-L764, for example has no input LogicalPlan, but perhaps you are saying \"even though `EmptyRelation` has no inputs (and thus could cause `inputs[0].schema()` to panic) it also has no Expressions then the potential panic'ing code won't be run. \r\n   \r\n   I guess I was thinking to the  future where we add expressions to root nodes (e.g. perhaps filtering *during* a table scan or something) which would then have expressions but no input.\r\n   \r\n    I think this code is fine as is. Perhaps we could make the code slightly easier to work with in the future if we did something like the assert suggestion here that there were no inputs if there were expressions rather than panic. \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-14T17:21:44.498+0000",
                    "updated": "2020-08-14T17:21:44.498+0000",
                    "started": "2020-08-14T17:21:44.498+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "470772",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/470824",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-674211939\n\n\n   @alamb , I have added a comment describing the algorithm. Could you take a look and evaluate if it helps at understanding the underlying code?\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-14T18:43:28.324+0000",
                    "updated": "2020-08-14T18:43:28.324+0000",
                    "started": "2020-08-14T18:43:28.324+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "470824",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/470877",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r470850721\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (tipically a scan).\n\nReview comment:\n       ```suggestion\r\n   and highest for the first operation (typically a scan).\r\n   ```\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n\nReview comment:\n       ```suggestion\r\n   A filter-commutative operation is an operation whose result of filter(op(data)) = op(filter(data)).\r\n   ```\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (tipically a scan).\n+\n+This optimizer commutes filters with filter-commutative operations to push the filters\n+to the maximum possible depth, consequently re-writing the filter expressions by every\n+projection that changes the filter's expression.\n+\n+    Selection: #b Gt Int64(10)\n+        Projection: #a AS b\n+\n+is optimized to\n+\n+    Projection: #a AS b\n+        Selection: #a Gt Int64(10)  <--- changed from #b to #a\n+\n+To perform such optimization, we first analyze the plan to identify three items:\n\nReview comment:\n       This comment / description helps a lot. Thank you @jorgecarleitao \n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (tipically a scan).\n+\n+This optimizer commutes filters with filter-commutative operations to push the filters\n+to the maximum possible depth, consequently re-writing the filter expressions by every\n+projection that changes the filter's expression.\n+\n+    Selection: #b Gt Int64(10)\n+        Projection: #a AS b\n+\n+is optimized to\n+\n+    Projection: #a AS b\n+        Selection: #a Gt Int64(10)  <--- changed from #b to #a\n+\n+To perform such optimization, we first analyze the plan to identify three items:\n+\n+1. Where are the filters located in the plan\n+2. Where are non-commutable operations' columns located in the plan (break_points)\n+3. Where are projections located in the plan\n+\n+With this information, we re-write the plan by:\n+\n+1. Computing the maximum possible depth of each column\n+2. Computing the maximum possible depth of each filter expression based on the columns it depends on\n+3. re-write the filter expression for every projection that it commutes with from its original depth to its max possible depth\n+*/\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let (break_points, selections, projections) = analyze_plan(plan, 0)?;\n+\n+        // compute max depth for each of the columns\n+        let mut breaks: HashMap<String, usize> = HashMap::new();\n\nReview comment:\n       I don't understand why this is needed -- `break_points` is already keyed on a column name (and thus the `breaks.get(key)` should never match. Indeed when I add an `assert!(false)` to this clause locally, all the tests still pass just fine. I may be missing something obvious\r\n   \r\n   ```\r\n   diff --git a/rust/datafusion/src/optimizer/filter_push_down.rs b/rust/datafusion/src/optimizer/filter_push_down.rs\r\n   index f6f36e4df..f2e306e0b 100644\r\n   --- a/rust/datafusion/src/optimizer/filter_push_down.rs\r\n   +++ b/rust/datafusion/src/optimizer/filter_push_down.rs\r\n   @@ -74,6 +74,7 @@ impl OptimizerRule for FilterPushDown {\r\n            for (key, depth) in break_points {\r\n                match breaks.get(&key) {\r\n                    Some(current_depth) => {\r\n   +                    assert!(false);\r\n                        if depth > *current_depth {\r\n                            breaks.insert(key, depth);\r\n                        }\r\n   alamb@MacBook-Pro arrow % \r\n   ```\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-14T20:50:53.275+0000",
                    "updated": "2020-08-14T20:50:53.275+0000",
                    "started": "2020-08-14T20:50:53.275+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "470877",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471009",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "houqp commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r470041733\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,467 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let (break_points, selections, projections) = analyze_plan(plan, 0)?;\n+\n+        // compute max depth for each of the columns\n+        let mut breaks: HashMap<String, usize> = HashMap::new();\n+        for (key, depth) in break_points {\n+            match breaks.get(&key) {\n+                Some(current_depth) => {\n+                    if depth > *current_depth {\n+                        breaks.insert(key, depth);\n+                    }\n+                }\n+                None => {\n+                    breaks.insert(key, depth);\n+                }\n+            }\n+        }\n+\n+        // construct optimized position of each of the new selections\n+        let mut new_selections: HashMap<usize, Expr> = HashMap::new();\n+        for (selection_depth, expr) in selections {\n+            let mut columns: HashSet<String> = HashSet::new();\n+            utils::expr_to_column_names(&expr, &mut columns)?;\n+\n+            // compute the depths of each of the observed columns and the respective maximum\n+            let depth = columns\n+                .iter()\n+                .filter_map(|column| breaks.get(column))\n+                .max_by_key(|depth| **depth);\n+\n+            let new_depth = match depth {\n+                None => selection_depth,\n+                Some(d) => *d,\n+            };\n+\n+            // re-write the new selections based on all projections that it crossed.\n+            // E.g. in `Selection: #b\\n  Projection: #a > 1 as b`, we can swap them, but the selection must be \"#a > 1\"\n+            let mut new_expression = expr.clone();\n+            for depth_i in selection_depth..new_depth {\n+                if let Some(projection) = projections.get(&depth_i) {\n+                    new_expression = rewrite(&new_expression, projection)?;\n+                }\n+            }\n+\n+            new_selections.insert(new_depth, new_expression);\n+        }\n+\n+        optimize_plan(plan, &new_selections, 0)\n+    }\n+}\n+\n+/// Recursively transverses the logical plan looking for depths that break filter pushdown\n+/// Returns a tuple:\n+/// 0: map \"column -> depth\" of the depth that each column is found up to.\n+/// 1: map \"depth -> filter expression\"\n+/// 2: map \"depth -> projection\"\n+fn analyze_plan(\n+    plan: &LogicalPlan,\n+    depth: usize,\n+) -> Result<(\n+    HashMap<String, usize>,\n+    HashMap<usize, Expr>,\n+    HashMap<usize, HashMap<String, Expr>>,\n+)> {\n+    match plan {\n+        LogicalPlan::Selection { input, expr } => {\n+            let mut result = analyze_plan(&input, depth + 1)?;\n+            result.1.insert(depth, expr.clone());\n\nReview comment:\n       I find `result.{0,1,2}.insert` a little bit hard to follow when going through the code, had to keep going back to the function comment to find out which index refers to which map. Would be better if we create a named struct to store the return value.\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (tipically a scan).\n+\n+This optimizer commutes filters with filter-commutative operations to push the filters\n+to the maximum possible depth, consequently re-writing the filter expressions by every\n+projection that changes the filter's expression.\n+\n+    Selection: #b Gt Int64(10)\n+        Projection: #a AS b\n+\n+is optimized to\n+\n+    Projection: #a AS b\n+        Selection: #a Gt Int64(10)  <--- changed from #b to #a\n+\n+To perform such optimization, we first analyze the plan to identify three items:\n+\n+1. Where are the filters located in the plan\n+2. Where are non-commutable operations' columns located in the plan (break_points)\n+3. Where are projections located in the plan\n+\n+With this information, we re-write the plan by:\n+\n+1. Computing the maximum possible depth of each column\n+2. Computing the maximum possible depth of each filter expression based on the columns it depends on\n+3. re-write the filter expression for every projection that it commutes with from its original depth to its max possible depth\n\nReview comment:\n       i think we missed step 4. selection push down? Would be helpful to mention push down starts from the the first operation (highest depth) in the plan as well.\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (tipically a scan).\n+\n+This optimizer commutes filters with filter-commutative operations to push the filters\n+to the maximum possible depth, consequently re-writing the filter expressions by every\n+projection that changes the filter's expression.\n+\n+    Selection: #b Gt Int64(10)\n+        Projection: #a AS b\n+\n+is optimized to\n+\n+    Projection: #a AS b\n+        Selection: #a Gt Int64(10)  <--- changed from #b to #a\n+\n+To perform such optimization, we first analyze the plan to identify three items:\n+\n+1. Where are the filters located in the plan\n+2. Where are non-commutable operations' columns located in the plan (break_points)\n+3. Where are projections located in the plan\n+\n+With this information, we re-write the plan by:\n+\n+1. Computing the maximum possible depth of each column\n+2. Computing the maximum possible depth of each filter expression based on the columns it depends on\n+3. re-write the filter expression for every projection that it commutes with from its original depth to its max possible depth\n+*/\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let (break_points, selections, projections) = analyze_plan(plan, 0)?;\n+\n+        // compute max depth for each of the columns\n+        let mut breaks: HashMap<String, usize> = HashMap::new();\n\nReview comment:\n       yes i think you are right, key is guaranteed to be unique for each iteration. hashmap is probably the wrong data structure to use for `break_points`. or we can do the depth comparison directly in `analyze_plan` on every map insertion.\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (tipically a scan).\n+\n+This optimizer commutes filters with filter-commutative operations to push the filters\n+to the maximum possible depth, consequently re-writing the filter expressions by every\n+projection that changes the filter's expression.\n+\n+    Selection: #b Gt Int64(10)\n+        Projection: #a AS b\n+\n+is optimized to\n+\n+    Projection: #a AS b\n+        Selection: #a Gt Int64(10)  <--- changed from #b to #a\n+\n+To perform such optimization, we first analyze the plan to identify three items:\n\nReview comment:\n       100% on this, great write up :+1: \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-15T05:12:28.622+0000",
                    "updated": "2020-08-15T05:12:28.622+0000",
                    "started": "2020-08-15T05:12:28.622+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471009",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471072",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r470975358\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,467 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let (break_points, selections, projections) = analyze_plan(plan, 0)?;\n+\n+        // compute max depth for each of the columns\n+        let mut breaks: HashMap<String, usize> = HashMap::new();\n+        for (key, depth) in break_points {\n+            match breaks.get(&key) {\n+                Some(current_depth) => {\n+                    if depth > *current_depth {\n+                        breaks.insert(key, depth);\n+                    }\n+                }\n+                None => {\n+                    breaks.insert(key, depth);\n+                }\n+            }\n+        }\n+\n+        // construct optimized position of each of the new selections\n+        let mut new_selections: HashMap<usize, Expr> = HashMap::new();\n+        for (selection_depth, expr) in selections {\n+            let mut columns: HashSet<String> = HashSet::new();\n+            utils::expr_to_column_names(&expr, &mut columns)?;\n+\n+            // compute the depths of each of the observed columns and the respective maximum\n+            let depth = columns\n+                .iter()\n+                .filter_map(|column| breaks.get(column))\n+                .max_by_key(|depth| **depth);\n+\n+            let new_depth = match depth {\n+                None => selection_depth,\n+                Some(d) => *d,\n+            };\n+\n+            // re-write the new selections based on all projections that it crossed.\n+            // E.g. in `Selection: #b\\n  Projection: #a > 1 as b`, we can swap them, but the selection must be \"#a > 1\"\n+            let mut new_expression = expr.clone();\n+            for depth_i in selection_depth..new_depth {\n+                if let Some(projection) = projections.get(&depth_i) {\n+                    new_expression = rewrite(&new_expression, projection)?;\n+                }\n+            }\n+\n+            new_selections.insert(new_depth, new_expression);\n+        }\n+\n+        optimize_plan(plan, &new_selections, 0)\n+    }\n+}\n+\n+/// Recursively transverses the logical plan looking for depths that break filter pushdown\n+/// Returns a tuple:\n+/// 0: map \"column -> depth\" of the depth that each column is found up to.\n+/// 1: map \"depth -> filter expression\"\n+/// 2: map \"depth -> projection\"\n+fn analyze_plan(\n+    plan: &LogicalPlan,\n+    depth: usize,\n+) -> Result<(\n+    HashMap<String, usize>,\n+    HashMap<usize, Expr>,\n+    HashMap<usize, HashMap<String, Expr>>,\n+)> {\n+    match plan {\n+        LogicalPlan::Selection { input, expr } => {\n+            let mut result = analyze_plan(&input, depth + 1)?;\n+            result.1.insert(depth, expr.clone());\n\nReview comment:\n       I agree -- having these as a named struct would improve the readability in my opinion.\r\n   \r\n   The other alternative is to use fields on `self` as well\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-15T12:58:46.700+0000",
                    "updated": "2020-08-15T12:58:46.700+0000",
                    "started": "2020-08-15T12:58:46.700+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471072",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471095",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r471004814\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (tipically a scan).\n+\n+This optimizer commutes filters with filter-commutative operations to push the filters\n+to the maximum possible depth, consequently re-writing the filter expressions by every\n+projection that changes the filter's expression.\n+\n+    Selection: #b Gt Int64(10)\n+        Projection: #a AS b\n+\n+is optimized to\n+\n+    Projection: #a AS b\n+        Selection: #a Gt Int64(10)  <--- changed from #b to #a\n+\n+To perform such optimization, we first analyze the plan to identify three items:\n+\n+1. Where are the filters located in the plan\n+2. Where are non-commutable operations' columns located in the plan (break_points)\n+3. Where are projections located in the plan\n+\n+With this information, we re-write the plan by:\n+\n+1. Computing the maximum possible depth of each column\n+2. Computing the maximum possible depth of each filter expression based on the columns it depends on\n+3. re-write the filter expression for every projection that it commutes with from its original depth to its max possible depth\n+*/\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let (break_points, selections, projections) = analyze_plan(plan, 0)?;\n+\n+        // compute max depth for each of the columns\n+        let mut breaks: HashMap<String, usize> = HashMap::new();\n\nReview comment:\n       Good catch. Yes, you are right. Leftovers from a previous iteration :/\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-15T15:47:06.750+0000",
                    "updated": "2020-08-15T15:47:06.750+0000",
                    "started": "2020-08-15T15:47:06.750+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471095",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471098",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r471007087\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,505 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is a operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (tipically a scan).\n+\n+This optimizer commutes filters with filter-commutative operations to push the filters\n+to the maximum possible depth, consequently re-writing the filter expressions by every\n+projection that changes the filter's expression.\n+\n+    Selection: #b Gt Int64(10)\n+        Projection: #a AS b\n+\n+is optimized to\n+\n+    Projection: #a AS b\n+        Selection: #a Gt Int64(10)  <--- changed from #b to #a\n+\n+To perform such optimization, we first analyze the plan to identify three items:\n+\n+1. Where are the filters located in the plan\n+2. Where are non-commutable operations' columns located in the plan (break_points)\n+3. Where are projections located in the plan\n+\n+With this information, we re-write the plan by:\n+\n+1. Computing the maximum possible depth of each column\n+2. Computing the maximum possible depth of each filter expression based on the columns it depends on\n+3. re-write the filter expression for every projection that it commutes with from its original depth to its max possible depth\n+*/\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let (break_points, selections, projections) = analyze_plan(plan, 0)?;\n+\n+        // compute max depth for each of the columns\n+        let mut breaks: HashMap<String, usize> = HashMap::new();\n\nReview comment:\n       The hashmap is updated on the analyze to be the lowest depth. I pushed another test just to make sure that we do not mix up depths.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-15T15:59:31.976+0000",
                    "updated": "2020-08-15T15:59:31.976+0000",
                    "started": "2020-08-15T15:59:31.975+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471098",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471100",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r471008766\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,467 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let (break_points, selections, projections) = analyze_plan(plan, 0)?;\n+\n+        // compute max depth for each of the columns\n+        let mut breaks: HashMap<String, usize> = HashMap::new();\n+        for (key, depth) in break_points {\n+            match breaks.get(&key) {\n+                Some(current_depth) => {\n+                    if depth > *current_depth {\n+                        breaks.insert(key, depth);\n+                    }\n+                }\n+                None => {\n+                    breaks.insert(key, depth);\n+                }\n+            }\n+        }\n+\n+        // construct optimized position of each of the new selections\n+        let mut new_selections: HashMap<usize, Expr> = HashMap::new();\n+        for (selection_depth, expr) in selections {\n+            let mut columns: HashSet<String> = HashSet::new();\n+            utils::expr_to_column_names(&expr, &mut columns)?;\n+\n+            // compute the depths of each of the observed columns and the respective maximum\n+            let depth = columns\n+                .iter()\n+                .filter_map(|column| breaks.get(column))\n+                .max_by_key(|depth| **depth);\n+\n+            let new_depth = match depth {\n+                None => selection_depth,\n+                Some(d) => *d,\n+            };\n+\n+            // re-write the new selections based on all projections that it crossed.\n+            // E.g. in `Selection: #b\\n  Projection: #a > 1 as b`, we can swap them, but the selection must be \"#a > 1\"\n+            let mut new_expression = expr.clone();\n+            for depth_i in selection_depth..new_depth {\n+                if let Some(projection) = projections.get(&depth_i) {\n+                    new_expression = rewrite(&new_expression, projection)?;\n+                }\n+            }\n+\n+            new_selections.insert(new_depth, new_expression);\n+        }\n+\n+        optimize_plan(plan, &new_selections, 0)\n+    }\n+}\n+\n+/// Recursively transverses the logical plan looking for depths that break filter pushdown\n+/// Returns a tuple:\n+/// 0: map \"column -> depth\" of the depth that each column is found up to.\n+/// 1: map \"depth -> filter expression\"\n+/// 2: map \"depth -> projection\"\n+fn analyze_plan(\n+    plan: &LogicalPlan,\n+    depth: usize,\n+) -> Result<(\n+    HashMap<String, usize>,\n+    HashMap<usize, Expr>,\n+    HashMap<usize, HashMap<String, Expr>>,\n+)> {\n+    match plan {\n+        LogicalPlan::Selection { input, expr } => {\n+            let mut result = analyze_plan(&input, depth + 1)?;\n+            result.1.insert(depth, expr.clone());\n\nReview comment:\n       Good call. I've made it a struct. I tend to stick to functions to avoid issues with the lifetime of `self` (e.g. two calls of `optimize_plan`).\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-15T16:01:48.779+0000",
                    "updated": "2020-08-15T16:01:48.779+0000",
                    "started": "2020-08-15T16:01:48.778+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471100",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471120",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-674429867\n\n\n   I have had this nagging sensation that the algorithm isn't quite right when the same column is used multiple times. I finally came up with an example that shows part of what I have been worrying about. \r\n   \r\n   Here is a new test that passes on this branch but I think is incorrect. Specifically, with two `Selection`s for the same variable separated by a `Limit`, one of the `Selection`s is  lost with the algorithm as written.  Am I missing something?\r\n   \r\n   ```\r\n   \r\n       #[test]\r\n       fn filter_2_breaks_limits() -> Result<()> {\r\n           let table_scan = test_table_scan()?;\r\n           let plan = LogicalPlanBuilder::from(&table_scan)\r\n               .project(vec![col(\"a\")])?\r\n               .filter(col(\"a\").lt_eq(&Expr::Literal(ScalarValue::Int64(1))))?\r\n               .limit(1)?\r\n               .project(vec![col(\"a\")])?\r\n               .filter(col(\"a\").gt_eq(&Expr::Literal(ScalarValue::Int64(1))))?\r\n               .build()?;\r\n           // Should be able to move both filters below the projections\r\n   \r\n           // not part of the test\r\n           assert_eq!(\r\n               format!(\"{:?}\", plan),\r\n               \"Selection: #a GtEq Int64(1)\\\r\n                \\n  Projection: #a\\\r\n                \\n    Limit: 1\\\r\n                \\n      Selection: #a LtEq Int64(1)\\\r\n                \\n        Projection: #a\\\r\n                \\n          TableScan: test projection=None\"\r\n           );\r\n   \r\n           // This just seems wong: we lost a selection....\r\n           let expected = \"\\\r\n           Projection: #a\\\r\n           \\n  Selection: #a GtEq Int64(1)\\\r\n           \\n    Limit: 1\\\r\n           \\n      Projection: #a\\\r\n           \\n        TableScan: test projection=None\";\r\n   \r\n           assert_optimized_plan_eq(&plan, expected);\r\n           Ok(())\r\n       }\r\n   ```\r\n   \r\n   FYI @jorgecarleitao \r\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-15T17:58:56.208+0000",
                    "updated": "2020-08-15T17:58:56.208+0000",
                    "started": "2020-08-15T17:58:56.208+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471120",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471126",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-674434146\n\n\n   @alamb , thank you so much for taking the time to think through this and come up with an example. I agree with you that it is wrong. I will evaluate whether the current approach is able to coupe with this, or whether we will have to scratch it and start from a different direction.\r\n   \r\n   I changed this PR back to draft as it is obviously out of spec.\r\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-15T18:42:22.630+0000",
                    "updated": "2020-08-15T18:42:22.630+0000",
                    "started": "2020-08-15T18:42:22.629+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471126",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471163",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-674472125\n\n\n   @alamb, @houqp @andygrove , I think that this is ready to re-review.\r\n   \r\n   I modified the result returned by analyze to ensure that we do not lose relevant information (that lead to the error @alamb found).\r\n   \r\n   I also found and fixed another error related to the placement of two filters in the same depth, that caused filters to be dropped: their expressions are now `ANDed` instead, which has the added bonus of gobbling filters together whenever possible.\r\n   \r\n   All the changes are in new commits, in case it is easier for the review.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-16T03:15:30.314+0000",
                    "updated": "2020-08-16T03:15:30.314+0000",
                    "started": "2020-08-16T03:15:30.313+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471163",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471174",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "houqp commented on a change in pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#discussion_r471062961\n\n\n\n##########\nFile path: rust/datafusion/src/optimizer/filter_push_down.rs\n##########\n@@ -0,0 +1,631 @@\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Filter Push Down optimizer rule ensures that filters are applied as early as possible in the plan\n+\n+use crate::error::Result;\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::{and, LogicalPlan};\n+use crate::optimizer::optimizer::OptimizerRule;\n+use crate::optimizer::utils;\n+use std::collections::{BTreeMap, HashMap, HashSet};\n+\n+/// Filter Push Down optimizer rule pushes filter clauses down the plan\n+///\n+/// This optimization looks for the maximum depth of each column in the plan where a filter can be applied and\n+/// re-writes the plan with filters on those locations.\n+/// It performs two passes on the plan:\n+/// 1. identify filters, which columns they use, and projections along the path\n+/// 2. move filters down, re-writing the expressions using the projections\n+/*\n+A filter-commutative operation is an operation whose result of filter(op(data)) = op(filter(data)).\n+An example of a filter-commutative operation is a projection; a counter-example is `limit`.\n+\n+The filter-commutative property is column-specific. An aggregate grouped by A on SUM(B)\n+can commute with a filter that depends on A only, but does not commute with a filter that depends\n+on SUM(B).\n+\n+A location in this module is identified by a number, depth, which is 0 for the last operation\n+and highest for the first operation (typically a scan).\n+\n+This optimizer commutes filters with filter-commutative operations to push the filters\n+to the maximum possible depth, consequently re-writing the filter expressions by every\n+projection that changes the filter's expression.\n+\n+    Selection: #b Gt Int64(10)\n+        Projection: #a AS b\n+\n+is optimized to\n+\n+    Projection: #a AS b\n+        Selection: #a Gt Int64(10)  <--- changed from #b to #a\n+\n+To perform such optimization, we first analyze the plan to identify three items:\n+\n+1. Where are the filters located in the plan\n+2. Where are non-commutable operations' columns located in the plan (break_points)\n+3. Where are projections located in the plan\n+\n+With this information, we re-write the plan by:\n+\n+1. Computing the maximum possible depth of each column between breakpoints\n+2. Computing the maximum possible depth of each filter expression based on the columns it depends on\n+3. re-write the filter expression for every projection that it commutes with from its original depth to its max possible depth\n+4. recursively re-write the plan by deleting old filter expressions and adding new filter expressions on their max possible depth.\n+*/\n+pub struct FilterPushDown {}\n+\n+impl OptimizerRule for FilterPushDown {\n+    fn name(&self) -> &str {\n+        return \"filter_push_down\";\n+    }\n+\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<LogicalPlan> {\n+        let result = analyze_plan(plan, 0)?;\n+        let break_points = result.break_points.clone();\n+\n+        // get max depth over all breakpoints\n+        let max_depth = break_points.keys().max();\n+        if max_depth.is_none() {\n+            // it is unlikely that the plan is correct without break points as all scans\n+            // adds breakpoints. We just return the plan and let others handle the error\n+            return Ok(plan.clone());\n\nReview comment:\n       shouldn't we return error here instead if the plan is not correct?\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-16T05:29:28.192+0000",
                    "updated": "2020-08-16T05:29:28.192+0000",
                    "started": "2020-08-16T05:29:28.192+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471174",
                    "issueId": "13320540"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/worklog/471175",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "houqp commented on pull request #7880:\nURL: https://github.com/apache/arrow/pull/7880#issuecomment-674481844\n\n\n   Something that can be left for future optimization: we can also go the other direction, i.e. break `And` filters into into individual boolean expressions so these filters can be partially pushed further down the plan.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-16T05:33:21.644+0000",
                    "updated": "2020-08-16T05:33:21.644+0000",
                    "started": "2020-08-16T05:33:21.643+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471175",
                    "issueId": "13320540"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 16800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@70712839[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@613953dc[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5eefb3e3[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@75e3ac51[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3d09b9b4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@2905a14c[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@a566f24[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@623fdb1b[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@128b0af1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@1600c62d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2f16b486[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@336d457c[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 16800,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Aug 19 01:33:49 UTC 2020",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2020-08-19T01:33:49.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-9619/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2020-08-02T12:08:27.000+0000",
        "updated": "2020-08-19T01:34:01.000+0000",
        "timeoriginalestimate": null,
        "description": "Like the title says, add an optimizer to push filters down the plan as farther as logically possible.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "4h 40m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 16800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Rust] [DataFusion] Add predicate push-down",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13320540/comment/17180200",
                    "id": "17180200",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "body": "Issue resolved by pull request 7880\n[https://github.com/apache/arrow/pull/7880]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "created": "2020-08-19T01:33:49.888+0000",
                    "updated": "2020-08-19T01:33:49.888+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0hei0:",
        "customfield_12314139": null
    }
}