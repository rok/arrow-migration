{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13081467",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467",
    "key": "ARROW-1134",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12341352",
                "id": "12341352",
                "name": "0.8.0",
                "archived": false,
                "released": true,
                "releaseDate": "2017-12-18"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
            "name": "Minor",
            "id": "4"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": null,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12516498",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12516498",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13106966",
                    "key": "ARROW-1641",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13106966",
                    "fields": {
                        "summary": "[C++] Do not include <mutex> in public headers",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": null,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
            "name": "tobyshaw",
            "key": "tobyshaw",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Toby Shaw",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
            "name": "tobyshaw",
            "key": "tobyshaw",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Toby Shaw",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1134/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 0,
            "worklogs": []
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": null,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@2ecd2cd7[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@474fbc2d[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5bfe2de5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@61ea5c63[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@8d7abae[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@4c010a1[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2efb84d9[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@1d41dc21[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@28fe8efc[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@2cbfd65a[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5e104258[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@4da33d6f[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": null,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Oct 25 13:08:48 UTC 2017",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2017-10-25T13:08:44.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1134/watchers",
            "watchCount": 4,
            "isWatching": false
        },
        "created": "2017-06-21T15:55:09.000+0000",
        "updated": "2017-10-25T13:08:48.000+0000",
        "timeoriginalestimate": null,
        "description": "Currently, the inclusion of <mutex> in some of Arrow's C++ headers prevents C++/CLI code from building against it.\n\nFrom a C++/CLI project:\n\n#include <arrow/io/file.h>\n...\n\n\"#error directive: <mutex> is not supported when compiling with /clr or /clr:pure.\"\n\nThis could be patched by optionally relying on Boost's mutex/lock_guard instead of std, or not exposing the #include <mutex> publically.",
        "customfield_10010": null,
        "timetracking": {},
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Allow C++/CLI projects to build with Arrow\u200b",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16057764",
                    "id": "16057764",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=uwe",
                        "name": "uwe",
                        "key": "xhochy",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xhochy&avatarId=30652",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xhochy&avatarId=30652",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xhochy&avatarId=30652",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xhochy&avatarId=30652"
                        },
                        "displayName": "Uwe Korn",
                        "active": true,
                        "timeZone": "Europe/Berlin"
                    },
                    "body": "[~tobyshaw] I would be in favour here to not expose the {{#include <mutex>}} publicly. This won't be straightforward as you would need to write private implementations then for some of the classes. Alternatively in some cases, we probably could implement functions without a mutex, e.g. https://github.com/apache/arrow/blob/master/cpp/src/arrow/io/memory.cc#L152 could be refactored to be lock-free. Can you make a patch for that?\n\nIs there a list showing what is disallowed in C++/CLI?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=uwe",
                        "name": "uwe",
                        "key": "xhochy",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xhochy&avatarId=30652",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xhochy&avatarId=30652",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xhochy&avatarId=30652",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xhochy&avatarId=30652"
                        },
                        "displayName": "Uwe Korn",
                        "active": true,
                        "timeZone": "Europe/Berlin"
                    },
                    "created": "2017-06-21T16:15:08.164+0000",
                    "updated": "2017-06-21T16:15:08.164+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16057806",
                    "id": "16057806",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
                        "name": "tobyshaw",
                        "key": "tobyshaw",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Toby Shaw",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "I agree that is probably the most sensible way to fix this.\n\nAs for the list, sadly I don't believe there exists one (if there is I would love to know about it). It's a fair point that simply hiding the mutex includes might not necessarily resolve all of the restrictions on C++/CLI. Having said that, I locally resolved the mutex issue by using Boost's headers instead, and there was one complaint about a nullptr (fixed by replacing it with NULL), and after that it worked fine. Of course I haven't touched the entire Arrow API, so certainly more digging might be needed.\n\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
                        "name": "tobyshaw",
                        "key": "tobyshaw",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Toby Shaw",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-06-21T16:38:14.808+0000",
                    "updated": "2017-06-21T16:38:24.902+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16058015",
                    "id": "16058015",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=uwe",
                        "name": "uwe",
                        "key": "xhochy",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xhochy&avatarId=30652",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xhochy&avatarId=30652",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xhochy&avatarId=30652",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xhochy&avatarId=30652"
                        },
                        "displayName": "Uwe Korn",
                        "active": true,
                        "timeZone": "Europe/Berlin"
                    },
                    "body": "It would be nice if we would have a compile check in our AppVeyor CI build that tests if you can use the Arrow API in C++/CLI.\nNote that you should be able to include all public Arrow headers in your code by {{#include <arrow/api.h>}}",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=uwe",
                        "name": "uwe",
                        "key": "xhochy",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xhochy&avatarId=30652",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xhochy&avatarId=30652",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xhochy&avatarId=30652",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xhochy&avatarId=30652"
                        },
                        "displayName": "Uwe Korn",
                        "active": true,
                        "timeZone": "Europe/Berlin"
                    },
                    "created": "2017-06-21T18:48:47.919+0000",
                    "updated": "2017-06-21T18:48:47.919+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16058315",
                    "id": "16058315",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "I'm OK with hiding mutexes in private implementations where needed",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-06-21T21:48:18.245+0000",
                    "updated": "2017-06-21T21:48:18.245+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16059300",
                    "id": "16059300",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
                        "name": "tobyshaw",
                        "key": "tobyshaw",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Toby Shaw",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "[~xhochy] Somewhat tangential, but I'm not sure where else to post this.\n\nYou mention all public Arrow headers can be included via #include <arrow/api.h>. Unless I'm mistaken, this doesn't seem to include arrow/io/file.h, which is included in parquet-cpp, see: [https://github.com/apache/parquet-cpp/blob/master/src/parquet/file/reader.cc#L26]. Should parquet depend on non-public Arrow headers?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
                        "name": "tobyshaw",
                        "key": "tobyshaw",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Toby Shaw",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-06-22T12:56:01.984+0000",
                    "updated": "2017-06-22T12:56:01.984+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16059341",
                    "id": "16059341",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "That's an artifact of that fact that libarrow used to be 3 libraries (libarrow/libarrow_io/libarrow_ipc). We should clean up the \"api.h\" files",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-06-22T13:19:32.068+0000",
                    "updated": "2017-06-22T13:19:32.068+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16059417",
                    "id": "16059417",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
                        "name": "tobyshaw",
                        "key": "tobyshaw",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Toby Shaw",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "I mentioned earlier about C++ CLI complaining about nullptr usage. C++ CLI already had a nullptr keyword when C++11 introduced its own. This leads to incompatibilities when building with native projects which use nullptrs in header files.\n\nAnnoyingly, the nullptrs which exist in header files are as default arguments, which need to live in headers. The recommended fix is to use __nullptr instead of nullptr, but I'm not sure if that is valid C++ outside of Visual C++\n\n\n(All these changes also apply to parquet-cpp)",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
                        "name": "tobyshaw",
                        "key": "tobyshaw",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Toby Shaw",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-06-22T13:47:44.782+0000",
                    "updated": "2017-06-22T13:50:59.086+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16059424",
                    "id": "16059424",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Is there some way to detect C++CLI with an ifdef?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-06-22T13:53:03.216+0000",
                    "updated": "2017-06-22T13:53:03.216+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16059432",
                    "id": "16059432",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
                        "name": "tobyshaw",
                        "key": "tobyshaw",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Toby Shaw",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "__cplusplus_cli seems to work, I'll do that.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=tobyshaw",
                        "name": "tobyshaw",
                        "key": "tobyshaw",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Toby Shaw",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-06-22T13:57:07.090+0000",
                    "updated": "2017-06-22T13:57:07.090+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16090148",
                    "id": "16090148",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "[~tobyshaw] marked for 0.6.0. Would you be able to submit a patch? ",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-07-17T17:36:49.751+0000",
                    "updated": "2017-07-17T17:36:49.751+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16164458",
                    "id": "16164458",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "GitHub user TobyShaw opened a pull request:\n\n    https://github.com/apache/arrow/pull/1098\n\n    ARROW-1134 - Allow C++/CLI projects to build with Arrow\n\n    I'm going to attempt a few things:\n    \n    In public headers:\n    Hide mutexes through pimpl pattern.\n    Replace nullptr with __nullptr when building in C++/CLI mode.\n    \n\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/TobyShaw/arrow master\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/arrow/pull/1098.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #1098\n    \n----\ncommit d8cb1f676452af940bbc963c43ab3c5809d85459\nAuthor: labuser <labuser@labwks10>\nDate:   2017-09-13T10:34:21Z\n\n    Attempt PImpl on FixedSizeBufferWriter\n\ncommit d1903d7aa768a27e304c3f00d59cb725dfddf0f2\nAuthor: labuser <labuser@labwks10>\nDate:   2017-09-13T10:35:29Z\n\n    remove mutex include\n\n----\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-09-13T10:38:20.734+0000",
                    "updated": "2017-09-13T10:38:20.734+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16164459",
                    "id": "16164459",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user TobyShaw commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    Work-in-progress.\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-09-13T10:38:46.807+0000",
                    "updated": "2017-09-13T10:38:46.807+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16164620",
                    "id": "16164620",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user wesm commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    I'm happy to help you with this, I will let you work for a bit, but if you add me as a collaborator on your fork I can do some work on e.g. hiding mutex\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-09-13T13:07:29.103+0000",
                    "updated": "2017-09-13T13:07:29.103+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16164779",
                    "id": "16164779",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user TobyShaw commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    Sure thing, added.\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-09-13T14:58:01.129+0000",
                    "updated": "2017-09-13T14:58:01.129+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16187611",
                    "id": "16187611",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user wesm commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    I've got all the mutexes excluded from public headers. What is the prescribed solution for `nullptr`?\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-02T01:20:04.463+0000",
                    "updated": "2017-10-02T01:20:04.463+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16191410",
                    "id": "16191410",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user wesm commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    I think I'm going to extract this mutex PIMPL-ization into a separate patch so this need not linger too long, and we can deal with nullptr in another patch\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-04T15:10:10.510+0000",
                    "updated": "2017-10-04T15:10:10.510+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16191412",
                    "id": "16191412",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user wesm commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    See ARROW-1641. I will put up a patch for this soon as I can\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-04T15:11:39.600+0000",
                    "updated": "2017-10-04T15:11:39.600+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16191415",
                    "id": "16191415",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user TobyShaw commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    Eek, sorry about the inactivity.\n    \n    As for the nullptr stuff, the solution I've tested is with macros\n    \n    ```\n    #ifdef __cplusplus_cli\n    #define NULLPTR __nullptr\n    #else\n    #define NULLPTR nullptr\n    #endif\n    ```\n    And using NULLPTR instead of nullptr.\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-04T15:15:08.050+0000",
                    "updated": "2017-10-04T15:15:08.050+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16191416",
                    "id": "16191416",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user wesm commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    Would\n    \n    ```\n    #ifdef __cplusplus_cli\n    #define nullptr __nullptr\n    #endif\n    ```\n    \n    work also? \n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-04T15:16:17.271+0000",
                    "updated": "2017-10-04T15:16:17.271+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16191438",
                    "id": "16191438",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Github user TobyShaw commented on the issue:\n\n    https://github.com/apache/arrow/pull/1098\n  \n    My compiler complained about redefining a keyword when I tried that.\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-04T15:33:36.522+0000",
                    "updated": "2017-10-04T15:33:36.522+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16213653",
                    "id": "16213653",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "GitHub user wesm opened a pull request:\n\n    https://github.com/apache/arrow/pull/1228\n\n    ARROW-1134: [C++] Support for C++/CLI compilation, add NULLPTR define to avoid using nullptr in public headers\n\n    cc @tobyshaw. Can you test this?\r\n    \r\n    Close #1098\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/wesm/arrow ARROW-1134\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/arrow/pull/1228.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #1228\n    \n----\ncommit 4b4ca46ad0cdad4ca5aa804a9cd309c540059c0b\nAuthor: Wes McKinney <wes.mckinney@twosigma.com>\nDate:   2017-10-21T01:48:33Z\n\n    Add NULLPTR macro to avoid using nullptr in public headers for C++/CLI users\n    \n    Change-Id: Id8ef2be3f293a5fbb1cc6e5b792b25b9391bcf6b\n\n----\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-21T01:49:40.713+0000",
                    "updated": "2017-10-21T01:49:40.713+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16214537",
                    "id": "16214537",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1098: ARROW-1134 - Allow C++/CLI projects to build with Arrow\nURL: https://github.com/apache/arrow/pull/1098#issuecomment-338530966\n \n \n   superseded by #1228 \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-23T02:21:00.039+0000",
                    "updated": "2017-10-23T02:21:00.039+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16214538",
                    "id": "16214538",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm closed pull request #1098: ARROW-1134 - Allow C++/CLI projects to build with Arrow\nURL: https://github.com/apache/arrow/pull/1098\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/ci/travis_before_script_cpp.sh b/ci/travis_before_script_cpp.sh\nindex a7f1d2756..65171a735 100755\n--- a/ci/travis_before_script_cpp.sh\n+++ b/ci/travis_before_script_cpp.sh\n@@ -90,12 +90,12 @@ fi\n if [ $TRAVIS_OS_NAME == \"linux\" ]; then\n     cmake $CMAKE_COMMON_FLAGS \\\n           $CMAKE_LINUX_FLAGS \\\n-          -DARROW_CXXFLAGS=\"-Wconversion -Wno-sign-conversion -Werror\" \\\n+          -DBUILD_WARNING_LEVEL=CHECKIN \\\n           $ARROW_CPP_DIR\n else\n     cmake $CMAKE_COMMON_FLAGS \\\n           $CMAKE_OSX_FLAGS \\\n-          -DARROW_CXXFLAGS=-Werror \\\n+          -DBUILD_WARNING_LEVEL=CHECKIN \\\n           $ARROW_CPP_DIR\n fi\n \ndiff --git a/cpp/CMakeLists.txt b/cpp/CMakeLists.txt\nindex 972132f29..0930ae4ce 100644\n--- a/cpp/CMakeLists.txt\n+++ b/cpp/CMakeLists.txt\n@@ -56,6 +56,14 @@ if (\"$ENV{CMAKE_EXPORT_COMPILE_COMMANDS}\" STREQUAL \"1\" OR CLANG_TIDY_FOUND)\n   set(CMAKE_EXPORT_COMPILE_COMMANDS 1)\n endif()\n \n+find_package(InferTools)\n+if (\"$ENV{CMAKE_EXPORT_COMPILE_COMMANDS}\" STREQUAL \"1\" OR INFER_FOUND)\n+  # Generate a Clang compile_commands.json \"compilation database\" file for use\n+  # with various development tools, such as Vim's YouCompleteMe plugin.\n+  # See http://clang.llvm.org/docs/JSONCompilationDatabase.html\n+  set(CMAKE_EXPORT_COMPILE_COMMANDS 1)\n+endif()\n+\n find_program(CCACHE_FOUND ccache)\n if(CCACHE_FOUND)\n   set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_FOUND})\n@@ -223,12 +231,13 @@ else()\n   set(ARROW_BOOST_HEADER_ONLY 1)\n endif()\n \n-include(BuildUtils)\n-\n ############################################################\n # Compiler flags\n ############################################################\n \n+# Determine compiler version\n+include(CompilerInfo)\n+\n if (ARROW_NO_DEPRECATED_API)\n   add_definitions(-DARROW_NO_DEPRECATED_API)\n endif()\n@@ -245,6 +254,9 @@ include(SetupCxxFlags)\n \n add_custom_target(arrow_dependencies)\n \n+include(BuildUtils)\n+enable_testing()\n+\n include(ThirdpartyToolchain)\n \n # Add common flags\n@@ -253,9 +265,6 @@ set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${ARROW_CXXFLAGS}\")\n \n message(STATUS \"CMAKE_CXX_FLAGS: ${CMAKE_CXX_FLAGS}\")\n \n-# Determine compiler version\n-include(CompilerInfo)\n-\n if (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n   # Using Clang with ccache causes a bunch of spurious warnings that are\n   # purportedly fixed in the next version of ccache. See the following for details:\n@@ -355,153 +364,6 @@ if (PARQUET_BUILD_SHARED)\n endif()\n \n ############################################################\n-# Benchmarking\n-############################################################\n-# Add a new micro benchmark, with or without an executable that should be built.\n-# If benchmarks are enabled then they will be run along side unit tests with ctest.\n-# 'make runbenchmark' and 'make unittest' to build/run only benchmark or unittests,\n-# respectively.\n-#\n-# REL_BENCHMARK_NAME is the name of the benchmark app. It may be a single component\n-# (e.g. monotime-benchmark) or contain additional components (e.g.\n-# net/net_util-benchmark). Either way, the last component must be a globally\n-# unique name.\n-\n-# The benchmark will registered as unit test with ctest with a label\n-# of 'benchmark'.\n-#\n-# Arguments after the test name will be passed to set_tests_properties().\n-function(ADD_ARROW_BENCHMARK REL_BENCHMARK_NAME)\n-  if(NO_BENCHMARKS)\n-    return()\n-  endif()\n-  get_filename_component(BENCHMARK_NAME ${REL_BENCHMARK_NAME} NAME_WE)\n-\n-  if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/${REL_BENCHMARK_NAME}.cc)\n-    # This benchmark has a corresponding .cc file, set it up as an executable.\n-    set(BENCHMARK_PATH \"${EXECUTABLE_OUTPUT_PATH}/${BENCHMARK_NAME}\")\n-    add_executable(${BENCHMARK_NAME} \"${REL_BENCHMARK_NAME}.cc\")\n-    target_link_libraries(${BENCHMARK_NAME} ${ARROW_BENCHMARK_LINK_LIBS})\n-    add_dependencies(runbenchmark ${BENCHMARK_NAME})\n-    set(NO_COLOR \"--color_print=false\")\n-  else()\n-    # No executable, just invoke the benchmark (probably a script) directly.\n-    set(BENCHMARK_PATH ${CMAKE_CURRENT_SOURCE_DIR}/${REL_BENCHMARK_NAME})\n-    set(NO_COLOR \"\")\n-  endif()\n-\n-  add_test(${BENCHMARK_NAME}\n-    ${BUILD_SUPPORT_DIR}/run-test.sh ${CMAKE_BINARY_DIR} benchmark ${BENCHMARK_PATH} ${NO_COLOR})\n-  set_tests_properties(${BENCHMARK_NAME} PROPERTIES LABELS \"benchmark\")\n-  if(ARGN)\n-    set_tests_properties(${BENCHMARK_NAME} PROPERTIES ${ARGN})\n-  endif()\n-endfunction()\n-\n-# A wrapper for add_dependencies() that is compatible with NO_BENCHMARKS.\n-function(ADD_ARROW_BENCHMARK_DEPENDENCIES REL_BENCHMARK_NAME)\n-  if(NO_BENCHMARKS)\n-    return()\n-  endif()\n-  get_filename_component(BENCMARK_NAME ${REL_BENCHMARK_NAME} NAME_WE)\n-\n-  add_dependencies(${BENCHMARK_NAME} ${ARGN})\n-endfunction()\n-\n-# A wrapper for target_link_libraries() that is compatible with NO_BENCHMARKS.\n-function(ARROW_BENCHMARK_LINK_LIBRARIES REL_BENCHMARK_NAME)\n-    if(NO_BENCHMARKS)\n-    return()\n-  endif()\n-  get_filename_component(BENCHMARK_NAME ${REL_BENCHMARK_NAME} NAME_WE)\n-\n-  target_link_libraries(${BENCHMARK_NAME} ${ARGN})\n-endfunction()\n-\n-\n-############################################################\n-# Testing\n-############################################################\n-# Add a new test case, with or without an executable that should be built.\n-#\n-# REL_TEST_NAME is the name of the test. It may be a single component\n-# (e.g. monotime-test) or contain additional components (e.g.\n-# net/net_util-test). Either way, the last component must be a globally\n-# unique name.\n-#\n-# The unit test is added with a label of \"unittest\" to support filtering with\n-# ctest.\n-#\n-# Arguments after the test name will be passed to set_tests_properties().\n-function(ADD_ARROW_TEST REL_TEST_NAME)\n-  set(options)\n-  set(single_value_args)\n-  set(multi_value_args STATIC_LINK_LIBS)\n-  cmake_parse_arguments(ARG \"${options}\" \"${one_value_args}\" \"${multi_value_args}\" ${ARGN})\n-  if(ARG_UNPARSED_ARGUMENTS)\n-    message(SEND_ERROR \"Error: unrecognized arguments: ${ARG_UNPARSED_ARGUMENTS}\")\n-  endif()\n-\n-  if(NO_TESTS OR NOT ARROW_BUILD_STATIC)\n-    return()\n-  endif()\n-  get_filename_component(TEST_NAME ${REL_TEST_NAME} NAME_WE)\n-\n-  if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/${REL_TEST_NAME}.cc)\n-    # This test has a corresponding .cc file, set it up as an executable.\n-    set(TEST_PATH \"${EXECUTABLE_OUTPUT_PATH}/${TEST_NAME}\")\n-    add_executable(${TEST_NAME} \"${REL_TEST_NAME}.cc\")\n-\n-    if (ARG_STATIC_LINK_LIBS)\n-      # Customize link libraries\n-      target_link_libraries(${TEST_NAME} ${ARG_STATIC_LINK_LIBS})\n-    else()\n-      target_link_libraries(${TEST_NAME} ${ARROW_TEST_LINK_LIBS})\n-    endif()\n-    add_dependencies(unittest ${TEST_NAME})\n-  else()\n-    # No executable, just invoke the test (probably a script) directly.\n-    set(TEST_PATH ${CMAKE_CURRENT_SOURCE_DIR}/${REL_TEST_NAME})\n-  endif()\n-\n-  if (ARROW_TEST_MEMCHECK)\n-    SET_PROPERTY(TARGET ${TEST_NAME}\n-      APPEND_STRING PROPERTY\n-      COMPILE_FLAGS \" -DARROW_VALGRIND\")\n-    add_test(${TEST_NAME}\n-      bash -c \"cd ${EXECUTABLE_OUTPUT_PATH}; valgrind --tool=memcheck --leak-check=full --leak-check-heuristics=stdstring --error-exitcode=1 ${TEST_PATH}\")\n-  elseif(MSVC)\n-    add_test(${TEST_NAME} ${TEST_PATH})\n-  else()\n-    add_test(${TEST_NAME}\n-      ${BUILD_SUPPORT_DIR}/run-test.sh ${CMAKE_BINARY_DIR} test ${TEST_PATH})\n-  endif()\n-  set_tests_properties(${TEST_NAME} PROPERTIES LABELS \"unittest\")\n-endfunction()\n-\n-# A wrapper for add_dependencies() that is compatible with NO_TESTS.\n-function(ADD_ARROW_TEST_DEPENDENCIES REL_TEST_NAME)\n-  if(NO_TESTS)\n-    return()\n-  endif()\n-  get_filename_component(TEST_NAME ${REL_TEST_NAME} NAME_WE)\n-\n-  add_dependencies(${TEST_NAME} ${ARGN})\n-endfunction()\n-\n-# A wrapper for target_link_libraries() that is compatible with NO_TESTS.\n-function(ARROW_TEST_LINK_LIBRARIES REL_TEST_NAME)\n-  if(NO_TESTS)\n-    return()\n-  endif()\n-  get_filename_component(TEST_NAME ${REL_TEST_NAME} NAME_WE)\n-\n-  target_link_libraries(${TEST_NAME} ${ARGN})\n-endfunction()\n-\n-enable_testing()\n-\n-############################################################\n # \"make ctags\" target\n ############################################################\n if (UNIX)\n@@ -597,6 +459,19 @@ if (${CLANG_TIDY_FOUND})\n endif()\n \n ############################################################\n+# \"make infer\" target\n+############################################################\n+\n+if (${INFER_FOUND})\n+  # runs infer capture\n+  add_custom_target(infer ${BUILD_SUPPORT_DIR}/run-infer.sh ${INFER_BIN} ${CMAKE_BINARY_DIR}/compile_commands.json 1)\n+  # runs infer analyze\n+  add_custom_target(infer-analyze ${BUILD_SUPPORT_DIR}/run-infer.sh ${INFER_BIN} ${CMAKE_BINARY_DIR}/compile_commands.json 2)\n+  # runs infer report\n+  add_custom_target(infer-report ${BUILD_SUPPORT_DIR}/run-infer.sh ${INFER_BIN} ${CMAKE_BINARY_DIR}/compile_commands.json 3)\n+endif()\n+\n+############################################################\n # \"make iwyu\" target\n ############################################################\n if(UNIX)\n@@ -733,121 +608,6 @@ if(NOT WIN32 AND ARROW_PLASMA)\n endif()\n \n add_subdirectory(src/arrow)\n-add_subdirectory(src/arrow/io)\n-\n-set(ARROW_SRCS\n-  src/arrow/array.cc\n-  src/arrow/buffer.cc\n-  src/arrow/builder.cc\n-  src/arrow/compare.cc\n-  src/arrow/memory_pool.cc\n-  src/arrow/pretty_print.cc\n-  src/arrow/status.cc\n-  src/arrow/table.cc\n-  src/arrow/tensor.cc\n-  src/arrow/type.cc\n-  src/arrow/visitor.cc\n-\n-  src/arrow/compute/cast.cc\n-  src/arrow/compute/context.cc\n-\n-  src/arrow/io/file.cc\n-  src/arrow/io/interfaces.cc\n-  src/arrow/io/memory.cc\n-\n-  src/arrow/util/bit-util.cc\n-  src/arrow/util/compression.cc\n-  src/arrow/util/cpu-info.cc\n-  src/arrow/util/decimal.cc\n-  src/arrow/util/key_value_metadata.cc\n-)\n-\n-if (ARROW_COMPUTE)\n-  add_subdirectory(src/arrow/compute)\n-  set(ARROW_SRCS ${ARROW_SRCS}\n-    src/arrow/compute/cast.cc\n-    src/arrow/compute/context.cc\n-  )\n-endif()\n-\n-if (ARROW_GPU)\n-  # IPC extensions required to build the GPU library\n-  set(ARROW_IPC ON)\n-  add_subdirectory(src/arrow/gpu)\n-endif()\n-\n-if (ARROW_IPC)\n-  add_subdirectory(src/arrow/ipc)\n-  add_dependencies(arrow_dependencies metadata_fbs)\n-endif()\n-\n-if (ARROW_WITH_BROTLI)\n-  add_definitions(-DARROW_WITH_BROTLI)\n-  SET(ARROW_SRCS src/arrow/util/compression_brotli.cc ${ARROW_SRCS})\n-endif()\n-\n-if (ARROW_WITH_LZ4)\n-  add_definitions(-DARROW_WITH_LZ4)\n-  SET(ARROW_SRCS src/arrow/util/compression_lz4.cc ${ARROW_SRCS})\n-endif()\n-\n-if (ARROW_WITH_SNAPPY)\n-  add_definitions(-DARROW_WITH_SNAPPY)\n-  SET(ARROW_SRCS src/arrow/util/compression_snappy.cc ${ARROW_SRCS})\n-endif()\n-\n-if (ARROW_WITH_ZLIB)\n-  add_definitions(-DARROW_WITH_ZLIB)\n-  SET(ARROW_SRCS src/arrow/util/compression_zlib.cc ${ARROW_SRCS})\n-endif()\n-\n-if (ARROW_WITH_ZSTD)\n-  add_definitions(-DARROW_WITH_ZSTD)\n-  SET(ARROW_SRCS src/arrow/util/compression_zstd.cc ${ARROW_SRCS})\n-endif()\n-\n-if (NOT ARROW_BOOST_HEADER_ONLY)\n-  set(ARROW_SRCS ${ARROW_SRCS}\n-    src/arrow/io/hdfs.cc\n-    src/arrow/io/hdfs-internal.cc\n-  )\n-endif()\n-\n-if (ARROW_IPC)\n-  set(ARROW_SRCS ${ARROW_SRCS}\n-    src/arrow/ipc/dictionary.cc\n-    src/arrow/ipc/feather.cc\n-    src/arrow/ipc/json.cc\n-    src/arrow/ipc/json-internal.cc\n-    src/arrow/ipc/message.cc\n-    src/arrow/ipc/metadata-internal.cc\n-    src/arrow/ipc/reader.cc\n-    src/arrow/ipc/writer.cc\n-  )\n-endif()\n-\n-\n-if(NOT APPLE AND NOT MSVC)\n-  # Localize thirdparty symbols using a linker version script. This hides them\n-  # from the client application. The OS X linker does not support the\n-  # version-script option.\n-  set(ARROW_SHARED_LINK_FLAGS \"-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/src/arrow/symbols.map\")\n-endif()\n-\n-set(ARROW_ALL_SRCS\n-  ${ARROW_SRCS})\n-\n-ADD_ARROW_LIB(arrow\n-  SOURCES ${ARROW_ALL_SRCS}\n-  DEPENDENCIES arrow_dependencies\n-  SHARED_LINK_FLAGS ${ARROW_SHARED_LINK_FLAGS}\n-  SHARED_LINK_LIBS ${ARROW_LINK_LIBS}\n-  SHARED_PRIVATE_LINK_LIBS ${ARROW_SHARED_PRIVATE_LINK_LIBS}\n-  STATIC_LINK_LIBS ${ARROW_STATIC_LINK_LIBS}\n-  STATIC_PRIVATE_LINK_LIBS ${ARROW_STATIC_PRIVATE_LINK_LIBS}\n-)\n-\n-add_subdirectory(src/arrow/util)\n \n if(ARROW_PYTHON)\n   find_package(PythonLibsNew REQUIRED)\ndiff --git a/cpp/README.md b/cpp/README.md\nindex 4a515079d..6e29e6f78 100644\n--- a/cpp/README.md\n+++ b/cpp/README.md\n@@ -238,6 +238,13 @@ build failures by running the following checks before submitting your pull reque\n     # before running it.\n     make format # requires clang-format is installed\n \n+We run our CI builds with more compiler warnings enabled for the Clang\n+compiler. Please run CMake with\n+\n+`-DBUILD_WARNING_LEVEL=CHECKIN`\n+\n+to avoid failures due to compiler warnings.\n+\n Note that the clang-tidy target may take a while to run.  You might consider\n running clang-tidy separately on the files you have added/changed before\n invoking the make target to reduce iteration time.  Also, it might generate warnings\ndiff --git a/cpp/build-support/run-infer.sh b/cpp/build-support/run-infer.sh\nnew file mode 100755\nindex 000000000..823685aef\n--- /dev/null\n+++ b/cpp/build-support/run-infer.sh\n@@ -0,0 +1,48 @@\n+#!/bin/bash\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+#\n+# Runs infer in the given directory\n+# Arguments:\n+#   $1 - Path to the infer binary\n+#   $2 - Path to the compile_commands.json to use\n+#   $3 - Apply infer step (1=capture, 2=analyze, 3=report)\n+#\n+INFER=$1\n+shift\n+COMPILE_COMMANDS=$1\n+shift\n+APPLY_STEP=$1\n+shift\n+\n+if [ \"$APPLY_STEP\" == \"1\" ]; then\n+  $INFER capture --compilation-database $COMPILE_COMMANDS\n+  echo \"\"\n+  echo \"Run 'make infer-analyze' next.\"\n+elif [ \"$APPLY_STEP\" == \"2\" ]; then\n+  # infer's analyze step can take a very long time to complete\n+  $INFER analyze\n+  echo \"\"\n+  echo \"Run 'make infer-report' next.\"\n+  echo \"See: http://fbinfer.com/docs/steps-for-ci.html\"\n+elif [ \"$APPLY_STEP\" == \"3\" ]; then\n+  $INFER report --issues-csv ./infer-out/report.csv 1> /dev/null\n+  $INFER report --issues-txt ./infer-out/report.txt 1> /dev/null\n+  $INFER report --issues-json ./infer-out/report.json 1> /dev/null\n+  echo \"\"\n+  echo \"Reports (report.txt, report.csv, report.json) can be found in the infer-out subdirectory.\"\n+else\n+  echo \"\"\n+  echo \"See: http://fbinfer.com/docs/steps-for-ci.html\"\n+fi\ndiff --git a/cpp/cmake_modules/BuildUtils.cmake b/cpp/cmake_modules/BuildUtils.cmake\nindex 8f92d73ba..e398dc182 100644\n--- a/cpp/cmake_modules/BuildUtils.cmake\n+++ b/cpp/cmake_modules/BuildUtils.cmake\n@@ -188,3 +188,149 @@ function(ADD_ARROW_LIB LIB_NAME)\n   endif()\n \n endfunction()\n+\n+\n+############################################################\n+# Benchmarking\n+############################################################\n+# Add a new micro benchmark, with or without an executable that should be built.\n+# If benchmarks are enabled then they will be run along side unit tests with ctest.\n+# 'make runbenchmark' and 'make unittest' to build/run only benchmark or unittests,\n+# respectively.\n+#\n+# REL_BENCHMARK_NAME is the name of the benchmark app. It may be a single component\n+# (e.g. monotime-benchmark) or contain additional components (e.g.\n+# net/net_util-benchmark). Either way, the last component must be a globally\n+# unique name.\n+\n+# The benchmark will registered as unit test with ctest with a label\n+# of 'benchmark'.\n+#\n+# Arguments after the test name will be passed to set_tests_properties().\n+function(ADD_ARROW_BENCHMARK REL_BENCHMARK_NAME)\n+  if(NO_BENCHMARKS)\n+    return()\n+  endif()\n+  get_filename_component(BENCHMARK_NAME ${REL_BENCHMARK_NAME} NAME_WE)\n+\n+  if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/${REL_BENCHMARK_NAME}.cc)\n+    # This benchmark has a corresponding .cc file, set it up as an executable.\n+    set(BENCHMARK_PATH \"${EXECUTABLE_OUTPUT_PATH}/${BENCHMARK_NAME}\")\n+    add_executable(${BENCHMARK_NAME} \"${REL_BENCHMARK_NAME}.cc\")\n+    target_link_libraries(${BENCHMARK_NAME} ${ARROW_BENCHMARK_LINK_LIBS})\n+    add_dependencies(runbenchmark ${BENCHMARK_NAME})\n+    set(NO_COLOR \"--color_print=false\")\n+  else()\n+    # No executable, just invoke the benchmark (probably a script) directly.\n+    set(BENCHMARK_PATH ${CMAKE_CURRENT_SOURCE_DIR}/${REL_BENCHMARK_NAME})\n+    set(NO_COLOR \"\")\n+  endif()\n+\n+  add_test(${BENCHMARK_NAME}\n+    ${BUILD_SUPPORT_DIR}/run-test.sh ${CMAKE_BINARY_DIR} benchmark ${BENCHMARK_PATH} ${NO_COLOR})\n+  set_tests_properties(${BENCHMARK_NAME} PROPERTIES LABELS \"benchmark\")\n+  if(ARGN)\n+    set_tests_properties(${BENCHMARK_NAME} PROPERTIES ${ARGN})\n+  endif()\n+endfunction()\n+\n+# A wrapper for add_dependencies() that is compatible with NO_BENCHMARKS.\n+function(ADD_ARROW_BENCHMARK_DEPENDENCIES REL_BENCHMARK_NAME)\n+  if(NO_BENCHMARKS)\n+    return()\n+  endif()\n+  get_filename_component(BENCMARK_NAME ${REL_BENCHMARK_NAME} NAME_WE)\n+\n+  add_dependencies(${BENCHMARK_NAME} ${ARGN})\n+endfunction()\n+\n+# A wrapper for target_link_libraries() that is compatible with NO_BENCHMARKS.\n+function(ARROW_BENCHMARK_LINK_LIBRARIES REL_BENCHMARK_NAME)\n+    if(NO_BENCHMARKS)\n+    return()\n+  endif()\n+  get_filename_component(BENCHMARK_NAME ${REL_BENCHMARK_NAME} NAME_WE)\n+\n+  target_link_libraries(${BENCHMARK_NAME} ${ARGN})\n+endfunction()\n+\n+\n+############################################################\n+# Testing\n+############################################################\n+# Add a new test case, with or without an executable that should be built.\n+#\n+# REL_TEST_NAME is the name of the test. It may be a single component\n+# (e.g. monotime-test) or contain additional components (e.g.\n+# net/net_util-test). Either way, the last component must be a globally\n+# unique name.\n+#\n+# The unit test is added with a label of \"unittest\" to support filtering with\n+# ctest.\n+#\n+# Arguments after the test name will be passed to set_tests_properties().\n+function(ADD_ARROW_TEST REL_TEST_NAME)\n+  set(options)\n+  set(single_value_args)\n+  set(multi_value_args STATIC_LINK_LIBS)\n+  cmake_parse_arguments(ARG \"${options}\" \"${one_value_args}\" \"${multi_value_args}\" ${ARGN})\n+  if(ARG_UNPARSED_ARGUMENTS)\n+    message(SEND_ERROR \"Error: unrecognized arguments: ${ARG_UNPARSED_ARGUMENTS}\")\n+  endif()\n+\n+  if(NO_TESTS OR NOT ARROW_BUILD_STATIC)\n+    return()\n+  endif()\n+  get_filename_component(TEST_NAME ${REL_TEST_NAME} NAME_WE)\n+\n+  if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/${REL_TEST_NAME}.cc)\n+    # This test has a corresponding .cc file, set it up as an executable.\n+    set(TEST_PATH \"${EXECUTABLE_OUTPUT_PATH}/${TEST_NAME}\")\n+    add_executable(${TEST_NAME} \"${REL_TEST_NAME}.cc\")\n+\n+    if (ARG_STATIC_LINK_LIBS)\n+      # Customize link libraries\n+      target_link_libraries(${TEST_NAME} ${ARG_STATIC_LINK_LIBS})\n+    else()\n+      target_link_libraries(${TEST_NAME} ${ARROW_TEST_LINK_LIBS})\n+    endif()\n+    add_dependencies(unittest ${TEST_NAME})\n+  else()\n+    # No executable, just invoke the test (probably a script) directly.\n+    set(TEST_PATH ${CMAKE_CURRENT_SOURCE_DIR}/${REL_TEST_NAME})\n+  endif()\n+\n+  if (ARROW_TEST_MEMCHECK)\n+    SET_PROPERTY(TARGET ${TEST_NAME}\n+      APPEND_STRING PROPERTY\n+      COMPILE_FLAGS \" -DARROW_VALGRIND\")\n+    add_test(${TEST_NAME}\n+      bash -c \"cd ${EXECUTABLE_OUTPUT_PATH}; valgrind --tool=memcheck --leak-check=full --leak-check-heuristics=stdstring --error-exitcode=1 ${TEST_PATH}\")\n+  elseif(MSVC)\n+    add_test(${TEST_NAME} ${TEST_PATH})\n+  else()\n+    add_test(${TEST_NAME}\n+      ${BUILD_SUPPORT_DIR}/run-test.sh ${CMAKE_BINARY_DIR} test ${TEST_PATH})\n+  endif()\n+  set_tests_properties(${TEST_NAME} PROPERTIES LABELS \"unittest\")\n+endfunction()\n+\n+# A wrapper for add_dependencies() that is compatible with NO_TESTS.\n+function(ADD_ARROW_TEST_DEPENDENCIES REL_TEST_NAME)\n+  if(NO_TESTS)\n+    return()\n+  endif()\n+  get_filename_component(TEST_NAME ${REL_TEST_NAME} NAME_WE)\n+\n+  add_dependencies(${TEST_NAME} ${ARGN})\n+endfunction()\n+\n+# A wrapper for target_link_libraries() that is compatible with NO_TESTS.\n+function(ARROW_TEST_LINK_LIBRARIES REL_TEST_NAME)\n+  if(NO_TESTS)\n+    return()\n+  endif()\n+  get_filename_component(TEST_NAME ${REL_TEST_NAME} NAME_WE)\n+\n+  target_link_libraries(${TEST_NAME} ${ARGN})\n+endfunction()\ndiff --git a/cpp/cmake_modules/CompilerInfo.cmake b/cpp/cmake_modules/CompilerInfo.cmake\nindex a1b470182..5ff1d8614 100644\n--- a/cpp/cmake_modules/CompilerInfo.cmake\n+++ b/cpp/cmake_modules/CompilerInfo.cmake\n@@ -64,6 +64,11 @@ elseif(\"${COMPILER_VERSION_FULL}\" MATCHES \".*clang-7\")\n   set(COMPILER_VERSION \"3.7.0svn\")\n \n # clang on Mac OS X, XCode 8.\n+elseif(\"${COMPILER_VERSION_FULL}\" MATCHES \".*clang-802\")\n+  set(COMPILER_FAMILY \"clang\")\n+  set(COMPILER_VERSION \"3.9.0svn\")\n+\n+# clang on Mac OS X, XCode 8.\n elseif(\"${COMPILER_VERSION_FULL}\" MATCHES \".*clang-8\")\n   set(COMPILER_FAMILY \"clang\")\n   set(COMPILER_VERSION \"3.8.0svn\")\ndiff --git a/cpp/cmake_modules/FindInferTools.cmake b/cpp/cmake_modules/FindInferTools.cmake\nnew file mode 100644\nindex 000000000..00c6709c6\n--- /dev/null\n+++ b/cpp/cmake_modules/FindInferTools.cmake\n@@ -0,0 +1,45 @@\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Tries to find the infer module\n+#\n+# Usage of this module as follows:\n+#\n+#  find_package(InferTools)\n+#\n+# Variables used by this module, they can change the default behaviour and need\n+# to be set before calling find_package:\n+#\n+#  InferTools_PATH -\n+#   When set, this path is inspected instead of standard library binary locations\n+#   to find infer\n+#\n+# This module defines\n+#  INFER_BIN, The  path to the clang tidy binary\n+#  INFER_FOUND, Whether clang tidy was found\n+\n+find_program(INFER_BIN\n+  NAMES infer\n+  PATHS ${InferTools_PATH} $ENV{INFER_TOOLS_PATH} /usr/local/bin /usr/bin\n+  /usr/local/homebrew/bin\n+  /opt/local/bin\n+  NO_DEFAULT_PATH\n+)\n+\n+if ( \"${INFER_BIN}\" STREQUAL \"INFER_BIN-NOTFOUND\" )\n+  set(INFER_FOUND 0)\n+  message(\"infer not found\")\n+else()\n+  set(INFER_FOUND 1)\n+  message(\"infer found at ${INFER_BIN}\")\n+endif()\ndiff --git a/cpp/cmake_modules/SetupCxxFlags.cmake b/cpp/cmake_modules/SetupCxxFlags.cmake\nindex 6e92c4b1c..77bfac835 100644\n--- a/cpp/cmake_modules/SetupCxxFlags.cmake\n+++ b/cpp/cmake_modules/SetupCxxFlags.cmake\n@@ -48,7 +48,110 @@ if (MSVC)\n     set(CXX_COMMON_FLAGS \"/W3\")\n   endif()\n else()\n-  set(CXX_COMMON_FLAGS \"-Wall -std=c++11\")\n+  # Common flags set below with warning level\n+  set(CXX_COMMON_FLAGS \"\")\n+endif()\n+\n+# Build warning level (CHECKIN, EVERYTHING, etc.)\n+\n+# if no build warning level is specified, default to development warning level\n+if (NOT BUILD_WARNING_LEVEL)\n+  set(BUILD_WARNING_LEVEL Production)\n+endif(NOT BUILD_WARNING_LEVEL)\n+\n+string(TOUPPER ${BUILD_WARNING_LEVEL} UPPERCASE_BUILD_WARNING_LEVEL)\n+\n+if (\"${UPPERCASE_BUILD_WARNING_LEVEL}\" STREQUAL \"CHECKIN\")\n+  # Pre-checkin builds\n+  if (\"${COMPILER_FAMILY}\" STREQUAL \"msvc\")\n+    string(REPLACE \"/W3\" \"\" CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS}\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /W3\")\n+    # Treat all compiler warnings as errors\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /WX\")\n+  elseif (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Weverything -Wno-c++98-compat \\\n+-Wno-c++98-compat-pedantic -Wno-deprecated -Wno-weak-vtables -Wno-padded \\\n+-Wno-unused-parameter -Wno-undef \\\n+-Wno-shadow -Wno-switch-enum -Wno-exit-time-destructors \\\n+-Wno-global-constructors -Wno-weak-template-vtables -Wno-undefined-reinterpret-cast \\\n+-Wno-implicit-fallthrough -Wno-unreachable-code-return \\\n+-Wno-float-equal -Wno-missing-prototypes \\\n+-Wno-old-style-cast -Wno-covered-switch-default \\\n+-Wno-cast-align -Wno-vla-extension -Wno-shift-sign-overflow \\\n+-Wno-used-but-marked-unused -Wno-missing-variable-declarations \\\n+-Wno-gnu-zero-variadic-macro-arguments -Wconversion -Wno-sign-conversion \\\n+-Wno-disabled-macro-expansion -Wno-shorten-64-to-32\")\n+\n+    # Version numbers where warnings are introduced\n+    if (\"${COMPILER_VERSION}\" VERSION_GREATER \"3.3\")\n+      set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wno-gnu-folding-constant\")\n+    endif()\n+    if (\"${COMPILER_VERSION}\" VERSION_GREATER \"3.6\")\n+      set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wno-reserved-id-macro\")\n+      set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wno-range-loop-analysis\")\n+    endif()\n+    if (\"${COMPILER_VERSION}\" VERSION_GREATER \"3.7\")\n+      set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wno-double-promotion\")\n+    endif()\n+    if (\"${COMPILER_VERSION}\" VERSION_GREATER \"3.8\")\n+      set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wno-undefined-func-template\")\n+    endif()\n+\n+    # Treat all compiler warnings as errors\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wno-unknown-warning-option -Werror\")\n+  elseif (\"${COMPILER_FAMILY}\" STREQUAL \"gcc\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wall -Wconversion -Wno-sign-conversion\")\n+    # Treat all compiler warnings as errors\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wno-unknown-warning-option -Werror\")\n+  else()\n+    message(FATAL_ERROR \"Unknown compiler. Version info:\\n${COMPILER_VERSION_FULL}\")\n+  endif()\n+elseif (\"${UPPERCASE_BUILD_WARNING_LEVEL}\" STREQUAL \"EVERYTHING\")\n+  # Pedantic builds for fixing warnings\n+  if (\"${COMPILER_FAMILY}\" STREQUAL \"msvc\")\n+    string(REPLACE \"/W3\" \"\" CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS}\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /Wall\")\n+    # https://docs.microsoft.com/en-us/cpp/build/reference/compiler-option-warning-level\n+    # /wdnnnn disables a warning where \"nnnn\" is a warning number\n+    # Treat all compiler warnings as errors\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS}  /WX\")\n+  elseif (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Weverything -Wno-c++98-compat -Wno-c++98-compat-pedantic\")\n+    # Treat all compiler warnings as errors\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Werror\")\n+  elseif (\"${COMPILER_FAMILY}\" STREQUAL \"gcc\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wall -Wpedantic -Wextra -Wno-unused-parameter\")\n+    # Treat all compiler warnings as errors\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Werror\")\n+  else()\n+    message(FATAL_ERROR \"Unknown compiler. Version info:\\n${COMPILER_VERSION_FULL}\")\n+  endif()\n+else()\n+  # Production builds (warning are not treated as errors)\n+  if (\"${COMPILER_FAMILY}\" STREQUAL \"msvc\")\n+    # https://docs.microsoft.com/en-us/cpp/build/reference/compiler-option-warning-level\n+    # TODO: Enable /Wall and disable individual warnings until build compiles without errors\n+    # /wdnnnn disables a warning where \"nnnn\" is a warning number\n+    string(REPLACE \"/W3\" \"\" CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS}\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} /W3\")\n+  elseif (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wall\")\n+  elseif (\"${COMPILER_FAMILY}\" STREQUAL \"gcc\")\n+    set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -Wall\")\n+  else()\n+    message(FATAL_ERROR \"Unknown compiler. Version info:\\n${COMPILER_VERSION_FULL}\")\n+  endif()\n+endif()\n+\n+# if build warning flags is set, add to CXX_COMMON_FLAGS\n+if (BUILD_WARNING_FLAGS)\n+  # Use BUILD_WARNING_FLAGS with BUILD_WARNING_LEVEL=everything to disable\n+  # warnings (use with Clang's -Weverything flag to find potential errors)\n+  set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} ${BUILD_WARNING_FLAGS}\")\n+endif(BUILD_WARNING_FLAGS)\n+\n+if (NOT (\"${COMPILER_FAMILY}\" STREQUAL \"msvc\"))\n+set(CXX_COMMON_FLAGS \"${CXX_COMMON_FLAGS} -std=c++11\")\n endif()\n \n # Only enable additional instruction sets if they are supported\ndiff --git a/cpp/cmake_modules/ThirdpartyToolchain.cmake b/cpp/cmake_modules/ThirdpartyToolchain.cmake\nindex 194156cc6..98186d014 100644\n--- a/cpp/cmake_modules/ThirdpartyToolchain.cmake\n+++ b/cpp/cmake_modules/ThirdpartyToolchain.cmake\n@@ -126,6 +126,7 @@ endif()\n set(Boost_DEBUG TRUE)\n set(Boost_USE_MULTITHREADED ON)\n set(Boost_ADDITIONAL_VERSIONS\n+  \"1.65.0\" \"1.65\"\n   \"1.64.0\" \"1.64\"\n   \"1.63.0\" \"1.63\"\n   \"1.62.0\" \"1.61\"\ndiff --git a/cpp/src/arrow/CMakeLists.txt b/cpp/src/arrow/CMakeLists.txt\nindex 6963b11e3..5c9033141 100644\n--- a/cpp/src/arrow/CMakeLists.txt\n+++ b/cpp/src/arrow/CMakeLists.txt\n@@ -15,6 +15,121 @@\n # specific language governing permissions and limitations\n # under the License.\n \n+set(ARROW_SRCS\n+  array.cc\n+  buffer.cc\n+  builder.cc\n+  compare.cc\n+  memory_pool.cc\n+  pretty_print.cc\n+  status.cc\n+  table.cc\n+  tensor.cc\n+  type.cc\n+  visitor.cc\n+\n+  io/file.cc\n+  io/interfaces.cc\n+  io/memory.cc\n+\n+  util/bit-util.cc\n+  util/compression.cc\n+  util/cpu-info.cc\n+  util/decimal.cc\n+  util/key_value_metadata.cc\n+)\n+\n+if (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n+  set_property(SOURCE io/file.cc\n+    APPEND_STRING\n+    PROPERTY COMPILE_FLAGS\n+    \" -Wno-unused-macros \")\n+endif()\n+\n+if (ARROW_COMPUTE)\n+  add_subdirectory(compute)\n+  set(ARROW_SRCS ${ARROW_SRCS}\n+    compute/cast.cc\n+    compute/context.cc\n+  )\n+endif()\n+\n+if (ARROW_GPU)\n+  # IPC extensions required to build the GPU library\n+  set(ARROW_IPC ON)\n+  add_subdirectory(gpu)\n+endif()\n+\n+if (ARROW_IPC)\n+  add_subdirectory(ipc)\n+  add_dependencies(arrow_dependencies metadata_fbs)\n+endif()\n+\n+if (ARROW_WITH_BROTLI)\n+  add_definitions(-DARROW_WITH_BROTLI)\n+  SET(ARROW_SRCS util/compression_brotli.cc ${ARROW_SRCS})\n+endif()\n+\n+if (ARROW_WITH_LZ4)\n+  add_definitions(-DARROW_WITH_LZ4)\n+  SET(ARROW_SRCS util/compression_lz4.cc ${ARROW_SRCS})\n+endif()\n+\n+if (ARROW_WITH_SNAPPY)\n+  add_definitions(-DARROW_WITH_SNAPPY)\n+  SET(ARROW_SRCS util/compression_snappy.cc ${ARROW_SRCS})\n+endif()\n+\n+if (ARROW_WITH_ZLIB)\n+  add_definitions(-DARROW_WITH_ZLIB)\n+  SET(ARROW_SRCS util/compression_zlib.cc ${ARROW_SRCS})\n+endif()\n+\n+if (ARROW_WITH_ZSTD)\n+  add_definitions(-DARROW_WITH_ZSTD)\n+  SET(ARROW_SRCS util/compression_zstd.cc ${ARROW_SRCS})\n+endif()\n+\n+if (NOT ARROW_BOOST_HEADER_ONLY)\n+  set(ARROW_SRCS ${ARROW_SRCS}\n+    io/hdfs.cc\n+    io/hdfs-internal.cc\n+  )\n+endif()\n+\n+if (ARROW_IPC)\n+  set(ARROW_SRCS ${ARROW_SRCS}\n+    ipc/dictionary.cc\n+    ipc/feather.cc\n+    ipc/json.cc\n+    ipc/json-internal.cc\n+    ipc/message.cc\n+    ipc/metadata-internal.cc\n+    ipc/reader.cc\n+    ipc/writer.cc\n+  )\n+endif()\n+\n+if(NOT APPLE AND NOT MSVC)\n+  # Localize thirdparty symbols using a linker version script. This hides them\n+  # from the client application. The OS X linker does not support the\n+  # version-script option.\n+  set(ARROW_SHARED_LINK_FLAGS \"-Wl,--version-script=${CMAKE_CURRENT_SOURCE_DIR}/symbols.map\")\n+endif()\n+\n+set(ARROW_ALL_SRCS\n+  ${ARROW_SRCS})\n+\n+ADD_ARROW_LIB(arrow\n+  SOURCES ${ARROW_ALL_SRCS}\n+  DEPENDENCIES arrow_dependencies\n+  SHARED_LINK_FLAGS ${ARROW_SHARED_LINK_FLAGS}\n+  SHARED_LINK_LIBS ${ARROW_LINK_LIBS}\n+  SHARED_PRIVATE_LINK_LIBS ${ARROW_SHARED_PRIVATE_LINK_LIBS}\n+  STATIC_LINK_LIBS ${ARROW_STATIC_LINK_LIBS}\n+  STATIC_PRIVATE_LINK_LIBS ${ARROW_STATIC_PRIVATE_LINK_LIBS}\n+)\n+\n # Headers: top level\n install(FILES\n   allocator.h\n@@ -60,3 +175,6 @@ ADD_ARROW_TEST(tensor-test)\n \n ADD_ARROW_BENCHMARK(builder-benchmark)\n ADD_ARROW_BENCHMARK(column-benchmark)\n+\n+add_subdirectory(io)\n+add_subdirectory(util)\ndiff --git a/cpp/src/arrow/allocator-test.cc b/cpp/src/arrow/allocator-test.cc\nindex e02741ec6..88f4f5e8a 100644\n--- a/cpp/src/arrow/allocator-test.cc\n+++ b/cpp/src/arrow/allocator-test.cc\n@@ -59,17 +59,16 @@ TEST(stl_allocator, FreeLargeMemory) {\n }\n \n TEST(stl_allocator, MaxMemory) {\n-  DefaultMemoryPool pool;\n+  auto pool = default_memory_pool();\n \n-  ASSERT_EQ(0, pool.max_memory());\n-  stl_allocator<uint8_t> alloc(&pool);\n-  uint8_t* data = alloc.allocate(100);\n-  uint8_t* data2 = alloc.allocate(100);\n+  stl_allocator<uint8_t> alloc(pool);\n+  uint8_t* data = alloc.allocate(1000);\n+  uint8_t* data2 = alloc.allocate(1000);\n \n-  alloc.deallocate(data, 100);\n-  alloc.deallocate(data2, 100);\n+  alloc.deallocate(data, 1000);\n+  alloc.deallocate(data2, 1000);\n \n-  ASSERT_EQ(200, pool.max_memory());\n+  ASSERT_EQ(2000, pool->max_memory());\n }\n \n #endif  // ARROW_VALGRIND\ndiff --git a/cpp/src/arrow/array-test.cc b/cpp/src/arrow/array-test.cc\nindex 97310830d..4ecf0f92b 100644\n--- a/cpp/src/arrow/array-test.cc\n+++ b/cpp/src/arrow/array-test.cc\n@@ -283,7 +283,7 @@ class TestPrimitiveBuilder : public TestBuilder {\n \n #define PINT_DECL(CapType, c_type, LOWER, UPPER)    \\\n   struct P##CapType {                               \\\n-    PTYPE_DECL(CapType, c_type);                    \\\n+    PTYPE_DECL(CapType, c_type)                     \\\n     static void draw(int64_t N, vector<T>* draws) { \\\n       test::randint<T>(N, LOWER, UPPER, draws);     \\\n     }                                               \\\n@@ -291,7 +291,7 @@ class TestPrimitiveBuilder : public TestBuilder {\n \n #define PFLOAT_DECL(CapType, c_type, LOWER, UPPER)     \\\n   struct P##CapType {                                  \\\n-    PTYPE_DECL(CapType, c_type);                       \\\n+    PTYPE_DECL(CapType, c_type)                        \\\n     static void draw(int64_t N, vector<T>* draws) {    \\\n       test::random_real<T>(N, 0, LOWER, UPPER, draws); \\\n     }                                                  \\\n@@ -311,7 +311,7 @@ PFLOAT_DECL(Float, float, -1000, 1000);\n PFLOAT_DECL(Double, double, -1000, 1000);\n \n struct PBoolean {\n-  PTYPE_DECL(Boolean, uint8_t);\n+  PTYPE_DECL(Boolean, uint8_t)\n };\n \n template <>\n@@ -378,8 +378,6 @@ TYPED_TEST_CASE(TestPrimitiveBuilder, Primitives);\n \n #define DECL_TYPE() typedef typename TestFixture::Type Type;\n \n-#define DECL_ARRAYTYPE() typedef typename TestFixture::ArrayType ArrayType;\n-\n TYPED_TEST(TestPrimitiveBuilder, TestInit) {\n   DECL_TYPE();\n \n@@ -1200,7 +1198,7 @@ class TestFWBinaryArray : public ::testing::Test {\n };\n \n TEST_F(TestFWBinaryArray, Builder) {\n-  const int32_t byte_width = 10;\n+  constexpr int32_t byte_width = 10;\n   int64_t length = 4096;\n \n   int64_t nbytes = length * byte_width;\n@@ -1215,8 +1213,7 @@ TEST_F(TestFWBinaryArray, Builder) {\n \n   std::shared_ptr<Array> result;\n \n-  auto CheckResult = [this, &length, &is_valid, &raw_data,\n-                      &byte_width](const Array& result) {\n+  auto CheckResult = [&length, &is_valid, &raw_data, byte_width](const Array& result) {\n     // Verify output\n     const auto& fw_result = static_cast<const FixedSizeBinaryArray&>(result);\n \n@@ -1847,6 +1844,93 @@ TEST(TestFixedSizeBinaryDictionaryBuilder, InvalidTypeAppend) {\n   ASSERT_RAISES(Invalid, builder.AppendArray(*fsb_array));\n }\n \n+TEST(TestDecimalDictionaryBuilder, Basic) {\n+  // Build the dictionary Array\n+  const auto& decimal_type = arrow::decimal(2, 0);\n+  DictionaryBuilder<FixedSizeBinaryType> builder(decimal_type, default_memory_pool());\n+\n+  // Test data\n+  std::vector<Decimal128> test{12, 12, 11, 12};\n+  for (const auto& value : test) {\n+    ASSERT_OK(builder.Append(value.ToBytes().data()));\n+  }\n+\n+  std::shared_ptr<Array> result;\n+  ASSERT_OK(builder.Finish(&result));\n+\n+  // Build expected data\n+  FixedSizeBinaryBuilder decimal_builder(decimal_type);\n+  ASSERT_OK(decimal_builder.Append(Decimal128(12).ToBytes()));\n+  ASSERT_OK(decimal_builder.Append(Decimal128(11).ToBytes()));\n+\n+  std::shared_ptr<Array> decimal_array;\n+  ASSERT_OK(decimal_builder.Finish(&decimal_array));\n+  auto dtype = arrow::dictionary(int8(), decimal_array);\n+\n+  Int8Builder int_builder;\n+  ASSERT_OK(int_builder.Append({0, 0, 1, 0}));\n+  std::shared_ptr<Array> int_array;\n+  ASSERT_OK(int_builder.Finish(&int_array));\n+\n+  DictionaryArray expected(dtype, int_array);\n+  ASSERT_TRUE(expected.Equals(result));\n+}\n+\n+TEST(TestDecimalDictionaryBuilder, DoubleTableSize) {\n+  const auto& decimal_type = arrow::decimal(21, 0);\n+\n+  // Build the dictionary Array\n+  DictionaryBuilder<FixedSizeBinaryType> builder(decimal_type, default_memory_pool());\n+\n+  // Build expected data\n+  FixedSizeBinaryBuilder fsb_builder(decimal_type);\n+  Int16Builder int_builder;\n+\n+  // Fill with 1024 different values\n+  for (int64_t i = 0; i < 1024; i++) {\n+    const uint8_t bytes[] = {0,\n+                             0,\n+                             0,\n+                             0,\n+                             0,\n+                             0,\n+                             0,\n+                             0,\n+                             0,\n+                             0,\n+                             0,\n+                             0,\n+                             12,\n+                             12,\n+                             static_cast<uint8_t>(i / 128),\n+                             static_cast<uint8_t>(i % 128)};\n+    ASSERT_OK(builder.Append(bytes));\n+    ASSERT_OK(fsb_builder.Append(bytes));\n+    ASSERT_OK(int_builder.Append(static_cast<uint16_t>(i)));\n+  }\n+  // Fill with an already existing value\n+  const uint8_t known_value[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 0, 1};\n+  for (int64_t i = 0; i < 1024; i++) {\n+    ASSERT_OK(builder.Append(known_value));\n+    ASSERT_OK(int_builder.Append(1));\n+  }\n+\n+  // Finalize result\n+  std::shared_ptr<Array> result;\n+  ASSERT_OK(builder.Finish(&result));\n+\n+  // Finalize expected data\n+  std::shared_ptr<Array> fsb_array;\n+  ASSERT_OK(fsb_builder.Finish(&fsb_array));\n+\n+  auto dtype = std::make_shared<DictionaryType>(int16(), fsb_array);\n+  std::shared_ptr<Array> int_array;\n+  ASSERT_OK(int_builder.Finish(&int_array));\n+\n+  DictionaryArray expected(dtype, int_array);\n+  ASSERT_TRUE(expected.Equals(result));\n+}\n+\n // ----------------------------------------------------------------------\n // List tests\n \ndiff --git a/cpp/src/arrow/array.cc b/cpp/src/arrow/array.cc\nindex 80188a14a..12922ae7b 100644\n--- a/cpp/src/arrow/array.cc\n+++ b/cpp/src/arrow/array.cc\n@@ -194,7 +194,8 @@ ListArray::ListArray(const std::shared_ptr<DataType>& type, int64_t length,\n   SetData(internal_data);\n }\n \n-Status ListArray::FromArrays(const Array& offsets, const Array& values, MemoryPool* pool,\n+Status ListArray::FromArrays(const Array& offsets, const Array& values,\n+                             MemoryPool* ARROW_ARG_UNUSED(pool),\n                              std::shared_ptr<Array>* out) {\n   if (ARROW_PREDICT_FALSE(offsets.length() == 0)) {\n     return Status::Invalid(\"List offsets must have non-zero length\");\n@@ -223,6 +224,8 @@ Status ListArray::FromArrays(const Array& offsets, const Array& values, MemoryPo\n \n void ListArray::SetData(const std::shared_ptr<ArrayData>& data) {\n   this->Array::SetData(data);\n+  DCHECK_EQ(data->buffers.size(), 2);\n+\n   auto value_offsets = data->buffers[1];\n   raw_value_offsets_ = value_offsets == nullptr\n                            ? nullptr\n@@ -239,15 +242,13 @@ std::shared_ptr<Array> ListArray::values() const { return values_; }\n // ----------------------------------------------------------------------\n // String and binary\n \n-static std::shared_ptr<DataType> kBinary = std::make_shared<BinaryType>();\n-static std::shared_ptr<DataType> kString = std::make_shared<StringType>();\n-\n BinaryArray::BinaryArray(const std::shared_ptr<ArrayData>& data) {\n   DCHECK_EQ(data->type->id(), Type::BINARY);\n   SetData(data);\n }\n \n void BinaryArray::SetData(const std::shared_ptr<ArrayData>& data) {\n+  DCHECK_EQ(data->buffers.size(), 3);\n   auto value_offsets = data->buffers[1];\n   auto value_data = data->buffers[2];\n   this->Array::SetData(data);\n@@ -261,8 +262,8 @@ BinaryArray::BinaryArray(int64_t length, const std::shared_ptr<Buffer>& value_of\n                          const std::shared_ptr<Buffer>& data,\n                          const std::shared_ptr<Buffer>& null_bitmap, int64_t null_count,\n                          int64_t offset)\n-    : BinaryArray(kBinary, length, value_offsets, data, null_bitmap, null_count, offset) {\n-}\n+    : BinaryArray(binary(), length, value_offsets, data, null_bitmap, null_count,\n+                  offset) {}\n \n BinaryArray::BinaryArray(const std::shared_ptr<DataType>& type, int64_t length,\n                          const std::shared_ptr<Buffer>& value_offsets,\n@@ -283,8 +284,7 @@ StringArray::StringArray(int64_t length, const std::shared_ptr<Buffer>& value_of\n                          const std::shared_ptr<Buffer>& data,\n                          const std::shared_ptr<Buffer>& null_bitmap, int64_t null_count,\n                          int64_t offset)\n-    : BinaryArray(kString, length, value_offsets, data, null_bitmap, null_count, offset) {\n-}\n+    : BinaryArray(utf8(), length, value_offsets, data, null_bitmap, null_count, offset) {}\n \n // ----------------------------------------------------------------------\n // Fixed width binary\n@@ -345,6 +345,7 @@ std::shared_ptr<Array> StructArray::field(int i) const {\n   if (!boxed_fields_[i]) {\n     DCHECK(MakeArray(data_->child_data[i], &boxed_fields_[i]).ok());\n   }\n+  DCHECK(boxed_fields_[i]);\n   return boxed_fields_[i];\n }\n \n@@ -354,6 +355,8 @@ std::shared_ptr<Array> StructArray::field(int i) const {\n void UnionArray::SetData(const std::shared_ptr<ArrayData>& data) {\n   this->Array::SetData(data);\n \n+  DCHECK_EQ(data->buffers.size(), 3);\n+\n   auto type_ids = data_->buffers[1];\n   auto value_offsets = data_->buffers[2];\n   raw_type_ids_ =\n@@ -388,6 +391,7 @@ std::shared_ptr<Array> UnionArray::child(int i) const {\n   if (!boxed_fields_[i]) {\n     DCHECK(MakeArray(data_->child_data[i], &boxed_fields_[i]).ok());\n   }\n+  DCHECK(boxed_fields_[i]);\n   return boxed_fields_[i];\n }\n \n@@ -437,13 +441,13 @@ Status Array::Accept(ArrayVisitor* visitor) const {\n namespace internal {\n \n struct ValidateVisitor {\n-  Status Visit(const NullArray& array) { return Status::OK(); }\n+  Status Visit(const NullArray&) { return Status::OK(); }\n \n-  Status Visit(const PrimitiveArray& array) { return Status::OK(); }\n+  Status Visit(const PrimitiveArray&) { return Status::OK(); }\n \n-  Status Visit(const DecimalArray& array) { return Status::OK(); }\n+  Status Visit(const DecimalArray&) { return Status::OK(); }\n \n-  Status Visit(const BinaryArray& array) {\n+  Status Visit(const BinaryArray&) {\n     // TODO(wesm): what to do here?\n     return Status::OK();\n   }\n@@ -585,7 +589,7 @@ class ArrayDataWrapper {\n       : data_(data), out_(out) {}\n \n   template <typename T>\n-  Status Visit(const T& type) {\n+  Status Visit(const T&) {\n     using ArrayType = typename TypeTraits<T>::ArrayType;\n     *out_ = std::make_shared<ArrayType>(data_);\n     return Status::OK();\n@@ -597,10 +601,11 @@ class ArrayDataWrapper {\n \n }  // namespace internal\n \n-// Remove enclosing namespace after 0.7.0\n Status MakeArray(const std::shared_ptr<ArrayData>& data, std::shared_ptr<Array>* out) {\n   internal::ArrayDataWrapper wrapper_visitor(data, out);\n-  return VisitTypeInline(*data->type, &wrapper_visitor);\n+  RETURN_NOT_OK(VisitTypeInline(*data->type, &wrapper_visitor));\n+  DCHECK(out);\n+  return Status::OK();\n }\n \n #ifndef ARROW_NO_DEPRECATED_API\ndiff --git a/cpp/src/arrow/array.h b/cpp/src/arrow/array.h\nindex e801b3586..4ad60eb77 100644\n--- a/cpp/src/arrow/array.h\n+++ b/cpp/src/arrow/array.h\n@@ -84,7 +84,7 @@ struct Decimal;\n /// input array and replace them with newly-allocated data, changing the output\n /// data type as well.\n struct ARROW_EXPORT ArrayData {\n-  ArrayData() {}\n+  ArrayData() : length(0) {}\n \n   ArrayData(const std::shared_ptr<DataType>& type, int64_t length,\n             int64_t null_count = kUnknownNullCount, int64_t offset = 0)\n@@ -674,7 +674,7 @@ ARROW_EXPORT\n Status ValidateArray(const Array& array);\n \n #ifndef ARROW_NO_DEPRECATED_API\n-// \\deprecated Since 0.7.0\n+// \\note Deprecated since 0.7.0\n \n /// Create new arrays for logical types that are backed by primitive arrays.\n ARROW_EXPORT\ndiff --git a/cpp/src/arrow/buffer-test.cc b/cpp/src/arrow/buffer-test.cc\nindex 334ad7bf7..5fd2706f0 100644\n--- a/cpp/src/arrow/buffer-test.cc\n+++ b/cpp/src/arrow/buffer-test.cc\n@@ -32,9 +32,7 @@ using std::string;\n \n namespace arrow {\n \n-class TestBuffer : public ::testing::Test {};\n-\n-TEST_F(TestBuffer, IsMutableFlag) {\n+TEST(TestBuffer, IsMutableFlag) {\n   Buffer buf(nullptr, 0);\n \n   ASSERT_FALSE(buf.is_mutable());\n@@ -46,7 +44,15 @@ TEST_F(TestBuffer, IsMutableFlag) {\n   ASSERT_TRUE(pbuf.is_mutable());\n }\n \n-TEST_F(TestBuffer, Resize) {\n+TEST(TestBuffer, FromStdString) {\n+  std::string val = \"hello, world\";\n+\n+  Buffer buf(val);\n+  ASSERT_EQ(0, memcmp(buf.data(), val.c_str(), val.size()));\n+  ASSERT_EQ(static_cast<int64_t>(val.size()), buf.size());\n+}\n+\n+TEST(TestBuffer, Resize) {\n   PoolBuffer buf;\n \n   ASSERT_EQ(0, buf.size());\n@@ -69,7 +75,7 @@ TEST_F(TestBuffer, Resize) {\n   ASSERT_EQ(128, buf.capacity());\n }\n \n-TEST_F(TestBuffer, TypedResize) {\n+TEST(TestBuffer, TypedResize) {\n   PoolBuffer buf;\n \n   ASSERT_EQ(0, buf.size());\n@@ -88,7 +94,7 @@ TEST_F(TestBuffer, TypedResize) {\n   ASSERT_EQ(832, buf.capacity());\n }\n \n-TEST_F(TestBuffer, ResizeOOM) {\n+TEST(TestBuffer, ResizeOOM) {\n // This test doesn't play nice with AddressSanitizer\n #ifndef ADDRESS_SANITIZER\n   // realloc fails, even though there may be no explicit limit\n@@ -99,7 +105,7 @@ TEST_F(TestBuffer, ResizeOOM) {\n #endif\n }\n \n-TEST_F(TestBuffer, EqualsWithSameContent) {\n+TEST(TestBuffer, EqualsWithSameContent) {\n   MemoryPool* pool = default_memory_pool();\n   const int32_t bufferSize = 128 * 1024;\n   uint8_t* rawBuffer1;\n@@ -123,7 +129,7 @@ TEST_F(TestBuffer, EqualsWithSameContent) {\n   pool->Free(rawBuffer3, bufferSize);\n }\n \n-TEST_F(TestBuffer, EqualsWithSameBuffer) {\n+TEST(TestBuffer, EqualsWithSameBuffer) {\n   MemoryPool* pool = default_memory_pool();\n   const int32_t bufferSize = 128 * 1024;\n   uint8_t* rawBuffer;\n@@ -142,7 +148,7 @@ TEST_F(TestBuffer, EqualsWithSameBuffer) {\n   pool->Free(rawBuffer, bufferSize);\n }\n \n-TEST_F(TestBuffer, Copy) {\n+TEST(TestBuffer, Copy) {\n   std::string data_str = \"some data to copy\";\n \n   auto data = reinterpret_cast<const uint8_t*>(data_str.c_str());\n@@ -157,7 +163,7 @@ TEST_F(TestBuffer, Copy) {\n   ASSERT_TRUE(out->Equals(expected));\n }\n \n-TEST_F(TestBuffer, SliceBuffer) {\n+TEST(TestBuffer, SliceBuffer) {\n   std::string data_str = \"some data to slice\";\n \n   auto data = reinterpret_cast<const uint8_t*>(data_str.c_str());\n@@ -171,7 +177,7 @@ TEST_F(TestBuffer, SliceBuffer) {\n   ASSERT_EQ(2, buf.use_count());\n }\n \n-TEST_F(TestBuffer, SliceMutableBuffer) {\n+TEST(TestBuffer, SliceMutableBuffer) {\n   std::string data_str = \"some data to slice\";\n   auto data = reinterpret_cast<const uint8_t*>(data_str.c_str());\n \ndiff --git a/cpp/src/arrow/buffer.cc b/cpp/src/arrow/buffer.cc\nindex cf533eb3b..e308ed260 100644\n--- a/cpp/src/arrow/buffer.cc\n+++ b/cpp/src/arrow/buffer.cc\n@@ -132,6 +132,8 @@ Status AllocateBuffer(MemoryPool* pool, const int64_t size,\n   return Status::OK();\n }\n \n+#ifndef ARROW_NO_DEPRECATED_API\n+\n Status AllocateBuffer(MemoryPool* pool, const int64_t size,\n                       std::shared_ptr<MutableBuffer>* out) {\n   std::shared_ptr<Buffer> buffer;\n@@ -140,6 +142,8 @@ Status AllocateBuffer(MemoryPool* pool, const int64_t size,\n   return Status::OK();\n }\n \n+#endif\n+\n Status AllocateResizableBuffer(MemoryPool* pool, const int64_t size,\n                                std::shared_ptr<ResizableBuffer>* out) {\n   auto buffer = std::make_shared<PoolBuffer>(pool);\ndiff --git a/cpp/src/arrow/buffer.h b/cpp/src/arrow/buffer.h\nindex d21526778..5f61ade95 100644\n--- a/cpp/src/arrow/buffer.h\n+++ b/cpp/src/arrow/buffer.h\n@@ -47,9 +47,25 @@ class MemoryPool;\n /// The following invariant is always true: Size < Capacity\n class ARROW_EXPORT Buffer {\n  public:\n+  /// \\brief Construct from buffer and size without copying memory\n+  ///\n+  /// \\param[in] data a memory buffer\n+  /// \\param[in] size buffer size\n+  ///\n+  /// \\note The passed memory must be kept alive through some other means\n   Buffer(const uint8_t* data, int64_t size)\n       : is_mutable_(false), data_(data), size_(size), capacity_(size) {}\n \n+  /// \\brief Construct from std::string without copying memory\n+  ///\n+  /// \\param[in] data a std::string object\n+  ///\n+  /// \\note The std::string must stay alive for the lifetime of the Buffer, so\n+  /// temporary rvalue strings must be stored in an lvalue somewhere\n+  explicit Buffer(const std::string& data)\n+      : Buffer(reinterpret_cast<const uint8_t*>(data.c_str()),\n+               static_cast<int64_t>(data.size())) {}\n+\n   virtual ~Buffer() = default;\n \n   /// An offset into data that is owned by another buffer, but we want to be\n@@ -69,6 +85,8 @@ class ARROW_EXPORT Buffer {\n   /// Return true if both buffers are the same size and contain the same bytes\n   /// up to the number of compared bytes\n   bool Equals(const Buffer& other, int64_t nbytes) const;\n+\n+  /// Return true if both buffers are the same size and contain the same bytes\n   bool Equals(const Buffer& other) const;\n \n   /// Copy a section of the buffer into a new Buffer.\n@@ -101,17 +119,6 @@ class ARROW_EXPORT Buffer {\n   ARROW_DISALLOW_COPY_AND_ASSIGN(Buffer);\n };\n \n-/// \\brief Create Buffer referencing std::string memory\n-///\n-/// Warning: string instance must stay alive\n-///\n-/// \\param str std::string instance\n-/// \\return std::shared_ptr<Buffer>\n-static inline std::shared_ptr<Buffer> GetBufferFromString(const std::string& str) {\n-  return std::make_shared<Buffer>(reinterpret_cast<const uint8_t*>(str.c_str()),\n-                                  static_cast<int64_t>(str.size()));\n-}\n-\n /// Construct a view on passed buffer at the indicated offset and length. This\n /// function cannot fail and does not error checking (except in debug builds)\n static inline std::shared_ptr<Buffer> SliceBuffer(const std::shared_ptr<Buffer>& buffer,\n@@ -194,7 +201,7 @@ class ARROW_EXPORT BufferBuilder {\n     if (elements == 0) {\n       return Status::OK();\n     }\n-    if (capacity_ == 0) {\n+    if (buffer_ == nullptr) {\n       buffer_ = std::make_shared<PoolBuffer>(pool_);\n     }\n     int64_t old_capacity = capacity_;\n@@ -331,11 +338,24 @@ Status AllocateResizableBuffer(MemoryPool* pool, const int64_t size,\n                                std::shared_ptr<ResizableBuffer>* out);\n \n #ifndef ARROW_NO_DEPRECATED_API\n+\n /// \\deprecated Since 0.7.0\n ARROW_EXPORT\n Status AllocateBuffer(MemoryPool* pool, const int64_t size,\n                       std::shared_ptr<MutableBuffer>* out);\n-#endif\n+\n+/// \\brief Create Buffer referencing std::string memory\n+/// \\deprecated Since 0.8.0\n+///\n+/// Warning: string instance must stay alive\n+///\n+/// \\param str std::string instance\n+/// \\return std::shared_ptr<Buffer>\n+static inline std::shared_ptr<Buffer> GetBufferFromString(const std::string& str) {\n+  return std::make_shared<Buffer>(str);\n+}\n+\n+#endif  // ARROW_NO_DEPRECATED_API\n \n }  // namespace arrow\n \ndiff --git a/cpp/src/arrow/builder.cc b/cpp/src/arrow/builder.cc\nindex a194ab745..0479dc555 100644\n--- a/cpp/src/arrow/builder.cc\n+++ b/cpp/src/arrow/builder.cc\n@@ -831,11 +831,11 @@ DictionaryBuilder<FixedSizeBinaryType>::DictionaryBuilder(\n       hash_table_(new PoolBuffer(pool)),\n       hash_slots_(nullptr),\n       dict_builder_(type, pool),\n-      values_builder_(pool) {\n+      values_builder_(pool),\n+      byte_width_(static_cast<const FixedSizeBinaryType&>(*type).byte_width()) {\n   if (!::arrow::CpuInfo::initialized()) {\n     ::arrow::CpuInfo::Init();\n   }\n-  byte_width_ = static_cast<const FixedSizeBinaryType&>(*type).byte_width();\n }\n \n #ifndef ARROW_NO_DEPRECATED_API\n@@ -921,7 +921,7 @@ Status DictionaryBuilder<T>::Append(const Scalar& value) {\n \n template <typename T>\n Status DictionaryBuilder<T>::AppendArray(const Array& array) {\n-  const NumericArray<T>& numeric_array = static_cast<const NumericArray<T>&>(array);\n+  const auto& numeric_array = static_cast<const NumericArray<T>&>(array);\n   for (int64_t i = 0; i < array.length(); i++) {\n     if (array.IsNull(i)) {\n       RETURN_NOT_OK(AppendNull());\n@@ -938,8 +938,7 @@ Status DictionaryBuilder<FixedSizeBinaryType>::AppendArray(const Array& array) {\n     return Status::Invalid(\"Cannot append FixedSizeBinary array with non-matching type\");\n   }\n \n-  const FixedSizeBinaryArray& numeric_array =\n-      static_cast<const FixedSizeBinaryArray&>(array);\n+  const auto& numeric_array = static_cast<const FixedSizeBinaryArray&>(array);\n   for (int64_t i = 0; i < array.length(); i++) {\n     if (array.IsNull(i)) {\n       RETURN_NOT_OK(AppendNull());\n@@ -1493,6 +1492,7 @@ Status MakeDictionaryBuilder(MemoryPool* pool, const std::shared_ptr<DataType>&\n     DICTIONARY_BUILDER_CASE(STRING, StringDictionaryBuilder);\n     DICTIONARY_BUILDER_CASE(BINARY, BinaryDictionaryBuilder);\n     DICTIONARY_BUILDER_CASE(FIXED_SIZE_BINARY, DictionaryBuilder<FixedSizeBinaryType>);\n+    DICTIONARY_BUILDER_CASE(DECIMAL, DictionaryBuilder<FixedSizeBinaryType>);\n     default:\n       return Status::NotImplemented(type->ToString());\n   }\n@@ -1528,6 +1528,7 @@ Status EncodeArrayToDictionary(const Array& input, MemoryPool* pool,\n     DICTIONARY_ARRAY_CASE(STRING, StringDictionaryBuilder);\n     DICTIONARY_ARRAY_CASE(BINARY, BinaryDictionaryBuilder);\n     DICTIONARY_ARRAY_CASE(FIXED_SIZE_BINARY, DictionaryBuilder<FixedSizeBinaryType>);\n+    DICTIONARY_ARRAY_CASE(DECIMAL, DictionaryBuilder<FixedSizeBinaryType>);\n     default:\n       std::stringstream ss;\n       ss << \"Cannot encode array of type \" << type->ToString();\ndiff --git a/cpp/src/arrow/builder.h b/cpp/src/arrow/builder.h\nindex 28f3cb972..da7386aaf 100644\n--- a/cpp/src/arrow/builder.h\n+++ b/cpp/src/arrow/builder.h\n@@ -937,8 +937,8 @@ class ARROW_EXPORT DictionaryBuilder : public ArrayBuilder {\n \n class ARROW_EXPORT BinaryDictionaryBuilder : public DictionaryBuilder<BinaryType> {\n  public:\n-  using DictionaryBuilder::DictionaryBuilder;\n   using DictionaryBuilder::Append;\n+  using DictionaryBuilder::DictionaryBuilder;\n \n   Status Append(const uint8_t* value, int32_t length) {\n     return Append(internal::WrappedBinary(value, length));\n@@ -958,8 +958,8 @@ class ARROW_EXPORT BinaryDictionaryBuilder : public DictionaryBuilder<BinaryType\n /// \\brief Dictionary array builder with convenience methods for strings\n class ARROW_EXPORT StringDictionaryBuilder : public DictionaryBuilder<StringType> {\n  public:\n-  using DictionaryBuilder::DictionaryBuilder;\n   using DictionaryBuilder::Append;\n+  using DictionaryBuilder::DictionaryBuilder;\n \n   Status Append(const uint8_t* value, int32_t length) {\n     return Append(internal::WrappedBinary(value, length));\ndiff --git a/cpp/src/arrow/compare.cc b/cpp/src/arrow/compare.cc\nindex 2aeb03b8b..515b8f62f 100644\n--- a/cpp/src/arrow/compare.cc\n+++ b/cpp/src/arrow/compare.cc\n@@ -584,7 +584,7 @@ class TypeEqualsVisitor {\n   typename std::enable_if<std::is_base_of<NoExtraMeta, T>::value ||\n                               std::is_base_of<PrimitiveCType, T>::value,\n                           Status>::type\n-  Visit(const T& type) {\n+  Visit(const T&) {\n     result_ = true;\n     return Status::OK();\n   }\ndiff --git a/cpp/src/arrow/compute/cast.cc b/cpp/src/arrow/compute/cast.cc\nindex ee838fa38..149cc36b6 100644\n--- a/cpp/src/arrow/compute/cast.cc\n+++ b/cpp/src/arrow/compute/cast.cc\n@@ -490,7 +490,10 @@ static Status AllocateIfNotPreallocated(FunctionContext* ctx, const Array& input\n   if (can_pre_allocate_values) {\n     std::shared_ptr<Buffer> out_data;\n \n-    if (!(is_primitive(out->type->id()) || out->type->id() == Type::FIXED_SIZE_BINARY)) {\n+    const Type::type type_id = out->type->id();\n+\n+    if (!(is_primitive(type_id) || type_id == Type::FIXED_SIZE_BINARY ||\n+          type_id == Type::DECIMAL)) {\n       std::stringstream ss;\n       ss << \"Cannot pre-allocate memory for type: \" << out->type->ToString();\n       return Status::NotImplemented(ss.str());\n@@ -614,6 +617,7 @@ class CastKernel : public UnaryKernel {\n   FN(IN_TYPE, FloatType);             \\\n   FN(IN_TYPE, DoubleType);            \\\n   FN(IN_TYPE, FixedSizeBinaryType);   \\\n+  FN(IN_TYPE, DecimalType);           \\\n   FN(IN_TYPE, BinaryType);            \\\n   FN(IN_TYPE, StringType);\n \ndiff --git a/cpp/src/arrow/compute/cast.h b/cpp/src/arrow/compute/cast.h\nindex 081cdd908..7a07512b2 100644\n--- a/cpp/src/arrow/compute/cast.h\n+++ b/cpp/src/arrow/compute/cast.h\n@@ -46,11 +46,11 @@ Status GetCastFunction(const DataType& in_type, const std::shared_ptr<DataType>&\n                        const CastOptions& options, std::unique_ptr<UnaryKernel>* kernel);\n \n /// \\brief Cast from one array type to another\n-/// \\param[in] context\n-/// \\param[in] array\n-/// \\param[in] to_type\n-/// \\param[in] options\n-/// \\param[out] out\n+/// \\param[in] context the FunctionContext\n+/// \\param[in] array array to cast\n+/// \\param[in] to_type type to cast to\n+/// \\param[in] options casting options\n+/// \\param[out] out resulting array\n ///\n /// \\since 0.7.0\n /// \\note API not yet finalized\ndiff --git a/cpp/src/arrow/io/file.cc b/cpp/src/arrow/io/file.cc\nindex ca536321b..234bd540e 100644\n--- a/cpp/src/arrow/io/file.cc\n+++ b/cpp/src/arrow/io/file.cc\n@@ -384,6 +384,8 @@ class OSFile {\n \n   FileMode::type mode() const { return mode_; }\n \n+  std::mutex& lock() { return lock_; }\n+\n  protected:\n   Status SetFileName(const std::string& file_name) {\n #if defined(_MSC_VER)\n@@ -461,6 +463,20 @@ Status ReadableFile::Read(int64_t nbytes, int64_t* bytes_read, uint8_t* out) {\n   return impl_->Read(nbytes, bytes_read, out);\n }\n \n+Status ReadableFile::ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                            uint8_t* out) {\n+  std::lock_guard<std::mutex> guard(impl_->lock());\n+  RETURN_NOT_OK(Seek(position));\n+  return Read(nbytes, bytes_read, out);\n+}\n+\n+Status ReadableFile::ReadAt(int64_t position, int64_t nbytes,\n+                            std::shared_ptr<Buffer>* out) {\n+  std::lock_guard<std::mutex> guard(impl_->lock());\n+  RETURN_NOT_OK(Seek(position));\n+  return Read(nbytes, out);\n+}\n+\n Status ReadableFile::Read(int64_t nbytes, std::shared_ptr<Buffer>* out) {\n   return impl_->ReadBuffer(nbytes, out);\n }\n@@ -590,6 +606,8 @@ class MemoryMappedFile::MemoryMap : public MutableBuffer {\n \n   int fd() const { return file_->fd(); }\n \n+  std::mutex& lock() { return file_->lock(); }\n+\n  private:\n   std::unique_ptr<OSFile> file_;\n   int64_t position_;\n@@ -671,10 +689,24 @@ Status MemoryMappedFile::Read(int64_t nbytes, std::shared_ptr<Buffer>* out) {\n   return Status::OK();\n }\n \n+Status MemoryMappedFile::ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                                uint8_t* out) {\n+  std::lock_guard<std::mutex> guard(memory_map_->lock());\n+  RETURN_NOT_OK(Seek(position));\n+  return Read(nbytes, bytes_read, out);\n+}\n+\n+Status MemoryMappedFile::ReadAt(int64_t position, int64_t nbytes,\n+                                std::shared_ptr<Buffer>* out) {\n+  std::lock_guard<std::mutex> guard(memory_map_->lock());\n+  RETURN_NOT_OK(Seek(position));\n+  return Read(nbytes, out);\n+}\n+\n bool MemoryMappedFile::supports_zero_copy() const { return true; }\n \n Status MemoryMappedFile::WriteAt(int64_t position, const uint8_t* data, int64_t nbytes) {\n-  std::lock_guard<std::mutex> guard(lock_);\n+  std::lock_guard<std::mutex> guard(memory_map_->lock());\n \n   if (!memory_map_->opened() || !memory_map_->writable()) {\n     return Status::IOError(\"Unable to write\");\n@@ -685,7 +717,7 @@ Status MemoryMappedFile::WriteAt(int64_t position, const uint8_t* data, int64_t\n }\n \n Status MemoryMappedFile::Write(const uint8_t* data, int64_t nbytes) {\n-  std::lock_guard<std::mutex> guard(lock_);\n+  std::lock_guard<std::mutex> guard(memory_map_->lock());\n \n   if (!memory_map_->opened() || !memory_map_->writable()) {\n     return Status::IOError(\"Unable to write\");\ndiff --git a/cpp/src/arrow/io/file.h b/cpp/src/arrow/io/file.h\nindex 1b1bbe0e4..47bed346c 100644\n--- a/cpp/src/arrow/io/file.h\n+++ b/cpp/src/arrow/io/file.h\n@@ -86,7 +86,7 @@ class ARROW_EXPORT ReadableFile : public RandomAccessFile {\n   /// \\param[in] pool a MemoryPool for memory allocations\n   /// \\param[out] file ReadableFile instance\n   /// Open file with one's own memory pool for memory allocations\n-  static Status Open(const std::string& path, MemoryPool* memory_pool,\n+  static Status Open(const std::string& path, MemoryPool* pool,\n                      std::shared_ptr<ReadableFile>* file);\n \n   Status Close() override;\n@@ -96,6 +96,12 @@ class ARROW_EXPORT ReadableFile : public RandomAccessFile {\n   Status Read(int64_t nbytes, int64_t* bytes_read, uint8_t* buffer) override;\n   Status Read(int64_t nbytes, std::shared_ptr<Buffer>* out) override;\n \n+  Status ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                uint8_t* out) override;\n+\n+  /// Default implementation is thread-safe\n+  Status ReadAt(int64_t position, int64_t nbytes, std::shared_ptr<Buffer>* out) override;\n+\n   Status GetSize(int64_t* size) override;\n   Status Seek(int64_t position) override;\n \n@@ -139,6 +145,12 @@ class ARROW_EXPORT MemoryMappedFile : public ReadWriteFileInterface {\n   // Zero copy read. Not thread-safe\n   Status Read(int64_t nbytes, std::shared_ptr<Buffer>* out) override;\n \n+  Status ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                uint8_t* out) override;\n+\n+  /// Default implementation is thread-safe\n+  Status ReadAt(int64_t position, int64_t nbytes, std::shared_ptr<Buffer>* out) override;\n+\n   bool supports_zero_copy() const override;\n \n   /// Write data at the current position in the file. Thread-safe\ndiff --git a/cpp/src/arrow/io/hdfs-internal.cc b/cpp/src/arrow/io/hdfs-internal.cc\nindex e6d0487a9..9cd1c5052 100644\n--- a/cpp/src/arrow/io/hdfs-internal.cc\n+++ b/cpp/src/arrow/io/hdfs-internal.cc\n@@ -44,6 +44,7 @@\n #include <boost/filesystem.hpp>  // NOLINT\n \n #include \"arrow/status.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace fs = boost::filesystem;\n \n@@ -346,6 +347,7 @@ bool LibHdfsShim::HasPread() {\n tSize LibHdfsShim::Pread(hdfsFS fs, hdfsFile file, tOffset position, void* buffer,\n                          tSize length) {\n   GET_SYMBOL(this, hdfsPread);\n+  DCHECK(this->hdfsPread);\n   return this->hdfsPread(fs, file, position, buffer, length);\n }\n \ndiff --git a/cpp/src/arrow/io/interfaces.cc b/cpp/src/arrow/io/interfaces.cc\nindex 694575b5f..58fcf7a81 100644\n--- a/cpp/src/arrow/io/interfaces.cc\n+++ b/cpp/src/arrow/io/interfaces.cc\n@@ -30,20 +30,6 @@ FileInterface::~FileInterface() {}\n \n RandomAccessFile::RandomAccessFile() { set_mode(FileMode::READ); }\n \n-Status RandomAccessFile::ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n-                                uint8_t* out) {\n-  std::lock_guard<std::mutex> guard(lock_);\n-  RETURN_NOT_OK(Seek(position));\n-  return Read(nbytes, bytes_read, out);\n-}\n-\n-Status RandomAccessFile::ReadAt(int64_t position, int64_t nbytes,\n-                                std::shared_ptr<Buffer>* out) {\n-  std::lock_guard<std::mutex> guard(lock_);\n-  RETURN_NOT_OK(Seek(position));\n-  return Read(nbytes, out);\n-}\n-\n Status Writeable::Write(const std::string& data) {\n   return Write(reinterpret_cast<const uint8_t*>(data.c_str()),\n                static_cast<int64_t>(data.size()));\ndiff --git a/cpp/src/arrow/io/interfaces.h b/cpp/src/arrow/io/interfaces.h\nindex 11441794e..1af35efe2 100644\n--- a/cpp/src/arrow/io/interfaces.h\n+++ b/cpp/src/arrow/io/interfaces.h\n@@ -20,7 +20,6 @@\n \n #include <cstdint>\n #include <memory>\n-#include <mutex>\n #include <string>\n #include <vector>\n \n@@ -86,11 +85,14 @@ class ARROW_EXPORT FileInterface {\n \n class ARROW_EXPORT Seekable {\n  public:\n+  virtual ~Seekable() = default;\n   virtual Status Seek(int64_t position) = 0;\n };\n \n class ARROW_EXPORT Writeable {\n  public:\n+  virtual ~Writeable() = default;\n+\n   virtual Status Write(const uint8_t* data, int64_t nbytes) = 0;\n \n   /// \\brief Flush buffered bytes, if any\n@@ -101,6 +103,8 @@ class ARROW_EXPORT Writeable {\n \n class ARROW_EXPORT Readable {\n  public:\n+  virtual ~Readable() = default;\n+\n   virtual Status Read(int64_t nbytes, int64_t* bytes_read, uint8_t* out) = 0;\n \n   // Does not copy if not necessary\n@@ -128,16 +132,13 @@ class ARROW_EXPORT RandomAccessFile : public InputStream, public Seekable {\n   ///\n   /// Default implementation is thread-safe\n   virtual Status ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n-                        uint8_t* out);\n+                        uint8_t* out) = 0;\n \n   /// Default implementation is thread-safe\n-  virtual Status ReadAt(int64_t position, int64_t nbytes, std::shared_ptr<Buffer>* out);\n-\n-  std::mutex& lock() { return lock_; }\n+  virtual Status ReadAt(int64_t position, int64_t nbytes,\n+                        std::shared_ptr<Buffer>* out) = 0;\n \n  protected:\n-  std::mutex lock_;\n-\n   RandomAccessFile();\n };\n \ndiff --git a/cpp/src/arrow/io/io-file-test.cc b/cpp/src/arrow/io/io-file-test.cc\nindex 636fbd842..ee3beabd9 100644\n--- a/cpp/src/arrow/io/io-file-test.cc\n+++ b/cpp/src/arrow/io/io-file-test.cc\n@@ -393,9 +393,9 @@ TEST_F(TestReadableFile, ThreadSafety) {\n   ASSERT_OK(ReadableFile::Open(path_, &pool, &file_));\n \n   std::atomic<int> correct_count(0);\n-  const int niter = 10000;\n+  constexpr int niter = 10000;\n \n-  auto ReadData = [&correct_count, &data, niter, this]() {\n+  auto ReadData = [&correct_count, &data, this, niter]() {\n     std::shared_ptr<Buffer> buffer;\n \n     for (int i = 0; i < niter; ++i) {\n@@ -587,9 +587,9 @@ TEST_F(TestMemoryMappedFile, ThreadSafety) {\n                         static_cast<int64_t>(data.size())));\n \n   std::atomic<int> correct_count(0);\n-  const int niter = 10000;\n+  constexpr int niter = 10000;\n \n-  auto ReadData = [&correct_count, &data, niter, &file]() {\n+  auto ReadData = [&correct_count, &data, &file, niter]() {\n     std::shared_ptr<Buffer> buffer;\n \n     for (int i = 0; i < niter; ++i) {\ndiff --git a/cpp/src/arrow/io/io-hdfs-test.cc b/cpp/src/arrow/io/io-hdfs-test.cc\nindex eaf638f20..5305b4774 100644\n--- a/cpp/src/arrow/io/io-hdfs-test.cc\n+++ b/cpp/src/arrow/io/io-hdfs-test.cc\n@@ -437,7 +437,7 @@ TYPED_TEST(TestHadoopFileSystem, ThreadSafety) {\n   ASSERT_OK(this->client_->OpenReadable(src_path, &file));\n \n   std::atomic<int> correct_count(0);\n-  const int niter = 1000;\n+  constexpr int niter = 1000;\n \n   auto ReadData = [&file, &correct_count, &data, niter]() {\n     for (int i = 0; i < niter; ++i) {\ndiff --git a/cpp/src/arrow/io/memory.cc b/cpp/src/arrow/io/memory.cc\nindex 0b91ab518..370d3e956 100644\n--- a/cpp/src/arrow/io/memory.cc\n+++ b/cpp/src/arrow/io/memory.cc\n@@ -20,6 +20,7 @@\n #include <algorithm>\n #include <cstdint>\n #include <cstring>\n+#include <mutex>\n \n #include \"arrow/buffer.h\"\n #include \"arrow/status.h\"\n@@ -127,67 +128,109 @@ static constexpr int kMemcopyDefaultNumThreads = 1;\n static constexpr int64_t kMemcopyDefaultBlocksize = 64;\n static constexpr int64_t kMemcopyDefaultThreshold = 1024 * 1024;\n \n-/// Input buffer must be mutable, will abort if not\n-FixedSizeBufferWriter::FixedSizeBufferWriter(const std::shared_ptr<Buffer>& buffer)\n-    : memcopy_num_threads_(kMemcopyDefaultNumThreads),\n-      memcopy_blocksize_(kMemcopyDefaultBlocksize),\n-      memcopy_threshold_(kMemcopyDefaultThreshold) {\n-  DCHECK(buffer) << \"Buffer was nullptr\";\n-  buffer_ = buffer;\n-  DCHECK(buffer->is_mutable()) << \"Must pass mutable buffer\";\n-  mutable_data_ = buffer->mutable_data();\n-  size_ = buffer->size();\n-  position_ = 0;\n-}\n+class FixedSizeBufferWriter::FixedSizeBufferWriterImpl {\n+ public:\n+  /// Input buffer must be mutable, will abort if not\n+\n+  /// Input buffer must be mutable, will abort if not\n+  explicit FixedSizeBufferWriterImpl(const std::shared_ptr<Buffer>& buffer)\n+      : memcopy_num_threads_(kMemcopyDefaultNumThreads),\n+        memcopy_blocksize_(kMemcopyDefaultBlocksize),\n+        memcopy_threshold_(kMemcopyDefaultThreshold) {\n+    buffer_ = buffer;\n+    DCHECK(buffer->is_mutable()) << \"Must pass mutable buffer\";\n+    mutable_data_ = buffer->mutable_data();\n+    size_ = buffer->size();\n+    position_ = 0;\n+  }\n \n-FixedSizeBufferWriter::~FixedSizeBufferWriter() {}\n+  ~FixedSizeBufferWriterImpl() {}\n \n-Status FixedSizeBufferWriter::Close() {\n-  // no-op\n-  return Status::OK();\n-}\n+  Status Close() {\n+    // No-op\n+    return Status::OK();\n+  }\n \n-Status FixedSizeBufferWriter::Seek(int64_t position) {\n-  if (position < 0 || position >= size_) {\n-    return Status::IOError(\"position out of bounds\");\n+  Status Seek(int64_t position) {\n+    if (position < 0 || position >= size_) {\n+      return Status::IOError(\"position out of bounds\");\n+    }\n+    position_ = position;\n+    return Status::OK();\n   }\n-  position_ = position;\n-  return Status::OK();\n-}\n+\n+  Status Tell(int64_t* position) {\n+    *position = position_;\n+    return Status::OK();\n+  }\n+\n+  Status Write(const uint8_t* data, int64_t nbytes) {\n+    if (nbytes > memcopy_threshold_ && memcopy_num_threads_ > 1) {\n+      internal::parallel_memcopy(mutable_data_ + position_, data, nbytes,\n+                                 memcopy_blocksize_, memcopy_num_threads_);\n+    } else {\n+      memcpy(mutable_data_ + position_, data, nbytes);\n+    }\n+    position_ += nbytes;\n+    return Status::OK();\n+  }\n+\n+  Status WriteAt(int64_t position, const uint8_t* data, int64_t nbytes) {\n+    std::lock_guard<std::mutex> guard(lock_);\n+    RETURN_NOT_OK(Seek(position));\n+    return Write(data, nbytes);\n+  }\n+\n+  void set_memcopy_threads(int num_threads) { memcopy_num_threads_ = num_threads; }\n+\n+  void set_memcopy_blocksize(int64_t blocksize) { memcopy_blocksize_ = blocksize; }\n+\n+  void set_memcopy_threshold(int64_t threshold) { memcopy_threshold_ = threshold; }\n+\n+ private:\n+  std::mutex lock_;\n+  std::shared_ptr<Buffer> buffer_;\n+  uint8_t* mutable_data_;\n+  int64_t size_;\n+  int64_t position_;\n+\n+  int memcopy_num_threads_;\n+  int64_t memcopy_blocksize_;\n+  int64_t memcopy_threshold_;\n+};\n+\n+FixedSizeBufferWriter::~FixedSizeBufferWriter() {}\n+\n+FixedSizeBufferWriter::FixedSizeBufferWriter(const std::shared_ptr<Buffer>& buffer)\n+    : impl_(new FixedSizeBufferWriterImpl(buffer)) {}\n+\n+Status FixedSizeBufferWriter::Close() { return impl_->Close(); }\n+\n+Status FixedSizeBufferWriter::Seek(int64_t position) { return impl_->Seek(position); }\n \n Status FixedSizeBufferWriter::Tell(int64_t* position) const {\n-  *position = position_;\n-  return Status::OK();\n+  return impl_->Tell(position);\n }\n \n Status FixedSizeBufferWriter::Write(const uint8_t* data, int64_t nbytes) {\n-  if (nbytes > memcopy_threshold_ && memcopy_num_threads_ > 1) {\n-    internal::parallel_memcopy(mutable_data_ + position_, data, nbytes,\n-                               memcopy_blocksize_, memcopy_num_threads_);\n-  } else {\n-    memcpy(mutable_data_ + position_, data, nbytes);\n-  }\n-  position_ += nbytes;\n-  return Status::OK();\n+  return impl_->Write(data, nbytes);\n }\n \n Status FixedSizeBufferWriter::WriteAt(int64_t position, const uint8_t* data,\n                                       int64_t nbytes) {\n-  std::lock_guard<std::mutex> guard(lock_);\n-  RETURN_NOT_OK(Seek(position));\n-  return Write(data, nbytes);\n+  return impl_->WriteAt(position, data, nbytes);\n }\n \n void FixedSizeBufferWriter::set_memcopy_threads(int num_threads) {\n-  memcopy_num_threads_ = num_threads;\n+  impl_->set_memcopy_threads(num_threads);\n }\n \n void FixedSizeBufferWriter::set_memcopy_blocksize(int64_t blocksize) {\n-  memcopy_blocksize_ = blocksize;\n+  impl_->set_memcopy_blocksize(blocksize);\n }\n \n void FixedSizeBufferWriter::set_memcopy_threshold(int64_t threshold) {\n-  memcopy_threshold_ = threshold;\n+  impl_->set_memcopy_threshold(threshold);\n }\n \n // ----------------------------------------------------------------------\n@@ -233,6 +276,18 @@ Status BufferReader::Read(int64_t nbytes, std::shared_ptr<Buffer>* out) {\n   return Status::OK();\n }\n \n+Status BufferReader::ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                            uint8_t* out) {\n+  RETURN_NOT_OK(Seek(position));\n+  return Read(nbytes, bytes_read, out);\n+}\n+\n+Status BufferReader::ReadAt(int64_t position, int64_t nbytes,\n+                            std::shared_ptr<Buffer>* out) {\n+  RETURN_NOT_OK(Seek(position));\n+  return Read(nbytes, out);\n+}\n+\n Status BufferReader::GetSize(int64_t* size) {\n   *size = size_;\n   return Status::OK();\ndiff --git a/cpp/src/arrow/io/memory.h b/cpp/src/arrow/io/memory.h\nindex 563000f77..978c198c2 100644\n--- a/cpp/src/arrow/io/memory.h\n+++ b/cpp/src/arrow/io/memory.h\n@@ -22,7 +22,6 @@\n \n #include <cstdint>\n #include <memory>\n-#include <mutex>\n \n #include \"arrow/io/interfaces.h\"\n #include \"arrow/util/visibility.h\"\n@@ -99,15 +98,8 @@ class ARROW_EXPORT FixedSizeBufferWriter : public WriteableFile {\n   void set_memcopy_threshold(int64_t threshold);\n \n  protected:\n-  std::mutex lock_;\n-  std::shared_ptr<Buffer> buffer_;\n-  uint8_t* mutable_data_;\n-  int64_t size_;\n-  int64_t position_;\n-\n-  int memcopy_num_threads_;\n-  int64_t memcopy_blocksize_;\n-  int64_t memcopy_threshold_;\n+  class FixedSizeBufferWriterImpl;\n+  std::unique_ptr<FixedSizeBufferWriterImpl> impl_;\n };\n \n /// \\class BufferReader\n@@ -125,6 +117,12 @@ class ARROW_EXPORT BufferReader : public RandomAccessFile {\n   // Zero copy read\n   Status Read(int64_t nbytes, std::shared_ptr<Buffer>* out) override;\n \n+  Status ReadAt(int64_t position, int64_t nbytes, int64_t* bytes_read,\n+                uint8_t* out) override;\n+\n+  /// Default implementation is thread-safe\n+  Status ReadAt(int64_t position, int64_t nbytes, std::shared_ptr<Buffer>* out) override;\n+\n   Status GetSize(int64_t* size) override;\n   Status Seek(int64_t position) override;\n \ndiff --git a/cpp/src/arrow/ipc/feather.cc b/cpp/src/arrow/ipc/feather.cc\nindex 31dc0e73e..9d244f115 100644\n--- a/cpp/src/arrow/ipc/feather.cc\n+++ b/cpp/src/arrow/ipc/feather.cc\n@@ -477,7 +477,6 @@ fbs::Type ToFlatbufferType(Type::type type) {\n       return fbs::Type_INT64;\n     default:\n       DCHECK(false) << \"Cannot reach this code\";\n-      break;\n   }\n   // prevent compiler warning\n   return fbs::Type_MIN;\n@@ -632,19 +631,19 @@ class TableWriter::TableWriterImpl : public ArrayVisitor {\n #define VISIT_PRIMITIVE(TYPE) \\\n   Status Visit(const TYPE& values) override { return WritePrimitiveValues(values); }\n \n-  VISIT_PRIMITIVE(BooleanArray);\n-  VISIT_PRIMITIVE(Int8Array);\n-  VISIT_PRIMITIVE(Int16Array);\n-  VISIT_PRIMITIVE(Int32Array);\n-  VISIT_PRIMITIVE(Int64Array);\n-  VISIT_PRIMITIVE(UInt8Array);\n-  VISIT_PRIMITIVE(UInt16Array);\n-  VISIT_PRIMITIVE(UInt32Array);\n-  VISIT_PRIMITIVE(UInt64Array);\n-  VISIT_PRIMITIVE(FloatArray);\n-  VISIT_PRIMITIVE(DoubleArray);\n-  VISIT_PRIMITIVE(BinaryArray);\n-  VISIT_PRIMITIVE(StringArray);\n+  VISIT_PRIMITIVE(BooleanArray)\n+  VISIT_PRIMITIVE(Int8Array)\n+  VISIT_PRIMITIVE(Int16Array)\n+  VISIT_PRIMITIVE(Int32Array)\n+  VISIT_PRIMITIVE(Int64Array)\n+  VISIT_PRIMITIVE(UInt8Array)\n+  VISIT_PRIMITIVE(UInt16Array)\n+  VISIT_PRIMITIVE(UInt32Array)\n+  VISIT_PRIMITIVE(UInt64Array)\n+  VISIT_PRIMITIVE(FloatArray)\n+  VISIT_PRIMITIVE(DoubleArray)\n+  VISIT_PRIMITIVE(BinaryArray)\n+  VISIT_PRIMITIVE(StringArray)\n \n #undef VISIT_PRIMITIVE\n \ndiff --git a/cpp/src/arrow/ipc/ipc-json-test.cc b/cpp/src/arrow/ipc/ipc-json-test.cc\nindex 7855aeafe..f2dd9e74e 100644\n--- a/cpp/src/arrow/ipc/ipc-json-test.cc\n+++ b/cpp/src/arrow/ipc/ipc-json-test.cc\n@@ -279,8 +279,7 @@ TEST(TestJsonFileReadWrite, BasicRoundTrip) {\n \n   std::unique_ptr<JsonReader> reader;\n \n-  auto buffer = std::make_shared<Buffer>(reinterpret_cast<const uint8_t*>(result.c_str()),\n-                                         static_cast<int>(result.size()));\n+  auto buffer = std::make_shared<Buffer>(result);\n \n   ASSERT_OK(JsonReader::Open(buffer, &reader));\n   ASSERT_TRUE(reader->schema()->Equals(*schema));\n@@ -395,8 +394,7 @@ void CheckRoundtrip(const RecordBatch& batch) {\n   std::string result;\n   ASSERT_OK(writer->Finish(&result));\n \n-  auto buffer = std::make_shared<Buffer>(reinterpret_cast<const uint8_t*>(result.c_str()),\n-                                         static_cast<int64_t>(result.size()));\n+  auto buffer = std::make_shared<Buffer>(result);\n \n   std::unique_ptr<JsonReader> reader;\n   ASSERT_OK(JsonReader::Open(buffer, &reader));\ndiff --git a/cpp/src/arrow/ipc/ipc-read-write-test.cc b/cpp/src/arrow/ipc/ipc-read-write-test.cc\nindex ad3af0fb6..d454d59b2 100644\n--- a/cpp/src/arrow/ipc/ipc-read-write-test.cc\n+++ b/cpp/src/arrow/ipc/ipc-read-write-test.cc\n@@ -63,10 +63,10 @@ TEST(TestMessage, Equals) {\n   std::string metadata = \"foo\";\n   std::string body = \"bar\";\n \n-  auto b1 = GetBufferFromString(metadata);\n-  auto b2 = GetBufferFromString(metadata);\n-  auto b3 = GetBufferFromString(body);\n-  auto b4 = GetBufferFromString(body);\n+  auto b1 = std::make_shared<Buffer>(metadata);\n+  auto b2 = std::make_shared<Buffer>(metadata);\n+  auto b3 = std::make_shared<Buffer>(body);\n+  auto b4 = std::make_shared<Buffer>(body);\n \n   Message msg1(b1, b3);\n   Message msg2(b2, b4);\ndiff --git a/cpp/src/arrow/ipc/message.cc b/cpp/src/arrow/ipc/message.cc\nindex 082c92556..0c587ab7b 100644\n--- a/cpp/src/arrow/ipc/message.cc\n+++ b/cpp/src/arrow/ipc/message.cc\n@@ -29,6 +29,7 @@\n #include \"arrow/ipc/Schema_generated.h\"\n #include \"arrow/ipc/metadata-internal.h\"\n #include \"arrow/status.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n namespace ipc {\n@@ -194,9 +195,18 @@ std::string FormatMessageType(Message::Type type) {\n \n Status ReadMessage(int64_t offset, int32_t metadata_length, io::RandomAccessFile* file,\n                    std::unique_ptr<Message>* message) {\n+  DCHECK_GT(static_cast<size_t>(metadata_length), sizeof(int32_t));\n+\n   std::shared_ptr<Buffer> buffer;\n   RETURN_NOT_OK(file->ReadAt(offset, metadata_length, &buffer));\n \n+  if (buffer->size() < metadata_length) {\n+    std::stringstream ss;\n+    ss << \"Expected to read \" << metadata_length << \" metadata bytes but got \"\n+       << buffer->size();\n+    return Status::Invalid(ss.str());\n+  }\n+\n   int32_t flatbuffer_size = *reinterpret_cast<const int32_t*>(buffer->data());\n \n   if (flatbuffer_size + static_cast<int>(sizeof(int32_t)) > metadata_length) {\ndiff --git a/cpp/src/arrow/ipc/message.h b/cpp/src/arrow/ipc/message.h\nindex 4bc4384f6..522b3bdcd 100644\n--- a/cpp/src/arrow/ipc/message.h\n+++ b/cpp/src/arrow/ipc/message.h\n@@ -82,11 +82,10 @@ class ARROW_EXPORT Message {\n   static Status ReadFrom(const std::shared_ptr<Buffer>& metadata, io::InputStream* stream,\n                          std::unique_ptr<Message>* out);\n \n-  /// \\brief Write length-prefixed metadata and body to output stream\n+  /// \\brief Return true if message type and contents are equal\n   ///\n-  /// \\param[in] file output stream to write to\n-  /// \\param[out] output_length the number of bytes written\n-  /// \\return Status\n+  /// \\param other another message\n+  /// \\return true if contents equal\n   bool Equals(const Message& other) const;\n \n   /// \\brief the Message metadata\ndiff --git a/cpp/src/arrow/ipc/reader.cc b/cpp/src/arrow/ipc/reader.cc\nindex 09def6ea6..e6ba50e74 100644\n--- a/cpp/src/arrow/ipc/reader.cc\n+++ b/cpp/src/arrow/ipc/reader.cc\n@@ -349,7 +349,6 @@ Status ReadDictionary(const Buffer& metadata, const DictionaryTypeMap& dictionar\n       reinterpret_cast<const flatbuf::RecordBatch*>(dictionary_batch->data());\n   RETURN_NOT_OK(\n       ReadRecordBatch(batch_meta, dummy_schema, kMaxNestingDepth, file, &batch));\n-\n   if (batch->num_columns() != 1) {\n     return Status::Invalid(\"Dictionary record batch must only contain one field\");\n   }\n@@ -526,6 +525,13 @@ class RecordBatchFileReader::RecordBatchFileReaderImpl {\n     int file_end_size = static_cast<int>(magic_size + sizeof(int32_t));\n     RETURN_NOT_OK(file_->ReadAt(footer_offset_ - file_end_size, file_end_size, &buffer));\n \n+    const int64_t expected_footer_size = magic_size + sizeof(int32_t);\n+    if (buffer->size() < expected_footer_size) {\n+      std::stringstream ss;\n+      ss << \"Unable to read \" << expected_footer_size << \"from end of file\";\n+      return Status::Invalid(ss.str());\n+    }\n+\n     if (memcmp(buffer->data() + sizeof(int32_t), kArrowMagicBytes, magic_size)) {\n       return Status::Invalid(\"Not an Arrow file\");\n     }\ndiff --git a/cpp/src/arrow/ipc/reader.h b/cpp/src/arrow/ipc/reader.h\nindex 54174f9ca..e90dc1e52 100644\n--- a/cpp/src/arrow/ipc/reader.h\n+++ b/cpp/src/arrow/ipc/reader.h\n@@ -54,8 +54,8 @@ class ARROW_EXPORT RecordBatchStreamReader : public RecordBatchReader {\n \n   /// Create batch reader from generic MessageReader\n   ///\n-  /// \\param(in) message_reader a MessageReader implementation\n-  /// \\param(out) out the created RecordBatchReader object\n+  /// \\param[in] message_reader a MessageReader implementation\n+  /// \\param[out] out the created RecordBatchReader object\n   /// \\return Status\n   static Status Open(std::unique_ptr<MessageReader> message_reader,\n                      std::shared_ptr<RecordBatchReader>* out);\n@@ -72,9 +72,9 @@ class ARROW_EXPORT RecordBatchStreamReader : public RecordBatchReader {\n \n   /// \\brief Record batch stream reader from InputStream\n   ///\n-  /// \\param(in) stream an input stream instance. Must stay alive throughout\n+  /// \\param[in] stream an input stream instance. Must stay alive throughout\n   /// lifetime of stream reader\n-  /// \\param(out) out the created RecordBatchStreamReader object\n+  /// \\param[out] out the created RecordBatchStreamReader object\n   /// \\return Status\n   static Status Open(io::InputStream* stream, std::shared_ptr<RecordBatchReader>* out);\n \n@@ -112,8 +112,8 @@ class ARROW_EXPORT RecordBatchFileReader {\n   /// metadata footer). The metadata must have been written with memory offsets\n   /// relative to the start of the containing file\n   ///\n-  /// @param file: the data source\n-  /// @param footer_offset: the position of the end of the Arrow \"file\"\n+  /// @param file the data source\n+  /// @param footer_offset the position of the end of the Arrow \"file\"\n   static Status Open(io::RandomAccessFile* file, int64_t footer_offset,\n                      std::shared_ptr<RecordBatchFileReader>* reader);\n \n@@ -138,8 +138,8 @@ class ARROW_EXPORT RecordBatchFileReader {\n   /// Read a record batch from the file. Does not copy memory if the input\n   /// source supports zero-copy.\n   ///\n-  /// \\param(in) i the index of the record batch to return\n-  /// \\param(out) batch the read batch\n+  /// \\param[in] i the index of the record batch to return\n+  /// \\param[out] batch the read batch\n   /// \\return Status\n   Status ReadRecordBatch(int i, std::shared_ptr<RecordBatch>* batch);\n \n@@ -165,20 +165,19 @@ Status ReadSchema(io::InputStream* stream, std::shared_ptr<Schema>* out);\n /// Read record batch as encapsulated IPC message with metadata size prefix and\n /// header\n ///\n-/// \\param(in) schema the record batch schema\n-/// \\param(in) offset the file location of the start of the message\n-/// \\param(in) file the file where the batch is located\n-/// \\param(out) out the read record batch\n+/// \\param[in] schema the record batch schema\n+/// \\param[in] stream the file where the batch is located\n+/// \\param[out] out the read record batch\n ARROW_EXPORT\n Status ReadRecordBatch(const std::shared_ptr<Schema>& schema, io::InputStream* stream,\n                        std::shared_ptr<RecordBatch>* out);\n \n /// \\brief Read record batch from file given metadata and schema\n ///\n-/// \\param(in) metadata a Message containing the record batch metadata\n-/// \\param(in) schema the record batch schema\n-/// \\param(in) file a random access file\n-/// \\param(out) out the read record batch\n+/// \\param[in] metadata a Message containing the record batch metadata\n+/// \\param[in] schema the record batch schema\n+/// \\param[in] file a random access file\n+/// \\param[out] out the read record batch\n ARROW_EXPORT\n Status ReadRecordBatch(const Buffer& metadata, const std::shared_ptr<Schema>& schema,\n                        io::RandomAccessFile* file, std::shared_ptr<RecordBatch>* out);\n@@ -186,7 +185,7 @@ Status ReadRecordBatch(const Buffer& metadata, const std::shared_ptr<Schema>& sc\n /// \\brief Read record batch from fully encapulated Message\n ///\n /// \\param[in] message a message instance containing metadata and body\n-/// \\param[in] schema\n+/// \\param[in] schema the record batch schema\n /// \\param[out] out the resulting RecordBatch\n /// \\return Status\n ARROW_EXPORT\n@@ -195,11 +194,11 @@ Status ReadRecordBatch(const Message& message, const std::shared_ptr<Schema>& sc\n \n /// Read record batch from file given metadata and schema\n ///\n-/// \\param(in) metadata a Message containing the record batch metadata\n-/// \\param(in) schema the record batch schema\n-/// \\param(in) file a random access file\n-/// \\param(in) max_recursion_depth the maximum permitted nesting depth\n-/// \\param(out) out the read record batch\n+/// \\param[in] metadata a Message containing the record batch metadata\n+/// \\param[in] schema the record batch schema\n+/// \\param[in] file a random access file\n+/// \\param[in] max_recursion_depth the maximum permitted nesting depth\n+/// \\param[out] out the read record batch\n ARROW_EXPORT\n Status ReadRecordBatch(const Buffer& metadata, const std::shared_ptr<Schema>& schema,\n                        int max_recursion_depth, io::RandomAccessFile* file,\n@@ -207,9 +206,9 @@ Status ReadRecordBatch(const Buffer& metadata, const std::shared_ptr<Schema>& sc\n \n /// EXPERIMENTAL: Read arrow::Tensor as encapsulated IPC message in file\n ///\n-/// \\param(in) offset the file location of the start of the message\n-/// \\param(in) file the file where the batch is located\n-/// \\param(out) out the read tensor\n+/// \\param[in] offset the file location of the start of the message\n+/// \\param[in] file the file where the batch is located\n+/// \\param[out] out the read tensor\n ARROW_EXPORT\n Status ReadTensor(int64_t offset, io::RandomAccessFile* file,\n                   std::shared_ptr<Tensor>* out);\ndiff --git a/cpp/src/arrow/ipc/writer.cc b/cpp/src/arrow/ipc/writer.cc\nindex 9f557f665..9531fd77a 100644\n--- a/cpp/src/arrow/ipc/writer.cc\n+++ b/cpp/src/arrow/ipc/writer.cc\n@@ -324,24 +324,24 @@ class RecordBatchSerializer : public ArrayVisitor {\n #define VISIT_FIXED_WIDTH(TYPE) \\\n   Status Visit(const TYPE& array) override { return VisitFixedWidth<TYPE>(array); }\n \n-  VISIT_FIXED_WIDTH(Int8Array);\n-  VISIT_FIXED_WIDTH(Int16Array);\n-  VISIT_FIXED_WIDTH(Int32Array);\n-  VISIT_FIXED_WIDTH(Int64Array);\n-  VISIT_FIXED_WIDTH(UInt8Array);\n-  VISIT_FIXED_WIDTH(UInt16Array);\n-  VISIT_FIXED_WIDTH(UInt32Array);\n-  VISIT_FIXED_WIDTH(UInt64Array);\n-  VISIT_FIXED_WIDTH(HalfFloatArray);\n-  VISIT_FIXED_WIDTH(FloatArray);\n-  VISIT_FIXED_WIDTH(DoubleArray);\n-  VISIT_FIXED_WIDTH(Date32Array);\n-  VISIT_FIXED_WIDTH(Date64Array);\n-  VISIT_FIXED_WIDTH(TimestampArray);\n-  VISIT_FIXED_WIDTH(Time32Array);\n-  VISIT_FIXED_WIDTH(Time64Array);\n-  VISIT_FIXED_WIDTH(FixedSizeBinaryArray);\n-  VISIT_FIXED_WIDTH(DecimalArray);\n+  VISIT_FIXED_WIDTH(Int8Array)\n+  VISIT_FIXED_WIDTH(Int16Array)\n+  VISIT_FIXED_WIDTH(Int32Array)\n+  VISIT_FIXED_WIDTH(Int64Array)\n+  VISIT_FIXED_WIDTH(UInt8Array)\n+  VISIT_FIXED_WIDTH(UInt16Array)\n+  VISIT_FIXED_WIDTH(UInt32Array)\n+  VISIT_FIXED_WIDTH(UInt64Array)\n+  VISIT_FIXED_WIDTH(HalfFloatArray)\n+  VISIT_FIXED_WIDTH(FloatArray)\n+  VISIT_FIXED_WIDTH(DoubleArray)\n+  VISIT_FIXED_WIDTH(Date32Array)\n+  VISIT_FIXED_WIDTH(Date64Array)\n+  VISIT_FIXED_WIDTH(TimestampArray)\n+  VISIT_FIXED_WIDTH(Time32Array)\n+  VISIT_FIXED_WIDTH(Time64Array)\n+  VISIT_FIXED_WIDTH(FixedSizeBinaryArray)\n+  VISIT_FIXED_WIDTH(DecimalArray)\n \n #undef VISIT_FIXED_WIDTH\n \ndiff --git a/cpp/src/arrow/ipc/writer.h b/cpp/src/arrow/ipc/writer.h\nindex df03f4f13..89d3f5f26 100644\n--- a/cpp/src/arrow/ipc/writer.h\n+++ b/cpp/src/arrow/ipc/writer.h\n@@ -61,7 +61,7 @@ class ARROW_EXPORT RecordBatchWriter {\n   virtual Status WriteRecordBatch(const RecordBatch& batch, bool allow_64bit = false) = 0;\n \n   /// \\brief Write possibly-chunked table by creating sequence of record batches\n-  /// \\param[in] table\n+  /// \\param[in] table table to write\n   /// \\return Status\n   Status WriteTable(const Table& table);\n \n@@ -87,9 +87,9 @@ class ARROW_EXPORT RecordBatchStreamWriter : public RecordBatchWriter {\n   /// Create a new writer from stream sink and schema. User is responsible for\n   /// closing the actual OutputStream.\n   ///\n-  /// \\param(in) sink output stream to write to\n-  /// \\param(in) schema the schema of the record batches to be written\n-  /// \\param(out) out the created stream writer\n+  /// \\param[in] sink output stream to write to\n+  /// \\param[in] schema the schema of the record batches to be written\n+  /// \\param[out] out the created stream writer\n   /// \\return Status\n   static Status Open(io::OutputStream* sink, const std::shared_ptr<Schema>& schema,\n                      std::shared_ptr<RecordBatchWriter>* out);\n@@ -121,9 +121,9 @@ class ARROW_EXPORT RecordBatchFileWriter : public RecordBatchStreamWriter {\n \n   /// Create a new writer from stream sink and schema\n   ///\n-  /// \\param(in) sink output stream to write to\n-  /// \\param(in) schema the schema of the record batches to be written\n-  /// \\param(out) out the created stream writer\n+  /// \\param[in] sink output stream to write to\n+  /// \\param[in] schema the schema of the record batches to be written\n+  /// \\param[out] out the created stream writer\n   /// \\return Status\n   static Status Open(io::OutputStream* sink, const std::shared_ptr<Schema>& schema,\n                      std::shared_ptr<RecordBatchWriter>* out);\n@@ -155,13 +155,13 @@ class ARROW_EXPORT RecordBatchFileWriter : public RecordBatchStreamWriter {\n /// to the end of the body and end of the metadata / data header (suffixed by\n /// the header size) is returned in out-variables\n ///\n-/// \\param(in) buffer_start_offset the start offset to use in the buffer metadata,\n+/// \\param[in] buffer_start_offset the start offset to use in the buffer metadata,\n /// default should be 0\n-/// \\param(in) allow_64bit permit field lengths exceeding INT32_MAX. May not be\n+/// \\param[in] allow_64bit permit field lengths exceeding INT32_MAX. May not be\n /// readable by other Arrow implementations\n-/// \\param(out) metadata_length: the size of the length-prefixed flatbuffer\n+/// \\param[out] metadata_length the size of the length-prefixed flatbuffer\n /// including padding to a 64-byte boundary\n-/// \\param(out) body_length: the size of the contiguous buffer block plus\n+/// \\param[out] body_length the size of the contiguous buffer block plus\n /// padding bytes\n /// \\return Status\n ///\n@@ -198,7 +198,7 @@ Status SerializeRecordBatch(const RecordBatch& batch, MemoryPool* pool,\n /// \\brief Serialize schema using stream writer as a sequence of one or more\n /// IPC messages\n ///\n-/// \\param[in] scheam the schema to write\n+/// \\param[in] schema the schema to write\n /// \\param[in] pool a MemoryPool to allocate memory from\n /// \\param[out] out the serialized schema\n /// \\return Status\ndiff --git a/cpp/src/arrow/memory_pool-test.cc b/cpp/src/arrow/memory_pool-test.cc\nindex 552c79b5a..0a4785d52 100644\n--- a/cpp/src/arrow/memory_pool-test.cc\n+++ b/cpp/src/arrow/memory_pool-test.cc\n@@ -59,39 +59,36 @@ TEST(DefaultMemoryPoolDeathTest, FreeLargeMemory) {\n }\n \n TEST(DefaultMemoryPoolDeathTest, MaxMemory) {\n-  DefaultMemoryPool pool;\n-\n-  ASSERT_EQ(0, pool.max_memory());\n+  MemoryPool* pool = default_memory_pool();\n \n   uint8_t* data;\n-  ASSERT_OK(pool.Allocate(100, &data));\n+  ASSERT_OK(pool->Allocate(100, &data));\n \n   uint8_t* data2;\n-  ASSERT_OK(pool.Allocate(100, &data2));\n+  ASSERT_OK(pool->Allocate(100, &data2));\n \n-  pool.Free(data, 100);\n-  pool.Free(data2, 100);\n+  pool->Free(data, 100);\n+  pool->Free(data2, 100);\n \n-  ASSERT_EQ(200, pool.max_memory());\n+  ASSERT_EQ(200, pool->max_memory());\n }\n \n #endif  // ARROW_VALGRIND\n \n TEST(LoggingMemoryPool, Logging) {\n-  DefaultMemoryPool pool;\n-  LoggingMemoryPool lp(&pool);\n+  MemoryPool* pool = default_memory_pool();\n \n-  ASSERT_EQ(0, lp.max_memory());\n+  LoggingMemoryPool lp(pool);\n \n   uint8_t* data;\n-  ASSERT_OK(pool.Allocate(100, &data));\n+  ASSERT_OK(pool->Allocate(100, &data));\n \n   uint8_t* data2;\n-  ASSERT_OK(pool.Allocate(100, &data2));\n+  ASSERT_OK(pool->Allocate(100, &data2));\n \n-  pool.Free(data, 100);\n-  pool.Free(data2, 100);\n+  pool->Free(data, 100);\n+  pool->Free(data2, 100);\n \n-  ASSERT_EQ(200, pool.max_memory());\n+  ASSERT_EQ(200, pool->max_memory());\n }\n }  // namespace arrow\ndiff --git a/cpp/src/arrow/memory_pool.cc b/cpp/src/arrow/memory_pool.cc\nindex d86fb08be..3496636a4 100644\n--- a/cpp/src/arrow/memory_pool.cc\n+++ b/cpp/src/arrow/memory_pool.cc\n@@ -85,73 +85,82 @@ MemoryPool::~MemoryPool() {}\n \n int64_t MemoryPool::max_memory() const { return -1; }\n \n-DefaultMemoryPool::DefaultMemoryPool() : bytes_allocated_(0) { max_memory_ = 0; }\n+class DefaultMemoryPool : public MemoryPool {\n+ public:\n+  DefaultMemoryPool() : bytes_allocated_(0) { max_memory_ = 0; }\n \n-Status DefaultMemoryPool::Allocate(int64_t size, uint8_t** out) {\n-  RETURN_NOT_OK(AllocateAligned(size, out));\n-  bytes_allocated_ += size;\n+  ~DefaultMemoryPool() {}\n \n-  {\n-    std::lock_guard<std::mutex> guard(lock_);\n-    if (bytes_allocated_ > max_memory_) {\n-      max_memory_ = bytes_allocated_.load();\n+  Status Allocate(int64_t size, uint8_t** out) override {\n+    RETURN_NOT_OK(AllocateAligned(size, out));\n+    bytes_allocated_ += size;\n+\n+    {\n+      std::lock_guard<std::mutex> guard(lock_);\n+      if (bytes_allocated_ > max_memory_) {\n+        max_memory_ = bytes_allocated_.load();\n+      }\n     }\n+    return Status::OK();\n   }\n-  return Status::OK();\n-}\n \n-Status DefaultMemoryPool::Reallocate(int64_t old_size, int64_t new_size, uint8_t** ptr) {\n+  Status Reallocate(int64_t old_size, int64_t new_size, uint8_t** ptr) override {\n #ifdef ARROW_JEMALLOC\n-  *ptr = reinterpret_cast<uint8_t*>(rallocx(*ptr, new_size, MALLOCX_ALIGN(kAlignment)));\n-  if (*ptr == NULL) {\n-    std::stringstream ss;\n-    ss << \"realloc of size \" << new_size << \" failed\";\n-    return Status::OutOfMemory(ss.str());\n-  }\n+    *ptr = reinterpret_cast<uint8_t*>(rallocx(*ptr, new_size, MALLOCX_ALIGN(kAlignment)));\n+    if (*ptr == NULL) {\n+      std::stringstream ss;\n+      ss << \"realloc of size \" << new_size << \" failed\";\n+      return Status::OutOfMemory(ss.str());\n+    }\n #else\n-  // Note: We cannot use realloc() here as it doesn't guarantee alignment.\n-\n-  // Allocate new chunk\n-  uint8_t* out;\n-  RETURN_NOT_OK(AllocateAligned(new_size, &out));\n-  // Copy contents and release old memory chunk\n-  memcpy(out, *ptr, static_cast<size_t>(std::min(new_size, old_size)));\n+    // Note: We cannot use realloc() here as it doesn't guarantee alignment.\n+\n+    // Allocate new chunk\n+    uint8_t* out = nullptr;\n+    RETURN_NOT_OK(AllocateAligned(new_size, &out));\n+    DCHECK(out);\n+    // Copy contents and release old memory chunk\n+    memcpy(out, *ptr, static_cast<size_t>(std::min(new_size, old_size)));\n #ifdef _MSC_VER\n-  _aligned_free(*ptr);\n+    _aligned_free(*ptr);\n #else\n-  std::free(*ptr);\n+    std::free(*ptr);\n #endif  // defined(_MSC_VER)\n-  *ptr = out;\n+    *ptr = out;\n #endif  // defined(ARROW_JEMALLOC)\n \n-  bytes_allocated_ += new_size - old_size;\n-  {\n-    std::lock_guard<std::mutex> guard(lock_);\n-    if (bytes_allocated_ > max_memory_) {\n-      max_memory_ = bytes_allocated_.load();\n+    bytes_allocated_ += new_size - old_size;\n+    {\n+      std::lock_guard<std::mutex> guard(lock_);\n+      if (bytes_allocated_ > max_memory_) {\n+        max_memory_ = bytes_allocated_.load();\n+      }\n     }\n-  }\n \n-  return Status::OK();\n-}\n+    return Status::OK();\n+  }\n \n-int64_t DefaultMemoryPool::bytes_allocated() const { return bytes_allocated_.load(); }\n+  int64_t bytes_allocated() const override { return bytes_allocated_.load(); }\n \n-void DefaultMemoryPool::Free(uint8_t* buffer, int64_t size) {\n-  DCHECK_GE(bytes_allocated_, size);\n+  void Free(uint8_t* buffer, int64_t size) override {\n+    DCHECK_GE(bytes_allocated_, size);\n #ifdef _MSC_VER\n-  _aligned_free(buffer);\n+    _aligned_free(buffer);\n #elif defined(ARROW_JEMALLOC)\n-  dallocx(buffer, MALLOCX_ALIGN(kAlignment));\n+    dallocx(buffer, MALLOCX_ALIGN(kAlignment));\n #else\n-  std::free(buffer);\n+    std::free(buffer);\n #endif\n-  bytes_allocated_ -= size;\n-}\n+    bytes_allocated_ -= size;\n+  }\n \n-int64_t DefaultMemoryPool::max_memory() const { return max_memory_.load(); }\n+  int64_t max_memory() const override { return max_memory_.load(); }\n \n-DefaultMemoryPool::~DefaultMemoryPool() {}\n+ private:\n+  mutable std::mutex lock_;\n+  std::atomic<int64_t> bytes_allocated_;\n+  std::atomic<int64_t> max_memory_;\n+};\n \n MemoryPool* default_memory_pool() {\n   static DefaultMemoryPool default_memory_pool_;\ndiff --git a/cpp/src/arrow/memory_pool.h b/cpp/src/arrow/memory_pool.h\nindex 5bb2b5669..52ec67fee 100644\n--- a/cpp/src/arrow/memory_pool.h\n+++ b/cpp/src/arrow/memory_pool.h\n@@ -20,7 +20,6 @@\n \n #include <atomic>\n #include <cstdint>\n-#include <mutex>\n \n #include \"arrow/util/visibility.h\"\n \n@@ -69,26 +68,6 @@ class ARROW_EXPORT MemoryPool {\n   MemoryPool();\n };\n \n-class ARROW_EXPORT DefaultMemoryPool : public MemoryPool {\n- public:\n-  DefaultMemoryPool();\n-  virtual ~DefaultMemoryPool();\n-\n-  Status Allocate(int64_t size, uint8_t** out) override;\n-  Status Reallocate(int64_t old_size, int64_t new_size, uint8_t** ptr) override;\n-\n-  void Free(uint8_t* buffer, int64_t size) override;\n-\n-  int64_t bytes_allocated() const override;\n-\n-  int64_t max_memory() const override;\n-\n- private:\n-  mutable std::mutex lock_;\n-  std::atomic<int64_t> bytes_allocated_;\n-  std::atomic<int64_t> max_memory_;\n-};\n-\n class ARROW_EXPORT LoggingMemoryPool : public MemoryPool {\n  public:\n   explicit LoggingMemoryPool(MemoryPool* pool);\ndiff --git a/cpp/src/arrow/pretty_print.cc b/cpp/src/arrow/pretty_print.cc\nindex cc1acf4be..31b934424 100644\n--- a/cpp/src/arrow/pretty_print.cc\n+++ b/cpp/src/arrow/pretty_print.cc\n@@ -199,7 +199,7 @@ class ArrayPrinter : public PrettyPrinter {\n     }\n   }\n \n-  Status Visit(const NullArray& array) { return Status::OK(); }\n+  Status Visit(const NullArray&) { return Status::OK(); }\n \n   template <typename T>\n   typename std::enable_if<std::is_base_of<PrimitiveArray, T>::value ||\n@@ -213,7 +213,7 @@ class ArrayPrinter : public PrettyPrinter {\n     return Status::OK();\n   }\n \n-  Status Visit(const IntervalArray& array) { return Status::NotImplemented(\"interval\"); }\n+  Status Visit(const IntervalArray&) { return Status::NotImplemented(\"interval\"); }\n \n   Status WriteValidityBitmap(const Array& array);\n \ndiff --git a/cpp/src/arrow/python/CMakeLists.txt b/cpp/src/arrow/python/CMakeLists.txt\nindex 84aad82e2..af53a1631 100644\n--- a/cpp/src/arrow/python/CMakeLists.txt\n+++ b/cpp/src/arrow/python/CMakeLists.txt\n@@ -40,7 +40,7 @@ endif()\n \n set(ARROW_PYTHON_MIN_TEST_LIBS\n   arrow_python_test_main\n-  arrow_python_shared\n+  arrow_python_static\n   arrow_shared)\n \n set(ARROW_PYTHON_TEST_LINK_LIBS ${ARROW_PYTHON_MIN_TEST_LIBS})\n@@ -57,7 +57,7 @@ set(ARROW_PYTHON_SRCS\n   init.cc\n   io.cc\n   numpy_convert.cc\n-  pandas_to_arrow.cc\n+  numpy_to_arrow.cc\n   python_to_arrow.cc\n   pyarrow.cc\n )\n@@ -100,7 +100,7 @@ install(FILES\n   io.h\n   numpy_convert.h\n   numpy_interop.h\n-  pandas_to_arrow.h\n+  numpy_to_arrow.h\n   python_to_arrow.h\n   platform.h\n   pyarrow.h\ndiff --git a/cpp/src/arrow/python/api.h b/cpp/src/arrow/python/api.h\nindex 4ceb3f1a4..a000ac5fa 100644\n--- a/cpp/src/arrow/python/api.h\n+++ b/cpp/src/arrow/python/api.h\n@@ -25,7 +25,7 @@\n #include \"arrow/python/helpers.h\"\n #include \"arrow/python/io.h\"\n #include \"arrow/python/numpy_convert.h\"\n-#include \"arrow/python/pandas_to_arrow.h\"\n+#include \"arrow/python/numpy_to_arrow.h\"\n #include \"arrow/python/python_to_arrow.h\"\n \n #endif  // ARROW_PYTHON_API_H\ndiff --git a/cpp/src/arrow/python/arrow_to_pandas.cc b/cpp/src/arrow/python/arrow_to_pandas.cc\nindex be738e7f9..88b594cac 100644\n--- a/cpp/src/arrow/python/arrow_to_pandas.cc\n+++ b/cpp/src/arrow/python/arrow_to_pandas.cc\n@@ -615,8 +615,8 @@ static Status ConvertDecimals(PandasOptions options, const ChunkedArray& data,\n   PyAcquireGIL lock;\n   OwnedRef decimal_ref;\n   OwnedRef Decimal_ref;\n-  RETURN_NOT_OK(ImportModule(\"decimal\", &decimal_ref));\n-  RETURN_NOT_OK(ImportFromModule(decimal_ref, \"Decimal\", &Decimal_ref));\n+  RETURN_NOT_OK(internal::ImportModule(\"decimal\", &decimal_ref));\n+  RETURN_NOT_OK(internal::ImportFromModule(decimal_ref, \"Decimal\", &Decimal_ref));\n   PyObject* Decimal = Decimal_ref.obj();\n \n   for (int c = 0; c < data.num_chunks(); c++) {\n@@ -633,7 +633,8 @@ static Status ConvertDecimals(PandasOptions options, const ChunkedArray& data,\n         const uint8_t* raw_value = arr->GetValue(i);\n         std::string decimal_string;\n         RETURN_NOT_OK(RawDecimalToString(raw_value, precision, scale, &decimal_string));\n-        RETURN_NOT_OK(DecimalFromString(Decimal, decimal_string, out_values++));\n+        *out_values++ = internal::DecimalFromString(Decimal, decimal_string);\n+        RETURN_IF_PYERROR();\n       }\n     }\n   }\ndiff --git a/cpp/src/arrow/python/arrow_to_python.cc b/cpp/src/arrow/python/arrow_to_python.cc\nindex a281fe3c6..b4f4a41f4 100644\n--- a/cpp/src/arrow/python/arrow_to_python.cc\n+++ b/cpp/src/arrow/python/arrow_to_python.cc\n@@ -59,6 +59,10 @@ Status DeserializeDict(PyObject* context, const Array& array, int64_t start_idx,\n   const auto& data = static_cast<const StructArray&>(array);\n   ScopedRef keys, vals;\n   ScopedRef result(PyDict_New());\n+  RETURN_IF_PYERROR();\n+\n+  DCHECK_EQ(2, data.num_fields());\n+\n   RETURN_NOT_OK(DeserializeList(context, *data.field(0), start_idx, stop_idx, base,\n                                 tensors, keys.ref()));\n   RETURN_NOT_OK(DeserializeList(context, *data.field(1), start_idx, stop_idx, base,\ndiff --git a/cpp/src/arrow/python/arrow_to_python.h b/cpp/src/arrow/python/arrow_to_python.h\nindex 3650c058e..7509f30eb 100644\n--- a/cpp/src/arrow/python/arrow_to_python.h\n+++ b/cpp/src/arrow/python/arrow_to_python.h\n@@ -54,7 +54,7 @@ Status ReadSerializedObject(io::RandomAccessFile* src, SerializedPyObject* out);\n /// _serialize_callback method for serialization and a _deserialize_callback\n /// method for deserialization. If context is None, no custom serialization\n /// will be attempted.\n-/// \\param[in] object\n+/// \\param[in] object object to deserialize\n /// \\param[in] base a Python object holding the underlying data that any NumPy\n /// arrays will reference, to avoid premature deallocation\n /// \\param[out] out the returned object\ndiff --git a/cpp/src/arrow/python/builtin_convert.cc b/cpp/src/arrow/python/builtin_convert.cc\nindex 747b872af..69a19e7a3 100644\n--- a/cpp/src/arrow/python/builtin_convert.cc\n+++ b/cpp/src/arrow/python/builtin_convert.cc\n@@ -20,6 +20,7 @@\n #include <datetime.h>\n \n #include <algorithm>\n+#include <limits>\n #include <sstream>\n #include <string>\n \n@@ -359,7 +360,11 @@ class TypedConverterVisitor : public TypedConverter<BuilderType> {\n     if (PySequence_Check(obj)) {\n       for (int64_t i = 0; i < size; ++i) {\n         OwnedRef ref(PySequence_GetItem(obj, i));\n-        RETURN_NOT_OK(static_cast<Derived*>(this)->AppendItem(ref));\n+        if (ref.obj() == Py_None) {\n+          RETURN_NOT_OK(this->typed_builder_->AppendNull());\n+        } else {\n+          RETURN_NOT_OK(static_cast<Derived*>(this)->AppendItem(ref));\n+        }\n       }\n     } else if (PyObject_HasAttrString(obj, \"__iter__\")) {\n       PyObject* iter = PyObject_GetIter(obj);\n@@ -370,7 +375,11 @@ class TypedConverterVisitor : public TypedConverter<BuilderType> {\n       // consuming at size.\n       while ((item = PyIter_Next(iter)) && i < size) {\n         OwnedRef ref(item);\n-        RETURN_NOT_OK(static_cast<Derived*>(this)->AppendItem(ref));\n+        if (ref.obj() == Py_None) {\n+          RETURN_NOT_OK(this->typed_builder_->AppendNull());\n+        } else {\n+          RETURN_NOT_OK(static_cast<Derived*>(this)->AppendItem(ref));\n+        }\n         ++i;\n       }\n       if (size != i) {\n@@ -388,52 +397,136 @@ class TypedConverterVisitor : public TypedConverter<BuilderType> {\n class NullConverter : public TypedConverterVisitor<NullBuilder, NullConverter> {\n  public:\n   inline Status AppendItem(const OwnedRef& item) {\n-    if (item.obj() == Py_None) {\n-      return typed_builder_->AppendNull();\n-    } else {\n-      return Status::Invalid(\"NullConverter: passed non-None value\");\n-    }\n+    return Status::Invalid(\"NullConverter: passed non-None value\");\n   }\n };\n \n class BoolConverter : public TypedConverterVisitor<BooleanBuilder, BoolConverter> {\n  public:\n   inline Status AppendItem(const OwnedRef& item) {\n-    if (item.obj() == Py_None) {\n-      return typed_builder_->AppendNull();\n-    } else {\n-      if (item.obj() == Py_True) {\n-        return typed_builder_->Append(true);\n-      } else {\n-        return typed_builder_->Append(false);\n-      }\n+    return typed_builder_->Append(item.obj() == Py_True);\n+  }\n+};\n+\n+class Int8Converter : public TypedConverterVisitor<Int8Builder, Int8Converter> {\n+ public:\n+  inline Status AppendItem(const OwnedRef& item) {\n+    int64_t val = static_cast<int64_t>(PyLong_AsLongLong(item.obj()));\n+\n+    if (ARROW_PREDICT_FALSE(val > std::numeric_limits<int8_t>::max() ||\n+                            val < std::numeric_limits<int8_t>::min())) {\n+      return Status::Invalid(\n+          \"Cannot coerce values to array type that would \"\n+          \"lose data\");\n     }\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(static_cast<int8_t>(val));\n+  }\n+};\n+\n+class Int16Converter : public TypedConverterVisitor<Int16Builder, Int16Converter> {\n+ public:\n+  inline Status AppendItem(const OwnedRef& item) {\n+    int64_t val = static_cast<int64_t>(PyLong_AsLongLong(item.obj()));\n+\n+    if (ARROW_PREDICT_FALSE(val > std::numeric_limits<int16_t>::max() ||\n+                            val < std::numeric_limits<int16_t>::min())) {\n+      return Status::Invalid(\n+          \"Cannot coerce values to array type that would \"\n+          \"lose data\");\n+    }\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(static_cast<int16_t>(val));\n+  }\n+};\n+\n+class Int32Converter : public TypedConverterVisitor<Int32Builder, Int32Converter> {\n+ public:\n+  inline Status AppendItem(const OwnedRef& item) {\n+    int64_t val = static_cast<int64_t>(PyLong_AsLongLong(item.obj()));\n+\n+    if (ARROW_PREDICT_FALSE(val > std::numeric_limits<int32_t>::max() ||\n+                            val < std::numeric_limits<int32_t>::min())) {\n+      return Status::Invalid(\n+          \"Cannot coerce values to array type that would \"\n+          \"lose data\");\n+    }\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(static_cast<int32_t>(val));\n   }\n };\n \n class Int64Converter : public TypedConverterVisitor<Int64Builder, Int64Converter> {\n  public:\n   inline Status AppendItem(const OwnedRef& item) {\n-    int64_t val;\n-    if (item.obj() == Py_None) {\n-      return typed_builder_->AppendNull();\n-    } else {\n-      val = static_cast<int64_t>(PyLong_AsLongLong(item.obj()));\n-      RETURN_IF_PYERROR();\n-      return typed_builder_->Append(val);\n+    int64_t val = static_cast<int64_t>(PyLong_AsLongLong(item.obj()));\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(val);\n+  }\n+};\n+\n+class UInt8Converter : public TypedConverterVisitor<UInt8Builder, UInt8Converter> {\n+ public:\n+  inline Status AppendItem(const OwnedRef& item) {\n+    uint64_t val = static_cast<uint64_t>(PyLong_AsLongLong(item.obj()));\n+\n+    if (ARROW_PREDICT_FALSE(val > std::numeric_limits<uint8_t>::max() ||\n+                            val < std::numeric_limits<uint8_t>::min())) {\n+      return Status::Invalid(\n+          \"Cannot coerce values to array type that would \"\n+          \"lose data\");\n     }\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(static_cast<uint8_t>(val));\n   }\n };\n \n-class DateConverter : public TypedConverterVisitor<Date64Builder, DateConverter> {\n+class UInt16Converter : public TypedConverterVisitor<UInt16Builder, UInt16Converter> {\n  public:\n   inline Status AppendItem(const OwnedRef& item) {\n-    if (item.obj() == Py_None) {\n-      return typed_builder_->AppendNull();\n-    } else {\n-      PyDateTime_Date* pydate = reinterpret_cast<PyDateTime_Date*>(item.obj());\n-      return typed_builder_->Append(PyDate_to_ms(pydate));\n+    uint64_t val = static_cast<uint64_t>(PyLong_AsLongLong(item.obj()));\n+\n+    if (ARROW_PREDICT_FALSE(val > std::numeric_limits<uint16_t>::max() ||\n+                            val < std::numeric_limits<uint16_t>::min())) {\n+      return Status::Invalid(\n+          \"Cannot coerce values to array type that would \"\n+          \"lose data\");\n     }\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(static_cast<uint16_t>(val));\n+  }\n+};\n+\n+class UInt32Converter : public TypedConverterVisitor<UInt32Builder, UInt32Converter> {\n+ public:\n+  inline Status AppendItem(const OwnedRef& item) {\n+    uint64_t val = static_cast<uint64_t>(PyLong_AsLongLong(item.obj()));\n+\n+    if (ARROW_PREDICT_FALSE(val > std::numeric_limits<uint32_t>::max() ||\n+                            val < std::numeric_limits<uint32_t>::min())) {\n+      return Status::Invalid(\n+          \"Cannot coerce values to array type that would \"\n+          \"lose data\");\n+    }\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(static_cast<uint32_t>(val));\n+  }\n+};\n+\n+class UInt64Converter : public TypedConverterVisitor<UInt64Builder, UInt64Converter> {\n+ public:\n+  inline Status AppendItem(const OwnedRef& item) {\n+    int64_t val = static_cast<int64_t>(PyLong_AsLongLong(item.obj()));\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(val);\n+  }\n+};\n+\n+class DateConverter : public TypedConverterVisitor<Date64Builder, DateConverter> {\n+ public:\n+  inline Status AppendItem(const OwnedRef& item) {\n+    auto pydate = reinterpret_cast<PyDateTime_Date*>(item.obj());\n+    return typed_builder_->Append(PyDate_to_ms(pydate));\n   }\n };\n \n@@ -441,27 +534,17 @@ class TimestampConverter\n     : public TypedConverterVisitor<Date64Builder, TimestampConverter> {\n  public:\n   inline Status AppendItem(const OwnedRef& item) {\n-    if (item.obj() == Py_None) {\n-      return typed_builder_->AppendNull();\n-    } else {\n-      PyDateTime_DateTime* pydatetime =\n-          reinterpret_cast<PyDateTime_DateTime*>(item.obj());\n-      return typed_builder_->Append(PyDateTime_to_us(pydatetime));\n-    }\n+    auto pydatetime = reinterpret_cast<PyDateTime_DateTime*>(item.obj());\n+    return typed_builder_->Append(PyDateTime_to_us(pydatetime));\n   }\n };\n \n class DoubleConverter : public TypedConverterVisitor<DoubleBuilder, DoubleConverter> {\n  public:\n   inline Status AppendItem(const OwnedRef& item) {\n-    double val;\n-    if (item.obj() == Py_None) {\n-      return typed_builder_->AppendNull();\n-    } else {\n-      val = PyFloat_AsDouble(item.obj());\n-      RETURN_IF_PYERROR();\n-      return typed_builder_->Append(val);\n-    }\n+    double val = PyFloat_AsDouble(item.obj());\n+    RETURN_IF_PYERROR();\n+    return typed_builder_->Append(val);\n   }\n };\n \n@@ -473,10 +556,7 @@ class BytesConverter : public TypedConverterVisitor<BinaryBuilder, BytesConverte\n     Py_ssize_t length;\n     OwnedRef tmp;\n \n-    if (item.obj() == Py_None) {\n-      RETURN_NOT_OK(typed_builder_->AppendNull());\n-      return Status::OK();\n-    } else if (PyUnicode_Check(item.obj())) {\n+    if (PyUnicode_Check(item.obj())) {\n       tmp.reset(PyUnicode_AsUTF8String(item.obj()));\n       RETURN_IF_PYERROR();\n       bytes_obj = tmp.obj();\n@@ -504,10 +584,7 @@ class FixedWidthBytesConverter\n     Py_ssize_t expected_length =\n         std::dynamic_pointer_cast<FixedSizeBinaryType>(typed_builder_->type())\n             ->byte_width();\n-    if (item.obj() == Py_None) {\n-      RETURN_NOT_OK(typed_builder_->AppendNull());\n-      return Status::OK();\n-    } else if (PyUnicode_Check(item.obj())) {\n+    if (PyUnicode_Check(item.obj())) {\n       tmp.reset(PyUnicode_AsUTF8String(item.obj()));\n       RETURN_IF_PYERROR();\n       bytes_obj = tmp.obj();\n@@ -535,9 +612,7 @@ class UTF8Converter : public TypedConverterVisitor<StringBuilder, UTF8Converter>\n     Py_ssize_t length;\n \n     PyObject* obj = item.obj();\n-    if (obj == Py_None) {\n-      return typed_builder_->AppendNull();\n-    } else if (PyBytes_Check(obj)) {\n+    if (PyBytes_Check(obj)) {\n       tmp.reset(\n           PyUnicode_FromStringAndSize(PyBytes_AS_STRING(obj), PyBytes_GET_SIZE(obj)));\n       RETURN_IF_PYERROR();\n@@ -565,14 +640,10 @@ class ListConverter : public TypedConverterVisitor<ListBuilder, ListConverter> {\n   Status Init(ArrayBuilder* builder) override;\n \n   inline Status AppendItem(const OwnedRef& item) override {\n-    if (item.obj() == Py_None) {\n-      return typed_builder_->AppendNull();\n-    } else {\n-      RETURN_NOT_OK(typed_builder_->Append());\n-      PyObject* item_obj = item.obj();\n-      int64_t list_size = static_cast<int64_t>(PySequence_Size(item_obj));\n-      return value_converter_->AppendData(item_obj, list_size);\n-    }\n+    RETURN_NOT_OK(typed_builder_->Append());\n+    PyObject* item_obj = item.obj();\n+    int64_t list_size = static_cast<int64_t>(PySequence_Size(item_obj));\n+    return value_converter_->AppendData(item_obj, list_size);\n   }\n \n  protected:\n@@ -584,16 +655,12 @@ class DecimalConverter\n  public:\n   inline Status AppendItem(const OwnedRef& item) {\n     /// TODO(phillipc): Check for nan?\n-    if (item.obj() != Py_None) {\n-      std::string string;\n-      RETURN_NOT_OK(PythonDecimalToString(item.obj(), &string));\n-\n-      Decimal128 value;\n-      RETURN_NOT_OK(Decimal128::FromString(string, &value));\n-      return typed_builder_->Append(value);\n-    }\n+    std::string string;\n+    RETURN_NOT_OK(internal::PythonDecimalToString(item.obj(), &string));\n \n-    return typed_builder_->AppendNull();\n+    Decimal128 value;\n+    RETURN_NOT_OK(Decimal128::FromString(string, &value));\n+    return typed_builder_->Append(value);\n   }\n };\n \n@@ -604,8 +671,22 @@ std::shared_ptr<SeqConverter> GetConverter(const std::shared_ptr<DataType>& type\n       return std::make_shared<NullConverter>();\n     case Type::BOOL:\n       return std::make_shared<BoolConverter>();\n+    case Type::INT8:\n+      return std::make_shared<Int8Converter>();\n+    case Type::INT16:\n+      return std::make_shared<Int16Converter>();\n+    case Type::INT32:\n+      return std::make_shared<Int32Converter>();\n     case Type::INT64:\n       return std::make_shared<Int64Converter>();\n+    case Type::UINT8:\n+      return std::make_shared<UInt8Converter>();\n+    case Type::UINT16:\n+      return std::make_shared<UInt16Converter>();\n+    case Type::UINT32:\n+      return std::make_shared<UInt32Converter>();\n+    case Type::UINT64:\n+      return std::make_shared<UInt64Converter>();\n     case Type::DATE64:\n       return std::make_shared<DateConverter>();\n     case Type::TIMESTAMP:\ndiff --git a/cpp/src/arrow/python/helpers.cc b/cpp/src/arrow/python/helpers.cc\nindex fb2fed7f0..ad6a7f125 100644\n--- a/cpp/src/arrow/python/helpers.cc\n+++ b/cpp/src/arrow/python/helpers.cc\n@@ -55,6 +55,8 @@ std::shared_ptr<DataType> GetPrimitiveType(Type::type type) {\n   }\n }\n \n+namespace internal {\n+\n Status ImportModule(const std::string& module_name, OwnedRef* ref) {\n   PyObject* module = PyImport_ImportModule(module_name.c_str());\n   RETURN_IF_PYERROR();\n@@ -106,10 +108,9 @@ Status InferDecimalPrecisionAndScale(PyObject* python_decimal, int* precision,\n   return Decimal128::FromString(c_string, nullptr, precision, scale);\n }\n \n-Status DecimalFromString(PyObject* decimal_constructor, const std::string& decimal_string,\n-                         PyObject** out) {\n+PyObject* DecimalFromString(PyObject* decimal_constructor,\n+                            const std::string& decimal_string) {\n   DCHECK_NE(decimal_constructor, nullptr);\n-  DCHECK_NE(out, nullptr);\n \n   auto string_size = decimal_string.size();\n   DCHECK_GT(string_size, 0);\n@@ -117,11 +118,10 @@ Status DecimalFromString(PyObject* decimal_constructor, const std::string& decim\n   auto string_bytes = decimal_string.c_str();\n   DCHECK_NE(string_bytes, nullptr);\n \n-  *out = PyObject_CallFunction(decimal_constructor, const_cast<char*>(\"s#\"), string_bytes,\n+  return PyObject_CallFunction(decimal_constructor, const_cast<char*>(\"s#\"), string_bytes,\n                                string_size);\n-  RETURN_IF_PYERROR();\n-  return Status::OK();\n }\n \n+}  // namespace internal\n }  // namespace py\n }  // namespace arrow\ndiff --git a/cpp/src/arrow/python/helpers.h b/cpp/src/arrow/python/helpers.h\nindex 8b8c6673c..01ab91657 100644\n--- a/cpp/src/arrow/python/helpers.h\n+++ b/cpp/src/arrow/python/helpers.h\n@@ -28,26 +28,28 @@\n #include \"arrow/util/visibility.h\"\n \n namespace arrow {\n-\n namespace py {\n \n class OwnedRef;\n \n-ARROW_EXPORT std::shared_ptr<DataType> GetPrimitiveType(Type::type type);\n+ARROW_EXPORT\n+std::shared_ptr<DataType> GetPrimitiveType(Type::type type);\n+\n+namespace internal {\n \n-Status ARROW_EXPORT ImportModule(const std::string& module_name, OwnedRef* ref);\n-Status ARROW_EXPORT ImportFromModule(const OwnedRef& module,\n-                                     const std::string& module_name, OwnedRef* ref);\n+Status ImportModule(const std::string& module_name, OwnedRef* ref);\n+Status ImportFromModule(const OwnedRef& module, const std::string& module_name,\n+                        OwnedRef* ref);\n \n-Status ARROW_EXPORT PythonDecimalToString(PyObject* python_decimal, std::string* out);\n+Status PythonDecimalToString(PyObject* python_decimal, std::string* out);\n \n-Status ARROW_EXPORT InferDecimalPrecisionAndScale(PyObject* python_decimal,\n-                                                  int* precision = nullptr,\n-                                                  int* scale = nullptr);\n+Status InferDecimalPrecisionAndScale(PyObject* python_decimal, int* precision = nullptr,\n+                                     int* scale = nullptr);\n \n-Status ARROW_EXPORT DecimalFromString(PyObject* decimal_constructor,\n-                                      const std::string& decimal_string, PyObject** out);\n+PyObject* DecimalFromString(PyObject* decimal_constructor,\n+                            const std::string& decimal_string);\n \n+}  // namespace internal\n }  // namespace py\n }  // namespace arrow\n \ndiff --git a/cpp/src/arrow/python/pandas_to_arrow.cc b/cpp/src/arrow/python/numpy_to_arrow.cc\nsimilarity index 87%\nrename from cpp/src/arrow/python/pandas_to_arrow.cc\nrename to cpp/src/arrow/python/numpy_to_arrow.cc\nindex dc5b67f53..c0ce61cca 100644\n--- a/cpp/src/arrow/python/pandas_to_arrow.cc\n+++ b/cpp/src/arrow/python/numpy_to_arrow.cc\n@@ -19,10 +19,9 @@\n \n #define ARROW_NO_DEFAULT_MEMORY_POOL\n \n+#include \"arrow/python/numpy_to_arrow.h\"\n #include \"arrow/python/numpy_interop.h\"\n \n-#include \"arrow/python/pandas_to_arrow.h\"\n-\n #include <algorithm>\n #include <cmath>\n #include <cstdint>\n@@ -60,10 +59,14 @@ namespace py {\n \n using internal::NumPyTypeSize;\n \n+constexpr int64_t kBinaryMemoryLimit = std::numeric_limits<int32_t>::max();\n+\n // ----------------------------------------------------------------------\n // Conversion utilities\n \n-static inline bool PyFloat_isnan(const PyObject* obj) {\n+namespace {\n+\n+inline bool PyFloat_isnan(const PyObject* obj) {\n   if (PyFloat_Check(obj)) {\n     double val = PyFloat_AS_DOUBLE(obj);\n     return val != val;\n@@ -71,11 +74,12 @@ static inline bool PyFloat_isnan(const PyObject* obj) {\n     return false;\n   }\n }\n-static inline bool PandasObjectIsNull(const PyObject* obj) {\n+\n+inline bool PandasObjectIsNull(const PyObject* obj) {\n   return obj == Py_None || obj == numpy_nan || PyFloat_isnan(obj);\n }\n \n-static inline bool PyObject_is_string(const PyObject* obj) {\n+inline bool PyObject_is_string(const PyObject* obj) {\n #if PY_MAJOR_VERSION >= 3\n   return PyUnicode_Check(obj) || PyBytes_Check(obj);\n #else\n@@ -83,14 +87,14 @@ static inline bool PyObject_is_string(const PyObject* obj) {\n #endif\n }\n \n-static inline bool PyObject_is_float(const PyObject* obj) { return PyFloat_Check(obj); }\n+inline bool PyObject_is_float(const PyObject* obj) { return PyFloat_Check(obj); }\n \n-static inline bool PyObject_is_integer(const PyObject* obj) {\n+inline bool PyObject_is_integer(const PyObject* obj) {\n   return (!PyBool_Check(obj)) && PyArray_IsIntegerScalar(obj);\n }\n \n template <int TYPE>\n-static int64_t ValuesToBitmap(PyArrayObject* arr, uint8_t* bitmap) {\n+inline int64_t ValuesToBitmap(PyArrayObject* arr, uint8_t* bitmap) {\n   typedef internal::npy_traits<TYPE> traits;\n   typedef typename traits::value_type T;\n \n@@ -109,7 +113,7 @@ static int64_t ValuesToBitmap(PyArrayObject* arr, uint8_t* bitmap) {\n }\n \n // Returns null count\n-static int64_t MaskToBitmap(PyArrayObject* mask, int64_t length, uint8_t* bitmap) {\n+int64_t MaskToBitmap(PyArrayObject* mask, int64_t length, uint8_t* bitmap) {\n   int64_t null_count = 0;\n \n   Ndarray1DIndexer<uint8_t> mask_values(mask);\n@@ -123,29 +127,6 @@ static int64_t MaskToBitmap(PyArrayObject* mask, int64_t length, uint8_t* bitmap\n   return null_count;\n }\n \n-template <int TYPE, typename BuilderType>\n-static Status AppendNdarrayToBuilder(PyArrayObject* array, BuilderType* builder) {\n-  typedef internal::npy_traits<TYPE> traits;\n-  typedef typename traits::value_type T;\n-\n-  // TODO(wesm): Vector append when not strided\n-  Ndarray1DIndexer<T> values(array);\n-  if (traits::supports_nulls) {\n-    for (int64_t i = 0; i < values.size(); ++i) {\n-      if (traits::isnull(values[i])) {\n-        RETURN_NOT_OK(builder->AppendNull());\n-      } else {\n-        RETURN_NOT_OK(builder->Append(values[i]));\n-      }\n-    }\n-  } else {\n-    for (int64_t i = 0; i < values.size(); ++i) {\n-      RETURN_NOT_OK(builder->Append(values[i]));\n-    }\n-  }\n-  return Status::OK();\n-}\n-\n Status CheckFlatNumpyArray(PyArrayObject* numpy_array, int np_type) {\n   if (PyArray_NDIM(numpy_array) != 1) {\n     return Status::Invalid(\"only handle 1-dimensional arrays\");\n@@ -162,13 +143,13 @@ Status CheckFlatNumpyArray(PyArrayObject* numpy_array, int np_type) {\n   return Status::OK();\n }\n \n-constexpr int64_t kBinaryMemoryLimit = std::numeric_limits<int32_t>::max();\n+}  // namespace\n \n /// Append as many string objects from NumPy arrays to a `StringBuilder` as we\n /// can fit\n ///\n /// \\param[in] offset starting offset for appending\n-/// \\param[out] values_consumed ending offset where we stopped appending. Will\n+/// \\param[out] end_offset ending offset where we stopped appending. Will\n /// be length of arr if fully consumed\n /// \\param[out] have_bytes true if we encountered any PyBytes object\n static Status AppendObjectStrings(PyArrayObject* arr, PyArrayObject* mask, int64_t offset,\n@@ -272,14 +253,15 @@ static Status AppendObjectFixedWidthBytes(PyArrayObject* arr, PyArrayObject* mas\n // ----------------------------------------------------------------------\n // Conversion from NumPy-in-Pandas to Arrow\n \n-class PandasConverter {\n+class NumPyConverter {\n  public:\n-  PandasConverter(MemoryPool* pool, PyObject* ao, PyObject* mo,\n-                  const std::shared_ptr<DataType>& type)\n+  NumPyConverter(MemoryPool* pool, PyObject* ao, PyObject* mo,\n+                 const std::shared_ptr<DataType>& type, bool use_pandas_null_sentinels)\n       : pool_(pool),\n         type_(type),\n         arr_(reinterpret_cast<PyArrayObject*>(ao)),\n-        mask_(nullptr) {\n+        mask_(nullptr),\n+        use_pandas_null_sentinels_(use_pandas_null_sentinels) {\n     if (mo != nullptr && mo != Py_None) {\n       mask_ = reinterpret_cast<PyArrayObject*>(mo);\n     }\n@@ -291,6 +273,39 @@ class PandasConverter {\n     return astrides[0] != PyArray_DESCR(arr_)->elsize;\n   }\n \n+  Status Convert();\n+\n+  const std::vector<std::shared_ptr<Array>>& result() const { return out_arrays_; }\n+\n+  template <typename T>\n+  typename std::enable_if<std::is_base_of<PrimitiveCType, T>::value ||\n+                              std::is_same<BooleanType, T>::value,\n+                          Status>::type\n+  Visit(const T& type) {\n+    return VisitNative<T>();\n+  }\n+\n+  Status Visit(const Date32Type& type) { return VisitNative<Date32Type>(); }\n+  Status Visit(const Date64Type& type) { return VisitNative<Int64Type>(); }\n+  Status Visit(const TimestampType& type) { return VisitNative<TimestampType>(); }\n+  Status Visit(const Time32Type& type) { return VisitNative<Int32Type>(); }\n+  Status Visit(const Time64Type& type) { return VisitNative<Int64Type>(); }\n+\n+  Status Visit(const NullType& type) { return TypeNotImplemented(type.ToString()); }\n+\n+  Status Visit(const BinaryType& type) { return TypeNotImplemented(type.ToString()); }\n+\n+  Status Visit(const FixedSizeBinaryType& type) {\n+    return TypeNotImplemented(type.ToString());\n+  }\n+\n+  Status Visit(const DecimalType& type) { return TypeNotImplemented(type.ToString()); }\n+\n+  Status Visit(const DictionaryType& type) { return TypeNotImplemented(type.ToString()); }\n+\n+  Status Visit(const NestedType& type) { return TypeNotImplemented(type.ToString()); }\n+\n+ protected:\n   Status InitNullBitmap() {\n     int64_t null_bytes = BitUtil::BytesForBits(length_);\n \n@@ -317,6 +332,32 @@ class PandasConverter {\n     return Status::OK();\n   }\n \n+  template <int TYPE, typename BuilderType>\n+  Status AppendNdarrayToBuilder(PyArrayObject* array, BuilderType* builder) {\n+    typedef internal::npy_traits<TYPE> traits;\n+    typedef typename traits::value_type T;\n+\n+    const bool null_sentinels_possible =\n+        (use_pandas_null_sentinels_ && traits::supports_nulls);\n+\n+    // TODO(wesm): Vector append when not strided\n+    Ndarray1DIndexer<T> values(array);\n+    if (null_sentinels_possible) {\n+      for (int64_t i = 0; i < values.size(); ++i) {\n+        if (traits::isnull(values[i])) {\n+          RETURN_NOT_OK(builder->AppendNull());\n+        } else {\n+          RETURN_NOT_OK(builder->Append(values[i]));\n+        }\n+      }\n+    } else {\n+      for (int64_t i = 0; i < values.size(); ++i) {\n+        RETURN_NOT_OK(builder->Append(values[i]));\n+      }\n+    }\n+    return Status::OK();\n+  }\n+\n   Status PushArray(const std::shared_ptr<ArrayData>& data) {\n     std::shared_ptr<Array> result;\n     RETURN_NOT_OK(MakeArray(data, &result));\n@@ -328,7 +369,10 @@ class PandasConverter {\n   Status VisitNative() {\n     using traits = internal::arrow_traits<ArrowType::type_id>;\n \n-    if (mask_ != nullptr || traits::supports_nulls) {\n+    const bool null_sentinels_possible =\n+        (use_pandas_null_sentinels_ && traits::supports_nulls);\n+\n+    if (mask_ != nullptr || null_sentinels_possible) {\n       RETURN_NOT_OK(InitNullBitmap());\n     }\n \n@@ -338,7 +382,7 @@ class PandasConverter {\n     int64_t null_count = 0;\n     if (mask_ != nullptr) {\n       null_count = MaskToBitmap(mask_, length_, null_bitmap_data_);\n-    } else if (traits::supports_nulls) {\n+    } else if (null_sentinels_possible) {\n       // TODO(wesm): this presumes the NumPy C type and arrow C type are the\n       // same\n       null_count = ValuesToBitmap<traits::npy_type>(arr_, null_bitmap_data_);\n@@ -350,58 +394,17 @@ class PandasConverter {\n     return PushArray(arr_data);\n   }\n \n-  template <typename T>\n-  typename std::enable_if<std::is_base_of<PrimitiveCType, T>::value ||\n-                              std::is_same<BooleanType, T>::value,\n-                          Status>::type\n-  Visit(const T& type) {\n-    return VisitNative<T>();\n-  }\n-\n-  Status Visit(const Date32Type& type) { return VisitNative<Date32Type>(); }\n-  Status Visit(const Date64Type& type) { return VisitNative<Int64Type>(); }\n-  Status Visit(const TimestampType& type) { return VisitNative<TimestampType>(); }\n-  Status Visit(const Time32Type& type) { return VisitNative<Int32Type>(); }\n-  Status Visit(const Time64Type& type) { return VisitNative<Int64Type>(); }\n-\n   Status TypeNotImplemented(std::string type_name) {\n     std::stringstream ss;\n-    ss << \"PandasConverter doesn't implement <\" << type_name << \"> conversion. \";\n+    ss << \"NumPyConverter doesn't implement <\" << type_name << \"> conversion. \";\n     return Status::NotImplemented(ss.str());\n   }\n \n-  Status Visit(const NullType& type) { return TypeNotImplemented(type.ToString()); }\n-\n-  Status Visit(const BinaryType& type) { return TypeNotImplemented(type.ToString()); }\n-\n-  Status Visit(const FixedSizeBinaryType& type) {\n-    return TypeNotImplemented(type.ToString());\n-  }\n-\n-  Status Visit(const DecimalType& type) { return TypeNotImplemented(type.ToString()); }\n-\n-  Status Visit(const DictionaryType& type) { return TypeNotImplemented(type.ToString()); }\n-\n-  Status Visit(const NestedType& type) { return TypeNotImplemented(type.ToString()); }\n-\n-  Status Convert() {\n-    if (PyArray_NDIM(arr_) != 1) {\n-      return Status::Invalid(\"only handle 1-dimensional arrays\");\n-    }\n-\n-    if (type_ == nullptr) {\n-      return Status::Invalid(\"Must pass data type\");\n-    }\n-\n-    // Visit the type to perform conversion\n-    return VisitTypeInline(*type_, this);\n-  }\n-\n-  const std::vector<std::shared_ptr<Array>>& result() const { return out_arrays_; }\n-\n   // ----------------------------------------------------------------------\n   // Conversion logic for various object dtype arrays\n \n+  Status ConvertObjects();\n+\n   template <int ITEM_TYPE, typename ArrowType>\n   Status ConvertTypedLists(const std::shared_ptr<DataType>& type, ListBuilder* builder,\n                            PyObject* list);\n@@ -419,17 +422,17 @@ class PandasConverter {\n                       PyObject* list);\n   Status ConvertDecimals();\n   Status ConvertTimes();\n-  Status ConvertObjects();\n   Status ConvertObjectsInfer();\n   Status ConvertObjectsInferAndCast();\n \n- protected:\n   MemoryPool* pool_;\n   std::shared_ptr<DataType> type_;\n   PyArrayObject* arr_;\n   PyArrayObject* mask_;\n   int64_t length_;\n \n+  bool use_pandas_null_sentinels_;\n+\n   // Used in visitor pattern\n   std::vector<std::shared_ptr<Array>> out_arrays_;\n \n@@ -437,6 +440,23 @@ class PandasConverter {\n   uint8_t* null_bitmap_data_;\n };\n \n+Status NumPyConverter::Convert() {\n+  if (PyArray_NDIM(arr_) != 1) {\n+    return Status::Invalid(\"only handle 1-dimensional arrays\");\n+  }\n+\n+  if (PyArray_DESCR(arr_)->type_num == NPY_OBJECT) {\n+    return ConvertObjects();\n+  }\n+\n+  if (type_ == nullptr) {\n+    return Status::Invalid(\"Must pass data type for non-object arrays\");\n+  }\n+\n+  // Visit the type to perform conversion\n+  return VisitTypeInline(*type_, this);\n+}\n+\n template <typename T, typename T2>\n void CopyStrided(T* input_data, int64_t length, int64_t stride, T2* output_data) {\n   // Passing input_data as non-const is a concession to PyObject*\n@@ -482,7 +502,7 @@ static Status CastBuffer(const std::shared_ptr<Buffer>& input, const int64_t len\n }\n \n template <typename ArrowType>\n-inline Status PandasConverter::ConvertData(std::shared_ptr<Buffer>* data) {\n+inline Status NumPyConverter::ConvertData(std::shared_ptr<Buffer>* data) {\n   using traits = internal::arrow_traits<ArrowType::type_id>;\n   using T = typename traits::T;\n \n@@ -513,7 +533,7 @@ inline Status PandasConverter::ConvertData(std::shared_ptr<Buffer>* data) {\n }\n \n template <>\n-inline Status PandasConverter::ConvertData<Date32Type>(std::shared_ptr<Buffer>* data) {\n+inline Status NumPyConverter::ConvertData<Date32Type>(std::shared_ptr<Buffer>* data) {\n   // Handle LONGLONG->INT64 and other fun things\n   int type_num_compat = cast_npy_type_compat(PyArray_DESCR(arr_)->type_num);\n   int type_size = NumPyTypeSize(type_num_compat);\n@@ -552,7 +572,7 @@ inline Status PandasConverter::ConvertData<Date32Type>(std::shared_ptr<Buffer>*\n }\n \n template <>\n-inline Status PandasConverter::ConvertData<BooleanType>(std::shared_ptr<Buffer>* data) {\n+inline Status NumPyConverter::ConvertData<BooleanType>(std::shared_ptr<Buffer>* data) {\n   int64_t nbytes = BitUtil::BytesForBits(length_);\n   auto buffer = std::make_shared<PoolBuffer>(pool_);\n   RETURN_NOT_OK(buffer->Resize(nbytes));\n@@ -590,7 +610,7 @@ struct UnboxDate<Date64Type> {\n };\n \n template <typename ArrowType>\n-Status PandasConverter::ConvertDates() {\n+Status NumPyConverter::ConvertDates() {\n   PyAcquireGIL lock;\n \n   using BuilderType = typename TypeTraits<ArrowType>::BuilderType;\n@@ -626,14 +646,14 @@ Status PandasConverter::ConvertDates() {\n   return PushBuilderResult(&builder);\n }\n \n-Status PandasConverter::ConvertDecimals() {\n+Status NumPyConverter::ConvertDecimals() {\n   PyAcquireGIL lock;\n \n   // Import the decimal module and Decimal class\n   OwnedRef decimal;\n   OwnedRef Decimal;\n-  RETURN_NOT_OK(ImportModule(\"decimal\", &decimal));\n-  RETURN_NOT_OK(ImportFromModule(decimal, \"Decimal\", &Decimal));\n+  RETURN_NOT_OK(internal::ImportModule(\"decimal\", &decimal));\n+  RETURN_NOT_OK(internal::ImportFromModule(decimal, \"Decimal\", &Decimal));\n \n   Ndarray1DIndexer<PyObject*> objects(arr_);\n   PyObject* object = objects[0];\n@@ -641,7 +661,7 @@ Status PandasConverter::ConvertDecimals() {\n   int precision;\n   int scale;\n \n-  RETURN_NOT_OK(InferDecimalPrecisionAndScale(object, &precision, &scale));\n+  RETURN_NOT_OK(internal::InferDecimalPrecisionAndScale(object, &precision, &scale));\n \n   type_ = std::make_shared<DecimalType>(precision, scale);\n \n@@ -652,7 +672,7 @@ Status PandasConverter::ConvertDecimals() {\n     object = objects[i];\n     if (PyObject_IsInstance(object, Decimal.obj())) {\n       std::string string;\n-      RETURN_NOT_OK(PythonDecimalToString(object, &string));\n+      RETURN_NOT_OK(internal::PythonDecimalToString(object, &string));\n \n       Decimal128 value;\n       RETURN_NOT_OK(Decimal128::FromString(string, &value));\n@@ -669,7 +689,7 @@ Status PandasConverter::ConvertDecimals() {\n   return PushBuilderResult(&builder);\n }\n \n-Status PandasConverter::ConvertTimes() {\n+Status NumPyConverter::ConvertTimes() {\n   // Convert array of datetime.time objects to Arrow\n   PyAcquireGIL lock;\n   PyDateTime_IMPORT;\n@@ -697,7 +717,7 @@ Status PandasConverter::ConvertTimes() {\n   return PushBuilderResult(&builder);\n }\n \n-Status PandasConverter::ConvertObjectStrings() {\n+Status NumPyConverter::ConvertObjectStrings() {\n   PyAcquireGIL lock;\n \n   // The output type at this point is inconclusive because there may be bytes\n@@ -729,7 +749,7 @@ Status PandasConverter::ConvertObjectStrings() {\n   return Status::OK();\n }\n \n-Status PandasConverter::ConvertObjectFloats() {\n+Status NumPyConverter::ConvertObjectFloats() {\n   PyAcquireGIL lock;\n \n   Ndarray1DIndexer<PyObject*> objects(arr_);\n@@ -764,7 +784,7 @@ Status PandasConverter::ConvertObjectFloats() {\n   return PushBuilderResult(&builder);\n }\n \n-Status PandasConverter::ConvertObjectIntegers() {\n+Status NumPyConverter::ConvertObjectIntegers() {\n   PyAcquireGIL lock;\n \n   Int64Builder builder(pool_);\n@@ -799,11 +819,11 @@ Status PandasConverter::ConvertObjectIntegers() {\n   return PushBuilderResult(&builder);\n }\n \n-Status PandasConverter::ConvertObjectFixedWidthBytes(\n+Status NumPyConverter::ConvertObjectFixedWidthBytes(\n     const std::shared_ptr<DataType>& type) {\n   PyAcquireGIL lock;\n \n-  int32_t byte_width = static_cast<const FixedSizeBinaryType&>(*type).byte_width();\n+  const int32_t byte_width = static_cast<const FixedSizeBinaryType&>(*type).byte_width();\n \n   // The output type at this point is inconclusive because there may be bytes\n   // and unicode mixed in the object array\n@@ -822,7 +842,7 @@ Status PandasConverter::ConvertObjectFixedWidthBytes(\n   return Status::OK();\n }\n \n-Status PandasConverter::ConvertBooleans() {\n+Status NumPyConverter::ConvertBooleans() {\n   PyAcquireGIL lock;\n \n   Ndarray1DIndexer<PyObject*> objects(arr_);\n@@ -864,7 +884,7 @@ Status PandasConverter::ConvertBooleans() {\n   return Status::OK();\n }\n \n-Status PandasConverter::ConvertObjectsInfer() {\n+Status NumPyConverter::ConvertObjectsInfer() {\n   Ndarray1DIndexer<PyObject*> objects;\n \n   PyAcquireGIL lock;\n@@ -873,8 +893,8 @@ Status PandasConverter::ConvertObjectsInfer() {\n \n   OwnedRef decimal;\n   OwnedRef Decimal;\n-  RETURN_NOT_OK(ImportModule(\"decimal\", &decimal));\n-  RETURN_NOT_OK(ImportFromModule(decimal, \"Decimal\", &Decimal));\n+  RETURN_NOT_OK(internal::ImportModule(\"decimal\", &decimal));\n+  RETURN_NOT_OK(internal::ImportFromModule(decimal, \"Decimal\", &Decimal));\n \n   for (int64_t i = 0; i < length_; ++i) {\n     PyObject* obj = objects[i];\n@@ -912,10 +932,10 @@ Status PandasConverter::ConvertObjectsInfer() {\n   return Status::OK();\n }\n \n-Status PandasConverter::ConvertObjectsInferAndCast() {\n+Status NumPyConverter::ConvertObjectsInferAndCast() {\n   size_t position = out_arrays_.size();\n   RETURN_NOT_OK(ConvertObjectsInfer());\n-\n+  DCHECK_EQ(position + 1, out_arrays_.size());\n   std::shared_ptr<Array> arr = out_arrays_[position];\n \n   // Perform cast\n@@ -932,7 +952,7 @@ Status PandasConverter::ConvertObjectsInferAndCast() {\n   return Status::OK();\n }\n \n-Status PandasConverter::ConvertObjects() {\n+Status NumPyConverter::ConvertObjects() {\n   // Python object arrays are annoying, since we could have one of:\n   //\n   // * Strings\n@@ -1005,8 +1025,8 @@ Status LoopPySequence(PyObject* sequence, T func) {\n }\n \n template <int ITEM_TYPE, typename ArrowType>\n-inline Status PandasConverter::ConvertTypedLists(const std::shared_ptr<DataType>& type,\n-                                                 ListBuilder* builder, PyObject* list) {\n+inline Status NumPyConverter::ConvertTypedLists(const std::shared_ptr<DataType>& type,\n+                                                ListBuilder* builder, PyObject* list) {\n   typedef internal::npy_traits<ITEM_TYPE> traits;\n   typedef typename traits::BuilderClass BuilderT;\n \n@@ -1050,7 +1070,7 @@ inline Status PandasConverter::ConvertTypedLists(const std::shared_ptr<DataType>\n }\n \n template <>\n-inline Status PandasConverter::ConvertTypedLists<NPY_OBJECT, NullType>(\n+inline Status NumPyConverter::ConvertTypedLists<NPY_OBJECT, NullType>(\n     const std::shared_ptr<DataType>& type, ListBuilder* builder, PyObject* list) {\n   PyAcquireGIL lock;\n \n@@ -1091,7 +1111,7 @@ inline Status PandasConverter::ConvertTypedLists<NPY_OBJECT, NullType>(\n }\n \n template <>\n-inline Status PandasConverter::ConvertTypedLists<NPY_OBJECT, StringType>(\n+inline Status NumPyConverter::ConvertTypedLists<NPY_OBJECT, StringType>(\n     const std::shared_ptr<DataType>& type, ListBuilder* builder, PyObject* list) {\n   PyAcquireGIL lock;\n   // TODO: If there are bytes involed, convert to Binary representation\n@@ -1145,8 +1165,8 @@ inline Status PandasConverter::ConvertTypedLists<NPY_OBJECT, StringType>(\n     return ConvertTypedLists<NUMPY_TYPE, ArrowType>(type, builder, list); \\\n   }\n \n-Status PandasConverter::ConvertLists(const std::shared_ptr<DataType>& type,\n-                                     ListBuilder* builder, PyObject* list) {\n+Status NumPyConverter::ConvertLists(const std::shared_ptr<DataType>& type,\n+                                    ListBuilder* builder, PyObject* list) {\n   switch (type->id()) {\n     LIST_CASE(NA, NPY_OBJECT, NullType)\n     LIST_CASE(UINT8, NPY_UINT8, UInt8Type)\n@@ -1162,10 +1182,10 @@ Status PandasConverter::ConvertLists(const std::shared_ptr<DataType>& type,\n     LIST_CASE(DOUBLE, NPY_DOUBLE, DoubleType)\n     LIST_CASE(STRING, NPY_OBJECT, StringType)\n     case Type::LIST: {\n-      const ListType& list_type = static_cast<const ListType&>(*type);\n+      const auto& list_type = static_cast<const ListType&>(*type);\n       auto value_builder = static_cast<ListBuilder*>(builder->value_builder());\n \n-      auto foreach_item = [&](PyObject* object) {\n+      auto foreach_item = [this, &builder, &value_builder, &list_type](PyObject* object) {\n         if (PandasObjectIsNull(object)) {\n           return builder->AppendNull();\n         } else {\n@@ -1185,7 +1205,7 @@ Status PandasConverter::ConvertLists(const std::shared_ptr<DataType>& type,\n   }\n }\n \n-Status PandasConverter::ConvertLists(const std::shared_ptr<DataType>& type) {\n+Status NumPyConverter::ConvertLists(const std::shared_ptr<DataType>& type) {\n   std::unique_ptr<ArrayBuilder> array_builder;\n   RETURN_NOT_OK(MakeBuilder(pool_, arrow::list(type), &array_builder));\n   ListBuilder* list_builder = static_cast<ListBuilder*>(array_builder.get());\n@@ -1193,21 +1213,15 @@ Status PandasConverter::ConvertLists(const std::shared_ptr<DataType>& type) {\n   return PushBuilderResult(list_builder);\n }\n \n-Status PandasToArrow(MemoryPool* pool, PyObject* ao, PyObject* mo,\n-                     const std::shared_ptr<DataType>& type, std::shared_ptr<Array>* out) {\n-  PandasConverter converter(pool, ao, mo, type);\n+Status NdarrayToArrow(MemoryPool* pool, PyObject* ao, PyObject* mo,\n+                      bool use_pandas_null_sentinels,\n+                      const std::shared_ptr<DataType>& type,\n+                      std::shared_ptr<ChunkedArray>* out) {\n+  NumPyConverter converter(pool, ao, mo, type, use_pandas_null_sentinels);\n   RETURN_NOT_OK(converter.Convert());\n-  *out = converter.result()[0];\n-  DCHECK(*out);\n-  return Status::OK();\n-}\n-\n-Status PandasObjectsToArrow(MemoryPool* pool, PyObject* ao, PyObject* mo,\n-                            const std::shared_ptr<DataType>& type,\n-                            std::shared_ptr<ChunkedArray>* out) {\n-  PandasConverter converter(pool, ao, mo, type);\n-  RETURN_NOT_OK(converter.ConvertObjects());\n-  *out = std::make_shared<ChunkedArray>(converter.result());\n+  const auto& output_arrays = converter.result();\n+  DCHECK_GT(output_arrays.size(), 0);\n+  *out = std::make_shared<ChunkedArray>(output_arrays);\n   return Status::OK();\n }\n \ndiff --git a/cpp/src/arrow/python/pandas_to_arrow.h b/cpp/src/arrow/python/numpy_to_arrow.h\nsimilarity index 69%\nrename from cpp/src/arrow/python/pandas_to_arrow.h\nrename to cpp/src/arrow/python/numpy_to_arrow.h\nindex 3e655ba3f..4a870fff9 100644\n--- a/cpp/src/arrow/python/pandas_to_arrow.h\n+++ b/cpp/src/arrow/python/numpy_to_arrow.h\n@@ -17,8 +17,8 @@\n \n // Converting from pandas memory representation to Arrow data structures\n \n-#ifndef ARROW_PYTHON_PANDAS_TO_ARROW_H\n-#define ARROW_PYTHON_PANDAS_TO_ARROW_H\n+#ifndef ARROW_PYTHON_NUMPY_TO_ARROW_H\n+#define ARROW_PYTHON_NUMPY_TO_ARROW_H\n \n #include \"arrow/python/platform.h\"\n \n@@ -36,24 +36,21 @@ class Status;\n \n namespace py {\n \n-ARROW_EXPORT\n-Status PandasToArrow(MemoryPool* pool, PyObject* ao, PyObject* mo,\n-                     const std::shared_ptr<DataType>& type, std::shared_ptr<Array>* out);\n-\n-/// Convert dtype=object arrays. If target data type is not known, pass a type\n-/// with nullptr\n+/// Convert NumPy arrays to Arrow. If target data type is not known, pass a\n+/// type with nullptr\n ///\n /// \\param[in] pool Memory pool for any memory allocations\n /// \\param[in] ao an ndarray with the array data\n /// \\param[in] mo an ndarray with a null mask (True is null), optional\n-/// \\param[in] type\n+/// \\param[in] type a specific type to cast to, may be null\n /// \\param[out] out a ChunkedArray, to accommodate chunked output\n ARROW_EXPORT\n-Status PandasObjectsToArrow(MemoryPool* pool, PyObject* ao, PyObject* mo,\n-                            const std::shared_ptr<DataType>& type,\n-                            std::shared_ptr<ChunkedArray>* out);\n+Status NdarrayToArrow(MemoryPool* pool, PyObject* ao, PyObject* mo,\n+                      bool use_pandas_null_sentinels,\n+                      const std::shared_ptr<DataType>& type,\n+                      std::shared_ptr<ChunkedArray>* out);\n \n }  // namespace py\n }  // namespace arrow\n \n-#endif  // ARROW_PYTHON_PANDAS_TO_ARROW_H\n+#endif  // ARROW_PYTHON_NUMPY_TO_ARROW_H\ndiff --git a/cpp/src/arrow/python/python-test.cc b/cpp/src/arrow/python/python-test.cc\nindex e1796c097..86391a185 100644\n--- a/cpp/src/arrow/python/python-test.cc\n+++ b/cpp/src/arrow/python/python-test.cc\n@@ -39,10 +39,10 @@ TEST(DecimalTest, TestPythonDecimalToString) {\n \n   OwnedRef decimal;\n   OwnedRef Decimal;\n-  ASSERT_OK(ImportModule(\"decimal\", &decimal));\n+  ASSERT_OK(internal::ImportModule(\"decimal\", &decimal));\n   ASSERT_NE(decimal.obj(), nullptr);\n \n-  ASSERT_OK(ImportFromModule(decimal, \"Decimal\", &Decimal));\n+  ASSERT_OK(internal::ImportFromModule(decimal, \"Decimal\", &Decimal));\n   ASSERT_NE(Decimal.obj(), nullptr);\n \n   std::string decimal_string(\"-39402950693754869342983\");\n@@ -61,7 +61,7 @@ TEST(DecimalTest, TestPythonDecimalToString) {\n   ASSERT_NE(python_object, nullptr);\n \n   std::string string_result;\n-  ASSERT_OK(PythonDecimalToString(python_object, &string_result));\n+  ASSERT_OK(internal::PythonDecimalToString(python_object, &string_result));\n }\n \n TEST(PandasConversionTest, TestObjectBlockWriteFails) {\ndiff --git a/cpp/src/arrow/python/python_to_arrow.cc b/cpp/src/arrow/python/python_to_arrow.cc\nindex c57091fc0..9ba78213b 100644\n--- a/cpp/src/arrow/python/python_to_arrow.cc\n+++ b/cpp/src/arrow/python/python_to_arrow.cc\n@@ -297,12 +297,11 @@ class DictBuilder {\n \n   /// Construct an Arrow StructArray representing the dictionary.\n   /// Contains a field \"keys\" for the keys and \"vals\" for the values.\n-\n-  /// \\param list_data\n+  /// \\param val_list_data\n   ///    List containing the data from nested lists in the value\n   ///   list of the dictionary\n   ///\n-  /// \\param dict_data\n+  /// \\param val_dict_data\n   ///   List containing the data from nested dictionaries in the\n   ///   value list of the dictionary\n   Status Finish(const Array* key_tuple_data, const Array* key_dict_data,\n@@ -543,7 +542,11 @@ Status SerializeSequences(PyObject* context, std::vector<PyObject*> sequences,\n     ScopedRef iterator(PyObject_GetIter(sequence));\n     RETURN_IF_PYERROR();\n     ScopedRef item;\n-    while (item.reset(PyIter_Next(iterator.get())), item.get()) {\n+    while (true) {\n+      item.reset(PyIter_Next(iterator.get()));\n+      if (!item.get()) {\n+        break;\n+      }\n       RETURN_NOT_OK(Append(context, item.get(), &builder, &sublists, &subtuples,\n                            &subdicts, &subsets, tensors_out));\n     }\ndiff --git a/cpp/src/arrow/python/util/datetime.h b/cpp/src/arrow/python/util/datetime.h\nindex de7515101..4ebef720f 100644\n--- a/cpp/src/arrow/python/util/datetime.h\n+++ b/cpp/src/arrow/python/util/datetime.h\n@@ -70,11 +70,14 @@ static inline Status PyTime_from_int(int64_t val, const TimeUnit::type unit,\n }\n \n static inline int64_t PyDate_to_ms(PyDateTime_Date* pydate) {\n-  struct tm date = {0};\n+  struct tm date;\n+  memset(&date, 0, sizeof(struct tm));\n   date.tm_year = PyDateTime_GET_YEAR(pydate) - 1900;\n   date.tm_mon = PyDateTime_GET_MONTH(pydate) - 1;\n   date.tm_mday = PyDateTime_GET_DAY(pydate);\n-  struct tm epoch = {0};\n+  struct tm epoch;\n+  memset(&epoch, 0, sizeof(struct tm));\n+\n   epoch.tm_year = 70;\n   epoch.tm_mday = 1;\n #ifdef _MSC_VER\n@@ -88,7 +91,8 @@ static inline int64_t PyDate_to_ms(PyDateTime_Date* pydate) {\n }\n \n static inline int64_t PyDateTime_to_us(PyDateTime_DateTime* pydatetime) {\n-  struct tm datetime = {0};\n+  struct tm datetime;\n+  memset(&datetime, 0, sizeof(struct tm));\n   datetime.tm_year = PyDateTime_GET_YEAR(pydatetime) - 1900;\n   datetime.tm_mon = PyDateTime_GET_MONTH(pydatetime) - 1;\n   datetime.tm_mday = PyDateTime_GET_DAY(pydatetime);\n@@ -96,7 +100,8 @@ static inline int64_t PyDateTime_to_us(PyDateTime_DateTime* pydatetime) {\n   datetime.tm_min = PyDateTime_DATE_GET_MINUTE(pydatetime);\n   datetime.tm_sec = PyDateTime_DATE_GET_SECOND(pydatetime);\n   int us = PyDateTime_DATE_GET_MICROSECOND(pydatetime);\n-  struct tm epoch = {0};\n+  struct tm epoch;\n+  memset(&epoch, 0, sizeof(struct tm));\n   epoch.tm_year = 70;\n   epoch.tm_mday = 1;\n #ifdef _MSC_VER\ndiff --git a/cpp/src/arrow/table-test.cc b/cpp/src/arrow/table-test.cc\nindex b0aeed192..b490310c2 100644\n--- a/cpp/src/arrow/table-test.cc\n+++ b/cpp/src/arrow/table-test.cc\n@@ -140,10 +140,6 @@ TEST_F(TestColumn, BasicAPI) {\n   ASSERT_EQ(300, column_->length());\n   ASSERT_EQ(30, column_->null_count());\n   ASSERT_EQ(3, column_->data()->num_chunks());\n-\n-  // nullptr array should not break\n-  column_.reset(new Column(f0, std::shared_ptr<Array>(nullptr)));\n-  ASSERT_NE(column_.get(), nullptr);\n }\n \n TEST_F(TestColumn, ChunksInhomogeneous) {\ndiff --git a/cpp/src/arrow/table.cc b/cpp/src/arrow/table.cc\nindex 3d3ecd273..009b5cf63 100644\n--- a/cpp/src/arrow/table.cc\n+++ b/cpp/src/arrow/table.cc\n@@ -42,6 +42,8 @@ ChunkedArray::ChunkedArray(const ArrayVector& chunks) : chunks_(chunks) {\n   }\n }\n \n+std::shared_ptr<DataType> ChunkedArray::type() const { return chunks_[0]->type(); }\n+\n bool ChunkedArray::Equals(const ChunkedArray& other) const {\n   if (length_ != other.length()) {\n     return false;\n@@ -105,10 +107,10 @@ Column::Column(const std::shared_ptr<Field>& field, const ArrayVector& chunks)\n \n Column::Column(const std::shared_ptr<Field>& field, const std::shared_ptr<Array>& data)\n     : field_(field) {\n-  if (data) {\n-    data_ = std::make_shared<ChunkedArray>(ArrayVector({data}));\n-  } else {\n+  if (!data) {\n     data_ = std::make_shared<ChunkedArray>(ArrayVector({}));\n+  } else {\n+    data_ = std::make_shared<ChunkedArray>(ArrayVector({data}));\n   }\n }\n \n@@ -153,13 +155,6 @@ Status Column::ValidateData() {\n // ----------------------------------------------------------------------\n // RecordBatch methods\n \n-void AssertBatchValid(const RecordBatch& batch) {\n-  Status s = batch.Validate();\n-  if (!s.ok()) {\n-    DCHECK(false) << s.ToString();\n-  }\n-}\n-\n RecordBatch::RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows)\n     : schema_(schema), num_rows_(num_rows) {\n   boxed_columns_.resize(schema->num_fields());\n@@ -199,6 +194,7 @@ std::shared_ptr<Array> RecordBatch::column(int i) const {\n   if (!boxed_columns_[i]) {\n     DCHECK(MakeArray(columns_[i], &boxed_columns_[i]).ok());\n   }\n+  DCHECK(boxed_columns_[i]);\n   return boxed_columns_[i];\n }\n \ndiff --git a/cpp/src/arrow/table.h b/cpp/src/arrow/table.h\nindex a66772e5a..324112bfb 100644\n--- a/cpp/src/arrow/table.h\n+++ b/cpp/src/arrow/table.h\n@@ -53,7 +53,7 @@ class ARROW_EXPORT ChunkedArray {\n \n   const ArrayVector& chunks() const { return chunks_; }\n \n-  std::shared_ptr<DataType> type() const { return chunks_[0]->type(); }\n+  std::shared_ptr<DataType> type() const;\n \n   bool Equals(const ChunkedArray& other) const;\n   bool Equals(const std::shared_ptr<ChunkedArray>& other) const;\n@@ -113,7 +113,7 @@ class ARROW_EXPORT Column {\n /// sequence of fields, each a contiguous Arrow array\n class ARROW_EXPORT RecordBatch {\n  public:\n-  /// \\param[in] schema\n+  /// \\param[in] schema The record batch schema\n   /// \\param[in] num_rows length of fields in the record batch. Each array\n   /// should have the same length as num_rows\n   /// \\param[in] columns the record batch fields as vector of arrays\n@@ -209,16 +209,16 @@ class ARROW_EXPORT Table {\n  public:\n   /// \\brief Construct Table from schema and columns\n   /// If columns is zero-length, the table's number of rows is zero\n-  /// \\param schema\n-  /// \\param columns\n-  /// \\param number of rows in table, -1 (default) to infer from columns\n+  /// \\param schema The table schema (column types)\n+  /// \\param columns The table's columns\n+  /// \\param num_rows number of rows in table, -1 (default) to infer from columns\n   Table(const std::shared_ptr<Schema>& schema,\n         const std::vector<std::shared_ptr<Column>>& columns, int64_t num_rows = -1);\n \n   /// \\brief Construct Table from schema and arrays\n-  /// \\param schema\n-  /// \\param arrays\n-  /// \\param number of rows in table, -1 (default) to infer from columns\n+  /// \\param schema The table schema (column types)\n+  /// \\param arrays The table's columns as arrays\n+  /// \\param num_rows number of rows in table, -1 (default) to infer from columns\n   Table(const std::shared_ptr<Schema>& schema,\n         const std::vector<std::shared_ptr<Array>>& arrays, int64_t num_rows = -1);\n \n@@ -231,7 +231,7 @@ class ARROW_EXPORT Table {\n   /// \\return the table's schema\n   std::shared_ptr<Schema> schema() const { return schema_; }\n \n-  /// \\param[i] i column index, does not boundscheck\n+  /// \\param[in] i column index, does not boundscheck\n   /// \\return the i-th column\n   std::shared_ptr<Column> column(int i) const { return columns_[i]; }\n \n@@ -283,7 +283,7 @@ class ARROW_EXPORT RecordBatchReader {\n   /// Read the next record batch in the stream. Return nullptr for batch when\n   /// reaching end of stream\n   ///\n-  /// \\param(out) batch the next loaded batch, nullptr at end of stream\n+  /// \\param[out] batch the next loaded batch, nullptr at end of stream\n   /// \\return Status\n   virtual Status ReadNext(std::shared_ptr<RecordBatch>* batch) = 0;\n \ndiff --git a/cpp/src/arrow/type-test.cc b/cpp/src/arrow/type-test.cc\nindex 710344ab0..3242fadd5 100644\n--- a/cpp/src/arrow/type-test.cc\n+++ b/cpp/src/arrow/type-test.cc\n@@ -68,8 +68,7 @@ TEST(TestField, TestAddMetadata) {\n       new KeyValueMetadata({\"foo\", \"bar\"}, {\"bizz\", \"buzz\"}));\n   auto f0 = field(\"f0\", int32());\n   auto f1 = field(\"f0\", int32(), true, metadata);\n-  std::shared_ptr<Field> f2;\n-  ASSERT_OK(f0->AddMetadata(metadata, &f2));\n+  std::shared_ptr<Field> f2 = f0->AddMetadata(metadata);\n \n   ASSERT_FALSE(f2->Equals(*f0));\n   ASSERT_TRUE(f2->Equals(*f1));\n@@ -184,8 +183,7 @@ TEST_F(TestSchema, TestAddMetadata) {\n   auto metadata = std::shared_ptr<KeyValueMetadata>(\n       new KeyValueMetadata({\"foo\", \"bar\"}, {\"bizz\", \"buzz\"}));\n   auto schema = std::make_shared<Schema>(fields);\n-  std::shared_ptr<Schema> new_schema;\n-  ASSERT_OK(schema->AddMetadata(metadata, &new_schema));\n+  std::shared_ptr<Schema> new_schema = schema->AddMetadata(metadata);\n   ASSERT_TRUE(metadata->Equals(*new_schema->metadata()));\n \n   // Not copied\ndiff --git a/cpp/src/arrow/type.cc b/cpp/src/arrow/type.cc\nindex 87bb73b1e..b9e314440 100644\n--- a/cpp/src/arrow/type.cc\n+++ b/cpp/src/arrow/type.cc\n@@ -36,12 +36,16 @@ std::shared_ptr<Field> Field::AddMetadata(\n   return std::make_shared<Field>(name_, type_, nullable_, metadata);\n }\n \n+#ifndef ARROW_NO_DEPRECATED_API\n+\n Status Field::AddMetadata(const std::shared_ptr<const KeyValueMetadata>& metadata,\n                           std::shared_ptr<Field>* out) const {\n   *out = AddMetadata(metadata);\n   return Status::OK();\n }\n \n+#endif\n+\n std::shared_ptr<Field> Field::RemoveMetadata() const {\n   return std::make_shared<Field>(name_, type_, nullable_);\n }\n@@ -307,12 +311,16 @@ std::shared_ptr<Schema> Schema::AddMetadata(\n   return std::make_shared<Schema>(fields_, metadata);\n }\n \n+#ifndef ARROW_NO_DEPRECATED_API\n+\n Status Schema::AddMetadata(const std::shared_ptr<const KeyValueMetadata>& metadata,\n                            std::shared_ptr<Schema>* out) const {\n   *out = AddMetadata(metadata);\n   return Status::OK();\n }\n \n+#endif\n+\n std::shared_ptr<const KeyValueMetadata> Schema::metadata() const { return metadata_; }\n \n std::shared_ptr<Schema> Schema::RemoveMetadata() const {\ndiff --git a/cpp/src/arrow/type.h b/cpp/src/arrow/type.h\nindex 7630f4815..c1da6e270 100644\n--- a/cpp/src/arrow/type.h\n+++ b/cpp/src/arrow/type.h\n@@ -227,9 +227,11 @@ class ARROW_EXPORT Field {\n \n   std::shared_ptr<const KeyValueMetadata> metadata() const { return metadata_; }\n \n-  /// \\deprecated\n+#ifndef ARROW_NO_DEPRECATED_API\n+  /// \\note Deprecated since 0.8.0\n   Status AddMetadata(const std::shared_ptr<const KeyValueMetadata>& metadata,\n                      std::shared_ptr<Field>* out) const;\n+#endif\n \n   std::shared_ptr<Field> AddMetadata(\n       const std::shared_ptr<const KeyValueMetadata>& metadata) const;\n@@ -749,9 +751,11 @@ class ARROW_EXPORT Schema {\n                   std::shared_ptr<Schema>* out) const;\n   Status RemoveField(int i, std::shared_ptr<Schema>* out) const;\n \n-  /// \\deprecated\n+#ifndef ARROW_NO_DEPRECATED_API\n+  /// \\note Deprecated since 0.8.0\n   Status AddMetadata(const std::shared_ptr<const KeyValueMetadata>& metadata,\n                      std::shared_ptr<Schema>* out) const;\n+#endif\n \n   /// \\brief Replace key-value metadata with new metadata\n   ///\ndiff --git a/cpp/src/arrow/util/bit-util.cc b/cpp/src/arrow/util/bit-util.cc\nindex 15bf3595a..e0116cc56 100644\n--- a/cpp/src/arrow/util/bit-util.cc\n+++ b/cpp/src/arrow/util/bit-util.cc\n@@ -31,6 +31,7 @@\n #include \"arrow/memory_pool.h\"\n #include \"arrow/status.h\"\n #include \"arrow/util/bit-util.h\"\n+#include \"arrow/util/logging.h\"\n \n namespace arrow {\n \n@@ -48,9 +49,9 @@ Status BitUtil::BytesToBits(const std::vector<uint8_t>& bytes, MemoryPool* pool,\n \n   std::shared_ptr<Buffer> buffer;\n   RETURN_NOT_OK(AllocateBuffer(pool, bit_length, &buffer));\n-\n-  memset(buffer->mutable_data(), 0, static_cast<size_t>(bit_length));\n-  FillBitsFromBytes(bytes, buffer->mutable_data());\n+  uint8_t* out_buf = buffer->mutable_data();\n+  memset(out_buf, 0, static_cast<size_t>(bit_length));\n+  FillBitsFromBytes(bytes, out_buf);\n \n   *out = buffer;\n   return Status::OK();\ndiff --git a/cpp/src/arrow/util/compression_brotli.cc b/cpp/src/arrow/util/compression_brotli.cc\nindex 1aaec11d9..196c19a66 100644\n--- a/cpp/src/arrow/util/compression_brotli.cc\n+++ b/cpp/src/arrow/util/compression_brotli.cc\n@@ -25,6 +25,7 @@\n #include <brotli/types.h>\n \n #include \"arrow/status.h\"\n+#include \"arrow/util/macros.h\"\n \n namespace arrow {\n \n@@ -41,7 +42,8 @@ Status BrotliCodec::Decompress(int64_t input_len, const uint8_t* input,\n   return Status::OK();\n }\n \n-int64_t BrotliCodec::MaxCompressedLen(int64_t input_len, const uint8_t* input) {\n+int64_t BrotliCodec::MaxCompressedLen(int64_t input_len,\n+                                      const uint8_t* ARROW_ARG_UNUSED(input)) {\n   return BrotliEncoderMaxCompressedSize(input_len);\n }\n \ndiff --git a/cpp/src/arrow/util/compression_lz4.cc b/cpp/src/arrow/util/compression_lz4.cc\nindex cda40ad8c..001edeb01 100644\n--- a/cpp/src/arrow/util/compression_lz4.cc\n+++ b/cpp/src/arrow/util/compression_lz4.cc\n@@ -22,6 +22,7 @@\n #include <lz4.h>\n \n #include \"arrow/status.h\"\n+#include \"arrow/util/macros.h\"\n \n namespace arrow {\n \n@@ -39,7 +40,8 @@ Status Lz4Codec::Decompress(int64_t input_len, const uint8_t* input, int64_t out\n   return Status::OK();\n }\n \n-int64_t Lz4Codec::MaxCompressedLen(int64_t input_len, const uint8_t* input) {\n+int64_t Lz4Codec::MaxCompressedLen(int64_t input_len,\n+                                   const uint8_t* ARROW_ARG_UNUSED(input)) {\n   return LZ4_compressBound(static_cast<int>(input_len));\n }\n \ndiff --git a/cpp/src/arrow/util/compression_snappy.cc b/cpp/src/arrow/util/compression_snappy.cc\nindex e284bd435..2edaef7cf 100644\n--- a/cpp/src/arrow/util/compression_snappy.cc\n+++ b/cpp/src/arrow/util/compression_snappy.cc\n@@ -33,7 +33,8 @@ namespace arrow {\n // Snappy implementation\n \n Status SnappyCodec::Decompress(int64_t input_len, const uint8_t* input,\n-                               int64_t output_len, uint8_t* output_buffer) {\n+                               int64_t ARROW_ARG_UNUSED(output_len),\n+                               uint8_t* output_buffer) {\n   if (!snappy::RawUncompress(reinterpret_cast<const char*>(input),\n                              static_cast<size_t>(input_len),\n                              reinterpret_cast<char*>(output_buffer))) {\n@@ -42,13 +43,14 @@ Status SnappyCodec::Decompress(int64_t input_len, const uint8_t* input,\n   return Status::OK();\n }\n \n-int64_t SnappyCodec::MaxCompressedLen(int64_t input_len, const uint8_t* input) {\n+int64_t SnappyCodec::MaxCompressedLen(int64_t input_len,\n+                                      const uint8_t* ARROW_ARG_UNUSED(input)) {\n   return snappy::MaxCompressedLength(input_len);\n }\n \n Status SnappyCodec::Compress(int64_t input_len, const uint8_t* input,\n-                             int64_t output_buffer_len, uint8_t* output_buffer,\n-                             int64_t* output_length) {\n+                             int64_t ARROW_ARG_UNUSED(output_buffer_len),\n+                             uint8_t* output_buffer, int64_t* output_length) {\n   size_t output_len;\n   snappy::RawCompress(reinterpret_cast<const char*>(input),\n                       static_cast<size_t>(input_len),\ndiff --git a/cpp/src/arrow/util/compression_zlib.cc b/cpp/src/arrow/util/compression_zlib.cc\nindex 0656fd64b..3a520240e 100644\n--- a/cpp/src/arrow/util/compression_zlib.cc\n+++ b/cpp/src/arrow/util/compression_zlib.cc\n@@ -165,7 +165,7 @@ class GZipCodec::GZipCodecImpl {\n     return Status::OK();\n   }\n \n-  int64_t MaxCompressedLen(int64_t input_length, const uint8_t* input) {\n+  int64_t MaxCompressedLen(int64_t input_length, const uint8_t* ARROW_ARG_UNUSED(input)) {\n     // Most be in compression mode\n     if (!compressor_initialized_) {\n       Status s = InitCompressor();\ndiff --git a/cpp/src/arrow/util/compression_zstd.cc b/cpp/src/arrow/util/compression_zstd.cc\nindex d19ac4344..20306f48e 100644\n--- a/cpp/src/arrow/util/compression_zstd.cc\n+++ b/cpp/src/arrow/util/compression_zstd.cc\n@@ -23,6 +23,7 @@\n #include <zstd.h>\n \n #include \"arrow/status.h\"\n+#include \"arrow/util/macros.h\"\n \n using std::size_t;\n \n@@ -42,7 +43,8 @@ Status ZSTDCodec::Decompress(int64_t input_len, const uint8_t* input, int64_t ou\n   return Status::OK();\n }\n \n-int64_t ZSTDCodec::MaxCompressedLen(int64_t input_len, const uint8_t* input) {\n+int64_t ZSTDCodec::MaxCompressedLen(int64_t input_len,\n+                                    const uint8_t* ARROW_ARG_UNUSED(input)) {\n   return ZSTD_compressBound(input_len);\n }\n \ndiff --git a/cpp/src/arrow/util/cpu-info.cc b/cpp/src/arrow/util/cpu-info.cc\nindex 639f02e48..822fcaea7 100644\n--- a/cpp/src/arrow/util/cpu-info.cc\n+++ b/cpp/src/arrow/util/cpu-info.cc\n@@ -76,6 +76,8 @@ static struct {\n };\n static const int64_t num_flags = sizeof(flag_mappings) / sizeof(flag_mappings[0]);\n \n+namespace {\n+\n // Helper function to parse for hardware flags.\n // values contains a list of space-seperated flags.  check to see if the flags we\n // care about are present.\n@@ -90,6 +92,8 @@ int64_t ParseCPUFlags(const string& values) {\n   return flags;\n }\n \n+}  // namespace\n+\n #ifdef _WIN32\n bool RetrieveCacheSize(int64_t* cache_sizes) {\n   if (!cache_sizes) {\ndiff --git a/cpp/src/arrow/util/decimal.h b/cpp/src/arrow/util/decimal.h\nindex a0dea0921..58496a874 100644\n--- a/cpp/src/arrow/util/decimal.h\n+++ b/cpp/src/arrow/util/decimal.h\n@@ -75,8 +75,8 @@ class ARROW_EXPORT Decimal128 {\n   ///  -21 /  5 -> -4, -1\n   ///   21 / -5 -> -4,  1\n   ///  -21 / -5 ->  4, -1\n-  /// @param right the number to divide by\n-  /// @param remainder the remainder after the division\n+  /// \\param divisor the number to divide by\n+  /// \\param remainder the remainder after the division\n   Status Divide(const Decimal128& divisor, Decimal128* result,\n                 Decimal128* remainder) const;\n \ndiff --git a/cpp/src/arrow/util/hash-util.h b/cpp/src/arrow/util/hash-util.h\nindex d5fb212f3..3bba07bb2 100644\n--- a/cpp/src/arrow/util/hash-util.h\n+++ b/cpp/src/arrow/util/hash-util.h\n@@ -199,7 +199,7 @@ class HashUtil {\n   static uint32_t FnvHash64to32(const void* data, int32_t bytes, uint32_t hash) {\n     // IMPALA-2270: this function should never be used for zero-byte inputs.\n     DCHECK_GT(bytes, 0);\n-    uint64_t hash_u64 = hash | ((uint64_t)hash << 32);\n+    uint64_t hash_u64 = hash | (static_cast<uint64_t>(hash) << 32);\n     hash_u64 = FnvHash64(data, bytes, hash_u64);\n     return static_cast<uint32_t>((hash_u64 >> 32) ^ (hash_u64 & 0xFFFFFFFF));\n   }\ndiff --git a/cpp/src/arrow/util/key_value_metadata.cc b/cpp/src/arrow/util/key_value_metadata.cc\nindex cf74ddf37..4f379537c 100644\n--- a/cpp/src/arrow/util/key_value_metadata.cc\n+++ b/cpp/src/arrow/util/key_value_metadata.cc\n@@ -89,12 +89,14 @@ int64_t KeyValueMetadata::size() const {\n \n std::string KeyValueMetadata::key(int64_t i) const {\n   DCHECK_GE(i, 0);\n-  return keys_[static_cast<size_t>(i)];\n+  DCHECK_LT(static_cast<size_t>(i), keys_.size());\n+  return keys_[i];\n }\n \n std::string KeyValueMetadata::value(int64_t i) const {\n   DCHECK_GE(i, 0);\n-  return values_[static_cast<size_t>(i)];\n+  DCHECK_LT(static_cast<size_t>(i), values_.size());\n+  return values_[i];\n }\n \n std::shared_ptr<KeyValueMetadata> KeyValueMetadata::Copy() const {\ndiff --git a/cpp/src/arrow/util/logging.h b/cpp/src/arrow/util/logging.h\nindex b7e2ceea9..39815f303 100644\n--- a/cpp/src/arrow/util/logging.h\n+++ b/cpp/src/arrow/util/logging.h\n@@ -45,7 +45,7 @@ namespace arrow {\n #define ARROW_CHECK(condition)                           \\\n   (condition) ? 0                                        \\\n               : ::arrow::internal::FatalLog(ARROW_FATAL) \\\n-                    << __FILE__ << __LINE__ << \" Check failed: \" #condition \" \"\n+                    << __FILE__ << \":\" << __LINE__ << \" Check failed: \" #condition \" \"\n \n #ifdef NDEBUG\n #define ARROW_DFATAL ARROW_WARNING\n@@ -90,7 +90,7 @@ namespace internal {\n class NullLog {\n  public:\n   template <class T>\n-  NullLog& operator<<(const T& t) {\n+  NullLog& operator<<(const T& ARROW_ARG_UNUSED(t)) {\n     return *this;\n   }\n };\ndiff --git a/cpp/src/arrow/util/macros.h b/cpp/src/arrow/util/macros.h\nindex 2f1db0924..a5f6e5707 100644\n--- a/cpp/src/arrow/util/macros.h\n+++ b/cpp/src/arrow/util/macros.h\n@@ -26,7 +26,7 @@\n #endif\n \n #define ARROW_UNUSED(x) (void)x\n-\n+#define ARROW_ARG_UNUSED(x)\n //\n // GCC can be told that a certain branch is not likely to be taken (for\n // instance, a CHECK failure), and use that information in static analysis.\ndiff --git a/cpp/src/arrow/util/sse-util.h b/cpp/src/arrow/util/sse-util.h\nindex a0ec8a2e9..32ac43f91 100644\n--- a/cpp/src/arrow/util/sse-util.h\n+++ b/cpp/src/arrow/util/sse-util.h\n@@ -176,27 +176,27 @@ static inline int SSE4_cmpestri(__m128i str1, int len1, __m128i str2, int len2)\n   return 0;\n }\n \n-static inline uint32_t SSE4_crc32_u8(uint32_t crc, uint8_t v) {\n+static inline uint32_t SSE4_crc32_u8(uint32_t, uint8_t) {\n   DCHECK(false) << \"CPU doesn't support SSE 4.2\";\n   return 0;\n }\n \n-static inline uint32_t SSE4_crc32_u16(uint32_t crc, uint16_t v) {\n+static inline uint32_t SSE4_crc32_u16(uint32_t, uint16_t) {\n   DCHECK(false) << \"CPU doesn't support SSE 4.2\";\n   return 0;\n }\n \n-static inline uint32_t SSE4_crc32_u32(uint32_t crc, uint32_t v) {\n+static inline uint32_t SSE4_crc32_u32(uint32_t, uint32_t) {\n   DCHECK(false) << \"CPU doesn't support SSE 4.2\";\n   return 0;\n }\n \n-static inline uint32_t SSE4_crc32_u64(uint32_t crc, uint64_t v) {\n+static inline uint32_t SSE4_crc32_u64(uint32_t, uint64_t) {\n   DCHECK(false) << \"CPU doesn't support SSE 4.2\";\n   return 0;\n }\n \n-static inline int64_t POPCNT_popcnt_u64(uint64_t a) {\n+static inline int64_t POPCNT_popcnt_u64(uint64_t) {\n   DCHECK(false) << \"CPU doesn't support SSE 4.2\";\n   return 0;\n }\n@@ -205,27 +205,27 @@ static inline int64_t POPCNT_popcnt_u64(uint64_t a) {\n \n #else\n \n-static inline uint32_t SSE4_crc32_u8(uint32_t crc, uint8_t v) {\n+static inline uint32_t SSE4_crc32_u8(uint32_t, uint8_t) {\n   DCHECK(false) << \"SSE support is not enabled\";\n   return 0;\n }\n \n-static inline uint32_t SSE4_crc32_u16(uint32_t crc, uint16_t v) {\n+static inline uint32_t SSE4_crc32_u16(uint32_t, uint16_t) {\n   DCHECK(false) << \"SSE support is not enabled\";\n   return 0;\n }\n \n-static inline uint32_t SSE4_crc32_u32(uint32_t crc, uint32_t v) {\n+static inline uint32_t SSE4_crc32_u32(uint32_t, uint32_t) {\n   DCHECK(false) << \"SSE support is not enabled\";\n   return 0;\n }\n \n-static inline uint32_t SSE4_crc32_u64(uint32_t crc, uint64_t v) {\n+static inline uint32_t SSE4_crc32_u64(uint32_t, uint64_t) {\n   DCHECK(false) << \"SSE support is not enabled\";\n   return 0;\n }\n \n-static inline int64_t POPCNT_popcnt_u64(uint64_t a) {\n+static inline int64_t POPCNT_popcnt_u64(uint64_t) {\n   DCHECK(false) << \"SSE support is not enabled\";\n   return 0;\n }\ndiff --git a/cpp/src/arrow/visitor.cc b/cpp/src/arrow/visitor.cc\nindex 53a0fee0a..a7b01b0f6 100644\n--- a/cpp/src/arrow/visitor.cc\n+++ b/cpp/src/arrow/visitor.cc\n@@ -56,10 +56,7 @@ ARRAY_VISITOR_DEFAULT(ListArray);\n ARRAY_VISITOR_DEFAULT(StructArray);\n ARRAY_VISITOR_DEFAULT(UnionArray);\n ARRAY_VISITOR_DEFAULT(DictionaryArray);\n-\n-Status ArrayVisitor::Visit(const DecimalArray& array) {\n-  return Status::NotImplemented(\"decimal\");\n-}\n+ARRAY_VISITOR_DEFAULT(DecimalArray);\n \n #undef ARRAY_VISITOR_DEFAULT\n \ndiff --git a/cpp/src/plasma/CMakeLists.txt b/cpp/src/plasma/CMakeLists.txt\nindex 7e9120262..c9339545f 100644\n--- a/cpp/src/plasma/CMakeLists.txt\n+++ b/cpp/src/plasma/CMakeLists.txt\n@@ -94,7 +94,14 @@ if (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n   set_property(SOURCE malloc.cc\n     APPEND_STRING\n     PROPERTY COMPILE_FLAGS\n-    \" -Wno-parentheses-equality -Wno-shorten-64-to-32\")\n+    \" -Wno-parentheses-equality \\\n+-Wno-shorten-64-to-32 \\\n+-Wno-unused-macros \")\n+\n+  set_property(SOURCE thirdparty/xxhash.cc\n+    APPEND_STRING\n+    PROPERTY COMPILE_FLAGS\n+    \"-Wno-unused-macros\")\n endif()\n \n if (\"${COMPILER_FAMILY}\" STREQUAL \"gcc\")\n@@ -111,6 +118,7 @@ target_link_libraries(plasma_store plasma_static)\n install(FILES\n   common.h\n   common_generated.h\n+  compat.h\n   client.h\n   events.h\n   plasma.h\ndiff --git a/cpp/src/plasma/client.cc b/cpp/src/plasma/client.cc\nindex 5e28d4f2a..e57a2a6f3 100644\n--- a/cpp/src/plasma/client.cc\n+++ b/cpp/src/plasma/client.cc\n@@ -286,7 +286,6 @@ Status PlasmaClient::Get(const ObjectID* object_ids, int64_t num_objects,\n /// calls will not do anything. The client will only send a message to the store\n /// releasing the object when the client is truly done with the object.\n ///\n-/// @param conn The plasma connection.\n /// @param object_id The object ID to attempt to release.\n Status PlasmaClient::PerformRelease(const ObjectID& object_id) {\n   // Decrement the count of the number of instances of this object that are\n@@ -357,6 +356,7 @@ Status PlasmaClient::Contains(const ObjectID& object_id, bool* has_object) {\n     std::vector<uint8_t> buffer;\n     RETURN_NOT_OK(PlasmaReceive(store_conn_, MessageType_PlasmaContainsReply, &buffer));\n     ObjectID object_id2;\n+    DCHECK_GT(buffer.size(), 0);\n     RETURN_NOT_OK(\n         ReadContainsReply(buffer.data(), buffer.size(), &object_id2, has_object));\n   }\n@@ -401,7 +401,8 @@ static inline bool compute_object_hash_parallel(XXH64_state_t* hash_state,\n     }\n   }\n \n-  XXH64_update(hash_state, (unsigned char*)threadhash, sizeof(threadhash));\n+  XXH64_update(hash_state, reinterpret_cast<unsigned char*>(threadhash),\n+               sizeof(threadhash));\n   return true;\n }\n \n@@ -409,12 +410,14 @@ static uint64_t compute_object_hash(const ObjectBuffer& obj_buffer) {\n   XXH64_state_t hash_state;\n   XXH64_reset(&hash_state, XXH64_DEFAULT_SEED);\n   if (obj_buffer.data_size >= kBytesInMB) {\n-    compute_object_hash_parallel(&hash_state, (unsigned char*)obj_buffer.data,\n+    compute_object_hash_parallel(&hash_state,\n+                                 reinterpret_cast<unsigned char*>(obj_buffer.data),\n                                  obj_buffer.data_size);\n   } else {\n-    XXH64_update(&hash_state, (unsigned char*)obj_buffer.data, obj_buffer.data_size);\n+    XXH64_update(&hash_state, reinterpret_cast<unsigned char*>(obj_buffer.data),\n+                 obj_buffer.data_size);\n   }\n-  XXH64_update(&hash_state, (unsigned char*)obj_buffer.metadata,\n+  XXH64_update(&hash_state, reinterpret_cast<unsigned char*>(obj_buffer.metadata),\n                obj_buffer.metadata_size);\n   return XXH64_digest(&hash_state);\n }\n@@ -548,8 +551,6 @@ Status PlasmaClient::Disconnect() {\n   return Status::OK();\n }\n \n-#define h_addr h_addr_list[0]\n-\n Status PlasmaClient::Transfer(const char* address, int port, const ObjectID& object_id) {\n   return SendDataRequest(manager_conn_, object_id, address, port);\n }\ndiff --git a/cpp/src/plasma/client.h b/cpp/src/plasma/client.h\nindex 50ec55f5e..145942441 100644\n--- a/cpp/src/plasma/client.h\n+++ b/cpp/src/plasma/client.h\n@@ -82,15 +82,15 @@ class ARROW_EXPORT PlasmaClient {\n   /// Connect to the local plasma store and plasma manager. Return\n   /// the resulting connection.\n   ///\n-  /// @param store_socket_name The name of the UNIX domain socket to use to\n+  /// \\param store_socket_name The name of the UNIX domain socket to use to\n   ///        connect to the Plasma store.\n-  /// @param manager_socket_name The name of the UNIX domain socket to use to\n+  /// \\param manager_socket_name The name of the UNIX domain socket to use to\n   ///        connect to the local Plasma manager. If this is \"\", then this\n   ///        function will not connect to a manager.\n-  /// @param release_delay Number of released objects that are kept around\n+  /// \\param release_delay Number of released objects that are kept around\n   ///        and not evicted to avoid too many munmaps.\n-  /// @param num_retries number of attempts to connect to IPC socket, default 50\n-  /// @return The return status.\n+  /// \\param num_retries number of attempts to connect to IPC socket, default 50\n+  /// \\return The return status.\n   Status Connect(const std::string& store_socket_name,\n                  const std::string& manager_socket_name, int release_delay,\n                  int num_retries = -1);\n@@ -98,17 +98,17 @@ class ARROW_EXPORT PlasmaClient {\n   /// Create an object in the Plasma Store. Any metadata for this object must be\n   /// be passed in when the object is created.\n   ///\n-  /// @param object_id The ID to use for the newly created object.\n-  /// @param data_size The size in bytes of the space to be allocated for this\n+  /// \\param object_id The ID to use for the newly created object.\n+  /// \\param data_size The size in bytes of the space to be allocated for this\n   /// object's\n   ///        data (this does not include space used for metadata).\n-  /// @param metadata The object's metadata. If there is no metadata, this\n+  /// \\param metadata The object's metadata. If there is no metadata, this\n   /// pointer\n   ///        should be NULL.\n-  /// @param metadata_size The size in bytes of the metadata. If there is no\n+  /// \\param metadata_size The size in bytes of the metadata. If there is no\n   ///        metadata, this should be 0.\n-  /// @param data The address of the newly created object will be written here.\n-  /// @return The return status.\n+  /// \\param data The address of the newly created object will be written here.\n+  /// \\return The return status.\n   Status Create(const ObjectID& object_id, int64_t data_size, uint8_t* metadata,\n                 int64_t metadata_size, uint8_t** data);\n \n@@ -119,14 +119,14 @@ class ARROW_EXPORT PlasmaClient {\n   /// but\n   /// the caller should not release objects that were not retrieved.\n   ///\n-  /// @param object_ids The IDs of the objects to get.\n-  /// @param num_object_ids The number of object IDs to get.\n-  /// @param timeout_ms The amount of time in milliseconds to wait before this\n+  /// \\param object_ids The IDs of the objects to get.\n+  /// \\param num_objects The number of object IDs to get.\n+  /// \\param timeout_ms The amount of time in milliseconds to wait before this\n   ///        request times out. If this value is -1, then no timeout is set.\n-  /// @param object_buffers An array where the results will be stored. If the\n+  /// \\param object_buffers An array where the results will be stored. If the\n   /// data\n   ///        size field is -1, then the object was not retrieved.\n-  /// @return The return status.\n+  /// \\return The return status.\n   Status Get(const ObjectID* object_ids, int64_t num_objects, int64_t timeout_ms,\n              ObjectBuffer* object_buffers);\n \n@@ -136,8 +136,8 @@ class ARROW_EXPORT PlasmaClient {\n   /// the address returned by Get is no longer valid. This should be called\n   /// once for each call to Get (with the same object ID).\n   ///\n-  /// @param object_id The ID of the object that is no longer needed.\n-  /// @return The return status.\n+  /// \\param object_id The ID of the object that is no longer needed.\n+  /// \\return The return status.\n   Status Release(const ObjectID& object_id);\n \n   /// Check if the object store contains a particular object and the object has\n@@ -146,18 +146,18 @@ class ARROW_EXPORT PlasmaClient {\n   /// @todo: We may want to indicate if the object has been created but not\n   /// sealed.\n   ///\n-  /// @param object_id The ID of the object whose presence we are checking.\n-  /// @param has_object The function will write true at this address if\n+  /// \\param object_id The ID of the object whose presence we are checking.\n+  /// \\param has_object The function will write true at this address if\n   ///        the object is present and false if it is not present.\n-  /// @return The return status.\n+  /// \\return The return status.\n   Status Contains(const ObjectID& object_id, bool* has_object);\n \n   /// Seal an object in the object store. The object will be immutable after\n   /// this\n   /// call.\n   ///\n-  /// @param object_id The ID of the object to seal.\n-  /// @return The return status.\n+  /// \\param object_id The ID of the object to seal.\n+  /// \\return The return status.\n   Status Seal(const ObjectID& object_id);\n \n   /// Delete an object from the object store. This currently assumes that the\n@@ -166,52 +166,51 @@ class ARROW_EXPORT PlasmaClient {\n   /// @todo We may want to allow the deletion of objects that are not present or\n   ///       haven't been sealed.\n   ///\n-  /// @param object_id The ID of the object to delete.\n-  /// @return The return status.\n+  /// \\param object_id The ID of the object to delete.\n+  /// \\return The return status.\n   Status Delete(const ObjectID& object_id);\n \n   /// Delete objects until we have freed up num_bytes bytes or there are no more\n   /// released objects that can be deleted.\n   ///\n-  /// @param num_bytes The number of bytes to try to free up.\n-  /// @param num_bytes_evicted Out parameter for total number of bytes of space\n+  /// \\param num_bytes The number of bytes to try to free up.\n+  /// \\param num_bytes_evicted Out parameter for total number of bytes of space\n   /// retrieved.\n-  /// @return The return status.\n+  /// \\return The return status.\n   Status Evict(int64_t num_bytes, int64_t& num_bytes_evicted);\n \n   /// Compute the hash of an object in the object store.\n   ///\n-  /// @param conn The object containing the connection state.\n-  /// @param object_id The ID of the object we want to hash.\n-  /// @param digest A pointer at which to return the hash digest of the object.\n+  /// \\param object_id The ID of the object we want to hash.\n+  /// \\param digest A pointer at which to return the hash digest of the object.\n   ///        The pointer must have at least kDigestSize bytes allocated.\n-  /// @return The return status.\n+  /// \\return The return status.\n   Status Hash(const ObjectID& object_id, uint8_t* digest);\n \n   /// Subscribe to notifications when objects are sealed in the object store.\n   /// Whenever an object is sealed, a message will be written to the client\n   /// socket that is returned by this method.\n   ///\n-  /// @param fd Out parameter for the file descriptor the client should use to\n+  /// \\param fd Out parameter for the file descriptor the client should use to\n   /// read notifications\n   ///         from the object store about sealed objects.\n-  /// @return The return status.\n+  /// \\return The return status.\n   Status Subscribe(int* fd);\n \n   /// Receive next object notification for this client if Subscribe has been called.\n   ///\n-  /// @param fd The file descriptor we are reading the notification from.\n-  /// @param object_id Out parameter, the object_id of the object that was sealed.\n-  /// @param data_size Out parameter, the data size of the object that was sealed.\n-  /// @param metadata_size Out parameter, the metadata size of the object that was sealed.\n-  /// @return The return status.\n+  /// \\param fd The file descriptor we are reading the notification from.\n+  /// \\param object_id Out parameter, the object_id of the object that was sealed.\n+  /// \\param data_size Out parameter, the data size of the object that was sealed.\n+  /// \\param metadata_size Out parameter, the metadata size of the object that was sealed.\n+  /// \\return The return status.\n   Status GetNotification(int fd, ObjectID* object_id, int64_t* data_size,\n                          int64_t* metadata_size);\n \n   /// Disconnect from the local plasma instance, including the local store and\n   /// manager.\n   ///\n-  /// @return The return status.\n+  /// \\return The return status.\n   Status Disconnect();\n \n   /// Attempt to initiate the transfer of some objects from remote Plasma\n@@ -236,17 +235,17 @@ class ARROW_EXPORT PlasmaClient {\n   /// This method is idempotent in the sense that it is ok to call it multiple\n   /// times.\n   ///\n-  /// @param num_object_ids The number of object IDs fetch is being called on.\n-  /// @param object_ids The IDs of the objects that fetch is being called on.\n-  /// @return The return status.\n+  /// \\param num_object_ids The number of object IDs fetch is being called on.\n+  /// \\param object_ids The IDs of the objects that fetch is being called on.\n+  /// \\return The return status.\n   Status Fetch(int num_object_ids, const ObjectID* object_ids);\n \n   /// Wait for (1) a specified number of objects to be available (sealed) in the\n   /// local Plasma Store or in a remote Plasma Store, or (2) for a timeout to\n   /// expire. This is a blocking call.\n   ///\n-  /// @param num_object_requests Size of the object_requests array.\n-  /// @param object_requests Object event array. Each element contains a request\n+  /// \\param num_object_requests Size of the object_requests array.\n+  /// \\param object_requests Object event array. Each element contains a request\n   ///        for a particular object_id. The type of request is specified in the\n   ///        \"type\" field.\n   ///        - A PLASMA_QUERY_LOCAL request is satisfied when object_id becomes\n@@ -260,36 +259,34 @@ class ARROW_EXPORT PlasmaClient {\n   ///          available either at the local Plasma Store or on a remote Plasma\n   ///          Store. In this case, the functions sets the \"status\" field to\n   ///          ObjectStatus_Local or ObjectStatus_Remote.\n-  /// @param num_ready_objects The number of requests in object_requests array\n+  /// \\param num_ready_objects The number of requests in object_requests array\n   /// that\n   ///        must be satisfied before the function returns, unless it timeouts.\n   ///        The num_ready_objects should be no larger than num_object_requests.\n-  /// @param timeout_ms Timeout value in milliseconds. If this timeout expires\n+  /// \\param timeout_ms Timeout value in milliseconds. If this timeout expires\n   ///        before min_num_ready_objects of requests are satisfied, the\n   ///        function\n   ///        returns.\n-  /// @param num_objects_ready Out parameter for number of satisfied requests in\n+  /// \\param num_objects_ready Out parameter for number of satisfied requests in\n   ///        the object_requests list. If the returned number is less than\n   ///        min_num_ready_objects this means that timeout expired.\n-  /// @return The return status.\n+  /// \\return The return status.\n   Status Wait(int64_t num_object_requests, ObjectRequest* object_requests,\n               int num_ready_objects, int64_t timeout_ms, int* num_objects_ready);\n \n   /// Transfer local object to a different plasma manager.\n   ///\n-  /// @param conn The object containing the connection state.\n-  /// @param addr IP address of the plasma manager we are transfering to.\n-  /// @param port Port of the plasma manager we are transfering to.\n-  /// @object_id ObjectID of the object we are transfering.\n-  /// @return The return status.\n+  /// \\param addr IP address of the plasma manager we are transfering to.\n+  /// \\param port Port of the plasma manager we are transfering to.\n+  /// \\param object_id ObjectID of the object we are transfering.\n+  /// \\return The return status.\n   Status Transfer(const char* addr, int port, const ObjectID& object_id);\n \n   /// Return the status of a given object. This method may query the object\n   /// table.\n   ///\n-  /// @param conn The object containing the connection state.\n-  /// @param object_id The ID of the object whose status we query.\n-  /// @param object_status Out parameter for object status. Can take the\n+  /// \\param object_id The ID of the object whose status we query.\n+  /// \\param object_status Out parameter for object status. Can take the\n   ///         following values.\n   ///         - PLASMA_CLIENT_LOCAL, if object is stored in the local Plasma\n   ///         Store.\n@@ -300,13 +297,12 @@ class ARROW_EXPORT PlasmaClient {\n   ///           Plasma Store.\n   ///         - PLASMA_CLIENT_DOES_NOT_EXIST, if the object doesn\u2019t exist in the\n   ///           system.\n-  /// @return The return status.\n+  /// \\return The return status.\n   Status Info(const ObjectID& object_id, int* object_status);\n \n   /// Get the file descriptor for the socket connection to the plasma manager.\n   ///\n-  /// @param conn The plasma connection.\n-  /// @return The file descriptor for the manager connection. If there is no\n+  /// \\return The file descriptor for the manager connection. If there is no\n   ///         connection to the manager, this is -1.\n   int get_manager_fd();\n \ndiff --git a/cpp/src/plasma/common.h b/cpp/src/plasma/common.h\nindex 66d5f3069..cc67ffe46 100644\n--- a/cpp/src/plasma/common.h\n+++ b/cpp/src/plasma/common.h\n@@ -26,6 +26,8 @@\n #define __STDC_FORMAT_MACROS\n #endif\n \n+#include \"plasma/compat.h\"\n+\n #include \"arrow/status.h\"\n #include \"arrow/util/logging.h\"\n \ndiff --git a/cpp/src/plasma/compat.h b/cpp/src/plasma/compat.h\nnew file mode 100644\nindex 000000000..ce751da1d\n--- /dev/null\n+++ b/cpp/src/plasma/compat.h\n@@ -0,0 +1,35 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#ifndef PLASMA_COMPAT_H\n+#define PLASMA_COMPAT_H\n+\n+// Workaround for multithreading on XCode 9, see\n+// https://issues.apache.org/jira/browse/ARROW-1622 and\n+// https://github.com/tensorflow/tensorflow/issues/13220#issuecomment-331579775\n+// This should be a short-term fix until the problem is fixed upstream.\n+#ifdef __APPLE__\n+#ifndef _MACH_PORT_T\n+#define _MACH_PORT_T\n+#include <sys/_types.h> /* __darwin_mach_port_t */\n+typedef __darwin_mach_port_t mach_port_t;\n+#include <pthread.h>\n+mach_port_t pthread_mach_thread_np(pthread_t);\n+#endif /* _MACH_PORT_T */\n+#endif /* __APPLE__ */\n+\n+#endif  // PLASMA_COMPAT_H\ndiff --git a/cpp/src/plasma/events.h b/cpp/src/plasma/events.h\nindex 429148486..419c2ebfb 100644\n--- a/cpp/src/plasma/events.h\n+++ b/cpp/src/plasma/events.h\n@@ -66,7 +66,6 @@ class EventLoop {\n   /// Remove a file event handler from the event loop.\n   ///\n   /// @param fd The file descriptor of the event handler.\n-  /// @return Void.\n   void RemoveFileEvent(int fd);\n \n   /// Register a handler that will be called after a time slice of\n@@ -84,8 +83,6 @@ class EventLoop {\n   int RemoveTimer(int64_t timer_id);\n \n   /// \\brief Run the event loop.\n-  ///\n-  /// @return Void.\n   void Start();\n \n   /// \\brief Stop the event loop\ndiff --git a/cpp/src/plasma/eviction_policy.h b/cpp/src/plasma/eviction_policy.h\nindex dd1c87346..de33dabcb 100644\n--- a/cpp/src/plasma/eviction_policy.h\n+++ b/cpp/src/plasma/eviction_policy.h\n@@ -70,7 +70,6 @@ class EvictionPolicy {\n   /// cache.\n   ///\n   /// @param object_id The object ID of the object that was created.\n-  /// @return Void.\n   void object_created(const ObjectID& object_id);\n \n   /// This method will be called when the Plasma store needs more space, perhaps\n@@ -94,7 +93,6 @@ class EvictionPolicy {\n   /// @param object_id The ID of the object that is now being used.\n   /// @param objects_to_evict The object IDs that were chosen for eviction will\n   ///        be stored into this vector.\n-  /// @return Void.\n   void begin_object_access(const ObjectID& object_id,\n                            std::vector<ObjectID>* objects_to_evict);\n \n@@ -106,7 +104,6 @@ class EvictionPolicy {\n   /// @param object_id The ID of the object that is no longer being used.\n   /// @param objects_to_evict The object IDs that were chosen for eviction will\n   ///        be stored into this vector.\n-  /// @return Void.\n   void end_object_access(const ObjectID& object_id,\n                          std::vector<ObjectID>* objects_to_evict);\n \ndiff --git a/cpp/src/plasma/fling.cc b/cpp/src/plasma/fling.cc\nindex 79da4f43a..b84648b25 100644\n--- a/cpp/src/plasma/fling.cc\n+++ b/cpp/src/plasma/fling.cc\n@@ -37,6 +37,9 @@ int send_fd(int conn, int fd) {\n   init_msg(&msg, &iov, buf, sizeof(buf));\n \n   struct cmsghdr* header = CMSG_FIRSTHDR(&msg);\n+  if (header == nullptr) {\n+    return -1;\n+  }\n   header->cmsg_level = SOL_SOCKET;\n   header->cmsg_type = SCM_RIGHTS;\n   header->cmsg_len = CMSG_LEN(sizeof(int));\n@@ -64,8 +67,9 @@ int recv_fd(int conn) {\n   for (struct cmsghdr* header = CMSG_FIRSTHDR(&msg); header != NULL;\n        header = CMSG_NXTHDR(&msg, header))\n     if (header->cmsg_level == SOL_SOCKET && header->cmsg_type == SCM_RIGHTS) {\n-      ssize_t count =\n-          (header->cmsg_len - (CMSG_DATA(header) - (unsigned char*)header)) / sizeof(int);\n+      ssize_t count = (header->cmsg_len -\n+                       (CMSG_DATA(header) - reinterpret_cast<unsigned char*>(header))) /\n+                      sizeof(int);\n       for (int i = 0; i < count; ++i) {\n         int fd = (reinterpret_cast<int*>(CMSG_DATA(header)))[i];\n         if (found_fd == -1) {\ndiff --git a/cpp/src/plasma/io.cc b/cpp/src/plasma/io.cc\nindex 9bb433990..afe705332 100644\n--- a/cpp/src/plasma/io.cc\n+++ b/cpp/src/plasma/io.cc\n@@ -26,10 +26,6 @@\n \n using arrow::Status;\n \n-/* Number of times we try binding to a socket. */\n-#define NUM_BIND_ATTEMPTS 5\n-#define BIND_TIMEOUT_MS 100\n-\n /* Number of times we try connecting to a socket. */\n #define NUM_CONNECT_ATTEMPTS 50\n #define CONNECT_TIMEOUT_MS 100\n@@ -134,7 +130,8 @@ int bind_ipc_sock(const std::string& pathname, bool shall_listen) {\n   }\n   strncpy(socket_address.sun_path, pathname.c_str(), pathname.size() + 1);\n \n-  if (bind(socket_fd, (struct sockaddr*)&socket_address, sizeof(socket_address)) != 0) {\n+  if (bind(socket_fd, reinterpret_cast<struct sockaddr*>(&socket_address),\n+           sizeof(socket_address)) != 0) {\n     ARROW_LOG(ERROR) << \"Bind failed for pathname \" << pathname;\n     close(socket_fd);\n     return -1;\n@@ -197,8 +194,8 @@ int connect_ipc_sock(const std::string& pathname) {\n   }\n   strncpy(socket_address.sun_path, pathname.c_str(), pathname.size() + 1);\n \n-  if (connect(socket_fd, (struct sockaddr*)&socket_address, sizeof(socket_address)) !=\n-      0) {\n+  if (connect(socket_fd, reinterpret_cast<struct sockaddr*>(&socket_address),\n+              sizeof(socket_address)) != 0) {\n     close(socket_fd);\n     return -1;\n   }\n@@ -227,6 +224,7 @@ uint8_t* read_message_async(int sock) {\n   uint8_t* message = reinterpret_cast<uint8_t*>(malloc(size));\n   s = ReadBytes(sock, message, size);\n   if (!s.ok()) {\n+    free(message);\n     /* The other side has closed the socket. */\n     ARROW_LOG(DEBUG) << \"Socket has been closed, or some other error has occurred.\";\n     close(sock);\ndiff --git a/cpp/src/plasma/io.h b/cpp/src/plasma/io.h\nindex ef96c06cc..4beb1346b 100644\n--- a/cpp/src/plasma/io.h\n+++ b/cpp/src/plasma/io.h\n@@ -27,6 +27,7 @@\n #include <vector>\n \n #include \"arrow/status.h\"\n+#include \"plasma/compat.h\"\n \n // TODO(pcm): Replace our own custom message header (message type,\n // message length, plasma protocol verion) with one that is serialized\ndiff --git a/cpp/src/plasma/plasma.h b/cpp/src/plasma/plasma.h\nindex 476002f68..603ff8a4f 100644\n--- a/cpp/src/plasma/plasma.h\n+++ b/cpp/src/plasma/plasma.h\n@@ -31,6 +31,8 @@\n #include <unordered_map>\n #include <unordered_set>\n \n+#include \"plasma/compat.h\"\n+\n #include \"arrow/status.h\"\n #include \"arrow/util/logging.h\"\n #include \"plasma/common.h\"\ndiff --git a/cpp/src/plasma/protocol.cc b/cpp/src/plasma/protocol.cc\nindex 305e3d5d9..2261b6a62 100644\n--- a/cpp/src/plasma/protocol.cc\n+++ b/cpp/src/plasma/protocol.cc\n@@ -79,8 +79,9 @@ Status SendCreateReply(int sock, ObjectID object_id, PlasmaObject* object,\n   PlasmaObjectSpec plasma_object(object->handle.store_fd, object->handle.mmap_size,\n                                  object->data_offset, object->data_size,\n                                  object->metadata_offset, object->metadata_size);\n-  auto message = CreatePlasmaCreateReply(fbb, fbb.CreateString(object_id.binary()),\n-                                         &plasma_object, (PlasmaError)error_code);\n+  auto message =\n+      CreatePlasmaCreateReply(fbb, fbb.CreateString(object_id.binary()), &plasma_object,\n+                              static_cast<PlasmaError>(error_code));\n   return PlasmaSend(sock, MessageType_PlasmaCreateReply, &fbb, message);\n }\n \n@@ -123,7 +124,7 @@ Status ReadSealRequest(uint8_t* data, size_t size, ObjectID* object_id,\n Status SendSealReply(int sock, ObjectID object_id, int error) {\n   flatbuffers::FlatBufferBuilder fbb;\n   auto message = CreatePlasmaSealReply(fbb, fbb.CreateString(object_id.binary()),\n-                                       (PlasmaError)error);\n+                                       static_cast<PlasmaError>(error));\n   return PlasmaSend(sock, MessageType_PlasmaSealReply, &fbb, message);\n }\n \n@@ -154,7 +155,7 @@ Status ReadReleaseRequest(uint8_t* data, size_t size, ObjectID* object_id) {\n Status SendReleaseReply(int sock, ObjectID object_id, int error) {\n   flatbuffers::FlatBufferBuilder fbb;\n   auto message = CreatePlasmaReleaseReply(fbb, fbb.CreateString(object_id.binary()),\n-                                          (PlasmaError)error);\n+                                          static_cast<PlasmaError>(error));\n   return PlasmaSend(sock, MessageType_PlasmaReleaseReply, &fbb, message);\n }\n \n@@ -185,7 +186,7 @@ Status ReadDeleteRequest(uint8_t* data, size_t size, ObjectID* object_id) {\n Status SendDeleteReply(int sock, ObjectID object_id, int error) {\n   flatbuffers::FlatBufferBuilder fbb;\n   auto message = CreatePlasmaDeleteReply(fbb, fbb.CreateString(object_id.binary()),\n-                                         (PlasmaError)error);\n+                                         static_cast<PlasmaError>(error));\n   return PlasmaSend(sock, MessageType_PlasmaDeleteReply, &fbb, message);\n }\n \n@@ -526,8 +527,8 @@ Status ReadDataReply(uint8_t* data, size_t size, ObjectID* object_id,\n   auto message = flatbuffers::GetRoot<PlasmaDataReply>(data);\n   DCHECK(verify_flatbuffer(message, data, size));\n   *object_id = ObjectID::from_binary(message->object_id()->str());\n-  *object_size = (int64_t)message->object_size();\n-  *metadata_size = (int64_t)message->metadata_size();\n+  *object_size = static_cast<int64_t>(message->object_size());\n+  *metadata_size = static_cast<int64_t>(message->metadata_size());\n   return Status::OK();\n }\n \ndiff --git a/cpp/src/plasma/store.cc b/cpp/src/plasma/store.cc\nindex aaa2bad67..210cce162 100644\n--- a/cpp/src/plasma/store.cc\n+++ b/cpp/src/plasma/store.cc\n@@ -458,8 +458,7 @@ void PlasmaStore::disconnect_client(int client_fd) {\n /// be\n /// buffered, and this will be called again when the send buffer has room.\n ///\n-/// @param client The client to send the notification to.\n-/// @return Void.\n+/// @param client_fd The client to send the notification to.\n void PlasmaStore::send_notifications(int client_fd) {\n   auto it = pending_notifications_.find(client_fd);\n \n@@ -666,7 +665,7 @@ class PlasmaStoreRunner {\n   std::unique_ptr<PlasmaStore> store_;\n };\n \n-static PlasmaStoreRunner* g_runner = nullptr;\n+static std::unique_ptr<PlasmaStoreRunner> g_runner = nullptr;\n \n void HandleSignal(int signal) {\n   if (signal == SIGTERM) {\n@@ -684,10 +683,9 @@ void start_server(char* socket_name, int64_t system_memory, std::string plasma_d\n   // to a client that has already died, the store could die.\n   signal(SIGPIPE, SIG_IGN);\n \n-  PlasmaStoreRunner runner;\n-  g_runner = &runner;\n+  g_runner.reset(new PlasmaStoreRunner());\n   signal(SIGTERM, HandleSignal);\n-  runner.Start(socket_name, system_memory, plasma_directory, hugepages_enabled);\n+  g_runner->Start(socket_name, system_memory, plasma_directory, hugepages_enabled);\n }\n \n }  // namespace plasma\ndiff --git a/cpp/src/plasma/store.h b/cpp/src/plasma/store.h\nindex 61a3a2456..d03d11f4e 100644\n--- a/cpp/src/plasma/store.h\n+++ b/cpp/src/plasma/store.h\n@@ -77,7 +77,6 @@ class PlasmaStore {\n   /// be called on objects that are returned by the eviction policy to evict.\n   ///\n   /// @param object_ids Object IDs of the objects to be deleted.\n-  /// @return Void.\n   void delete_objects(const std::vector<ObjectID>& object_ids);\n \n   /// Process a get request from a client. This method assumes that we will\n@@ -91,7 +90,6 @@ class PlasmaStore {\n   /// @param client The client making this request.\n   /// @param object_ids Object IDs of the objects to be gotten.\n   /// @param timeout_ms The timeout for the get request in milliseconds.\n-  /// @return Void.\n   void process_get_request(Client* client, const std::vector<ObjectID>& object_ids,\n                            int64_t timeout_ms);\n \n@@ -101,7 +99,6 @@ class PlasmaStore {\n   /// @param digest The digest of the object. This is used to tell if two\n   /// objects\n   ///        with the same object ID are the same.\n-  /// @return Void.\n   void seal_object(const ObjectID& object_id, unsigned char digest[]);\n \n   /// Check if the plasma store contains an object:\n@@ -115,25 +112,21 @@ class PlasmaStore {\n   ///\n   /// @param object_id The object ID of the object that is being released.\n   /// @param client The client making this request.\n-  /// @param Void.\n   void release_object(const ObjectID& object_id, Client* client);\n \n   /// Subscribe a file descriptor to updates about new sealed objects.\n   ///\n   /// @param client The client making this request.\n-  /// @return Void.\n   void subscribe_to_updates(Client* client);\n \n   /// Connect a new client to the PlasmaStore.\n   ///\n   /// @param listener_sock The socket that is listening to incoming connections.\n-  /// @return Void.\n   void connect_client(int listener_sock);\n \n   /// Disconnect a client from the PlasmaStore.\n   ///\n   /// @param client_fd The client file descriptor that is disconnected.\n-  /// @return Void.\n   void disconnect_client(int client_fd);\n \n   void send_notifications(int client_fd);\ndiff --git a/cpp/src/plasma/test/serialization_tests.cc b/cpp/src/plasma/test/serialization_tests.cc\nindex c76f5ce10..7c9d90133 100644\n--- a/cpp/src/plasma/test/serialization_tests.cc\n+++ b/cpp/src/plasma/test/serialization_tests.cc\n@@ -43,7 +43,7 @@ int create_temp_file(void) {\n  * Seek to the beginning of a file and read a message from it.\n  *\n  * @param fd File descriptor of the file.\n- * @param message type Message type that we expect in the file.\n+ * @param message_type Message type that we expect in the file.\n  *\n  * @return Pointer to the content of the message. Needs to be freed by the\n  * caller.\n@@ -226,7 +226,7 @@ TEST(PlasmaSerialization, DeleteReply) {\n \n TEST(PlasmaSerialization, StatusRequest) {\n   int fd = create_temp_file();\n-  int64_t num_objects = 2;\n+  constexpr int64_t num_objects = 2;\n   ObjectID object_ids[num_objects];\n   object_ids[0] = ObjectID::from_random();\n   object_ids[1] = ObjectID::from_random();\n@@ -249,10 +249,11 @@ TEST(PlasmaSerialization, StatusReply) {\n   ARROW_CHECK_OK(SendStatusReply(fd, object_ids, object_statuses, 2));\n   std::vector<uint8_t> data = read_message_from_file(fd, MessageType_PlasmaStatusReply);\n   int64_t num_objects = ReadStatusReply_num_objects(data.data(), data.size());\n-  ObjectID object_ids_read[num_objects];\n-  int object_statuses_read[num_objects];\n-  ARROW_CHECK_OK(ReadStatusReply(data.data(), data.size(), object_ids_read,\n-                                 object_statuses_read, num_objects));\n+\n+  std::vector<ObjectID> object_ids_read(num_objects);\n+  std::vector<int> object_statuses_read(num_objects);\n+  ARROW_CHECK_OK(ReadStatusReply(data.data(), data.size(), object_ids_read.data(),\n+                                 object_statuses_read.data(), num_objects));\n   ASSERT_EQ(object_ids[0], object_ids_read[0]);\n   ASSERT_EQ(object_ids[1], object_ids_read[1]);\n   ASSERT_EQ(object_statuses[0], object_statuses_read[0]);\ndiff --git a/cpp/src/plasma/thirdparty/ae/ae_epoll.c b/cpp/src/plasma/thirdparty/ae/ae_epoll.c\nindex 410aac70d..2f70550a9 100644\n--- a/cpp/src/plasma/thirdparty/ae/ae_epoll.c\n+++ b/cpp/src/plasma/thirdparty/ae/ae_epoll.c\n@@ -72,7 +72,8 @@ static void aeApiFree(aeEventLoop *eventLoop) {\n \n static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) {\n     aeApiState *state = eventLoop->apidata;\n-    struct epoll_event ee = {0}; /* avoid valgrind warning */\n+    struct epoll_event ee;\n+    memset(&ee, 0, sizeof(struct epoll_event)); // avoid valgrind warning\n     /* If the fd was already monitored for some event, we need a MOD\n      * operation. Otherwise we need an ADD operation. */\n     int op = eventLoop->events[fd].mask == AE_NONE ?\n@@ -89,7 +90,8 @@ static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) {\n \n static void aeApiDelEvent(aeEventLoop *eventLoop, int fd, int delmask) {\n     aeApiState *state = eventLoop->apidata;\n-    struct epoll_event ee = {0}; /* avoid valgrind warning */\n+    struct epoll_event ee;\n+    memset(&ee, 0, sizeof(struct epoll_event)); // avoid valgrind warning\n     int mask = eventLoop->events[fd].mask & (~delmask);\n \n     ee.events = 0;\ndiff --git a/java/memory/src/main/java/org/apache/arrow/memory/AllocationManager.java b/java/memory/src/main/java/org/apache/arrow/memory/AllocationManager.java\nindex c528937bf..6877c18f6 100644\n--- a/java/memory/src/main/java/org/apache/arrow/memory/AllocationManager.java\n+++ b/java/memory/src/main/java/org/apache/arrow/memory/AllocationManager.java\n@@ -140,7 +140,7 @@ private BufferLedger associate(final BaseAllocator allocator, final boolean reta\n         return existingLedger;\n       }\n \n-      final BufferLedger ledger = new BufferLedger(allocator, new ReleaseListener(allocator));\n+      final BufferLedger ledger = new BufferLedger(allocator);\n       if (retain) {\n         ledger.inc();\n       }\n@@ -151,54 +151,41 @@ private BufferLedger associate(final BaseAllocator allocator, final boolean reta\n     }\n   }\n \n-\n   /**\n    * The way that a particular BufferLedger communicates back to the AllocationManager that it\n    * now longer needs to hold\n    * a reference to particular piece of memory.\n+   * Can only be called when you already hold the writeLock.\n    */\n-  private class ReleaseListener {\n-\n-    private final BufferAllocator allocator;\n-\n-    public ReleaseListener(BufferAllocator allocator) {\n-      this.allocator = allocator;\n-    }\n-\n-    /**\n-     * Can only be called when you already hold the writeLock.\n-     */\n-    public void release() {\n-      allocator.assertOpen();\n+  private void release(final BufferLedger ledger) {\n+    final BaseAllocator allocator = ledger.getAllocator();\n+    allocator.assertOpen();\n \n-      final BufferLedger oldLedger = map.remove(allocator);\n-      oldLedger.allocator.dissociateLedger(oldLedger);\n+    final BufferLedger oldLedger = map.remove(allocator);\n+    oldLedger.allocator.dissociateLedger(oldLedger);\n \n-      if (oldLedger == owningLedger) {\n-        if (map.isEmpty()) {\n-          // no one else owns, lets release.\n-          oldLedger.allocator.releaseBytes(size);\n-          underlying.release();\n-          amDestructionTime = System.nanoTime();\n-          owningLedger = null;\n-        } else {\n-          // we need to change the owning allocator. we've been removed so we'll get whatever is\n-          // top of list\n-          BufferLedger newLedger = map.values().iterator().next();\n-\n-          // we'll forcefully transfer the ownership and not worry about whether we exceeded the\n-          // limit\n-          // since this consumer can't do anything with this.\n-          oldLedger.transferBalance(newLedger);\n-        }\n+    if (oldLedger == owningLedger) {\n+      if (map.isEmpty()) {\n+        // no one else owns, lets release.\n+        oldLedger.allocator.releaseBytes(size);\n+        underlying.release();\n+        amDestructionTime = System.nanoTime();\n+        owningLedger = null;\n       } else {\n-        if (map.isEmpty()) {\n-          throw new IllegalStateException(\"The final removal of a ledger should be connected to \" +\n-              \"the owning ledger.\");\n-        }\n+        // we need to change the owning allocator. we've been removed so we'll get whatever is\n+        // top of list\n+        BufferLedger newLedger = map.values().iterator().next();\n+\n+        // we'll forcefully transfer the ownership and not worry about whether we exceeded the\n+        // limit\n+        // since this consumer can't do anything with this.\n+        oldLedger.transferBalance(newLedger);\n+      }\n+    } else {\n+      if (map.isEmpty()) {\n+        throw new IllegalStateException(\"The final removal of a ledger should be connected to \" +\n+                \"the owning ledger.\");\n       }\n-\n-\n     }\n   }\n \n@@ -221,16 +208,22 @@ public void release() {\n     // correctly\n     private final long lCreationTime = System.nanoTime();\n     private final BaseAllocator allocator;\n-    private final ReleaseListener listener;\n     private final HistoricalLog historicalLog = BaseAllocator.DEBUG ? new HistoricalLog\n         (BaseAllocator.DEBUG_LOG_LENGTH,\n             \"BufferLedger[%d]\", 1)\n         : null;\n     private volatile long lDestructionTime = 0;\n \n-    private BufferLedger(BaseAllocator allocator, ReleaseListener listener) {\n+    private BufferLedger(BaseAllocator allocator) {\n       this.allocator = allocator;\n-      this.listener = listener;\n+    }\n+\n+    /**\n+     * Get the allocator for this ledger\n+     * @return allocator\n+     */\n+    private BaseAllocator getAllocator() {\n+      return allocator;\n     }\n \n     /**\n@@ -340,7 +333,7 @@ public int decrement(int decrement) {\n         outcome = bufRefCnt.addAndGet(-decrement);\n         if (outcome == 0) {\n           lDestructionTime = System.nanoTime();\n-          listener.release();\n+          release(this);\n         }\n       }\n \ndiff --git a/java/vector/src/main/java/org/apache/arrow/vector/complex/ListVector.java b/java/vector/src/main/java/org/apache/arrow/vector/complex/ListVector.java\nindex 470317f8c..6511efcb7 100644\n--- a/java/vector/src/main/java/org/apache/arrow/vector/complex/ListVector.java\n+++ b/java/vector/src/main/java/org/apache/arrow/vector/complex/ListVector.java\n@@ -368,7 +368,7 @@ public UnionVector promoteToUnion() {\n     return vector;\n   }\n \n-  private int lastSet;\n+  private int lastSet = 0;\n \n   public class Accessor extends BaseRepeatedAccessor {\n \ndiff --git a/java/vector/src/main/java/org/apache/arrow/vector/file/json/JsonFileReader.java b/java/vector/src/main/java/org/apache/arrow/vector/file/json/JsonFileReader.java\nindex 8bb0f26d9..c6ebd61aa 100644\n--- a/java/vector/src/main/java/org/apache/arrow/vector/file/json/JsonFileReader.java\n+++ b/java/vector/src/main/java/org/apache/arrow/vector/file/json/JsonFileReader.java\n@@ -44,6 +44,8 @@\n import org.apache.arrow.vector.Float4Vector;\n import org.apache.arrow.vector.Float8Vector;\n import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.NullableVarBinaryVector;\n+import org.apache.arrow.vector.NullableVarCharVector;\n import org.apache.arrow.vector.SmallIntVector;\n import org.apache.arrow.vector.TimeMicroVector;\n import org.apache.arrow.vector.TimeMilliVector;\n@@ -63,10 +65,10 @@\n import org.apache.arrow.vector.UInt4Vector;\n import org.apache.arrow.vector.UInt8Vector;\n import org.apache.arrow.vector.ValueVector;\n-import org.apache.arrow.vector.ValueVector.Mutator;\n import org.apache.arrow.vector.VarBinaryVector;\n import org.apache.arrow.vector.VarCharVector;\n import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.complex.ListVector;\n import org.apache.arrow.vector.dictionary.Dictionary;\n import org.apache.arrow.vector.dictionary.DictionaryProvider;\n import org.apache.arrow.vector.schema.ArrowVectorType;\n@@ -84,7 +86,6 @@\n import com.google.common.base.Objects;\n \n public class JsonFileReader implements AutoCloseable, DictionaryProvider {\n-  private final File inputFile;\n   private final JsonParser parser;\n   private final BufferAllocator allocator;\n   private Schema schema;\n@@ -93,7 +94,6 @@\n \n   public JsonFileReader(File inputFile, BufferAllocator allocator) throws JsonParseException, IOException {\n     super();\n-    this.inputFile = inputFile;\n     this.allocator = allocator;\n     MappingJsonFactory jsonFactory = new MappingJsonFactory();\n     this.parser = jsonFactory.createParser(inputFile);\n@@ -216,10 +216,9 @@ public VectorSchemaRoot read() throws IOException {\n     }\n   }\n \n-  /*\n-   * TODO: This method doesn't load some vectors correctly. For instance, it doesn't initialize\n-   * `lastSet` in ListVector, VarCharVector, NullableVarBinaryVector A better way of implementing\n-   * this function is to use `loadFieldBuffers` methods in FieldVector.\n+  /**\n+   * TODO: A better way of implementing this function is to use `loadFieldBuffers` methods in\n+   * FieldVector to set the inner-vector data as done in `ArrowFileReader`.\n    */\n   private void readVector(Field field, FieldVector vector) throws JsonParseException, IOException {\n     List<ArrowVectorType> vectorTypes = field.getTypeLayout().getVectorTypes();\n@@ -234,29 +233,42 @@ private void readVector(Field field, FieldVector vector) throws JsonParseExcepti\n       if (started && !Objects.equal(field.getName(), name)) {\n         throw new IllegalArgumentException(\"Expected field \" + field.getName() + \" but got \" + name);\n       }\n+\n+      // Initialize the vector with required capacity\n       int count = readNextField(\"count\", Integer.class);\n+      vector.setInitialCapacity(count);\n       vector.allocateNew();\n-      vector.getMutator().setValueCount(count);\n+\n+      // Read inner vectors\n       for (int v = 0; v < vectorTypes.size(); v++) {\n         ArrowVectorType vectorType = vectorTypes.get(v);\n-        BufferBacked innerVector = fieldInnerVectors.get(v);\n+        ValueVector valueVector = (ValueVector) fieldInnerVectors.get(v);\n         nextFieldIs(vectorType.getName());\n         readToken(START_ARRAY);\n-        ValueVector valueVector = (ValueVector) innerVector;\n-\n         int innerVectorCount = vectorType.equals(OFFSET) ? count + 1 : count;\n-        valueVector.setInitialCapacity(innerVectorCount);\n-        valueVector.allocateNew();\n-\n         for (int i = 0; i < innerVectorCount; i++) {\n           parser.nextToken();\n           setValueFromParser(valueVector, i);\n         }\n-        Mutator mutator = valueVector.getMutator();\n-        mutator.setValueCount(innerVectorCount);\n         readToken(END_ARRAY);\n       }\n-      // if children\n+\n+      // Set lastSet before valueCount to prevent setValueCount from filling empty values\n+      switch (vector.getMinorType()) {\n+        case LIST:\n+          // ListVector starts lastSet from index 0, so lastSet value is always last index written + 1\n+          ((ListVector) vector).getMutator().setLastSet(count);\n+          break;\n+        case VARBINARY:\n+          ((NullableVarBinaryVector) vector).getMutator().setLastSet(count - 1);\n+          break;\n+        case VARCHAR:\n+          ((NullableVarCharVector) vector).getMutator().setLastSet(count - 1);\n+          break;\n+      }\n+      vector.getMutator().setValueCount(count);\n+\n+      // read child vectors, if any\n       List<Field> fields = field.getChildren();\n       if (!fields.isEmpty()) {\n         List<FieldVector> vectorChildren = vector.getChildrenFromFields();\ndiff --git a/java/vector/src/test/java/org/apache/arrow/vector/file/BaseFileTest.java b/java/vector/src/test/java/org/apache/arrow/vector/file/BaseFileTest.java\nindex c05d59049..ba62de0a6 100644\n--- a/java/vector/src/test/java/org/apache/arrow/vector/file/BaseFileTest.java\n+++ b/java/vector/src/test/java/org/apache/arrow/vector/file/BaseFileTest.java\n@@ -32,6 +32,7 @@\n import org.apache.arrow.vector.NullableDecimalVector;\n import org.apache.arrow.vector.NullableIntVector;\n import org.apache.arrow.vector.NullableTimeMilliVector;\n+import org.apache.arrow.vector.NullableVarBinaryVector;\n import org.apache.arrow.vector.NullableVarCharVector;\n import org.apache.arrow.vector.ValueVector.Accessor;\n import org.apache.arrow.vector.VectorSchemaRoot;\n@@ -541,4 +542,52 @@ public void writeUnionData(int count, NullableMapVector parent) {\n     writer.setValueCount(count);\n     varchar.release();\n   }\n+\n+  protected void writeVarBinaryData(int count, NullableMapVector parent) {\n+    Assert.assertTrue(count < 100);\n+    ComplexWriter writer = new ComplexWriterImpl(\"root\", parent);\n+    MapWriter rootWriter = writer.rootAsMap();\n+    ListWriter listWriter = rootWriter.list(\"list\");\n+    ArrowBuf varbin = allocator.buffer(count);\n+    for (int i = 0; i < count; i++) {\n+      varbin.setByte(i, i);\n+      listWriter.setPosition(i);\n+      listWriter.startList();\n+      for (int j = 0; j < i % 3; j++) {\n+        listWriter.varBinary().writeVarBinary(0, i + 1, varbin);\n+      }\n+      listWriter.endList();\n+    }\n+    writer.setValueCount(count);\n+    varbin.release();\n+  }\n+\n+  protected void validateVarBinary(int count, VectorSchemaRoot root) {\n+    Assert.assertEquals(count, root.getRowCount());\n+    ListVector listVector = (ListVector) root.getVector(\"list\");\n+    byte[] expectedArray = new byte[count];\n+    int numVarBinaryValues = 0;\n+    for (int i = 0; i < count; i++) {\n+      expectedArray[i] = (byte) i;\n+      Object obj = listVector.getAccessor().getObject(i);\n+      List<?> objList = (List) obj;\n+      if (i % 3 == 0) {\n+        Assert.assertTrue(objList.isEmpty());\n+      } else {\n+        byte[] expected = Arrays.copyOfRange(expectedArray, 0, i + 1);\n+        for (int j = 0; j < i % 3; j++) {\n+          byte[] result = (byte[]) objList.get(j);\n+          Assert.assertArrayEquals(result, expected);\n+          numVarBinaryValues++;\n+        }\n+      }\n+    }\n+\n+    // ListVector lastSet should be the index of last value + 1\n+    Assert.assertEquals(listVector.getMutator().getLastSet(), count);\n+\n+    // NullableVarBinaryVector lastSet should be the index of last value\n+    NullableVarBinaryVector binaryVector = (NullableVarBinaryVector) listVector.getChildrenFromFields().get(0);\n+    Assert.assertEquals(binaryVector.getMutator().getLastSet(), numVarBinaryValues - 1);\n+  }\n }\ndiff --git a/java/vector/src/test/java/org/apache/arrow/vector/file/TestArrowFile.java b/java/vector/src/test/java/org/apache/arrow/vector/file/TestArrowFile.java\nindex c483ba7de..81e58989f 100644\n--- a/java/vector/src/test/java/org/apache/arrow/vector/file/TestArrowFile.java\n+++ b/java/vector/src/test/java/org/apache/arrow/vector/file/TestArrowFile.java\n@@ -622,6 +622,47 @@ public void testWriteReadFixedSizeList() throws IOException {\n     }\n   }\n \n+  @Test\n+  public void testWriteReadVarBin() throws IOException {\n+    File file = new File(\"target/mytest_varbin.arrow\");\n+    ByteArrayOutputStream stream = new ByteArrayOutputStream();\n+    int count = COUNT;\n+\n+    // write\n+    try (\n+        BufferAllocator vectorAllocator = allocator.newChildAllocator(\"original vectors\", 0, Integer.MAX_VALUE);\n+        NullableMapVector parent = NullableMapVector.empty(\"parent\", vectorAllocator)) {\n+      writeVarBinaryData(count, parent);\n+      VectorSchemaRoot root = new VectorSchemaRoot(parent.getChild(\"root\"));\n+      validateVarBinary(count, root);\n+      write(parent.getChild(\"root\"), file, stream);\n+    }\n+\n+    // read from file\n+    try (\n+        BufferAllocator readerAllocator = allocator.newChildAllocator(\"reader\", 0, Integer.MAX_VALUE);\n+        FileInputStream fileInputStream = new FileInputStream(file);\n+        ArrowFileReader arrowReader = new ArrowFileReader(fileInputStream.getChannel(), readerAllocator)) {\n+      VectorSchemaRoot root = arrowReader.getVectorSchemaRoot();\n+      Schema schema = root.getSchema();\n+      LOGGER.debug(\"reading schema: \" + schema);\n+      Assert.assertTrue(arrowReader.loadNextBatch());\n+      validateVarBinary(count, root);\n+    }\n+\n+    // read from stream\n+    try (\n+        BufferAllocator readerAllocator = allocator.newChildAllocator(\"reader\", 0, Integer.MAX_VALUE);\n+        ByteArrayInputStream input = new ByteArrayInputStream(stream.toByteArray());\n+        ArrowStreamReader arrowReader = new ArrowStreamReader(input, readerAllocator)) {\n+      VectorSchemaRoot root = arrowReader.getVectorSchemaRoot();\n+      Schema schema = root.getSchema();\n+      LOGGER.debug(\"reading schema: \" + schema);\n+      Assert.assertTrue(arrowReader.loadNextBatch());\n+      validateVarBinary(count, root);\n+    }\n+  }\n+\n \n   /**\n    * Writes the contents of parents to file. If outStream is non-null, also writes it\ndiff --git a/java/vector/src/test/java/org/apache/arrow/vector/file/json/TestJSONFile.java b/java/vector/src/test/java/org/apache/arrow/vector/file/json/TestJSONFile.java\nindex 960567fc8..ee90d340d 100644\n--- a/java/vector/src/test/java/org/apache/arrow/vector/file/json/TestJSONFile.java\n+++ b/java/vector/src/test/java/org/apache/arrow/vector/file/json/TestJSONFile.java\n@@ -285,4 +285,33 @@ public void testSetStructLength() throws IOException {\n     }\n   }\n \n+  @Test\n+  public void testWriteReadVarBinJSON() throws IOException {\n+    File file = new File(\"target/mytest_varbin.json\");\n+    int count = COUNT;\n+\n+    // write\n+    try (\n+        BufferAllocator vectorAllocator = allocator.newChildAllocator(\"original vectors\", 0, Integer.MAX_VALUE);\n+        NullableMapVector parent = NullableMapVector.empty(\"parent\", vectorAllocator)) {\n+      writeVarBinaryData(count, parent);\n+      VectorSchemaRoot root = new VectorSchemaRoot(parent.getChild(\"root\"));\n+      validateVarBinary(count, root);\n+      writeJSON(file, new VectorSchemaRoot(parent.getChild(\"root\")), null);\n+    }\n+\n+    // read\n+    try (\n+        BufferAllocator readerAllocator = allocator.newChildAllocator(\"reader\", 0, Integer.MAX_VALUE)) {\n+      JsonFileReader reader = new JsonFileReader(file, readerAllocator);\n+      Schema schema = reader.start();\n+      LOGGER.debug(\"reading schema: \" + schema);\n+\n+      // initialize vectors\n+      try (VectorSchemaRoot root = reader.read();) {\n+        validateVarBinary(count, root);\n+      }\n+      reader.close();\n+    }\n+  }\n }\ndiff --git a/python/CMakeLists.txt b/python/CMakeLists.txt\nindex a636d51b4..d148d1105 100644\n--- a/python/CMakeLists.txt\n+++ b/python/CMakeLists.txt\n@@ -72,8 +72,8 @@ endif(CCACHE_FOUND)\n ############################################################\n \n include(BuildUtils)\n-include(SetupCxxFlags)\n include(CompilerInfo)\n+include(SetupCxxFlags)\n \n # Add common flags\n set(CMAKE_CXX_FLAGS \"${CXX_COMMON_FLAGS} ${CMAKE_CXX_FLAGS}\")\ndiff --git a/python/manylinux1/scripts/build_boost.sh b/python/manylinux1/scripts/build_boost.sh\nindex 3c11f3aeb..4650cde95 100755\n--- a/python/manylinux1/scripts/build_boost.sh\n+++ b/python/manylinux1/scripts/build_boost.sh\n@@ -16,10 +16,13 @@\n # specific language governing permissions and limitations\n # under the License.\n \n-wget --no-check-certificate http://downloads.sourceforge.net/project/boost/boost/1.60.0/boost_1_60_0.tar.gz -O /boost_1_60_0.tar.gz\n-tar xf boost_1_60_0.tar.gz\n-pushd /boost_1_60_0\n+BOOST_VERSION=1.65.1\n+BOOST_VERSION_UNDERSCORE=${BOOST_VERSION//\\./_}\n+\n+wget --no-check-certificate https://dl.bintray.com/boostorg/release/${BOOST_VERSION}/source/boost_${BOOST_VERSION_UNDERSCORE}.tar.gz -O /boost_${BOOST_VERSION_UNDERSCORE}.tar.gz\n+tar xf boost_${BOOST_VERSION_UNDERSCORE}.tar.gz\n+pushd /boost_${BOOST_VERSION_UNDERSCORE}\n ./bootstrap.sh\n ./bjam cxxflags=-fPIC cflags=-fPIC --prefix=/usr --with-filesystem --with-date_time --with-system --with-regex install\n popd\n-rm -rf boost_1_60_0.tar.gz boost_1_60_0\n+rm -rf boost_${BOOST_VERSION_UNDERSCORE}.tar.gz boost_${BOOST_VERSION_UNDERSCORE}\ndiff --git a/python/pyarrow/__init__.py b/python/pyarrow/__init__.py\nindex 0d76a35f4..ac0694822 100644\n--- a/python/pyarrow/__init__.py\n+++ b/python/pyarrow/__init__.py\n@@ -36,7 +36,7 @@\n                          time32, time64, timestamp, date32, date64,\n                          float16, float32, float64,\n                          binary, string, decimal,\n-                         list_, struct, dictionary, field,\n+                         list_, struct, dictionary, field, type_for_alias,\n                          DataType, NAType,\n                          Field,\n                          Schema,\ndiff --git a/python/pyarrow/array.pxi b/python/pyarrow/array.pxi\nindex eec618016..f402defc9 100644\n--- a/python/pyarrow/array.pxi\n+++ b/python/pyarrow/array.pxi\n@@ -16,58 +16,161 @@\n # under the License.\n \n \n-def array(object sequence, DataType type=None, MemoryPool memory_pool=None,\n-          size=None):\n+cdef _sequence_to_array(object sequence, object size, DataType type,\n+                        CMemoryPool* pool):\n+    cdef shared_ptr[CArray] out\n+    cdef int64_t c_size\n+    if type is None:\n+        with nogil:\n+            check_status(ConvertPySequence(sequence, pool, &out))\n+    else:\n+        if size is None:\n+            with nogil:\n+                check_status(\n+                    ConvertPySequence(\n+                        sequence, pool, &out, type.sp_type\n+                    )\n+                )\n+        else:\n+            c_size = size\n+            with nogil:\n+                check_status(\n+                    ConvertPySequence(\n+                        sequence, pool, &out, type.sp_type, c_size\n+                    )\n+                )\n+\n+    return pyarrow_wrap_array(out)\n+\n+\n+cdef _is_array_like(obj):\n+    try:\n+        import pandas\n+        return isinstance(obj, (np.ndarray, pd.Series, pd.Index, Categorical))\n+    except:\n+        return isinstance(obj, np.ndarray)\n+\n+\n+cdef _ndarray_to_array(object values, object mask, DataType type,\n+                       c_bool use_pandas_null_sentinels,\n+                       CMemoryPool* pool):\n+    cdef shared_ptr[CChunkedArray] chunked_out\n+    cdef shared_ptr[CDataType] c_type\n+\n+    dtype = values.dtype\n+\n+    if type is None and dtype != object:\n+        with nogil:\n+            check_status(NumPyDtypeToArrow(dtype, &c_type))\n+\n+    if type is not None:\n+        c_type = type.sp_type\n+\n+    with nogil:\n+        check_status(NdarrayToArrow(pool, values, mask,\n+                                    use_pandas_null_sentinels,\n+                                    c_type, &chunked_out))\n+\n+    if chunked_out.get().num_chunks() > 1:\n+        return pyarrow_wrap_chunked_array(chunked_out)\n+    else:\n+        return pyarrow_wrap_array(chunked_out.get().chunk(0))\n+\n+\n+cdef DataType _ensure_type(object type):\n+    if type is None:\n+        return None\n+    elif not isinstance(type, DataType):\n+        return type_for_alias(type)\n+    else:\n+        return type\n+\n+\n+def array(object obj, type=None, mask=None,\n+          MemoryPool memory_pool=None, size=None,\n+          from_pandas=False):\n     \"\"\"\n-    Create pyarrow.Array instance from a Python sequence\n+    Create pyarrow.Array instance from a Python object\n \n     Parameters\n     ----------\n-    sequence : sequence-like or iterable object of Python objects.\n-        If both type and size are specified may be a single use iterable.\n-    type : pyarrow.DataType, optional\n-        If not passed, will be inferred from the data\n+    obj : sequence, iterable, ndarray or Series\n+        If both type and size are specified may be a single use iterable. If\n+        not strongly-typed, Arrow type will be inferred for resulting array\n+    mask : array (boolean), optional\n+        Indicate which values are null (True) or not null (False).\n+    type : pyarrow.DataType\n+        Explicit type to attempt to coerce to, otherwise will be inferred from\n+        the data\n     memory_pool : pyarrow.MemoryPool, optional\n         If not passed, will allocate memory from the currently-set default\n         memory pool\n+\n     size : int64, optional\n         Size of the elements. If the imput is larger than size bail at this\n         length. For iterators, if size is larger than the input iterator this\n         will be treated as a \"max size\", but will involve an initial allocation\n         of size followed by a resize to the actual size (so if you know the\n         exact size specifying it correctly will give you better performance).\n+    from_pandas : boolean, default False\n+        Use pandas's semantics for inferring nulls from values in ndarray-like\n+        data. If passed, the mask tasks precendence, but if a value is unmasked\n+        (not-null), but still null according to pandas semantics, then it is\n+        null\n+\n+    Notes\n+    -----\n+    Localized timestamps will currently be returned as UTC (pandas's native\n+    representation).  Timezone-naive data will be implicitly interpreted as\n+    UTC.\n+\n+    Examples\n+    --------\n+    >>> import pandas as pd\n+    >>> import pyarrow as pa\n+    >>> pa.array(pd.Series([1, 2]))\n+    <pyarrow.array.Int64Array object at 0x7f674e4c0e10>\n+    [\n+      1,\n+      2\n+    ]\n+\n+    >>> import numpy as np\n+    >>> pa.array(pd.Series([1, 2]), np.array([0, 1],\n+    ... dtype=bool))\n+    <pyarrow.array.Int64Array object at 0x7f9019e11208>\n+    [\n+      1,\n+      NA\n+    ]\n \n     Returns\n     -------\n-    array : pyarrow.Array\n+    array : pyarrow.Array or pyarrow.ChunkedArray (if object data\n+    overflowed binary storage)\n     \"\"\"\n-    cdef:\n-        shared_ptr[CArray] sp_array\n-        CMemoryPool* pool\n-        int64_t c_size\n+    type = _ensure_type(type)\n+    cdef CMemoryPool* pool = maybe_unbox_memory_pool(memory_pool)\n \n-    pool = maybe_unbox_memory_pool(memory_pool)\n-    if type is None:\n-        with nogil:\n-            check_status(ConvertPySequence(sequence, pool, &sp_array))\n-    else:\n-        if size is None:\n-            with nogil:\n-                check_status(\n-                    ConvertPySequence(\n-                        sequence, pool, &sp_array, type.sp_type\n-                    )\n-                )\n-        else:\n-            c_size = size\n-            with nogil:\n-                check_status(\n-                    ConvertPySequence(\n-                        sequence, pool, &sp_array, type.sp_type, c_size\n-                    )\n-                )\n+    if _is_array_like(obj):\n+        if mask is not None:\n+            mask = get_series_values(mask)\n+\n+        values = get_series_values(obj)\n \n-    return pyarrow_wrap_array(sp_array)\n+        if isinstance(values, Categorical):\n+            return DictionaryArray.from_arrays(\n+                values.codes, values.categories.values,\n+                mask=mask, ordered=values.ordered,\n+                memory_pool=memory_pool)\n+        else:\n+            values, type = pdcompat.get_datetimetz_type(values, obj.dtype,\n+                                                        type)\n+            return _ndarray_to_array(values, mask, type, from_pandas, pool)\n+    else:\n+        if mask is not None:\n+            raise ValueError(\"Masks only supported with ndarray-like inputs\")\n+        return _sequence_to_array(obj, size, type, pool)\n \n \n def _normalize_slice(object arrow_obj, slice key):\n@@ -112,7 +215,7 @@ cdef class Array:\n         with nogil:\n             check_status(DebugPrint(deref(self.ap), 0))\n \n-    def cast(self, DataType target_type, safe=True):\n+    def cast(self, object target_type, safe=True):\n         \"\"\"\n         Cast array values to another data type\n \n@@ -130,42 +233,37 @@ cdef class Array:\n         cdef:\n             CCastOptions options\n             shared_ptr[CArray] result\n+            DataType type\n+\n+        type = _ensure_type(target_type)\n \n         if not safe:\n             options.allow_int_overflow = 1\n \n         with nogil:\n-            check_status(Cast(_context(), self.ap[0], target_type.sp_type,\n+            check_status(Cast(_context(), self.ap[0], type.sp_type,\n                               options, &result))\n \n         return pyarrow_wrap_array(result)\n \n     @staticmethod\n-    def from_pandas(obj, mask=None, DataType type=None,\n-                    timestamps_to_ms=False,\n-                    MemoryPool memory_pool=None):\n+    def from_pandas(obj, mask=None, type=None, MemoryPool memory_pool=None):\n         \"\"\"\n-        Convert pandas.Series to an Arrow Array.\n+        Convert pandas.Series to an Arrow Array, using pandas's semantics about\n+        what values indicate nulls. See pyarrow.array for more general\n+        conversion from arrays or sequences to Arrow arrays\n \n         Parameters\n         ----------\n-        series : pandas.Series or numpy.ndarray\n-\n-        mask : pandas.Series or numpy.ndarray, optional\n-            boolean mask if the object is null (True) or valid (False)\n-\n+        sequence : ndarray, Inded Series\n+        mask : array (boolean), optional\n+            Indicate which values are null (True) or not null (False)\n         type : pyarrow.DataType\n-            Explicit type to attempt to coerce to\n-\n-        timestamps_to_ms : bool, optional\n-            Convert datetime columns to ms resolution. This is needed for\n-            compatibility with other functionality like Parquet I/O which\n-            only supports milliseconds.\n-\n-            .. deprecated:: 0.7.0\n-\n-        memory_pool: MemoryPool, optional\n-            Specific memory pool to use to allocate the resulting Arrow array.\n+            Explicit type to attempt to coerce to, otherwise will be inferred\n+            from the data\n+        memory_pool : pyarrow.MemoryPool, optional\n+            If not passed, will allocate memory from the currently-set default\n+            memory pool\n \n         Notes\n         -----\n@@ -173,78 +271,13 @@ cdef class Array:\n         representation).  Timezone-naive data will be implicitly interpreted as\n         UTC.\n \n-        Examples\n-        --------\n-\n-        >>> import pandas as pd\n-        >>> import pyarrow as pa\n-        >>> pa.Array.from_pandas(pd.Series([1, 2]))\n-        <pyarrow.array.Int64Array object at 0x7f674e4c0e10>\n-        [\n-          1,\n-          2\n-        ]\n-\n-        >>> import numpy as np\n-        >>> pa.Array.from_pandas(pd.Series([1, 2]), np.array([0, 1],\n-        ... dtype=bool))\n-        <pyarrow.array.Int64Array object at 0x7f9019e11208>\n-        [\n-          1,\n-          NA\n-        ]\n-\n         Returns\n         -------\n         array : pyarrow.Array or pyarrow.ChunkedArray (if object data\n-        overflowed binary storage)\n+        overflows binary buffer)\n         \"\"\"\n-        cdef:\n-            shared_ptr[CArray] out\n-            shared_ptr[CChunkedArray] chunked_out\n-            shared_ptr[CDataType] c_type\n-            CMemoryPool* pool\n-\n-        if mask is not None:\n-            mask = get_series_values(mask)\n-\n-        values = get_series_values(obj)\n-        pool = maybe_unbox_memory_pool(memory_pool)\n-\n-        if isinstance(values, Categorical):\n-            return DictionaryArray.from_arrays(\n-                values.codes, values.categories.values,\n-                mask=mask, ordered=values.ordered,\n-                memory_pool=memory_pool)\n-        elif values.dtype == object:\n-            # Object dtype undergoes a different conversion path as more type\n-            # inference may be needed\n-            if type is not None:\n-                c_type = type.sp_type\n-            with nogil:\n-                check_status(PandasObjectsToArrow(\n-                    pool, values, mask, c_type, &chunked_out))\n-\n-            if chunked_out.get().num_chunks() > 1:\n-                return pyarrow_wrap_chunked_array(chunked_out)\n-            else:\n-                out = chunked_out.get().chunk(0)\n-        else:\n-            values, type = pdcompat.maybe_coerce_datetime64(\n-                values, obj.dtype, type, timestamps_to_ms=timestamps_to_ms)\n-\n-            if type is None:\n-                dtype = values.dtype\n-                with nogil:\n-                    check_status(NumPyDtypeToArrow(dtype, &c_type))\n-            else:\n-                c_type = type.sp_type\n-\n-            with nogil:\n-                check_status(PandasToArrow(\n-                    pool, values, mask, c_type, &out))\n-\n-        return pyarrow_wrap_array(out)\n+        return array(obj, mask=mask, type=type, memory_pool=memory_pool,\n+                     from_pandas=True)\n \n     property null_count:\n \ndiff --git a/python/pyarrow/includes/libarrow.pxd b/python/pyarrow/includes/libarrow.pxd\nindex 5e6708871..fc17d1c06 100644\n--- a/python/pyarrow/includes/libarrow.pxd\n+++ b/python/pyarrow/includes/libarrow.pxd\n@@ -766,13 +766,10 @@ cdef extern from \"arrow/python/api.h\" namespace \"arrow::py\" nogil:\n \n     CStatus NumPyDtypeToArrow(object dtype, shared_ptr[CDataType]* type)\n \n-    CStatus PandasToArrow(CMemoryPool* pool, object ao, object mo,\n-                          const shared_ptr[CDataType]& type,\n-                          shared_ptr[CArray]* out)\n-\n-    CStatus PandasObjectsToArrow(CMemoryPool* pool, object ao, object mo,\n-                                 const shared_ptr[CDataType]& type,\n-                                 shared_ptr[CChunkedArray]* out)\n+    CStatus NdarrayToArrow(CMemoryPool* pool, object ao, object mo,\n+                           c_bool use_pandas_null_sentinels,\n+                           const shared_ptr[CDataType]& type,\n+                           shared_ptr[CChunkedArray]* out)\n \n     CStatus NdarrayToTensor(CMemoryPool* pool, object ao,\n                             shared_ptr[CTensor]* out)\ndiff --git a/python/pyarrow/pandas_compat.py b/python/pyarrow/pandas_compat.py\nindex d1e6f5a80..be48aeb44 100644\n--- a/python/pyarrow/pandas_compat.py\n+++ b/python/pyarrow/pandas_compat.py\n@@ -203,7 +203,7 @@ def construct_metadata(df, column_names, index_levels, preserve_index, types):\n     }\n \n \n-def dataframe_to_arrays(df, timestamps_to_ms, schema, preserve_index):\n+def dataframe_to_arrays(df, schema, preserve_index):\n     names = []\n     arrays = []\n     index_columns = []\n@@ -223,15 +223,13 @@ def dataframe_to_arrays(df, timestamps_to_ms, schema, preserve_index):\n             field = schema.field_by_name(name)\n             type = getattr(field, \"type\", None)\n \n-        array = pa.Array.from_pandas(\n-            col, type=type, timestamps_to_ms=timestamps_to_ms\n-        )\n+        array = pa.array(col, from_pandas=True, type=type)\n         arrays.append(array)\n         names.append(name)\n         types.append(array.type)\n \n     for i, column in enumerate(index_columns):\n-        array = pa.Array.from_pandas(column, timestamps_to_ms=timestamps_to_ms)\n+        array = pa.array(column)\n         arrays.append(array)\n         names.append(index_level_name(column, i))\n         types.append(array.type)\n@@ -242,25 +240,15 @@ def dataframe_to_arrays(df, timestamps_to_ms, schema, preserve_index):\n     return names, arrays, metadata\n \n \n-def maybe_coerce_datetime64(values, dtype, type_, timestamps_to_ms=False):\n-    if timestamps_to_ms:\n-        import warnings\n-        warnings.warn('timestamps_to_ms=True is deprecated', FutureWarning)\n-\n+def get_datetimetz_type(values, dtype, type_):\n     from pyarrow.compat import DatetimeTZDtype\n \n     if values.dtype.type != np.datetime64:\n         return values, type_\n \n-    coerce_ms = timestamps_to_ms and values.dtype != 'datetime64[ms]'\n-\n-    if coerce_ms:\n-        values = values.astype('datetime64[ms]')\n-        type_ = pa.timestamp('ms')\n-\n     if isinstance(dtype, DatetimeTZDtype):\n         tz = dtype.tz\n-        unit = 'ms' if coerce_ms else dtype.unit\n+        unit = dtype.unit\n         type_ = pa.timestamp(unit, tz)\n     elif type_ is None:\n         # Trust the NumPy dtype\ndiff --git a/python/pyarrow/scalar.pxi b/python/pyarrow/scalar.pxi\nindex 3a847f77c..c37ed3b20 100644\n--- a/python/pyarrow/scalar.pxi\n+++ b/python/pyarrow/scalar.pxi\n@@ -348,10 +348,10 @@ cdef class StructValue(ArrayValue):\n \n cdef dict _scalar_classes = {\n     _Type_BOOL: BooleanValue,\n-    _Type_UINT8: Int8Value,\n-    _Type_UINT16: Int16Value,\n-    _Type_UINT32: Int32Value,\n-    _Type_UINT64: Int64Value,\n+    _Type_UINT8: UInt8Value,\n+    _Type_UINT16: UInt16Value,\n+    _Type_UINT32: UInt32Value,\n+    _Type_UINT64: UInt64Value,\n     _Type_INT8: Int8Value,\n     _Type_INT16: Int16Value,\n     _Type_INT32: Int32Value,\ndiff --git a/python/pyarrow/table.pxi b/python/pyarrow/table.pxi\nindex 028797e45..e5422a5be 100644\n--- a/python/pyarrow/table.pxi\n+++ b/python/pyarrow/table.pxi\n@@ -575,7 +575,7 @@ cdef class RecordBatch:\n         pyarrow.RecordBatch\n         \"\"\"\n         names, arrays, metadata = pdcompat.dataframe_to_arrays(\n-            df, False, schema, preserve_index\n+            df, schema, preserve_index\n         )\n         return cls.from_arrays(arrays, names, metadata)\n \n@@ -714,21 +714,13 @@ cdef class Table:\n         return result\n \n     @classmethod\n-    def from_pandas(cls, df, bint timestamps_to_ms=False,\n-                    Schema schema=None, bint preserve_index=True):\n+    def from_pandas(cls, df, Schema schema=None, bint preserve_index=True):\n         \"\"\"\n         Convert pandas.DataFrame to an Arrow Table\n \n         Parameters\n         ----------\n         df : pandas.DataFrame\n-        timestamps_to_ms : bool\n-            Convert datetime columns to ms resolution. This is needed for\n-            compability with other functionality like Parquet I/O which\n-            only supports milliseconds.\n-\n-            .. deprecated:: 0.7.0\n-\n         schema : pyarrow.Schema, optional\n             The expected schema of the Arrow Table. This can be used to\n             indicate the type of columns if we cannot infer it automatically.\n@@ -754,7 +746,6 @@ cdef class Table:\n         \"\"\"\n         names, arrays, metadata = pdcompat.dataframe_to_arrays(\n             df,\n-            timestamps_to_ms=timestamps_to_ms,\n             schema=schema,\n             preserve_index=preserve_index\n         )\ndiff --git a/python/pyarrow/tests/test_array.py b/python/pyarrow/tests/test_array.py\nindex f316417ca..3bf392686 100644\n--- a/python/pyarrow/tests/test_array.py\n+++ b/python/pyarrow/tests/test_array.py\n@@ -149,6 +149,14 @@ def test_array_factory_invalid_type():\n         pa.array(arr)\n \n \n+def test_array_ref_to_ndarray_base():\n+    arr = np.array([1, 2, 3])\n+\n+    refcount = sys.getrefcount(arr)\n+    arr2 = pa.array(arr)  # noqa\n+    assert sys.getrefcount(arr) == (refcount + 1)\n+\n+\n def test_dictionary_from_numpy():\n     indices = np.repeat([0, 1, 2], 2)\n     dictionary = np.array(['foo', 'bar', 'baz'], dtype=object)\n@@ -170,8 +178,8 @@ def test_dictionary_from_boxed_arrays():\n     indices = np.repeat([0, 1, 2], 2)\n     dictionary = np.array(['foo', 'bar', 'baz'], dtype=object)\n \n-    iarr = pa.Array.from_pandas(indices)\n-    darr = pa.Array.from_pandas(dictionary)\n+    iarr = pa.array(indices)\n+    darr = pa.array(dictionary)\n \n     d1 = pa.DictionaryArray.from_arrays(iarr, darr)\n \n@@ -201,9 +209,9 @@ def test_dictionary_with_pandas():\n \n def test_list_from_arrays():\n     offsets_arr = np.array([0, 2, 5, 8], dtype='i4')\n-    offsets = pa.Array.from_pandas(offsets_arr, type=pa.int32())\n+    offsets = pa.array(offsets_arr, type='int32')\n     pyvalues = [b'a', b'b', b'c', b'd', b'e', b'f', b'g', b'h']\n-    values = pa.array(pyvalues, type=pa.binary())\n+    values = pa.array(pyvalues, type='binary')\n \n     result = pa.ListArray.from_arrays(offsets, values)\n     expected = pa.array([pyvalues[:2], pyvalues[2:5], pyvalues[5:8]])\n@@ -214,22 +222,22 @@ def test_list_from_arrays():\n def _check_cast_case(case, safe=True):\n     in_data, in_type, out_data, out_type = case\n \n-    in_arr = pa.Array.from_pandas(in_data, type=in_type)\n+    in_arr = pa.array(in_data, type=in_type)\n \n     casted = in_arr.cast(out_type, safe=safe)\n-    expected = pa.Array.from_pandas(out_data, type=out_type)\n+    expected = pa.array(out_data, type=out_type)\n     assert casted.equals(expected)\n \n \n def test_cast_integers_safe():\n     safe_cases = [\n-        (np.array([0, 1, 2, 3], dtype='i1'), pa.int8(),\n+        (np.array([0, 1, 2, 3], dtype='i1'), 'int8',\n          np.array([0, 1, 2, 3], dtype='i4'), pa.int32()),\n-        (np.array([0, 1, 2, 3], dtype='i1'), pa.int8(),\n+        (np.array([0, 1, 2, 3], dtype='i1'), 'int8',\n          np.array([0, 1, 2, 3], dtype='u4'), pa.uint16()),\n-        (np.array([0, 1, 2, 3], dtype='i1'), pa.int8(),\n+        (np.array([0, 1, 2, 3], dtype='i1'), 'int8',\n          np.array([0, 1, 2, 3], dtype='u1'), pa.uint8()),\n-        (np.array([0, 1, 2, 3], dtype='i1'), pa.int8(),\n+        (np.array([0, 1, 2, 3], dtype='i1'), 'int8',\n          np.array([0, 1, 2, 3], dtype='f8'), pa.float64())\n     ]\n \n@@ -237,13 +245,13 @@ def test_cast_integers_safe():\n         _check_cast_case(case)\n \n     unsafe_cases = [\n-        (np.array([50000], dtype='i4'), pa.int32(), pa.int16()),\n-        (np.array([70000], dtype='i4'), pa.int32(), pa.uint16()),\n-        (np.array([-1], dtype='i4'), pa.int32(), pa.uint16()),\n-        (np.array([50000], dtype='u2'), pa.uint16(), pa.int16())\n+        (np.array([50000], dtype='i4'), 'int32', 'int16'),\n+        (np.array([70000], dtype='i4'), 'int32', 'uint16'),\n+        (np.array([-1], dtype='i4'), 'int32', 'uint16'),\n+        (np.array([50000], dtype='u2'), 'uint16', 'int16')\n     ]\n     for in_data, in_type, out_type in unsafe_cases:\n-        in_arr = pa.Array.from_pandas(in_data, type=in_type)\n+        in_arr = pa.array(in_data, type=in_type)\n \n         with pytest.raises(pa.ArrowInvalid):\n             in_arr.cast(out_type)\n@@ -252,11 +260,11 @@ def test_cast_integers_safe():\n def test_cast_integers_unsafe():\n     # We let NumPy do the unsafe casting\n     unsafe_cases = [\n-        (np.array([50000], dtype='i4'), pa.int32(),\n+        (np.array([50000], dtype='i4'), 'int32',\n          np.array([50000], dtype='i2'), pa.int16()),\n-        (np.array([70000], dtype='i4'), pa.int32(),\n+        (np.array([70000], dtype='i4'), 'int32',\n          np.array([70000], dtype='u2'), pa.uint16()),\n-        (np.array([-1], dtype='i4'), pa.int32(),\n+        (np.array([-1], dtype='i4'), 'int32',\n          np.array([-1], dtype='u2'), pa.uint16()),\n         (np.array([50000], dtype='u2'), pa.uint16(),\n          np.array([50000], dtype='i2'), pa.int16())\n@@ -315,3 +323,17 @@ def test_simple_type_construction():\n )\n def test_logical_type(type, expected):\n     assert get_logical_type(type) == expected\n+\n+\n+def test_array_conversions_no_sentinel_values():\n+    arr = np.array([1, 2, 3, 4], dtype='int8')\n+    refcount = sys.getrefcount(arr)\n+    arr2 = pa.array(arr)  # noqa\n+    assert sys.getrefcount(arr) == (refcount + 1)\n+\n+    assert arr2.type == 'int8'\n+\n+    arr3 = pa.array(np.array([1, np.nan, 2, 3, np.nan, 4], dtype='float32'),\n+                    type='float32')\n+    assert arr3.type == 'float32'\n+    assert arr3.null_count == 0\ndiff --git a/python/pyarrow/tests/test_convert_pandas.py b/python/pyarrow/tests/test_convert_pandas.py\nindex 5d56cde7d..182f3afc7 100644\n--- a/python/pyarrow/tests/test_convert_pandas.py\n+++ b/python/pyarrow/tests/test_convert_pandas.py\n@@ -18,7 +18,7 @@\n \n from collections import OrderedDict\n \n-from datetime import datetime, date, time\n+from datetime import date, time\n import unittest\n import decimal\n import json\n@@ -82,7 +82,7 @@ def _check_pandas_roundtrip(self, df, expected=None, nthreads=1,\n         tm.assert_frame_equal(result, expected, check_dtype=check_dtype)\n \n     def _check_series_roundtrip(self, s, type_=None):\n-        arr = pa.Array.from_pandas(s, type=type_)\n+        arr = pa.array(s, from_pandas=True, type=type_)\n \n         result = pd.Series(arr.to_pandas(), name=s.name)\n         if isinstance(arr.type, pa.TimestampType) and arr.type.tz is not None:\n@@ -93,7 +93,7 @@ def _check_series_roundtrip(self, s, type_=None):\n \n     def _check_array_roundtrip(self, values, expected=None, mask=None,\n                                type=None):\n-        arr = pa.Array.from_pandas(values, mask=mask, type=type)\n+        arr = pa.array(values, from_pandas=True, mask=mask, type=type)\n         result = arr.to_pandas()\n \n         values_nulls = pd.isnull(values)\n@@ -152,7 +152,7 @@ def test_float_nulls(self):\n         for name, arrow_dtype in dtypes:\n             values = np.random.randn(num_values).astype(name)\n \n-            arr = pa.Array.from_pandas(values, null_mask)\n+            arr = pa.array(values, from_pandas=True, mask=null_mask)\n             arrays.append(arr)\n             fields.append(pa.field(name, arrow_dtype))\n             values[null_mask] = np.nan\n@@ -223,7 +223,7 @@ def test_integer_with_nulls(self):\n         for name in int_dtypes:\n             values = np.random.randint(0, 100, size=num_values)\n \n-            arr = pa.Array.from_pandas(values, null_mask)\n+            arr = pa.array(values, mask=null_mask)\n             arrays.append(arr)\n \n             expected = values.astype('f8')\n@@ -244,8 +244,8 @@ def test_array_from_pandas_type_cast(self):\n \n         target_type = pa.int8()\n \n-        result = pa.Array.from_pandas(arr, type=target_type)\n-        expected = pa.Array.from_pandas(arr.astype('int8'))\n+        result = pa.array(arr, type=target_type)\n+        expected = pa.array(arr.astype('int8'))\n         assert result.equals(expected)\n \n     def test_boolean_no_nulls(self):\n@@ -266,7 +266,7 @@ def test_boolean_nulls(self):\n         mask = np.random.randint(0, 10, size=num_values) < 3\n         values = np.random.randint(0, 10, size=num_values) < 5\n \n-        arr = pa.Array.from_pandas(values, mask)\n+        arr = pa.array(values, mask=mask)\n \n         expected = values.astype(object)\n         expected[mask] = None\n@@ -292,7 +292,7 @@ def test_all_nulls_cast_numeric(self):\n         arr = np.array([None], dtype=object)\n \n         def _check_type(t):\n-            a2 = pa.Array.from_pandas(arr, type=t)\n+            a2 = pa.array(arr, type=t)\n             assert a2.type == t\n             assert a2[0].as_py() is None\n \n@@ -325,7 +325,7 @@ def test_bytes_exceed_2gb(self):\n         df = pd.DataFrame({\n             'strings': np.array([val] * 4000, dtype=object)\n         })\n-        arr = pa.Array.from_pandas(df['strings'])\n+        arr = pa.array(df['strings'])\n         assert isinstance(arr, pa.ChunkedArray)\n         assert arr.num_chunks == 2\n         arr = None\n@@ -365,19 +365,6 @@ def test_timestamps_notimezone_no_nulls(self):\n             expected_schema=schema,\n         )\n \n-    def test_timestamps_to_ms_explicit_schema(self):\n-        # ARROW-1328\n-        df = pd.DataFrame({'datetime': [datetime(2017, 1, 1)]})\n-        pa_type = pa.from_numpy_dtype(df['datetime'].dtype)\n-\n-        with tm.assert_produces_warning(FutureWarning,\n-                                        check_stacklevel=False):\n-            arr = pa.Array.from_pandas(df['datetime'], type=pa_type,\n-                                       timestamps_to_ms=True)\n-\n-        tm.assert_almost_equal(df['datetime'].values.astype('M8[ms]'),\n-                               arr.to_pandas())\n-\n     def test_timestamps_notimezone_nulls(self):\n         df = pd.DataFrame({\n             'datetime64': np.array([\n@@ -450,11 +437,11 @@ def test_date_objects_typed(self):\n         t32 = pa.date32()\n         t64 = pa.date64()\n \n-        a32 = pa.Array.from_pandas(arr, type=t32)\n-        a64 = pa.Array.from_pandas(arr, type=t64)\n+        a32 = pa.array(arr, type=t32)\n+        a64 = pa.array(arr, type=t64)\n \n-        a32_expected = pa.Array.from_pandas(arr_i4, mask=mask, type=t32)\n-        a64_expected = pa.Array.from_pandas(arr_i8, mask=mask, type=t64)\n+        a32_expected = pa.array(arr_i4, mask=mask, type=t32)\n+        a64_expected = pa.array(arr_i8, mask=mask, type=t64)\n \n         assert a32.equals(a32_expected)\n         assert a64.equals(a64_expected)\n@@ -481,8 +468,8 @@ def test_dates_from_integers(self):\n         arr = np.array([17259, 17260, 17261], dtype='int32')\n         arr2 = arr.astype('int64') * 86400000\n \n-        a1 = pa.Array.from_pandas(arr, type=t1)\n-        a2 = pa.Array.from_pandas(arr2, type=t2)\n+        a1 = pa.array(arr, type=t1)\n+        a2 = pa.array(arr2, type=t2)\n \n         expected = date(2017, 4, 3)\n         assert a1[0].as_py() == expected\n@@ -520,7 +507,7 @@ def test_column_of_arrays_to_py(self):\n             np.arange(1, dtype=dtype)\n         ])\n         type_ = pa.list_(pa.int8())\n-        parr = pa.Array.from_pandas(arr, type=type_)\n+        parr = pa.array(arr, type=type_)\n \n         assert parr[0].as_py() == list(range(10))\n         assert parr[1].as_py() == list(range(5))\n@@ -592,7 +579,7 @@ def test_column_of_lists_strided(self):\n     def test_nested_lists_all_none(self):\n         data = np.array([[None, None], None], dtype=object)\n \n-        arr = pa.Array.from_pandas(data)\n+        arr = pa.array(data)\n         expected = pa.array(list(data))\n         assert arr.equals(expected)\n         assert arr.type == pa.list_(pa.null())\n@@ -600,7 +587,7 @@ def test_nested_lists_all_none(self):\n         data2 = np.array([None, None, [None, None],\n                           np.array([None, None], dtype=object)],\n                          dtype=object)\n-        arr = pa.Array.from_pandas(data2)\n+        arr = pa.array(data2)\n         expected = pa.array([None, None, [None, None], [None, None]])\n         assert arr.equals(expected)\n \n@@ -760,7 +747,7 @@ def test_pytime_from_pandas(self):\n         t1 = pa.time64('us')\n \n         aobjs = np.array(pytimes + [None], dtype=object)\n-        parr = pa.Array.from_pandas(aobjs)\n+        parr = pa.array(aobjs)\n         assert parr.type == t1\n         assert parr[0].as_py() == pytimes[0]\n         assert parr[1].as_py() == pytimes[1]\n@@ -775,18 +762,18 @@ def test_pytime_from_pandas(self):\n         arr = np.array([_pytime_to_micros(v) for v in pytimes],\n                        dtype='int64')\n \n-        a1 = pa.Array.from_pandas(arr, type=pa.time64('us'))\n+        a1 = pa.array(arr, type=pa.time64('us'))\n         assert a1[0].as_py() == pytimes[0]\n \n-        a2 = pa.Array.from_pandas(arr * 1000, type=pa.time64('ns'))\n+        a2 = pa.array(arr * 1000, type=pa.time64('ns'))\n         assert a2[0].as_py() == pytimes[0]\n \n-        a3 = pa.Array.from_pandas((arr / 1000).astype('i4'),\n-                                  type=pa.time32('ms'))\n+        a3 = pa.array((arr / 1000).astype('i4'),\n+                      type=pa.time32('ms'))\n         assert a3[0].as_py() == pytimes[0].replace(microsecond=1000)\n \n-        a4 = pa.Array.from_pandas((arr / 1000000).astype('i4'),\n-                                  type=pa.time32('s'))\n+        a4 = pa.array((arr / 1000000).astype('i4'),\n+                      type=pa.time32('s'))\n         assert a4[0].as_py() == pytimes[0].replace(microsecond=0)\n \n     def test_arrow_time_to_pandas(self):\n@@ -809,14 +796,14 @@ def test_arrow_time_to_pandas(self):\n \n         null_mask = np.array([False, False, True], dtype=bool)\n \n-        a1 = pa.Array.from_pandas(arr, mask=null_mask, type=pa.time64('us'))\n-        a2 = pa.Array.from_pandas(arr * 1000, mask=null_mask,\n-                                  type=pa.time64('ns'))\n+        a1 = pa.array(arr, mask=null_mask, type=pa.time64('us'))\n+        a2 = pa.array(arr * 1000, mask=null_mask,\n+                      type=pa.time64('ns'))\n \n-        a3 = pa.Array.from_pandas((arr / 1000).astype('i4'), mask=null_mask,\n-                                  type=pa.time32('ms'))\n-        a4 = pa.Array.from_pandas((arr / 1000000).astype('i4'), mask=null_mask,\n-                                  type=pa.time32('s'))\n+        a3 = pa.array((arr / 1000).astype('i4'), mask=null_mask,\n+                      type=pa.time32('ms'))\n+        a4 = pa.array((arr / 1000000).astype('i4'), mask=null_mask,\n+                      type=pa.time32('s'))\n \n         names = ['time64[us]', 'time64[ns]', 'time32[ms]', 'time32[s]']\n         batch = pa.RecordBatch.from_arrays([a1, a2, a3, a4], names)\n@@ -841,8 +828,8 @@ def test_arrow_time_to_pandas(self):\n \n         tm.assert_frame_equal(df, expected_df)\n \n-    def _check_numpy_array_roundtrip(self, np_array):\n-        arr = pa.Array.from_pandas(np_array)\n+    def _check_array_from_pandas_roundtrip(self, np_array):\n+        arr = pa.array(np_array, from_pandas=True)\n         result = arr.to_pandas()\n         npt.assert_array_equal(result, np_array)\n \n@@ -853,7 +840,7 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-13T12:34:56.432539784',\n                 '2010-08-13T05:46:57.437699912'],\n                 dtype='datetime64[ns]')\n-        self._check_numpy_array_roundtrip(datetime64_ns)\n+        self._check_array_from_pandas_roundtrip(datetime64_ns)\n \n         datetime64_us = np.array([\n                 '2007-07-13T01:23:34.123456',\n@@ -861,7 +848,7 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-13T12:34:56.432539',\n                 '2010-08-13T05:46:57.437699'],\n                 dtype='datetime64[us]')\n-        self._check_numpy_array_roundtrip(datetime64_us)\n+        self._check_array_from_pandas_roundtrip(datetime64_us)\n \n         datetime64_ms = np.array([\n                 '2007-07-13T01:23:34.123',\n@@ -869,7 +856,7 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-13T12:34:56.432',\n                 '2010-08-13T05:46:57.437'],\n                 dtype='datetime64[ms]')\n-        self._check_numpy_array_roundtrip(datetime64_ms)\n+        self._check_array_from_pandas_roundtrip(datetime64_ms)\n \n         datetime64_s = np.array([\n                 '2007-07-13T01:23:34',\n@@ -877,7 +864,7 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-13T12:34:56',\n                 '2010-08-13T05:46:57'],\n                 dtype='datetime64[s]')\n-        self._check_numpy_array_roundtrip(datetime64_s)\n+        self._check_array_from_pandas_roundtrip(datetime64_s)\n \n         datetime64_d = np.array([\n                 '2007-07-13',\n@@ -885,11 +872,11 @@ def test_numpy_datetime64_columns(self):\n                 '2006-01-15',\n                 '2010-08-19'],\n                 dtype='datetime64[D]')\n-        self._check_numpy_array_roundtrip(datetime64_d)\n+        self._check_array_from_pandas_roundtrip(datetime64_d)\n \n     def test_all_nones(self):\n         def _check_series(s):\n-            converted = pa.Array.from_pandas(s)\n+            converted = pa.array(s)\n             assert isinstance(converted, pa.NullArray)\n             assert len(converted) == 3\n             assert converted.null_count == 3\ndiff --git a/python/pyarrow/tests/test_parquet.py b/python/pyarrow/tests/test_parquet.py\nindex eb23894f4..b0593fe88 100644\n--- a/python/pyarrow/tests/test_parquet.py\n+++ b/python/pyarrow/tests/test_parquet.py\n@@ -457,8 +457,26 @@ def test_column_of_arrays(tmpdir):\n \n @parquet\n def test_coerce_timestamps(tmpdir):\n+    from collections import OrderedDict\n     # ARROW-622\n-    df, schema = dataframe_with_arrays()\n+    arrays = OrderedDict()\n+    fields = [pa.field('datetime64',\n+                       pa.list_(pa.timestamp('ms')))]\n+    arrays['datetime64'] = [\n+        np.array(['2007-07-13T01:23:34.123456789',\n+                  None,\n+                  '2010-08-13T05:46:57.437699912'],\n+                 dtype='datetime64[ms]'),\n+        None,\n+        None,\n+        np.array(['2007-07-13T02',\n+                  None,\n+                  '2010-08-13T05:46:57.437699912'],\n+                 dtype='datetime64[ms]'),\n+    ]\n+\n+    df = pd.DataFrame(arrays)\n+    schema = pa.schema(fields)\n \n     filename = tmpdir.join('pandas_rountrip.parquet')\n     arrow_table = pa.Table.from_pandas(df, schema=schema)\n@@ -497,41 +515,41 @@ def test_column_of_lists(tmpdir):\n def test_date_time_types():\n     t1 = pa.date32()\n     data1 = np.array([17259, 17260, 17261], dtype='int32')\n-    a1 = pa.Array.from_pandas(data1, type=t1)\n+    a1 = pa.array(data1, type=t1)\n \n     t2 = pa.date64()\n     data2 = data1.astype('int64') * 86400000\n-    a2 = pa.Array.from_pandas(data2, type=t2)\n+    a2 = pa.array(data2, type=t2)\n \n     t3 = pa.timestamp('us')\n     start = pd.Timestamp('2000-01-01').value / 1000\n     data3 = np.array([start, start + 1, start + 2], dtype='int64')\n-    a3 = pa.Array.from_pandas(data3, type=t3)\n+    a3 = pa.array(data3, type=t3)\n \n     t4 = pa.time32('ms')\n     data4 = np.arange(3, dtype='i4')\n-    a4 = pa.Array.from_pandas(data4, type=t4)\n+    a4 = pa.array(data4, type=t4)\n \n     t5 = pa.time64('us')\n-    a5 = pa.Array.from_pandas(data4.astype('int64'), type=t5)\n+    a5 = pa.array(data4.astype('int64'), type=t5)\n \n     t6 = pa.time32('s')\n-    a6 = pa.Array.from_pandas(data4, type=t6)\n+    a6 = pa.array(data4, type=t6)\n \n     ex_t6 = pa.time32('ms')\n-    ex_a6 = pa.Array.from_pandas(data4 * 1000, type=ex_t6)\n+    ex_a6 = pa.array(data4 * 1000, type=ex_t6)\n \n     t7 = pa.timestamp('ns')\n     start = pd.Timestamp('2001-01-01').value\n     data7 = np.array([start, start + 1000, start + 2000],\n                      dtype='int64')\n-    a7 = pa.Array.from_pandas(data7, type=t7)\n+    a7 = pa.array(data7, type=t7)\n \n     t7_us = pa.timestamp('us')\n     start = pd.Timestamp('2001-01-01').value\n     data7_us = np.array([start, start + 1000, start + 2000],\n                         dtype='int64') // 1000\n-    a7_us = pa.Array.from_pandas(data7_us, type=t7_us)\n+    a7_us = pa.array(data7_us, type=t7_us)\n \n     table = pa.Table.from_arrays([a1, a2, a3, a4, a5, a6, a7],\n                                  ['date32', 'date64', 'timestamp[us]',\n@@ -575,7 +593,7 @@ def _assert_unsupported(array):\n             _write_table(table, buf, version=\"2.0\")\n \n     t7 = pa.time64('ns')\n-    a7 = pa.Array.from_pandas(data4.astype('int64'), type=t7)\n+    a7 = pa.array(data4.astype('int64'), type=t7)\n \n     _assert_unsupported(a7)\n \n@@ -1295,7 +1313,7 @@ def test_large_table_int32_overflow():\n \n     arr = np.ones(size, dtype='uint8')\n \n-    parr = pa.Array.from_pandas(arr, type=pa.uint8())\n+    parr = pa.array(arr, type=pa.uint8())\n \n     table = pa.Table.from_arrays([parr], names=['one'])\n     f = io.BytesIO()\ndiff --git a/python/pyarrow/tests/test_schema.py b/python/pyarrow/tests/test_schema.py\nindex 4bb6a5af7..c77be9805 100644\n--- a/python/pyarrow/tests/test_schema.py\n+++ b/python/pyarrow/tests/test_schema.py\n@@ -69,6 +69,56 @@ def test_type_list():\n     assert str(l2) == 'list<my_item: string>'\n \n \n+def test_type_comparisons():\n+    val = pa.int32()\n+    assert val == pa.int32()\n+    assert val == 'int32'\n+\n+    with pytest.raises(TypeError):\n+        val == 5\n+\n+\n+def test_type_for_alias():\n+    cases = [\n+        ('i1', pa.int8()),\n+        ('int8', pa.int8()),\n+        ('i2', pa.int16()),\n+        ('int16', pa.int16()),\n+        ('i4', pa.int32()),\n+        ('int32', pa.int32()),\n+        ('i8', pa.int64()),\n+        ('int64', pa.int64()),\n+        ('u1', pa.uint8()),\n+        ('uint8', pa.uint8()),\n+        ('u2', pa.uint16()),\n+        ('uint16', pa.uint16()),\n+        ('u4', pa.uint32()),\n+        ('uint32', pa.uint32()),\n+        ('u8', pa.uint64()),\n+        ('uint64', pa.uint64()),\n+        ('f4', pa.float32()),\n+        ('float32', pa.float32()),\n+        ('f8', pa.float64()),\n+        ('float64', pa.float64()),\n+        ('date32', pa.date32()),\n+        ('date64', pa.date64()),\n+        ('string', pa.string()),\n+        ('str', pa.string()),\n+        ('binary', pa.binary()),\n+        ('time32[s]', pa.time32('s')),\n+        ('time32[ms]', pa.time32('ms')),\n+        ('time64[us]', pa.time64('us')),\n+        ('time64[ns]', pa.time64('ns')),\n+        ('timestamp[s]', pa.timestamp('s')),\n+        ('timestamp[ms]', pa.timestamp('ms')),\n+        ('timestamp[us]', pa.timestamp('us')),\n+        ('timestamp[ns]', pa.timestamp('ns')),\n+    ]\n+\n+    for val, expected in cases:\n+        assert pa.type_for_alias(val) == expected\n+\n+\n def test_type_string():\n     t = pa.string()\n     assert str(t) == 'string'\ndiff --git a/python/pyarrow/types.pxi b/python/pyarrow/types.pxi\nindex b298e7402..316e09a6e 100644\n--- a/python/pyarrow/types.pxi\n+++ b/python/pyarrow/types.pxi\n@@ -72,11 +72,19 @@ cdef class DataType:\n     def __repr__(self):\n         return '{0.__class__.__name__}({0})'.format(self)\n \n-    def __richcmp__(DataType self, DataType other, int op):\n+    def __richcmp__(DataType self, object other, int op):\n+        cdef DataType other_type\n+        if not isinstance(other, DataType):\n+            if not isinstance(other, six.string_types):\n+                raise TypeError(other)\n+            other_type = type_for_alias(other)\n+        else:\n+            other_type = other\n+\n         if op == cp.Py_EQ:\n-            return self.type.Equals(deref(other.type))\n+            return self.type.Equals(deref(other_type.type))\n         elif op == cp.Py_NE:\n-            return not self.type.Equals(deref(other.type))\n+            return not self.type.Equals(deref(other_type.type))\n         else:\n             raise TypeError('Invalid comparison')\n \n@@ -922,6 +930,64 @@ def struct(fields):\n     return pyarrow_wrap_data_type(struct_type)\n \n \n+cdef dict _type_aliases = {\n+    'null': null,\n+    'i1': int8,\n+    'int8': int8,\n+    'i2': int16,\n+    'int16': int16,\n+    'i4': int32,\n+    'int32': int32,\n+    'i8': int64,\n+    'int64': int64,\n+    'u1': uint8,\n+    'uint8': uint8,\n+    'u2': uint16,\n+    'uint16': uint16,\n+    'u4': uint32,\n+    'uint32': uint32,\n+    'u8': uint64,\n+    'uint64': uint64,\n+    'f4': float32,\n+    'float32': float32,\n+    'f8': float64,\n+    'float64': float64,\n+    'string': string,\n+    'str': string,\n+    'utf8': string,\n+    'binary': binary,\n+    'date32': date32,\n+    'date64': date64,\n+    'time32[s]': time32('s'),\n+    'time32[ms]': time32('ms'),\n+    'time64[us]': time64('us'),\n+    'time64[ns]': time64('ns'),\n+    'timestamp[s]': timestamp('s'),\n+    'timestamp[ms]': timestamp('ms'),\n+    'timestamp[us]': timestamp('us'),\n+    'timestamp[ns]': timestamp('ns'),\n+}\n+\n+\n+def type_for_alias(name):\n+    \"\"\"\n+    Return DataType given a string alias if one exists\n+\n+    Returns\n+    -------\n+    type : DataType\n+    \"\"\"\n+    name = name.lower()\n+    try:\n+        alias = _type_aliases[name]\n+    except KeyError:\n+        raise ValueError('No type alias for {0}'.format(name))\n+\n+    if isinstance(alias, DataType):\n+        return alias\n+    return alias()\n+\n+\n def schema(fields):\n     \"\"\"\n     Construct pyarrow.Schema from collection of fields\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-23T02:21:01.016+0000",
                    "updated": "2017-10-23T02:21:01.016+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16215726",
                    "id": "16215726",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1228: ARROW-1134: [C++] Support for C++/CLI compilation, add NULLPTR define to avoid using nullptr in public headers\nURL: https://github.com/apache/arrow/pull/1228#issuecomment-338773816\n \n \n   We should probably have some kind of linting script to check that nullptr or `<mutex>` does not creep into a public header. I could write some kind of Python script for this if \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-23T19:41:44.881+0000",
                    "updated": "2017-10-23T19:41:44.881+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16215727",
                    "id": "16215727",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1228: ARROW-1134: [C++] Support for C++/CLI compilation, add NULLPTR define to avoid using nullptr in public headers\nURL: https://github.com/apache/arrow/pull/1228#issuecomment-338773816\n \n \n   We should probably have some kind of linting script to check that nullptr or `<mutex>` does not creep into a public header. I could write some kind of Python script for this \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-23T19:41:49.797+0000",
                    "updated": "2017-10-23T19:41:49.797+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16217838",
                    "id": "16217838",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1228: ARROW-1134: [C++] Support for C++/CLI compilation, add NULLPTR define to avoid using nullptr in public headers\nURL: https://github.com/apache/arrow/pull/1228#issuecomment-339159697\n \n \n   @xhochy @cpcloud I would suggest we should merge this and wait for more feedback from C++/CLI users\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-24T22:48:57.649+0000",
                    "updated": "2017-10-24T22:48:57.649+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16217839",
                    "id": "16217839",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1228: ARROW-1134: [C++] Support for C++/CLI compilation, add NULLPTR define to avoid using nullptr in public headers\nURL: https://github.com/apache/arrow/pull/1228#issuecomment-339159755\n \n \n   In the meantime we'll need to add a linting script so that this work does not get undone by a future patch\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-24T22:49:14.872+0000",
                    "updated": "2017-10-24T22:49:14.872+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16217967",
                    "id": "16217967",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "cpcloud commented on issue #1228: ARROW-1134: [C++] Support for C++/CLI compilation, add NULLPTR define to avoid using nullptr in public headers\nURL: https://github.com/apache/arrow/pull/1228#issuecomment-339185435\n \n \n   +1 LGTM\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-25T01:29:50.684+0000",
                    "updated": "2017-10-25T01:29:50.684+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16218587",
                    "id": "16218587",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 1228\n[https://github.com/apache/arrow/pull/1228]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-10-25T13:08:44.461+0000",
                    "updated": "2017-10-25T13:08:44.461+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13081467/comment/16218588",
                    "id": "16218588",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm closed pull request #1228: ARROW-1134: [C++] Support for C++/CLI compilation, add NULLPTR define to avoid using nullptr in public headers\nURL: https://github.com/apache/arrow/pull/1228\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/cpp/src/arrow/allocator.h b/cpp/src/arrow/allocator.h\nindex e00023dc4..c7780f19e 100644\n--- a/cpp/src/arrow/allocator.h\n+++ b/cpp/src/arrow/allocator.h\n@@ -24,6 +24,7 @@\n \n #include \"arrow/memory_pool.h\"\n #include \"arrow/status.h\"\n+#include \"arrow/util/macros.h\"\n \n namespace arrow {\n \n@@ -49,13 +50,13 @@ class stl_allocator {\n   template <class U>\n   stl_allocator(const stl_allocator<U>& rhs) noexcept : pool_(rhs.pool_) {}\n \n-  ~stl_allocator() { pool_ = nullptr; }\n+  ~stl_allocator() { pool_ = NULLPTR; }\n \n   pointer address(reference r) const noexcept { return std::addressof(r); }\n \n   const_pointer address(const_reference r) const noexcept { return std::addressof(r); }\n \n-  pointer allocate(size_type n, const void* /*hint*/ = nullptr) {\n+  pointer allocate(size_type n, const void* /*hint*/ = NULLPTR) {\n     uint8_t* data;\n     Status s = pool_->Allocate(n * sizeof(T), &data);\n     if (!s.ok()) throw std::bad_alloc();\ndiff --git a/cpp/src/arrow/array.h b/cpp/src/arrow/array.h\nindex 75dda4a75..a45563aa9 100644\n--- a/cpp/src/arrow/array.h\n+++ b/cpp/src/arrow/array.h\n@@ -183,14 +183,14 @@ class ARROW_EXPORT Array {\n \n   /// \\brief Return true if value at index is null. Does not boundscheck\n   bool IsNull(int64_t i) const {\n-    return null_bitmap_data_ != nullptr &&\n+    return null_bitmap_data_ != NULLPTR &&\n            BitUtil::BitNotSet(null_bitmap_data_, i + data_->offset);\n   }\n \n   /// \\brief Return true if value at index is valid (not null). Does not\n   /// boundscheck\n   bool IsValid(int64_t i) const {\n-    return null_bitmap_data_ != nullptr &&\n+    return null_bitmap_data_ != NULLPTR &&\n            BitUtil::GetBit(null_bitmap_data_, i + data_->offset);\n   }\n \n@@ -212,13 +212,13 @@ class ARROW_EXPORT Array {\n \n   /// Buffer for the null bitmap.\n   ///\n-  /// Note that for `null_count == 0`, this can be a `nullptr`.\n+  /// Note that for `null_count == 0`, this can be null.\n   /// This buffer does not account for any slice offset\n   std::shared_ptr<Buffer> null_bitmap() const { return data_->buffers[0]; }\n \n   /// Raw pointer to the null bitmap.\n   ///\n-  /// Note that for `null_count == 0`, this can be a `nullptr`.\n+  /// Note that for `null_count == 0`, this can be null.\n   /// This buffer does not account for any slice offset\n   const uint8_t* null_bitmap_data() const { return null_bitmap_data_; }\n \n@@ -270,7 +270,7 @@ class ARROW_EXPORT Array {\n     if (data->buffers.size() > 0 && data->buffers[0]) {\n       null_bitmap_data_ = data->buffers[0]->data();\n     } else {\n-      null_bitmap_data_ = nullptr;\n+      null_bitmap_data_ = NULLPTR;\n     }\n     data_ = data;\n   }\n@@ -299,7 +299,7 @@ class ARROW_EXPORT NullArray : public FlatArray {\n \n  private:\n   inline void SetData(const std::shared_ptr<ArrayData>& data) {\n-    null_bitmap_data_ = nullptr;\n+    null_bitmap_data_ = NULLPTR;\n     data->null_count = data->length;\n     data_ = data;\n   }\n@@ -310,7 +310,7 @@ class ARROW_EXPORT PrimitiveArray : public FlatArray {\n  public:\n   PrimitiveArray(const std::shared_ptr<DataType>& type, int64_t length,\n                  const std::shared_ptr<Buffer>& data,\n-                 const std::shared_ptr<Buffer>& null_bitmap = nullptr,\n+                 const std::shared_ptr<Buffer>& null_bitmap = NULLPTR,\n                  int64_t null_count = 0, int64_t offset = 0);\n \n   /// Does not account for any slice offset\n@@ -325,7 +325,7 @@ class ARROW_EXPORT PrimitiveArray : public FlatArray {\n   inline void SetData(const std::shared_ptr<ArrayData>& data) {\n     auto values = data->buffers[1];\n     this->Array::SetData(data);\n-    raw_values_ = values == nullptr ? nullptr : values->data();\n+    raw_values_ = values == NULLPTR ? NULLPTR : values->data();\n   }\n \n   explicit inline PrimitiveArray(const std::shared_ptr<ArrayData>& data) {\n@@ -349,7 +349,7 @@ class ARROW_EXPORT NumericArray : public PrimitiveArray {\n   NumericArray(\n       typename std::enable_if<TypeTraits<T1>::is_parameter_free, int64_t>::type length,\n       const std::shared_ptr<Buffer>& data,\n-      const std::shared_ptr<Buffer>& null_bitmap = nullptr, int64_t null_count = 0,\n+      const std::shared_ptr<Buffer>& null_bitmap = NULLPTR, int64_t null_count = 0,\n       int64_t offset = 0)\n       : PrimitiveArray(TypeTraits<T1>::type_singleton(), length, data, null_bitmap,\n                        null_count, offset) {}\n@@ -371,7 +371,7 @@ class ARROW_EXPORT BooleanArray : public PrimitiveArray {\n   explicit BooleanArray(const std::shared_ptr<ArrayData>& data);\n \n   BooleanArray(int64_t length, const std::shared_ptr<Buffer>& data,\n-               const std::shared_ptr<Buffer>& null_bitmap = nullptr,\n+               const std::shared_ptr<Buffer>& null_bitmap = NULLPTR,\n                int64_t null_count = 0, int64_t offset = 0);\n \n   bool Value(int64_t i) const {\n@@ -395,7 +395,7 @@ class ARROW_EXPORT ListArray : public Array {\n   ListArray(const std::shared_ptr<DataType>& type, int64_t length,\n             const std::shared_ptr<Buffer>& value_offsets,\n             const std::shared_ptr<Array>& values,\n-            const std::shared_ptr<Buffer>& null_bitmap = nullptr, int64_t null_count = 0,\n+            const std::shared_ptr<Buffer>& null_bitmap = NULLPTR, int64_t null_count = 0,\n             int64_t offset = 0);\n \n   /// \\brief Construct ListArray from array of offsets and child value array\n@@ -448,7 +448,7 @@ class ARROW_EXPORT BinaryArray : public FlatArray {\n \n   BinaryArray(int64_t length, const std::shared_ptr<Buffer>& value_offsets,\n               const std::shared_ptr<Buffer>& data,\n-              const std::shared_ptr<Buffer>& null_bitmap = nullptr,\n+              const std::shared_ptr<Buffer>& null_bitmap = NULLPTR,\n               int64_t null_count = 0, int64_t offset = 0);\n \n   // Return the pointer to the given elements bytes\n@@ -500,7 +500,7 @@ class ARROW_EXPORT BinaryArray : public FlatArray {\n   BinaryArray(const std::shared_ptr<DataType>& type, int64_t length,\n               const std::shared_ptr<Buffer>& value_offsets,\n               const std::shared_ptr<Buffer>& data,\n-              const std::shared_ptr<Buffer>& null_bitmap = nullptr,\n+              const std::shared_ptr<Buffer>& null_bitmap = NULLPTR,\n               int64_t null_count = 0, int64_t offset = 0);\n \n   const int32_t* raw_value_offsets_;\n@@ -515,7 +515,7 @@ class ARROW_EXPORT StringArray : public BinaryArray {\n \n   StringArray(int64_t length, const std::shared_ptr<Buffer>& value_offsets,\n               const std::shared_ptr<Buffer>& data,\n-              const std::shared_ptr<Buffer>& null_bitmap = nullptr,\n+              const std::shared_ptr<Buffer>& null_bitmap = NULLPTR,\n               int64_t null_count = 0, int64_t offset = 0);\n \n   // Construct a std::string\n@@ -538,7 +538,7 @@ class ARROW_EXPORT FixedSizeBinaryArray : public PrimitiveArray {\n \n   FixedSizeBinaryArray(const std::shared_ptr<DataType>& type, int64_t length,\n                        const std::shared_ptr<Buffer>& data,\n-                       const std::shared_ptr<Buffer>& null_bitmap = nullptr,\n+                       const std::shared_ptr<Buffer>& null_bitmap = NULLPTR,\n                        int64_t null_count = 0, int64_t offset = 0);\n \n   const uint8_t* GetValue(int64_t i) const;\n@@ -580,7 +580,7 @@ class ARROW_EXPORT StructArray : public Array {\n \n   StructArray(const std::shared_ptr<DataType>& type, int64_t length,\n               const std::vector<std::shared_ptr<Array>>& children,\n-              std::shared_ptr<Buffer> null_bitmap = nullptr, int64_t null_count = 0,\n+              std::shared_ptr<Buffer> null_bitmap = NULLPTR, int64_t null_count = 0,\n               int64_t offset = 0);\n \n   // Return a shared pointer in case the requestor desires to share ownership\n@@ -605,8 +605,8 @@ class ARROW_EXPORT UnionArray : public Array {\n   UnionArray(const std::shared_ptr<DataType>& type, int64_t length,\n              const std::vector<std::shared_ptr<Array>>& children,\n              const std::shared_ptr<Buffer>& type_ids,\n-             const std::shared_ptr<Buffer>& value_offsets = nullptr,\n-             const std::shared_ptr<Buffer>& null_bitmap = nullptr, int64_t null_count = 0,\n+             const std::shared_ptr<Buffer>& value_offsets = NULLPTR,\n+             const std::shared_ptr<Buffer>& null_bitmap = NULLPTR, int64_t null_count = 0,\n              int64_t offset = 0);\n \n   /// Note that this buffer does not account for any slice offset\ndiff --git a/cpp/src/arrow/buffer.h b/cpp/src/arrow/buffer.h\nindex b745812b1..8e989064b 100644\n--- a/cpp/src/arrow/buffer.h\n+++ b/cpp/src/arrow/buffer.h\n@@ -112,7 +112,7 @@ class ARROW_EXPORT Buffer {\n   int64_t size_;\n   int64_t capacity_;\n \n-  // nullptr by default, but may be set\n+  // null by default, but may be set\n   std::shared_ptr<Buffer> parent_;\n \n  private:\n@@ -145,7 +145,7 @@ class ARROW_EXPORT MutableBuffer : public Buffer {\n                 const int64_t size);\n \n  protected:\n-  MutableBuffer() : Buffer(nullptr, 0) {}\n+  MutableBuffer() : Buffer(NULLPTR, 0) {}\n };\n \n class ARROW_EXPORT ResizableBuffer : public MutableBuffer {\n@@ -180,7 +180,7 @@ class ARROW_EXPORT ResizableBuffer : public MutableBuffer {\n /// A Buffer whose lifetime is tied to a particular MemoryPool\n class ARROW_EXPORT PoolBuffer : public ResizableBuffer {\n  public:\n-  explicit PoolBuffer(MemoryPool* pool = nullptr);\n+  explicit PoolBuffer(MemoryPool* pool = NULLPTR);\n   virtual ~PoolBuffer();\n \n   Status Resize(const int64_t new_size, bool shrink_to_fit = true) override;\n@@ -193,7 +193,7 @@ class ARROW_EXPORT PoolBuffer : public ResizableBuffer {\n class ARROW_EXPORT BufferBuilder {\n  public:\n   explicit BufferBuilder(MemoryPool* pool)\n-      : pool_(pool), data_(nullptr), capacity_(0), size_(0) {}\n+      : pool_(pool), data_(NULLPTR), capacity_(0), size_(0) {}\n \n   /// Resizes the buffer to the nearest multiple of 64 bytes per Layout.md\n   Status Resize(const int64_t elements) {\n@@ -201,7 +201,7 @@ class ARROW_EXPORT BufferBuilder {\n     if (elements == 0) {\n       return Status::OK();\n     }\n-    if (buffer_ == nullptr) {\n+    if (buffer_ == NULLPTR) {\n       buffer_ = std::make_shared<PoolBuffer>(pool_);\n     }\n     int64_t old_capacity = capacity_;\n@@ -264,7 +264,7 @@ class ARROW_EXPORT BufferBuilder {\n   }\n \n   void Reset() {\n-    buffer_ = nullptr;\n+    buffer_ = NULLPTR;\n     capacity_ = size_ = 0;\n   }\n \ndiff --git a/cpp/src/arrow/builder.h b/cpp/src/arrow/builder.h\nindex 1720c0014..c580eeb3b 100644\n--- a/cpp/src/arrow/builder.h\n+++ b/cpp/src/arrow/builder.h\n@@ -59,9 +59,9 @@ class ARROW_EXPORT ArrayBuilder {\n   explicit ArrayBuilder(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n       : type_(type),\n         pool_(pool),\n-        null_bitmap_(nullptr),\n+        null_bitmap_(NULLPTR),\n         null_count_(0),\n-        null_bitmap_data_(nullptr),\n+        null_bitmap_data_(NULLPTR),\n         length_(0),\n         capacity_(0) {}\n \n@@ -188,7 +188,7 @@ class ARROW_EXPORT PrimitiveBuilder : public ArrayBuilder {\n   using value_type = typename Type::c_type;\n \n   explicit PrimitiveBuilder(const std::shared_ptr<DataType>& type, MemoryPool* pool)\n-      : ArrayBuilder(type, pool), data_(nullptr), raw_data_(nullptr) {}\n+      : ArrayBuilder(type, pool), data_(NULLPTR), raw_data_(NULLPTR) {}\n \n   using ArrayBuilder::Advance;\n \n@@ -214,7 +214,7 @@ class ARROW_EXPORT PrimitiveBuilder : public ArrayBuilder {\n   /// indicates a valid (non-null) value\n   /// \\return Status\n   Status Append(const value_type* values, int64_t length,\n-                const uint8_t* valid_bytes = nullptr);\n+                const uint8_t* valid_bytes = NULLPTR);\n \n   /// \\brief Append a sequence of elements in one shot\n   /// \\param[in] values a contiguous C array of values\n@@ -430,7 +430,7 @@ class ARROW_EXPORT AdaptiveUIntBuilder : public internal::AdaptiveIntBuilderBase\n   /// indicates a valid (non-null) value\n   /// \\return Status\n   Status Append(const uint64_t* values, int64_t length,\n-                const uint8_t* valid_bytes = nullptr);\n+                const uint8_t* valid_bytes = NULLPTR);\n \n   Status FinishInternal(std::shared_ptr<ArrayData>* out) override;\n \n@@ -492,7 +492,7 @@ class ARROW_EXPORT AdaptiveIntBuilder : public internal::AdaptiveIntBuilderBase\n   /// indicates a valid (non-null) value\n   /// \\return Status\n   Status Append(const int64_t* values, int64_t length,\n-                const uint8_t* valid_bytes = nullptr);\n+                const uint8_t* valid_bytes = NULLPTR);\n \n   Status FinishInternal(std::shared_ptr<ArrayData>* out) override;\n \n@@ -557,7 +557,7 @@ class ARROW_EXPORT BooleanBuilder : public ArrayBuilder {\n   /// indicates a valid (non-null) value\n   /// \\return Status\n   Status Append(const uint8_t* values, int64_t length,\n-                const uint8_t* valid_bytes = nullptr);\n+                const uint8_t* valid_bytes = NULLPTR);\n \n   /// \\brief Append a sequence of elements in one shot\n   /// \\param[in] values a contiguous C array of values\n@@ -624,7 +624,7 @@ class ARROW_EXPORT ListBuilder : public ArrayBuilder {\n   /// Use this constructor to incrementally build the value array along with offsets and\n   /// null bitmap.\n   ListBuilder(MemoryPool* pool, std::unique_ptr<ArrayBuilder> value_builder,\n-              const std::shared_ptr<DataType>& type = nullptr);\n+              const std::shared_ptr<DataType>& type = NULLPTR);\n \n   Status Init(int64_t elements) override;\n   Status Resize(int64_t capacity) override;\n@@ -635,7 +635,7 @@ class ARROW_EXPORT ListBuilder : public ArrayBuilder {\n   /// If passed, valid_bytes is of equal length to values, and any zero byte\n   /// will be considered as a null for that slot\n   Status Append(const int32_t* offsets, int64_t length,\n-                const uint8_t* valid_bytes = nullptr);\n+                const uint8_t* valid_bytes = NULLPTR);\n \n   /// \\brief Start a new variable-length list slot\n   ///\n@@ -732,7 +732,7 @@ class ARROW_EXPORT FixedSizeBinaryBuilder : public ArrayBuilder {\n   }\n \n   Status Append(const uint8_t* data, int64_t length,\n-                const uint8_t* valid_bytes = nullptr);\n+                const uint8_t* valid_bytes = NULLPTR);\n   Status Append(const std::string& value);\n   Status AppendNull();\n \ndiff --git a/cpp/src/arrow/io/hdfs.h b/cpp/src/arrow/io/hdfs.h\nindex aaaafc819..0708b11cc 100644\n--- a/cpp/src/arrow/io/hdfs.h\n+++ b/cpp/src/arrow/io/hdfs.h\n@@ -121,8 +121,8 @@ class ARROW_EXPORT HadoopFileSystem : public FileSystem {\n   /// Change\n   ///\n   /// @param path file path to change\n-  /// @param owner pass nullptr for no change\n-  /// @param group pass nullptr for no change\n+  /// @param owner pass null for no change\n+  /// @param group pass null for no change\n   Status Chown(const std::string& path, const char* owner, const char* group);\n \n   /// Change path permissions\n@@ -199,7 +199,7 @@ class ARROW_EXPORT HdfsReadableFile : public RandomAccessFile {\n   void set_memory_pool(MemoryPool* pool);\n \n  private:\n-  explicit HdfsReadableFile(MemoryPool* pool = nullptr);\n+  explicit HdfsReadableFile(MemoryPool* pool = NULLPTR);\n \n   class ARROW_NO_EXPORT HdfsReadableFileImpl;\n   std::unique_ptr<HdfsReadableFileImpl> impl_;\ndiff --git a/cpp/src/arrow/ipc/message.h b/cpp/src/arrow/ipc/message.h\nindex 67a95c7d2..a1b6c07a4 100644\n--- a/cpp/src/arrow/ipc/message.h\n+++ b/cpp/src/arrow/ipc/message.h\n@@ -69,7 +69,7 @@ class ARROW_EXPORT Message {\n   /// \\brief Create and validate a Message instance from two buffers\n   ///\n   /// \\param[in] metadata a buffer containing the Flatbuffer metadata\n-  /// \\param[in] body a buffer containing the message body, which may be nullptr\n+  /// \\param[in] body a buffer containing the message body, which may be null\n   /// \\param[out] out the created message\n   /// \\return Status\n   static Status Open(const std::shared_ptr<Buffer>& metadata,\n@@ -98,7 +98,7 @@ class ARROW_EXPORT Message {\n \n   /// \\brief the Message body, if any\n   ///\n-  /// \\return buffer is nullptr if no body\n+  /// \\return buffer is null if no body\n   std::shared_ptr<Buffer> body() const;\n \n   /// \\brief The Message type\n@@ -179,7 +179,7 @@ Status ReadMessage(const int64_t offset, const int32_t metadata_length,\n \n /// \\brief Read encapulated RPC message (metadata and body) from InputStream\n ///\n-/// Read length-prefixed message with as-yet unknown length. Returns nullptr if\n+/// Read length-prefixed message with as-yet unknown length. Returns null if\n /// there are not enough bytes available or the message length is 0 (e.g. EOS\n /// in a stream)\n ARROW_EXPORT\ndiff --git a/cpp/src/arrow/python/common.h b/cpp/src/arrow/python/common.h\nindex e3fe2ef42..146864ffd 100644\n--- a/cpp/src/arrow/python/common.h\n+++ b/cpp/src/arrow/python/common.h\n@@ -63,7 +63,7 @@ class ARROW_EXPORT PyAcquireGIL {\n \n class ARROW_EXPORT OwnedRef {\n  public:\n-  OwnedRef() : obj_(nullptr) {}\n+  OwnedRef() : obj_(NULLPTR) {}\n \n   explicit OwnedRef(PyObject* obj) : obj_(obj) {}\n \n@@ -82,7 +82,7 @@ class ARROW_EXPORT OwnedRef {\n \n   void release() {\n     Py_XDECREF(obj_);\n-    obj_ = nullptr;\n+    obj_ = NULLPTR;\n   }\n \n   PyObject* obj() const { return obj_; }\n@@ -96,7 +96,7 @@ class ARROW_EXPORT OwnedRef {\n // reference count when release is called.\n class ARROW_EXPORT ScopedRef {\n  public:\n-  ScopedRef() : obj_(nullptr) {}\n+  ScopedRef() : obj_(NULLPTR) {}\n \n   explicit ScopedRef(PyObject* obj) : obj_(obj) {}\n \n@@ -109,7 +109,7 @@ class ARROW_EXPORT ScopedRef {\n \n   PyObject* release() {\n     PyObject* result = obj_;\n-    obj_ = nullptr;\n+    obj_ = NULLPTR;\n     return result;\n   }\n \n@@ -137,7 +137,7 @@ struct ARROW_EXPORT PyObjectStringify {\n       bytes = PyBytes_AsString(obj);\n       size = PyBytes_GET_SIZE(obj);\n     } else {\n-      bytes = nullptr;\n+      bytes = NULLPTR;\n       size = -1;\n     }\n   }\ndiff --git a/cpp/src/arrow/python/helpers.h b/cpp/src/arrow/python/helpers.h\nindex 01ab91657..719ed796e 100644\n--- a/cpp/src/arrow/python/helpers.h\n+++ b/cpp/src/arrow/python/helpers.h\n@@ -25,6 +25,7 @@\n #include <utility>\n \n #include \"arrow/type.h\"\n+#include \"arrow/util/macros.h\"\n #include \"arrow/util/visibility.h\"\n \n namespace arrow {\n@@ -43,8 +44,8 @@ Status ImportFromModule(const OwnedRef& module, const std::string& module_name,\n \n Status PythonDecimalToString(PyObject* python_decimal, std::string* out);\n \n-Status InferDecimalPrecisionAndScale(PyObject* python_decimal, int* precision = nullptr,\n-                                     int* scale = nullptr);\n+Status InferDecimalPrecisionAndScale(PyObject* python_decimal, int* precision = NULLPTR,\n+                                     int* scale = NULLPTR);\n \n PyObject* DecimalFromString(PyObject* decimal_constructor,\n                             const std::string& decimal_string);\ndiff --git a/cpp/src/arrow/python/numpy_to_arrow.h b/cpp/src/arrow/python/numpy_to_arrow.h\nindex 4a870fff9..5bcbea325 100644\n--- a/cpp/src/arrow/python/numpy_to_arrow.h\n+++ b/cpp/src/arrow/python/numpy_to_arrow.h\n@@ -37,7 +37,7 @@ class Status;\n namespace py {\n \n /// Convert NumPy arrays to Arrow. If target data type is not known, pass a\n-/// type with nullptr\n+/// type with null\n ///\n /// \\param[in] pool Memory pool for any memory allocations\n /// \\param[in] ao an ndarray with the array data\ndiff --git a/cpp/src/arrow/table.h b/cpp/src/arrow/table.h\nindex d40bdb856..d3145ff10 100644\n--- a/cpp/src/arrow/table.h\n+++ b/cpp/src/arrow/table.h\n@@ -288,10 +288,10 @@ class ARROW_EXPORT RecordBatchReader {\n   /// \\return the shared schema of the record batches in the stream\n   virtual std::shared_ptr<Schema> schema() const = 0;\n \n-  /// Read the next record batch in the stream. Return nullptr for batch when\n+  /// Read the next record batch in the stream. Return null for batch when\n   /// reaching end of stream\n   ///\n-  /// \\param[out] batch the next loaded batch, nullptr at end of stream\n+  /// \\param[out] batch the next loaded batch, null at end of stream\n   /// \\return Status\n   virtual Status ReadNext(std::shared_ptr<RecordBatch>* batch) = 0;\n };\ndiff --git a/cpp/src/arrow/type.h b/cpp/src/arrow/type.h\nindex 443828423..2030f371d 100644\n--- a/cpp/src/arrow/type.h\n+++ b/cpp/src/arrow/type.h\n@@ -241,7 +241,7 @@ class ARROW_EXPORT Field {\n  public:\n   Field(const std::string& name, const std::shared_ptr<DataType>& type,\n         bool nullable = true,\n-        const std::shared_ptr<const KeyValueMetadata>& metadata = nullptr)\n+        const std::shared_ptr<const KeyValueMetadata>& metadata = NULLPTR)\n       : name_(name), type_(type), nullable_(nullable), metadata_(metadata) {}\n \n   std::shared_ptr<const KeyValueMetadata> metadata() const { return metadata_; }\n@@ -737,10 +737,10 @@ class ARROW_EXPORT DictionaryType : public FixedWidthType {\n class ARROW_EXPORT Schema {\n  public:\n   explicit Schema(const std::vector<std::shared_ptr<Field>>& fields,\n-                  const std::shared_ptr<const KeyValueMetadata>& metadata = nullptr);\n+                  const std::shared_ptr<const KeyValueMetadata>& metadata = NULLPTR);\n \n   explicit Schema(std::vector<std::shared_ptr<Field>>&& fields,\n-                  const std::shared_ptr<const KeyValueMetadata>& metadata = nullptr);\n+                  const std::shared_ptr<const KeyValueMetadata>& metadata = NULLPTR);\n \n   virtual ~Schema() = default;\n \n@@ -750,7 +750,7 @@ class ARROW_EXPORT Schema {\n   /// Return the ith schema element. Does not boundscheck\n   std::shared_ptr<Field> field(int i) const { return fields_[i]; }\n \n-  /// Returns nullptr if name not found\n+  /// Returns null if name not found\n   std::shared_ptr<Field> GetFieldByName(const std::string& name) const;\n \n   /// Returns -1 if name not found\n@@ -760,7 +760,7 @@ class ARROW_EXPORT Schema {\n \n   /// \\brief The custom key-value metadata, if any\n   ///\n-  /// \\return metadata may be nullptr\n+  /// \\return metadata may be null\n   std::shared_ptr<const KeyValueMetadata> metadata() const;\n \n   /// \\brief Render a string representation of the schema suitable for debugging\n@@ -850,30 +850,30 @@ dictionary(const std::shared_ptr<DataType>& index_type,\n /// \\param name the field name\n /// \\param type the field value type\n /// \\param nullable whether the values are nullable, default true\n-/// \\param metadata any custom key-value metadata, default nullptr\n+/// \\param metadata any custom key-value metadata, default null\n std::shared_ptr<Field> ARROW_EXPORT field(\n     const std::string& name, const std::shared_ptr<DataType>& type, bool nullable = true,\n-    const std::shared_ptr<const KeyValueMetadata>& metadata = nullptr);\n+    const std::shared_ptr<const KeyValueMetadata>& metadata = NULLPTR);\n \n /// \\brief Create a Schema instance\n ///\n /// \\param fields the schema's fields\n-/// \\param metadata any custom key-value metadata, default nullptr\n+/// \\param metadata any custom key-value metadata, default null\n /// \\return schema shared_ptr to Schema\n ARROW_EXPORT\n std::shared_ptr<Schema> schema(\n     const std::vector<std::shared_ptr<Field>>& fields,\n-    const std::shared_ptr<const KeyValueMetadata>& metadata = nullptr);\n+    const std::shared_ptr<const KeyValueMetadata>& metadata = NULLPTR);\n \n /// \\brief Create a Schema instance\n ///\n /// \\param fields the schema's fields (rvalue reference)\n-/// \\param metadata any custom key-value metadata, default nullptr\n+/// \\param metadata any custom key-value metadata, default null\n /// \\return schema shared_ptr to Schema\n ARROW_EXPORT\n std::shared_ptr<Schema> schema(\n     std::vector<std::shared_ptr<Field>>&& fields,\n-    const std::shared_ptr<const KeyValueMetadata>& metadata = nullptr);\n+    const std::shared_ptr<const KeyValueMetadata>& metadata = NULLPTR);\n \n }  // namespace arrow\n \ndiff --git a/cpp/src/arrow/util/decimal.h b/cpp/src/arrow/util/decimal.h\nindex 58496a874..6f8d5a46c 100644\n--- a/cpp/src/arrow/util/decimal.h\n+++ b/cpp/src/arrow/util/decimal.h\n@@ -24,6 +24,7 @@\n #include <type_traits>\n \n #include \"arrow/status.h\"\n+#include \"arrow/util/macros.h\"\n #include \"arrow/util/visibility.h\"\n \n namespace arrow {\n@@ -114,7 +115,7 @@ class ARROW_EXPORT Decimal128 {\n   /// \\brief Convert a decimal string to an Decimal128 value, optionally including\n   /// precision and scale if they're passed in and not null.\n   static Status FromString(const std::string& s, Decimal128* out,\n-                           int* precision = nullptr, int* scale = nullptr);\n+                           int* precision = NULLPTR, int* scale = NULLPTR);\n \n  private:\n   int64_t high_bits_;\ndiff --git a/cpp/src/arrow/util/io-util.h b/cpp/src/arrow/util/io-util.h\nindex 6fe3a5c17..dbca0d8be 100644\n--- a/cpp/src/arrow/util/io-util.h\n+++ b/cpp/src/arrow/util/io-util.h\n@@ -75,7 +75,7 @@ class StdinStream : public InputStream {\n   }\n \n   Status Read(int64_t nbytes, std::shared_ptr<Buffer>* out) override {\n-    auto buffer = std::make_shared<PoolBuffer>(nullptr);\n+    auto buffer = std::make_shared<PoolBuffer>(NULLPTR);\n     RETURN_NOT_OK(buffer->Resize(nbytes));\n     int64_t bytes_read;\n     RETURN_NOT_OK(Read(nbytes, &bytes_read, buffer->mutable_data()));\ndiff --git a/cpp/src/arrow/util/macros.h b/cpp/src/arrow/util/macros.h\nindex a5f6e5707..8b1125d02 100644\n--- a/cpp/src/arrow/util/macros.h\n+++ b/cpp/src/arrow/util/macros.h\n@@ -58,6 +58,21 @@\n #define ARROW_MUST_USE_RESULT\n #endif\n \n+// ----------------------------------------------------------------------\n+// C++/CLI support macros (see ARROW-1134)\n+\n+#ifndef NULLPTR\n+\n+#ifdef __cplusplus_cli\n+#define NULLPTR __nullptr\n+#else\n+#define NULLPTR nullptr\n+#endif\n+\n+#endif  // ifndef NULLPTR\n+\n+// ----------------------------------------------------------------------\n+\n // macros to disable padding\n // these macros are portable across different compilers and platforms\n //[https://github.com/google/flatbuffers/blob/master/include/flatbuffers/flatbuffers.h#L1355]\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-25T13:08:48.970+0000",
                    "updated": "2017-10-25T13:08:48.970+0000"
                }
            ],
            "maxResults": 30,
            "total": 30,
            "startAt": 0
        },
        "customfield_12311820": "0|i3gjyn:",
        "customfield_12314139": null
    }
}