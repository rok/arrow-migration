{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13252760",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760",
    "key": "ARROW-6341",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12340948",
                "id": "12340948",
                "description": "",
                "name": "0.16.0",
                "archived": false,
                "released": true,
                "releaseDate": "2020-02-07"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "dataset",
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12568472",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12568472",
                "type": {
                    "id": "10001",
                    "name": "dependent",
                    "inward": "is depended upon by",
                    "outward": "depends upon",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10001"
                },
                "outwardIssue": {
                    "id": "13250886",
                    "key": "ARROW-6242",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13250886",
                    "fields": {
                        "summary": "[C++] Implements basic Dataset/Scanner/ScannerBuilder",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
            "name": "kszucs",
            "key": "kszucs",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Krisztian Szucs",
            "active": true,
            "timeZone": "Europe/Budapest"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=fsaintjacques",
            "name": "fsaintjacques",
            "key": "fsaintjacques",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=fsaintjacques&avatarId=37276",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=fsaintjacques&avatarId=37276",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=fsaintjacques&avatarId=37276",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=fsaintjacques&avatarId=37276"
            },
            "displayName": "Francois Saint-Jacques",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=fsaintjacques",
            "name": "fsaintjacques",
            "key": "fsaintjacques",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=fsaintjacques&avatarId=37276",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=fsaintjacques&avatarId=37276",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=fsaintjacques&avatarId=37276",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=fsaintjacques&avatarId=37276"
            },
            "displayName": "Francois Saint-Jacques",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 56400,
            "total": 56400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 56400,
            "total": 56400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-6341/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 114,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/304246",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237\n \n \n   \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-08-30T13:07:12.745+0000",
                    "updated": "2019-08-30T13:07:12.745+0000",
                    "started": "2019-08-30T13:07:12.744+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "304246",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/333054",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on issue #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#issuecomment-545732476\n \n \n   @kszucs is this still WIP (I assume so based on CI builds?)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-24T03:58:01.620+0000",
                    "updated": "2019-10-24T03:58:01.620+0000",
                    "started": "2019-10-24T03:58:01.620+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "333054",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/333386",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs commented on issue #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#issuecomment-545911350\n \n \n   @emkornfield yes, I'll continue to work on it after the 0.15.1 release. Theoretically the Dataset API should be ready for having bindings. \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-10-24T13:13:15.973+0000",
                    "updated": "2019-10-24T13:13:15.973+0000",
                    "started": "2019-10-24T13:13:15.973+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "333386",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/351268",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on issue #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#issuecomment-559712449\n \n \n   Something I ran into yesterday: trying to access the `partition_scheme` attribute of the discovery segfaults:\r\n   \r\n   ```\r\n   from pyarrow.dataset import FileSystemDataSourceDiscovery, ParquetFileFormat\r\n   from pyarrow.fs import Selector, LocalFileSystem\r\n   \r\n   fs = LocalFileSystem()\r\n   selector = Selector('test_dataset/', recursive=True)\r\n   parquet_format = ParquetFileFormat()\r\n   discovery = FileSystemDataSourceDiscovery(fs, selector, parquet_format)\r\n   discovery.partition_scheme\r\n   ```\r\n   \r\n   where \"test_dataset\" is a simple directory with a single small parquet file in it. \r\n   (the actual tests you wrote do pass though)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-11-29T08:57:59.094+0000",
                    "updated": "2019-11-29T08:57:59.094+0000",
                    "started": "2019-11-29T08:57:59.094+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "351268",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/351425",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs commented on issue #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#issuecomment-559852117\n \n \n   @jorisvandenbossche I've fixed that.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-11-29T17:35:13.511+0000",
                    "updated": "2019-11-29T17:35:13.511+0000",
                    "started": "2019-11-29T17:35:13.511+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "351425",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/351958",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352632444\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/discovery.h\n ##########\n @@ -85,6 +85,7 @@ class ARROW_DS_EXPORT DataSourceDiscovery {\n   ExpressionPtr root_partition_;\n };\n \n+// TODO(kszucs): a static Defaults method would be nice\n \n Review comment:\n   @pitrou why do we use a default static method in other options struct?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T14:43:07.552+0000",
                    "updated": "2019-12-02T14:43:07.552+0000",
                    "started": "2019-12-02T14:43:07.552+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "351958",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/351959",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352636126\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/discovery.h\n ##########\n @@ -85,6 +85,7 @@ class ARROW_DS_EXPORT DataSourceDiscovery {\n   ExpressionPtr root_partition_;\n };\n \n+// TODO(kszucs): a static Defaults method would be nice\n \n Review comment:\n   Just being explicit :-)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T14:49:38.127+0000",
                    "updated": "2019-12-02T14:49:38.127+0000",
                    "started": "2019-12-02T14:49:38.127+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "351959",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/351964",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352645172\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n \n Review comment:\n   Can you allow a list of FileStats here as well? \r\n   \r\n   (there is an alternative FileSystemDataSourceDiscovery constructor that accepts a vector of FileStats)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T15:04:55.694+0000",
                    "updated": "2019-12-02T15:04:55.694+0000",
                    "started": "2019-12-02T15:04:55.694+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "351964",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/351966",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352646147\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n+        \"\"\"Returns an iterator of ScanTasks\n+\n+        Each of ScanTask yields RecordBatches from this DataFragment.\n+\n+        Parameters\n+        ----------\n+        context : ScanContext, default None\n+\n+        Returns\n+        -------\n+        scan_tasks: Iterator[ScanTask]\n+        \"\"\"\n+        cdef:\n+            CResult[CScanTaskIterator] iterator_result\n+            CScanTaskIterator iterator\n+            CScanTaskPtr task\n+\n+        context = context or ScanContext()\n+        iterator_result = self.fragment.Scan(context.unwrap())\n+        iterator = move(GetResultValue(move(iterator_result)))\n+\n+        while True:\n+            iterator.Next(&task)\n+            if task.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield ScanTask.wrap(task)\n+\n+    @property\n+    def splittable(self):\n+        \"\"\"Get whether the DataFragment supports parallel scanning.\"\"\"\n+        return self.fragment.splittable()\n+\n+    @property\n+    def scan_options(self):\n+        \"\"\"Get the scan options used for scanning the data fragment.\n+\n+        Filtering, schema reconciliation, and partition options. None indicates\n+        that no filtering or schema reconciliation will be performed and all\n+        partitions will be scanned.\n+        \"\"\"\n+        cdef shared_ptr[CScanOptions] options = self.fragment.scan_options()\n+        if options.get() == nullptr:\n+            return None\n+        else:\n+            return ScanOptions.wrap(options)\n+\n+\n+cdef class SimpleDataFragment(DataFragment):\n+    \"\"\"A fragment that yields ScanTasks out of a fixed set of RecordBatches.\"\"\"\n+\n+    cdef:\n+        CSimpleDataFragment* simple_fragment\n+\n+    def __init__(self, record_batches):\n+        \"\"\"Create a simple data fragment from record batches.\n+\n+        Parameters\n+        ----------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            RecordBatch batch\n+            vector[shared_ptr[CRecordBatch]] batches\n+            shared_ptr[CSimpleDataFragment] simple_fragment\n+\n+        for batch in record_batches:\n+            batches.push_back(batch.sp_batch)\n+\n+        simple_fragment = make_shared[CSimpleDataFragment](batches)\n+        self.init(<shared_ptr[CDataFragment]> simple_fragment)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.simple_fragment = <CSimpleDataFragment*> sp.get()\n+\n+\n+cdef class FileDataFragment(DataFragment):\n+\n+    cdef:\n+        CFileDataFragment* file_fragment\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.file_fragment = <CFileDataFragment*> sp.get()\n+\n+    @property\n+    def source(self):\n+        cdef shared_ptr[CFileSource] source\n+        source = make_shared[CFileSource](self.file_fragment.source())\n+        return FileSource.wrap(source)\n+\n+\n+cdef class ParquetDataFragment(FileDataFragment):\n+\n+    def __init__(self, FileSource source not None,\n+                 ParquetScanOptions scan_options not None):\n+        cdef shared_ptr[CDataFragment] fragment\n+        fragment.reset(new CParquetDataFragment(\n+            deref(source.unwrap()),\n+            scan_options.unwrap()\n+        ))\n+        self.init(fragment)\n+\n+\n+cdef class DataSource:\n+    \"\"\"Basic component of a Dataset which yields zero or more DataFragments.\n+\n+    A DataSource acts as a discovery mechanism of DataFragments and partitions,\n+    e.g. files deeply nested in a directory.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataSource] wrapped\n+        CDataSource* source\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSource]& sp):\n+        cdef DataSource self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'tree_data_source':\n+            self = TreeDataSource.__new__(TreeDataSource)\n+        elif typ == 'simple_data_source':\n+            self = SimpleDataSource.__new__(SimpleDataSource)\n+        elif typ == 'filesystem_data_source':\n+            self = FileSystemDataSource.__new__(FileSystemDataSource)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef shared_ptr[CDataSource] unwrap(self):\n+        return self.wrapped\n+\n+    def fragments(self, ScanOptions scan_options=None):\n+        \"\"\"Get the data fragments of this data source.\n+\n+        Parameters\n+        ----------\n+        scan_options : ScanOptions, default None\n+            Controls filtering and schema inference.\n+\n+        Returns\n+        -------\n+        fragments : iterator of DataFragments\n+        \"\"\"\n+        cdef:\n+            CDataFragmentIterator iterator\n+            CDataFragmentPtr fragment\n+\n+        scan_options = scan_options or ScanOptions()\n+        iterator = self.source.GetFragments(scan_options.unwrap())\n+\n+        while True:\n+            iterator.Next(&fragment)\n+            if fragment.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield DataFragment.wrap(fragment)\n+\n+    @property\n+    def partition_expression(self):\n+        cdef shared_ptr[CExpression] expression\n+        expression = self.source.partition_expression()\n+        if expression.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expression)\n+\n+\n+cdef class SimpleDataSource(DataSource):\n+    \"\"\"A DataSource consisting of a flat sequence of DataFragments.\"\"\"\n+\n+    cdef:\n+        CSimpleDataSource* simple_source\n+\n+    def __init__(self, data_fragments):\n+        cdef:\n+            DataFragment fragment\n+            CDataFragmentVector fragments\n+            shared_ptr[CSimpleDataSource] simple_source\n+\n+        for fragment in data_fragments:\n+            fragments.push_back(fragment.unwrap())\n+\n+        simple_source = make_shared[CSimpleDataSource](fragments)\n+        self.init(<shared_ptr[CDataSource]> simple_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.simple_source = <CSimpleDataSource*> sp.get()\n+\n+\n+cdef class TreeDataSource(DataSource):\n+\n+    cdef:\n+        CTreeDataSource* tree_source\n+\n+    def __init__(self, data_sources):\n+        cdef:\n+            DataSource child\n+            CDataSourceVector children\n+            shared_ptr[CTreeDataSource] tree_source\n+\n+        for child in data_sources:\n+            children.push_back(child.wrapped)\n+\n+        tree_source = make_shared[CTreeDataSource](children)\n+        self.init(<shared_ptr[CDataSource]> tree_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.tree_source = <CTreeDataSource*> sp.get()\n+\n+\n+cdef class FileSystemDataSource(DataSource):\n+\n+    cdef:\n+        CFileSystemDataSource* filesystem_source\n+\n+    def __init__(self, FileSystem filesystem not None, file_stats,\n+                 Expression source_partition, dict path_partitions,\n+                 FileFormat file_format not None):\n+        cdef:\n+            FileStats stats\n+            Expression expression\n+            vector[CFileStats] c_file_stats\n+            shared_ptr[CExpression] c_source_partition\n+            unordered_map[c_string, shared_ptr[CExpression]] c_path_partitions\n+            CResult[shared_ptr[CDataSource]] result\n+\n+        for stats in file_stats:\n+            c_file_stats.push_back(stats.unwrap())\n+\n+        for path, expression in path_partitions.items():\n+            c_path_partitions[tobytes(path)] = expression.unwrap()\n+\n+        if source_partition is not None:\n+            c_source_partition = source_partition.unwrap()\n+\n+        result = CFileSystemDataSource.Make(\n+            filesystem.unwrap(),\n+            c_file_stats,\n+            c_source_partition,\n+            c_path_partitions,\n+            file_format.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.filesystem_source = <CFileSystemDataSource*> sp.get()\n+\n+\n+cdef class Dataset:\n+    \"\"\"Collection of data fragments coming from possibly multiple sources.\"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataset] wrapped\n+        CDataset* dataset\n+\n+    def __init__(self, data_sources, Schema schema not None):\n+        \"\"\"Create a dataset\n+\n+        Parameters\n+        ----------\n+        data_sources : list of DataSource\n+            One or more input data sources\n+        schema : Schema\n+            A known schema to conform to. The data sources must conform their\n+            output to this schema with projections and filters taken into\n+            account.\n+        \"\"\"\n+        cdef:\n+            DataSource source\n+            CDataSourceVector sources\n+            CResult[CDatasetPtr] result\n+            shared_ptr[CSchema] sp_schema\n+\n+        for source in data_sources:\n+            sources.push_back(source.wrapped)\n+\n+        result = CDataset.Make(sources, pyarrow_unwrap_schema(schema))\n+\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataset]& sp):\n+        self.wrapped = sp\n+        self.dataset = sp.get()\n+\n+    cdef inline shared_ptr[CDataset] unwrap(self):\n+        return self.wrapped\n+\n+    # TODO(kszucs): pass ScanContext\n+    def new_scan(self):\n+        \"\"\"Begin to build a new Scan operation against this Dataset.\"\"\"\n+        cdef shared_ptr[CScannerBuilder] builder\n+        builder = GetResultValue(self.dataset.NewScan())\n+        return ScannerBuilder.wrap(builder)\n+\n+    @property\n+    def sources(self):\n+        cdef vector[shared_ptr[CDataSource]] sources = self.dataset.sources()\n+        return [DataSource.wrap(source) for source in sources]\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.dataset.schema())\n+\n+\n+cdef class ScanOptions:\n+\n+    cdef:\n+        shared_ptr[CScanOptions] wrapped\n+        CScanOptions* options\n+\n+    def __init__(self, Schema schema=None):\n+        self.init(CScanOptions.Defaults())\n+        if schema is not None:\n+            self.schema = schema\n+\n+    cdef init(self, const shared_ptr[CScanOptions]& sp):\n+        self.wrapped = sp\n+        self.options = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CScanOptions]& sp):\n+        cdef ScanOptions self = ScanOptions.__new__(ScanOptions)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanOptions] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        \"\"\"Schema to which record batches will be reconciled\"\"\"\n+        if self.options.schema == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(self.options.schema)\n+\n+    @schema.setter\n+    def schema(self, Schema value):\n+        self.options.schema = pyarrow_unwrap_schema(value)\n+\n+    @property\n+    def use_threads(self):\n+        \"\"\"Make use of the thread pool found in the scan context\"\"\"\n+        return self.options.use_threads\n+\n+    @use_threads.setter\n+    def use_threads(self, bint value):\n+        self.options.use_threads = value\n+\n+\n+cdef class FileScanOptions(ScanOptions):\n+    pass\n+\n+\n+cdef class ParquetScanOptions(FileScanOptions):\n+    pass\n+\n+\n+cdef class WriteOptions:\n+    pass\n+\n+\n+cdef class FileWriteOptions(WriteOptions):\n+    pass\n+\n+\n+cdef class ParquetWriterOptions(FileWriteOptions):\n+    pass\n+\n+\n+cdef class ScanContext:\n+\n+    cdef:\n+        shared_ptr[CScanContext] wrapped\n+        CScanContext *context\n+\n+    def __init__(self, MemoryPool memory_pool=None):\n+        cdef shared_ptr[CScanContext] context = make_shared[CScanContext]()\n+        self.init(context)\n+        if memory_pool is not None:\n+            self.memory_pool = memory_pool\n+\n+    cdef init(self, shared_ptr[CScanContext]& sp):\n+        self.wrapped = sp\n+        self.context = sp.get()\n+\n+    @staticmethod\n+    cdef ScanContext wrap(shared_ptr[CScanContext]& sp):\n+        cdef ScanContext self = ScanContext.__new__(ScanContext)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanContext] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def memory_pool(self):\n+        # TODO(kszucs): there's no function to box/wrap memory pool\n+        cdef:\n+            MemoryPool pool = MemoryPool.__new__(MemoryPool)\n+        # this might be unsafe because of the raw pointers\n+        pool.init(self.context.pool)\n+        return pool\n+\n+    @memory_pool.setter\n+    def memory_pool(self, MemoryPool pool):\n+        self.context.pool = maybe_unbox_memory_pool(pool)\n+\n+\n+cdef class ScanTask:\n+    \"\"\"Read record batches from a range of a single data fragment.\n+\n+    A ScanTask is meant to be a unit of work to be dispatched.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CScanTask] wrapped\n+        CScanTask* task\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__, subclasses_instead=False)\n+\n+    cdef init(self, shared_ptr[CScanTask]& sp):\n+        self.wrapped = sp\n+        self.task = self.wrapped.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CScanTask]& sp):\n+        cdef SimpleScanTask self = SimpleScanTask.__new__(SimpleScanTask)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanTask] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self):\n+        \"\"\"Iterate through sequence of materialized record batches.\n+\n+        Execution semantics are encapsulated in the particular ScanTask\n+        implementation.\n+\n+        Returns\n+        -------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            CRecordBatchIterator iterator\n+            shared_ptr[CRecordBatch] record_batch\n+\n+        iterator = move(GetResultValue(move(self.task.Scan())))\n+\n+        while True:\n+            iterator.Next(&record_batch)\n+            if record_batch.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield pyarrow_wrap_batch(record_batch)\n+\n+\n+cdef class SimpleScanTask(ScanTask):\n+    \"\"\"A trivial ScanTask that yields the RecordBatch of an array.\"\"\"\n+\n+    cdef:\n+        CSimpleScanTask* simple_task\n+\n+    cdef init(self, shared_ptr[CScanTask]& sp):\n+        ScanTask.init(self, sp)\n+        self.simple_task = <CSimpleScanTask*> sp.get()\n+\n+\n+cdef class ScannerBuilder:\n+    \"\"\"Factory class to construct a Scanner.\n+\n+    It is used to pass information, notably a potential filter expression and a\n+    subset of columns to materialize.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CScannerBuilder] wrapped\n+        CScannerBuilder* builder\n+\n+    def __init__(self, Dataset dataset not None, ScanContext context not None):\n+        cdef shared_ptr[CScannerBuilder] builder\n+        builder = make_shared[CScannerBuilder](\n+            dataset.unwrap(), context.unwrap()\n+        )\n+        self.init(builder)\n+\n+    cdef void init(self, shared_ptr[CScannerBuilder]& sp):\n+        self.wrapped = sp\n+        self.builder = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CScannerBuilder]& sp):\n+        cdef ScannerBuilder self = ScannerBuilder.__new__(ScannerBuilder)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScannerBuilder] unwrap(self):\n+        return self.wrapped\n+\n+    def project(self, columns):\n+        \"\"\"Set the subset of columns to materialize.\n+\n+        This subset will be passed down to DataSources and corresponding\n+        DataFragments. The goal is to avoid loading, copying, and deserializing\n+        columns that will not be required further down the compute chain.\n+\n+        Raises exception if any column name does not exists in the dataset's\n+        Schema.\n+\n+        Parameters\n+        ----------\n+        columns : list of str\n+            List of columns to project. Order and duplicates will be preserved.\n+\n+        Returns\n+        -------\n+        self : ScannerBuilder\n+        \"\"\"\n+        cdef vector[c_string] cols = [tobytes(c) for c in columns]\n+        check_status(self.builder.Project(cols))\n+        return self\n+\n+    def finish(self):\n+        \"\"\"Return the constructed now-immutable Scanner object\"\"\"\n+        return Scanner.wrap(GetResultValue(self.builder.Finish()))\n+\n+    def filter(self, Expression filter_expression not None):\n+        \"\"\"Set the filter expression to return only rows matching the filter.\n+\n+        The predicate will be passed down to DataSources and corresponding\n+        DataFragments to exploit predicate pushdown if possible using partition\n+        information or DataFragment internal metadata, e.g. Parquet statistics.\n+\n+        Raises exception if any of the referenced columns does not exists in\n+        the dataset's schema.\n+\n+        Parameters\n+        ----------\n+        filter_expression : Expression\n+            Boolean expression to filter rows with.\n+\n+        Returns\n+        -------\n+        self : ScannerBuilder\n+        \"\"\"\n+        check_status(self.builder.Filter(filter_expression.unwrap()))\n+        return self\n+\n+    def use_threads(self, bint value):\n+        \"\"\"Set whether the Scanner should make use of the thread pool.\n+\n+        Parameters\n+        ----------\n+        value : boolean\n+\n+        Returns\n+        -------\n+        self : ScannerBuilder\n+        \"\"\"\n+        check_status(self.builder.UseThreads(value))\n+        return self\n+\n+    def schema(self):\n \n Review comment:\n   Should this be a property? (or is it something that needs to be calculated (potentially expensive))\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T15:06:36.629+0000",
                    "updated": "2019-12-02T15:06:36.629+0000",
                    "started": "2019-12-02T15:06:36.629+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "351966",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352011",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352653315\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n \n Review comment:\n   Do we need to expose the ScanTask to python consumers? I think this is only relevant for scheduling and dispatching internally in C++.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T16:39:14.122+0000",
                    "updated": "2019-12-02T16:39:14.122+0000",
                    "started": "2019-12-02T16:39:14.122+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352011",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352012",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352659432\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n+        \"\"\"Returns an iterator of ScanTasks\n+\n+        Each of ScanTask yields RecordBatches from this DataFragment.\n+\n+        Parameters\n+        ----------\n+        context : ScanContext, default None\n+\n+        Returns\n+        -------\n+        scan_tasks: Iterator[ScanTask]\n+        \"\"\"\n+        cdef:\n+            CResult[CScanTaskIterator] iterator_result\n+            CScanTaskIterator iterator\n+            CScanTaskPtr task\n+\n+        context = context or ScanContext()\n+        iterator_result = self.fragment.Scan(context.unwrap())\n+        iterator = move(GetResultValue(move(iterator_result)))\n+\n+        while True:\n+            iterator.Next(&task)\n+            if task.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield ScanTask.wrap(task)\n+\n+    @property\n+    def splittable(self):\n+        \"\"\"Get whether the DataFragment supports parallel scanning.\"\"\"\n+        return self.fragment.splittable()\n+\n+    @property\n+    def scan_options(self):\n+        \"\"\"Get the scan options used for scanning the data fragment.\n+\n+        Filtering, schema reconciliation, and partition options. None indicates\n+        that no filtering or schema reconciliation will be performed and all\n+        partitions will be scanned.\n+        \"\"\"\n+        cdef shared_ptr[CScanOptions] options = self.fragment.scan_options()\n+        if options.get() == nullptr:\n+            return None\n+        else:\n+            return ScanOptions.wrap(options)\n+\n+\n+cdef class SimpleDataFragment(DataFragment):\n+    \"\"\"A fragment that yields ScanTasks out of a fixed set of RecordBatches.\"\"\"\n+\n+    cdef:\n+        CSimpleDataFragment* simple_fragment\n+\n+    def __init__(self, record_batches):\n+        \"\"\"Create a simple data fragment from record batches.\n+\n+        Parameters\n+        ----------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            RecordBatch batch\n+            vector[shared_ptr[CRecordBatch]] batches\n+            shared_ptr[CSimpleDataFragment] simple_fragment\n+\n+        for batch in record_batches:\n+            batches.push_back(batch.sp_batch)\n+\n+        simple_fragment = make_shared[CSimpleDataFragment](batches)\n+        self.init(<shared_ptr[CDataFragment]> simple_fragment)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.simple_fragment = <CSimpleDataFragment*> sp.get()\n+\n+\n+cdef class FileDataFragment(DataFragment):\n+\n+    cdef:\n+        CFileDataFragment* file_fragment\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.file_fragment = <CFileDataFragment*> sp.get()\n+\n+    @property\n+    def source(self):\n+        cdef shared_ptr[CFileSource] source\n+        source = make_shared[CFileSource](self.file_fragment.source())\n+        return FileSource.wrap(source)\n+\n+\n+cdef class ParquetDataFragment(FileDataFragment):\n+\n+    def __init__(self, FileSource source not None,\n+                 ParquetScanOptions scan_options not None):\n+        cdef shared_ptr[CDataFragment] fragment\n+        fragment.reset(new CParquetDataFragment(\n+            deref(source.unwrap()),\n+            scan_options.unwrap()\n+        ))\n+        self.init(fragment)\n+\n+\n+cdef class DataSource:\n+    \"\"\"Basic component of a Dataset which yields zero or more DataFragments.\n+\n+    A DataSource acts as a discovery mechanism of DataFragments and partitions,\n+    e.g. files deeply nested in a directory.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataSource] wrapped\n+        CDataSource* source\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSource]& sp):\n+        cdef DataSource self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'tree_data_source':\n+            self = TreeDataSource.__new__(TreeDataSource)\n+        elif typ == 'simple_data_source':\n+            self = SimpleDataSource.__new__(SimpleDataSource)\n+        elif typ == 'filesystem_data_source':\n+            self = FileSystemDataSource.__new__(FileSystemDataSource)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef shared_ptr[CDataSource] unwrap(self):\n+        return self.wrapped\n+\n+    def fragments(self, ScanOptions scan_options=None):\n+        \"\"\"Get the data fragments of this data source.\n+\n+        Parameters\n+        ----------\n+        scan_options : ScanOptions, default None\n+            Controls filtering and schema inference.\n+\n+        Returns\n+        -------\n+        fragments : iterator of DataFragments\n+        \"\"\"\n+        cdef:\n+            CDataFragmentIterator iterator\n+            CDataFragmentPtr fragment\n+\n+        scan_options = scan_options or ScanOptions()\n+        iterator = self.source.GetFragments(scan_options.unwrap())\n+\n+        while True:\n+            iterator.Next(&fragment)\n+            if fragment.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield DataFragment.wrap(fragment)\n+\n+    @property\n+    def partition_expression(self):\n+        cdef shared_ptr[CExpression] expression\n+        expression = self.source.partition_expression()\n+        if expression.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expression)\n+\n+\n+cdef class SimpleDataSource(DataSource):\n+    \"\"\"A DataSource consisting of a flat sequence of DataFragments.\"\"\"\n+\n+    cdef:\n+        CSimpleDataSource* simple_source\n+\n+    def __init__(self, data_fragments):\n+        cdef:\n+            DataFragment fragment\n+            CDataFragmentVector fragments\n+            shared_ptr[CSimpleDataSource] simple_source\n+\n+        for fragment in data_fragments:\n+            fragments.push_back(fragment.unwrap())\n+\n+        simple_source = make_shared[CSimpleDataSource](fragments)\n+        self.init(<shared_ptr[CDataSource]> simple_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.simple_source = <CSimpleDataSource*> sp.get()\n+\n+\n+cdef class TreeDataSource(DataSource):\n+\n+    cdef:\n+        CTreeDataSource* tree_source\n+\n+    def __init__(self, data_sources):\n+        cdef:\n+            DataSource child\n+            CDataSourceVector children\n+            shared_ptr[CTreeDataSource] tree_source\n+\n+        for child in data_sources:\n+            children.push_back(child.wrapped)\n+\n+        tree_source = make_shared[CTreeDataSource](children)\n+        self.init(<shared_ptr[CDataSource]> tree_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.tree_source = <CTreeDataSource*> sp.get()\n+\n+\n+cdef class FileSystemDataSource(DataSource):\n+\n+    cdef:\n+        CFileSystemDataSource* filesystem_source\n+\n+    def __init__(self, FileSystem filesystem not None, file_stats,\n+                 Expression source_partition, dict path_partitions,\n+                 FileFormat file_format not None):\n+        cdef:\n+            FileStats stats\n+            Expression expression\n+            vector[CFileStats] c_file_stats\n+            shared_ptr[CExpression] c_source_partition\n+            unordered_map[c_string, shared_ptr[CExpression]] c_path_partitions\n+            CResult[shared_ptr[CDataSource]] result\n+\n+        for stats in file_stats:\n+            c_file_stats.push_back(stats.unwrap())\n+\n+        for path, expression in path_partitions.items():\n+            c_path_partitions[tobytes(path)] = expression.unwrap()\n+\n+        if source_partition is not None:\n+            c_source_partition = source_partition.unwrap()\n+\n+        result = CFileSystemDataSource.Make(\n+            filesystem.unwrap(),\n+            c_file_stats,\n+            c_source_partition,\n+            c_path_partitions,\n+            file_format.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.filesystem_source = <CFileSystemDataSource*> sp.get()\n+\n+\n+cdef class Dataset:\n+    \"\"\"Collection of data fragments coming from possibly multiple sources.\"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataset] wrapped\n+        CDataset* dataset\n+\n+    def __init__(self, data_sources, Schema schema not None):\n+        \"\"\"Create a dataset\n+\n+        Parameters\n+        ----------\n+        data_sources : list of DataSource\n+            One or more input data sources\n+        schema : Schema\n+            A known schema to conform to. The data sources must conform their\n+            output to this schema with projections and filters taken into\n+            account.\n+        \"\"\"\n+        cdef:\n+            DataSource source\n+            CDataSourceVector sources\n+            CResult[CDatasetPtr] result\n+            shared_ptr[CSchema] sp_schema\n+\n+        for source in data_sources:\n+            sources.push_back(source.wrapped)\n+\n+        result = CDataset.Make(sources, pyarrow_unwrap_schema(schema))\n+\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataset]& sp):\n+        self.wrapped = sp\n+        self.dataset = sp.get()\n+\n+    cdef inline shared_ptr[CDataset] unwrap(self):\n+        return self.wrapped\n+\n+    # TODO(kszucs): pass ScanContext\n+    def new_scan(self):\n+        \"\"\"Begin to build a new Scan operation against this Dataset.\"\"\"\n+        cdef shared_ptr[CScannerBuilder] builder\n+        builder = GetResultValue(self.dataset.NewScan())\n+        return ScannerBuilder.wrap(builder)\n+\n+    @property\n+    def sources(self):\n+        cdef vector[shared_ptr[CDataSource]] sources = self.dataset.sources()\n+        return [DataSource.wrap(source) for source in sources]\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.dataset.schema())\n+\n+\n+cdef class ScanOptions:\n \n Review comment:\n   I question if we should expose ScanOptions (and any methods requiring it). This is almost a private object constructed by scanner builder.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T16:39:14.170+0000",
                    "updated": "2019-12-02T16:39:14.170+0000",
                    "started": "2019-12-02T16:39:14.169+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352012",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352013",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352657811\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n+        \"\"\"Returns an iterator of ScanTasks\n+\n+        Each of ScanTask yields RecordBatches from this DataFragment.\n+\n+        Parameters\n+        ----------\n+        context : ScanContext, default None\n+\n+        Returns\n+        -------\n+        scan_tasks: Iterator[ScanTask]\n+        \"\"\"\n+        cdef:\n+            CResult[CScanTaskIterator] iterator_result\n+            CScanTaskIterator iterator\n+            CScanTaskPtr task\n+\n+        context = context or ScanContext()\n+        iterator_result = self.fragment.Scan(context.unwrap())\n+        iterator = move(GetResultValue(move(iterator_result)))\n+\n+        while True:\n+            iterator.Next(&task)\n+            if task.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield ScanTask.wrap(task)\n+\n+    @property\n+    def splittable(self):\n+        \"\"\"Get whether the DataFragment supports parallel scanning.\"\"\"\n+        return self.fragment.splittable()\n+\n+    @property\n+    def scan_options(self):\n+        \"\"\"Get the scan options used for scanning the data fragment.\n+\n+        Filtering, schema reconciliation, and partition options. None indicates\n+        that no filtering or schema reconciliation will be performed and all\n+        partitions will be scanned.\n+        \"\"\"\n+        cdef shared_ptr[CScanOptions] options = self.fragment.scan_options()\n+        if options.get() == nullptr:\n+            return None\n+        else:\n+            return ScanOptions.wrap(options)\n+\n+\n+cdef class SimpleDataFragment(DataFragment):\n+    \"\"\"A fragment that yields ScanTasks out of a fixed set of RecordBatches.\"\"\"\n+\n+    cdef:\n+        CSimpleDataFragment* simple_fragment\n+\n+    def __init__(self, record_batches):\n+        \"\"\"Create a simple data fragment from record batches.\n+\n+        Parameters\n+        ----------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            RecordBatch batch\n+            vector[shared_ptr[CRecordBatch]] batches\n+            shared_ptr[CSimpleDataFragment] simple_fragment\n+\n+        for batch in record_batches:\n+            batches.push_back(batch.sp_batch)\n+\n+        simple_fragment = make_shared[CSimpleDataFragment](batches)\n+        self.init(<shared_ptr[CDataFragment]> simple_fragment)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.simple_fragment = <CSimpleDataFragment*> sp.get()\n+\n+\n+cdef class FileDataFragment(DataFragment):\n+\n+    cdef:\n+        CFileDataFragment* file_fragment\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.file_fragment = <CFileDataFragment*> sp.get()\n+\n+    @property\n+    def source(self):\n+        cdef shared_ptr[CFileSource] source\n+        source = make_shared[CFileSource](self.file_fragment.source())\n+        return FileSource.wrap(source)\n+\n+\n+cdef class ParquetDataFragment(FileDataFragment):\n \n Review comment:\n   I'm not sure this should be exposed.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T16:39:14.184+0000",
                    "updated": "2019-12-02T16:39:14.184+0000",
                    "started": "2019-12-02T16:39:14.183+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352013",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352016",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352702732\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n \n Review comment:\n   I concur, this is an important feature because selector can't express all filters (and likely never will). This constructor exists as a convenience to crawl a directory.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T16:40:44.405+0000",
                    "updated": "2019-12-02T16:40:44.405+0000",
                    "started": "2019-12-02T16:40:44.404+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352016",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352111",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "fsaintjacques commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352773932\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n+        \"\"\"Returns an iterator of ScanTasks\n+\n+        Each of ScanTask yields RecordBatches from this DataFragment.\n+\n+        Parameters\n+        ----------\n+        context : ScanContext, default None\n+\n+        Returns\n+        -------\n+        scan_tasks: Iterator[ScanTask]\n+        \"\"\"\n+        cdef:\n+            CResult[CScanTaskIterator] iterator_result\n+            CScanTaskIterator iterator\n+            CScanTaskPtr task\n+\n+        context = context or ScanContext()\n+        iterator_result = self.fragment.Scan(context.unwrap())\n+        iterator = move(GetResultValue(move(iterator_result)))\n+\n+        while True:\n+            iterator.Next(&task)\n+            if task.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield ScanTask.wrap(task)\n+\n+    @property\n+    def splittable(self):\n+        \"\"\"Get whether the DataFragment supports parallel scanning.\"\"\"\n+        return self.fragment.splittable()\n+\n+    @property\n+    def scan_options(self):\n+        \"\"\"Get the scan options used for scanning the data fragment.\n+\n+        Filtering, schema reconciliation, and partition options. None indicates\n+        that no filtering or schema reconciliation will be performed and all\n+        partitions will be scanned.\n+        \"\"\"\n+        cdef shared_ptr[CScanOptions] options = self.fragment.scan_options()\n+        if options.get() == nullptr:\n+            return None\n+        else:\n+            return ScanOptions.wrap(options)\n+\n+\n+cdef class SimpleDataFragment(DataFragment):\n+    \"\"\"A fragment that yields ScanTasks out of a fixed set of RecordBatches.\"\"\"\n+\n+    cdef:\n+        CSimpleDataFragment* simple_fragment\n+\n+    def __init__(self, record_batches):\n+        \"\"\"Create a simple data fragment from record batches.\n+\n+        Parameters\n+        ----------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            RecordBatch batch\n+            vector[shared_ptr[CRecordBatch]] batches\n+            shared_ptr[CSimpleDataFragment] simple_fragment\n+\n+        for batch in record_batches:\n+            batches.push_back(batch.sp_batch)\n+\n+        simple_fragment = make_shared[CSimpleDataFragment](batches)\n+        self.init(<shared_ptr[CDataFragment]> simple_fragment)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.simple_fragment = <CSimpleDataFragment*> sp.get()\n+\n+\n+cdef class FileDataFragment(DataFragment):\n+\n+    cdef:\n+        CFileDataFragment* file_fragment\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.file_fragment = <CFileDataFragment*> sp.get()\n+\n+    @property\n+    def source(self):\n+        cdef shared_ptr[CFileSource] source\n+        source = make_shared[CFileSource](self.file_fragment.source())\n+        return FileSource.wrap(source)\n+\n+\n+cdef class ParquetDataFragment(FileDataFragment):\n+\n+    def __init__(self, FileSource source not None,\n+                 ParquetScanOptions scan_options not None):\n+        cdef shared_ptr[CDataFragment] fragment\n+        fragment.reset(new CParquetDataFragment(\n+            deref(source.unwrap()),\n+            scan_options.unwrap()\n+        ))\n+        self.init(fragment)\n+\n+\n+cdef class DataSource:\n+    \"\"\"Basic component of a Dataset which yields zero or more DataFragments.\n+\n+    A DataSource acts as a discovery mechanism of DataFragments and partitions,\n+    e.g. files deeply nested in a directory.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataSource] wrapped\n+        CDataSource* source\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSource]& sp):\n+        cdef DataSource self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'tree_data_source':\n+            self = TreeDataSource.__new__(TreeDataSource)\n+        elif typ == 'simple_data_source':\n+            self = SimpleDataSource.__new__(SimpleDataSource)\n+        elif typ == 'filesystem_data_source':\n+            self = FileSystemDataSource.__new__(FileSystemDataSource)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef shared_ptr[CDataSource] unwrap(self):\n+        return self.wrapped\n+\n+    def fragments(self, ScanOptions scan_options=None):\n+        \"\"\"Get the data fragments of this data source.\n+\n+        Parameters\n+        ----------\n+        scan_options : ScanOptions, default None\n+            Controls filtering and schema inference.\n+\n+        Returns\n+        -------\n+        fragments : iterator of DataFragments\n+        \"\"\"\n+        cdef:\n+            CDataFragmentIterator iterator\n+            CDataFragmentPtr fragment\n+\n+        scan_options = scan_options or ScanOptions()\n+        iterator = self.source.GetFragments(scan_options.unwrap())\n+\n+        while True:\n+            iterator.Next(&fragment)\n+            if fragment.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield DataFragment.wrap(fragment)\n+\n+    @property\n+    def partition_expression(self):\n+        cdef shared_ptr[CExpression] expression\n+        expression = self.source.partition_expression()\n+        if expression.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expression)\n+\n+\n+cdef class SimpleDataSource(DataSource):\n+    \"\"\"A DataSource consisting of a flat sequence of DataFragments.\"\"\"\n+\n+    cdef:\n+        CSimpleDataSource* simple_source\n+\n+    def __init__(self, data_fragments):\n+        cdef:\n+            DataFragment fragment\n+            CDataFragmentVector fragments\n+            shared_ptr[CSimpleDataSource] simple_source\n+\n+        for fragment in data_fragments:\n+            fragments.push_back(fragment.unwrap())\n+\n+        simple_source = make_shared[CSimpleDataSource](fragments)\n+        self.init(<shared_ptr[CDataSource]> simple_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.simple_source = <CSimpleDataSource*> sp.get()\n+\n+\n+cdef class TreeDataSource(DataSource):\n+\n+    cdef:\n+        CTreeDataSource* tree_source\n+\n+    def __init__(self, data_sources):\n+        cdef:\n+            DataSource child\n+            CDataSourceVector children\n+            shared_ptr[CTreeDataSource] tree_source\n+\n+        for child in data_sources:\n+            children.push_back(child.wrapped)\n+\n+        tree_source = make_shared[CTreeDataSource](children)\n+        self.init(<shared_ptr[CDataSource]> tree_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.tree_source = <CTreeDataSource*> sp.get()\n+\n+\n+cdef class FileSystemDataSource(DataSource):\n+\n+    cdef:\n+        CFileSystemDataSource* filesystem_source\n+\n+    def __init__(self, FileSystem filesystem not None, file_stats,\n+                 Expression source_partition, dict path_partitions,\n+                 FileFormat file_format not None):\n+        cdef:\n+            FileStats stats\n+            Expression expression\n+            vector[CFileStats] c_file_stats\n+            shared_ptr[CExpression] c_source_partition\n+            unordered_map[c_string, shared_ptr[CExpression]] c_path_partitions\n+            CResult[shared_ptr[CDataSource]] result\n+\n+        for stats in file_stats:\n+            c_file_stats.push_back(stats.unwrap())\n+\n+        for path, expression in path_partitions.items():\n+            c_path_partitions[tobytes(path)] = expression.unwrap()\n+\n+        if source_partition is not None:\n+            c_source_partition = source_partition.unwrap()\n+\n+        result = CFileSystemDataSource.Make(\n+            filesystem.unwrap(),\n+            c_file_stats,\n+            c_source_partition,\n+            c_path_partitions,\n+            file_format.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.filesystem_source = <CFileSystemDataSource*> sp.get()\n+\n+\n+cdef class Dataset:\n+    \"\"\"Collection of data fragments coming from possibly multiple sources.\"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataset] wrapped\n+        CDataset* dataset\n+\n+    def __init__(self, data_sources, Schema schema not None):\n+        \"\"\"Create a dataset\n+\n+        Parameters\n+        ----------\n+        data_sources : list of DataSource\n+            One or more input data sources\n+        schema : Schema\n+            A known schema to conform to. The data sources must conform their\n+            output to this schema with projections and filters taken into\n+            account.\n+        \"\"\"\n+        cdef:\n+            DataSource source\n+            CDataSourceVector sources\n+            CResult[CDatasetPtr] result\n+            shared_ptr[CSchema] sp_schema\n+\n+        for source in data_sources:\n+            sources.push_back(source.wrapped)\n+\n+        result = CDataset.Make(sources, pyarrow_unwrap_schema(schema))\n+\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataset]& sp):\n+        self.wrapped = sp\n+        self.dataset = sp.get()\n+\n+    cdef inline shared_ptr[CDataset] unwrap(self):\n+        return self.wrapped\n+\n+    # TODO(kszucs): pass ScanContext\n+    def new_scan(self):\n+        \"\"\"Begin to build a new Scan operation against this Dataset.\"\"\"\n+        cdef shared_ptr[CScannerBuilder] builder\n+        builder = GetResultValue(self.dataset.NewScan())\n+        return ScannerBuilder.wrap(builder)\n+\n+    @property\n+    def sources(self):\n+        cdef vector[shared_ptr[CDataSource]] sources = self.dataset.sources()\n+        return [DataSource.wrap(source) for source in sources]\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.dataset.schema())\n+\n+\n+cdef class ScanOptions:\n+\n+    cdef:\n+        shared_ptr[CScanOptions] wrapped\n+        CScanOptions* options\n+\n+    def __init__(self, Schema schema=None):\n+        self.init(CScanOptions.Defaults())\n+        if schema is not None:\n+            self.schema = schema\n+\n+    cdef init(self, const shared_ptr[CScanOptions]& sp):\n+        self.wrapped = sp\n+        self.options = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CScanOptions]& sp):\n+        cdef ScanOptions self = ScanOptions.__new__(ScanOptions)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanOptions] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        \"\"\"Schema to which record batches will be reconciled\"\"\"\n+        if self.options.schema == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(self.options.schema)\n+\n+    @schema.setter\n+    def schema(self, Schema value):\n+        self.options.schema = pyarrow_unwrap_schema(value)\n+\n+    @property\n+    def use_threads(self):\n+        \"\"\"Make use of the thread pool found in the scan context\"\"\"\n+        return self.options.use_threads\n+\n+    @use_threads.setter\n+    def use_threads(self, bint value):\n+        self.options.use_threads = value\n+\n+\n+cdef class FileScanOptions(ScanOptions):\n+    pass\n+\n+\n+cdef class ParquetScanOptions(FileScanOptions):\n+    pass\n+\n+\n+cdef class WriteOptions:\n+    pass\n+\n+\n+cdef class FileWriteOptions(WriteOptions):\n+    pass\n+\n+\n+cdef class ParquetWriterOptions(FileWriteOptions):\n+    pass\n+\n+\n+cdef class ScanContext:\n+\n+    cdef:\n+        shared_ptr[CScanContext] wrapped\n+        CScanContext *context\n+\n+    def __init__(self, MemoryPool memory_pool=None):\n+        cdef shared_ptr[CScanContext] context = make_shared[CScanContext]()\n+        self.init(context)\n+        if memory_pool is not None:\n+            self.memory_pool = memory_pool\n+\n+    cdef init(self, shared_ptr[CScanContext]& sp):\n+        self.wrapped = sp\n+        self.context = sp.get()\n+\n+    @staticmethod\n+    cdef ScanContext wrap(shared_ptr[CScanContext]& sp):\n+        cdef ScanContext self = ScanContext.__new__(ScanContext)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanContext] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def memory_pool(self):\n+        # TODO(kszucs): there's no function to box/wrap memory pool\n+        cdef:\n+            MemoryPool pool = MemoryPool.__new__(MemoryPool)\n+        # this might be unsafe because of the raw pointers\n+        pool.init(self.context.pool)\n+        return pool\n+\n+    @memory_pool.setter\n+    def memory_pool(self, MemoryPool pool):\n+        self.context.pool = maybe_unbox_memory_pool(pool)\n+\n+\n+cdef class ScanTask:\n+    \"\"\"Read record batches from a range of a single data fragment.\n+\n+    A ScanTask is meant to be a unit of work to be dispatched.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CScanTask] wrapped\n+        CScanTask* task\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__, subclasses_instead=False)\n+\n+    cdef init(self, shared_ptr[CScanTask]& sp):\n+        self.wrapped = sp\n+        self.task = self.wrapped.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CScanTask]& sp):\n+        cdef SimpleScanTask self = SimpleScanTask.__new__(SimpleScanTask)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanTask] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self):\n+        \"\"\"Iterate through sequence of materialized record batches.\n+\n+        Execution semantics are encapsulated in the particular ScanTask\n+        implementation.\n+\n+        Returns\n+        -------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            CRecordBatchIterator iterator\n+            shared_ptr[CRecordBatch] record_batch\n+\n+        iterator = move(GetResultValue(move(self.task.Scan())))\n+\n+        while True:\n+            iterator.Next(&record_batch)\n+            if record_batch.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield pyarrow_wrap_batch(record_batch)\n+\n+\n+cdef class SimpleScanTask(ScanTask):\n+    \"\"\"A trivial ScanTask that yields the RecordBatch of an array.\"\"\"\n+\n+    cdef:\n+        CSimpleScanTask* simple_task\n+\n+    cdef init(self, shared_ptr[CScanTask]& sp):\n+        ScanTask.init(self, sp)\n+        self.simple_task = <CSimpleScanTask*> sp.get()\n+\n+\n+cdef class ScannerBuilder:\n+    \"\"\"Factory class to construct a Scanner.\n+\n+    It is used to pass information, notably a potential filter expression and a\n+    subset of columns to materialize.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CScannerBuilder] wrapped\n+        CScannerBuilder* builder\n+\n+    def __init__(self, Dataset dataset not None, ScanContext context not None):\n+        cdef shared_ptr[CScannerBuilder] builder\n+        builder = make_shared[CScannerBuilder](\n+            dataset.unwrap(), context.unwrap()\n+        )\n+        self.init(builder)\n+\n+    cdef void init(self, shared_ptr[CScannerBuilder]& sp):\n+        self.wrapped = sp\n+        self.builder = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CScannerBuilder]& sp):\n+        cdef ScannerBuilder self = ScannerBuilder.__new__(ScannerBuilder)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScannerBuilder] unwrap(self):\n+        return self.wrapped\n+\n+    def project(self, columns):\n+        \"\"\"Set the subset of columns to materialize.\n+\n+        This subset will be passed down to DataSources and corresponding\n+        DataFragments. The goal is to avoid loading, copying, and deserializing\n+        columns that will not be required further down the compute chain.\n+\n+        Raises exception if any column name does not exists in the dataset's\n+        Schema.\n+\n+        Parameters\n+        ----------\n+        columns : list of str\n+            List of columns to project. Order and duplicates will be preserved.\n+\n+        Returns\n+        -------\n+        self : ScannerBuilder\n+        \"\"\"\n+        cdef vector[c_string] cols = [tobytes(c) for c in columns]\n+        check_status(self.builder.Project(cols))\n+        return self\n+\n+    def finish(self):\n+        \"\"\"Return the constructed now-immutable Scanner object\"\"\"\n+        return Scanner.wrap(GetResultValue(self.builder.Finish()))\n+\n+    def filter(self, Expression filter_expression not None):\n+        \"\"\"Set the filter expression to return only rows matching the filter.\n+\n+        The predicate will be passed down to DataSources and corresponding\n+        DataFragments to exploit predicate pushdown if possible using partition\n+        information or DataFragment internal metadata, e.g. Parquet statistics.\n+\n+        Raises exception if any of the referenced columns does not exists in\n+        the dataset's schema.\n+\n+        Parameters\n+        ----------\n+        filter_expression : Expression\n+            Boolean expression to filter rows with.\n+\n+        Returns\n+        -------\n+        self : ScannerBuilder\n+        \"\"\"\n+        check_status(self.builder.Filter(filter_expression.unwrap()))\n \n Review comment:\n   You need to add an [implicit cast like in R](https://github.com/apache/arrow/blob/master/r/src/dataset.cpp#L92) for this to be bearable. Otherwise you get annoying errors:\r\n   \r\n   ```\r\n   In [41]: cond = ds.ComparisonExpression(ds.CompareOperator.Greater, ds.FieldExpression(\"total_amount\"), ds.ScalarExpression(1000.0))                           \r\n   \r\n   In [42]: scanner_builder.filter(cond)                                          \r\n   ---------------------------------------------------------------------------\r\n   ArrowTypeError                            Traceback (most recent call last)\r\n   <ipython-input-42-bb6fba558cf8> in <module>\r\n   ----> 1 scanner_builder.filter(cond)\r\n   \r\n   ~/src/db/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.ScannerBuilder.filter()\r\n       951         self : ScannerBuilder\r\n       952         \"\"\"\r\n   --> 953         check_status(self.builder.Filter(filter_expression.unwrap()))\r\n       954         return self\r\n       955 \r\n   \r\n   ~/src/db/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n        86             raise ArrowNotImplementedError(message)\r\n        87         elif status.IsTypeError():\r\n   ---> 88             raise ArrowTypeError(message)\r\n        89         elif status.IsCapacityError():\r\n        90             raise ArrowCapacityError(message)\r\n   \r\n   ArrowTypeError: cannot compare expressions of differing type, float vs double\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T19:05:19.620+0000",
                    "updated": "2019-12-02T19:05:19.620+0000",
                    "started": "2019-12-02T19:05:19.620+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352111",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352141",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352790544\n \n \n\n ##########\n File path: cpp/src/arrow/dataset/discovery.h\n ##########\n @@ -85,6 +85,7 @@ class ARROW_DS_EXPORT DataSourceDiscovery {\n   ExpressionPtr root_partition_;\n };\n \n+// TODO(kszucs): a static Defaults method would be nice\n \n Review comment:\n   I like the defaults static method BTW, rust has a trait for that which provides a pretty and explicit API.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T19:41:31.364+0000",
                    "updated": "2019-12-02T19:41:31.364+0000",
                    "started": "2019-12-02T19:41:31.363+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352141",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352142",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352782664\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n \n Review comment:\n   And let the `scan()` here directly iterate thought the scan tasks, execute them and returns RecordBatches? \r\n   \r\n   (that would certainly make the python api easier to use for the basic use case if you can go directly from Scanner -> list of RecordBatches)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T19:42:05.429+0000",
                    "updated": "2019-12-02T19:42:05.429+0000",
                    "started": "2019-12-02T19:42:05.428+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352142",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352143",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352781624\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n \n Review comment:\n   Maybe mention this is not directly created but is only created from `DataSource.fragments()`\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T19:42:05.509+0000",
                    "updated": "2019-12-02T19:42:05.509+0000",
                    "started": "2019-12-02T19:42:05.508+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352143",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352144",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352786771\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n+        \"\"\"Returns an iterator of ScanTasks\n+\n+        Each of ScanTask yields RecordBatches from this DataFragment.\n+\n+        Parameters\n+        ----------\n+        context : ScanContext, default None\n+\n+        Returns\n+        -------\n+        scan_tasks: Iterator[ScanTask]\n+        \"\"\"\n+        cdef:\n+            CResult[CScanTaskIterator] iterator_result\n+            CScanTaskIterator iterator\n+            CScanTaskPtr task\n+\n+        context = context or ScanContext()\n+        iterator_result = self.fragment.Scan(context.unwrap())\n+        iterator = move(GetResultValue(move(iterator_result)))\n+\n+        while True:\n+            iterator.Next(&task)\n+            if task.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield ScanTask.wrap(task)\n+\n+    @property\n+    def splittable(self):\n+        \"\"\"Get whether the DataFragment supports parallel scanning.\"\"\"\n+        return self.fragment.splittable()\n+\n+    @property\n+    def scan_options(self):\n+        \"\"\"Get the scan options used for scanning the data fragment.\n+\n+        Filtering, schema reconciliation, and partition options. None indicates\n+        that no filtering or schema reconciliation will be performed and all\n+        partitions will be scanned.\n+        \"\"\"\n+        cdef shared_ptr[CScanOptions] options = self.fragment.scan_options()\n+        if options.get() == nullptr:\n+            return None\n+        else:\n+            return ScanOptions.wrap(options)\n+\n+\n+cdef class SimpleDataFragment(DataFragment):\n+    \"\"\"A fragment that yields ScanTasks out of a fixed set of RecordBatches.\"\"\"\n+\n+    cdef:\n+        CSimpleDataFragment* simple_fragment\n+\n+    def __init__(self, record_batches):\n+        \"\"\"Create a simple data fragment from record batches.\n+\n+        Parameters\n+        ----------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            RecordBatch batch\n+            vector[shared_ptr[CRecordBatch]] batches\n+            shared_ptr[CSimpleDataFragment] simple_fragment\n+\n+        for batch in record_batches:\n+            batches.push_back(batch.sp_batch)\n+\n+        simple_fragment = make_shared[CSimpleDataFragment](batches)\n+        self.init(<shared_ptr[CDataFragment]> simple_fragment)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.simple_fragment = <CSimpleDataFragment*> sp.get()\n+\n+\n+cdef class FileDataFragment(DataFragment):\n+\n+    cdef:\n+        CFileDataFragment* file_fragment\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.file_fragment = <CFileDataFragment*> sp.get()\n+\n+    @property\n+    def source(self):\n+        cdef shared_ptr[CFileSource] source\n+        source = make_shared[CFileSource](self.file_fragment.source())\n+        return FileSource.wrap(source)\n+\n+\n+cdef class ParquetDataFragment(FileDataFragment):\n+\n+    def __init__(self, FileSource source not None,\n+                 ParquetScanOptions scan_options not None):\n+        cdef shared_ptr[CDataFragment] fragment\n+        fragment.reset(new CParquetDataFragment(\n+            deref(source.unwrap()),\n+            scan_options.unwrap()\n+        ))\n+        self.init(fragment)\n+\n+\n+cdef class DataSource:\n+    \"\"\"Basic component of a Dataset which yields zero or more DataFragments.\n+\n+    A DataSource acts as a discovery mechanism of DataFragments and partitions,\n+    e.g. files deeply nested in a directory.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataSource] wrapped\n+        CDataSource* source\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSource]& sp):\n+        cdef DataSource self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'tree_data_source':\n+            self = TreeDataSource.__new__(TreeDataSource)\n+        elif typ == 'simple_data_source':\n+            self = SimpleDataSource.__new__(SimpleDataSource)\n+        elif typ == 'filesystem_data_source':\n+            self = FileSystemDataSource.__new__(FileSystemDataSource)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef shared_ptr[CDataSource] unwrap(self):\n+        return self.wrapped\n+\n+    def fragments(self, ScanOptions scan_options=None):\n+        \"\"\"Get the data fragments of this data source.\n+\n+        Parameters\n+        ----------\n+        scan_options : ScanOptions, default None\n+            Controls filtering and schema inference.\n+\n+        Returns\n+        -------\n+        fragments : iterator of DataFragments\n+        \"\"\"\n+        cdef:\n+            CDataFragmentIterator iterator\n+            CDataFragmentPtr fragment\n+\n+        scan_options = scan_options or ScanOptions()\n+        iterator = self.source.GetFragments(scan_options.unwrap())\n+\n+        while True:\n+            iterator.Next(&fragment)\n+            if fragment.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield DataFragment.wrap(fragment)\n+\n+    @property\n+    def partition_expression(self):\n+        cdef shared_ptr[CExpression] expression\n+        expression = self.source.partition_expression()\n+        if expression.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expression)\n+\n+\n+cdef class SimpleDataSource(DataSource):\n+    \"\"\"A DataSource consisting of a flat sequence of DataFragments.\"\"\"\n+\n+    cdef:\n+        CSimpleDataSource* simple_source\n+\n+    def __init__(self, data_fragments):\n+        cdef:\n+            DataFragment fragment\n+            CDataFragmentVector fragments\n+            shared_ptr[CSimpleDataSource] simple_source\n+\n+        for fragment in data_fragments:\n+            fragments.push_back(fragment.unwrap())\n+\n+        simple_source = make_shared[CSimpleDataSource](fragments)\n+        self.init(<shared_ptr[CDataSource]> simple_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.simple_source = <CSimpleDataSource*> sp.get()\n+\n+\n+cdef class TreeDataSource(DataSource):\n+\n+    cdef:\n+        CTreeDataSource* tree_source\n+\n+    def __init__(self, data_sources):\n+        cdef:\n+            DataSource child\n+            CDataSourceVector children\n+            shared_ptr[CTreeDataSource] tree_source\n+\n+        for child in data_sources:\n+            children.push_back(child.wrapped)\n+\n+        tree_source = make_shared[CTreeDataSource](children)\n+        self.init(<shared_ptr[CDataSource]> tree_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.tree_source = <CTreeDataSource*> sp.get()\n+\n+\n+cdef class FileSystemDataSource(DataSource):\n+\n+    cdef:\n+        CFileSystemDataSource* filesystem_source\n+\n+    def __init__(self, FileSystem filesystem not None, file_stats,\n+                 Expression source_partition, dict path_partitions,\n+                 FileFormat file_format not None):\n+        cdef:\n+            FileStats stats\n+            Expression expression\n+            vector[CFileStats] c_file_stats\n+            shared_ptr[CExpression] c_source_partition\n+            unordered_map[c_string, shared_ptr[CExpression]] c_path_partitions\n+            CResult[shared_ptr[CDataSource]] result\n+\n+        for stats in file_stats:\n+            c_file_stats.push_back(stats.unwrap())\n+\n+        for path, expression in path_partitions.items():\n+            c_path_partitions[tobytes(path)] = expression.unwrap()\n+\n+        if source_partition is not None:\n+            c_source_partition = source_partition.unwrap()\n+\n+        result = CFileSystemDataSource.Make(\n+            filesystem.unwrap(),\n+            c_file_stats,\n+            c_source_partition,\n+            c_path_partitions,\n+            file_format.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.filesystem_source = <CFileSystemDataSource*> sp.get()\n+\n+\n+cdef class Dataset:\n+    \"\"\"Collection of data fragments coming from possibly multiple sources.\"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataset] wrapped\n+        CDataset* dataset\n+\n+    def __init__(self, data_sources, Schema schema not None):\n+        \"\"\"Create a dataset\n+\n+        Parameters\n+        ----------\n+        data_sources : list of DataSource\n+            One or more input data sources\n+        schema : Schema\n+            A known schema to conform to. The data sources must conform their\n+            output to this schema with projections and filters taken into\n+            account.\n \n Review comment:\n   It's not fully clear to me what this last sentence exactly means. \r\n   Does it mean this scheme will be used to filter/project the output of the data sources, or that the other projections/filters specified when scanning should result in something that matches this schema?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T19:42:05.584+0000",
                    "updated": "2019-12-02T19:42:05.584+0000",
                    "started": "2019-12-02T19:42:05.584+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352144",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352145",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352788692\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n+        \"\"\"Returns an iterator of ScanTasks\n+\n+        Each of ScanTask yields RecordBatches from this DataFragment.\n+\n+        Parameters\n+        ----------\n+        context : ScanContext, default None\n+\n+        Returns\n+        -------\n+        scan_tasks: Iterator[ScanTask]\n+        \"\"\"\n+        cdef:\n+            CResult[CScanTaskIterator] iterator_result\n+            CScanTaskIterator iterator\n+            CScanTaskPtr task\n+\n+        context = context or ScanContext()\n+        iterator_result = self.fragment.Scan(context.unwrap())\n+        iterator = move(GetResultValue(move(iterator_result)))\n+\n+        while True:\n+            iterator.Next(&task)\n+            if task.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield ScanTask.wrap(task)\n+\n+    @property\n+    def splittable(self):\n+        \"\"\"Get whether the DataFragment supports parallel scanning.\"\"\"\n+        return self.fragment.splittable()\n+\n+    @property\n+    def scan_options(self):\n+        \"\"\"Get the scan options used for scanning the data fragment.\n+\n+        Filtering, schema reconciliation, and partition options. None indicates\n+        that no filtering or schema reconciliation will be performed and all\n+        partitions will be scanned.\n+        \"\"\"\n+        cdef shared_ptr[CScanOptions] options = self.fragment.scan_options()\n+        if options.get() == nullptr:\n+            return None\n+        else:\n+            return ScanOptions.wrap(options)\n+\n+\n+cdef class SimpleDataFragment(DataFragment):\n+    \"\"\"A fragment that yields ScanTasks out of a fixed set of RecordBatches.\"\"\"\n+\n+    cdef:\n+        CSimpleDataFragment* simple_fragment\n+\n+    def __init__(self, record_batches):\n+        \"\"\"Create a simple data fragment from record batches.\n+\n+        Parameters\n+        ----------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            RecordBatch batch\n+            vector[shared_ptr[CRecordBatch]] batches\n+            shared_ptr[CSimpleDataFragment] simple_fragment\n+\n+        for batch in record_batches:\n+            batches.push_back(batch.sp_batch)\n+\n+        simple_fragment = make_shared[CSimpleDataFragment](batches)\n+        self.init(<shared_ptr[CDataFragment]> simple_fragment)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.simple_fragment = <CSimpleDataFragment*> sp.get()\n+\n+\n+cdef class FileDataFragment(DataFragment):\n+\n+    cdef:\n+        CFileDataFragment* file_fragment\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.file_fragment = <CFileDataFragment*> sp.get()\n+\n+    @property\n+    def source(self):\n+        cdef shared_ptr[CFileSource] source\n+        source = make_shared[CFileSource](self.file_fragment.source())\n+        return FileSource.wrap(source)\n+\n+\n+cdef class ParquetDataFragment(FileDataFragment):\n+\n+    def __init__(self, FileSource source not None,\n+                 ParquetScanOptions scan_options not None):\n+        cdef shared_ptr[CDataFragment] fragment\n+        fragment.reset(new CParquetDataFragment(\n+            deref(source.unwrap()),\n+            scan_options.unwrap()\n+        ))\n+        self.init(fragment)\n+\n+\n+cdef class DataSource:\n+    \"\"\"Basic component of a Dataset which yields zero or more DataFragments.\n+\n+    A DataSource acts as a discovery mechanism of DataFragments and partitions,\n+    e.g. files deeply nested in a directory.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataSource] wrapped\n+        CDataSource* source\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSource]& sp):\n+        cdef DataSource self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'tree_data_source':\n+            self = TreeDataSource.__new__(TreeDataSource)\n+        elif typ == 'simple_data_source':\n+            self = SimpleDataSource.__new__(SimpleDataSource)\n+        elif typ == 'filesystem_data_source':\n+            self = FileSystemDataSource.__new__(FileSystemDataSource)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef shared_ptr[CDataSource] unwrap(self):\n+        return self.wrapped\n+\n+    def fragments(self, ScanOptions scan_options=None):\n+        \"\"\"Get the data fragments of this data source.\n+\n+        Parameters\n+        ----------\n+        scan_options : ScanOptions, default None\n+            Controls filtering and schema inference.\n+\n+        Returns\n+        -------\n+        fragments : iterator of DataFragments\n+        \"\"\"\n+        cdef:\n+            CDataFragmentIterator iterator\n+            CDataFragmentPtr fragment\n+\n+        scan_options = scan_options or ScanOptions()\n+        iterator = self.source.GetFragments(scan_options.unwrap())\n+\n+        while True:\n+            iterator.Next(&fragment)\n+            if fragment.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield DataFragment.wrap(fragment)\n+\n+    @property\n+    def partition_expression(self):\n+        cdef shared_ptr[CExpression] expression\n+        expression = self.source.partition_expression()\n+        if expression.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expression)\n+\n+\n+cdef class SimpleDataSource(DataSource):\n+    \"\"\"A DataSource consisting of a flat sequence of DataFragments.\"\"\"\n+\n+    cdef:\n+        CSimpleDataSource* simple_source\n+\n+    def __init__(self, data_fragments):\n+        cdef:\n+            DataFragment fragment\n+            CDataFragmentVector fragments\n+            shared_ptr[CSimpleDataSource] simple_source\n+\n+        for fragment in data_fragments:\n+            fragments.push_back(fragment.unwrap())\n+\n+        simple_source = make_shared[CSimpleDataSource](fragments)\n+        self.init(<shared_ptr[CDataSource]> simple_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.simple_source = <CSimpleDataSource*> sp.get()\n+\n+\n+cdef class TreeDataSource(DataSource):\n+\n+    cdef:\n+        CTreeDataSource* tree_source\n+\n+    def __init__(self, data_sources):\n+        cdef:\n+            DataSource child\n+            CDataSourceVector children\n+            shared_ptr[CTreeDataSource] tree_source\n+\n+        for child in data_sources:\n+            children.push_back(child.wrapped)\n+\n+        tree_source = make_shared[CTreeDataSource](children)\n+        self.init(<shared_ptr[CDataSource]> tree_source)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.tree_source = <CTreeDataSource*> sp.get()\n+\n+\n+cdef class FileSystemDataSource(DataSource):\n+\n+    cdef:\n+        CFileSystemDataSource* filesystem_source\n+\n+    def __init__(self, FileSystem filesystem not None, file_stats,\n+                 Expression source_partition, dict path_partitions,\n+                 FileFormat file_format not None):\n+        cdef:\n+            FileStats stats\n+            Expression expression\n+            vector[CFileStats] c_file_stats\n+            shared_ptr[CExpression] c_source_partition\n+            unordered_map[c_string, shared_ptr[CExpression]] c_path_partitions\n+            CResult[shared_ptr[CDataSource]] result\n+\n+        for stats in file_stats:\n+            c_file_stats.push_back(stats.unwrap())\n+\n+        for path, expression in path_partitions.items():\n+            c_path_partitions[tobytes(path)] = expression.unwrap()\n+\n+        if source_partition is not None:\n+            c_source_partition = source_partition.unwrap()\n+\n+        result = CFileSystemDataSource.Make(\n+            filesystem.unwrap(),\n+            c_file_stats,\n+            c_source_partition,\n+            c_path_partitions,\n+            file_format.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        DataSource.init(self, sp)\n+        self.filesystem_source = <CFileSystemDataSource*> sp.get()\n+\n+\n+cdef class Dataset:\n+    \"\"\"Collection of data fragments coming from possibly multiple sources.\"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataset] wrapped\n+        CDataset* dataset\n+\n+    def __init__(self, data_sources, Schema schema not None):\n+        \"\"\"Create a dataset\n+\n+        Parameters\n+        ----------\n+        data_sources : list of DataSource\n+            One or more input data sources\n+        schema : Schema\n+            A known schema to conform to. The data sources must conform their\n+            output to this schema with projections and filters taken into\n+            account.\n+        \"\"\"\n+        cdef:\n+            DataSource source\n+            CDataSourceVector sources\n+            CResult[CDatasetPtr] result\n+            shared_ptr[CSchema] sp_schema\n+\n+        for source in data_sources:\n+            sources.push_back(source.wrapped)\n+\n+        result = CDataset.Make(sources, pyarrow_unwrap_schema(schema))\n+\n+        self.init(GetResultValue(result))\n+\n+    cdef void init(self, const shared_ptr[CDataset]& sp):\n+        self.wrapped = sp\n+        self.dataset = sp.get()\n+\n+    cdef inline shared_ptr[CDataset] unwrap(self):\n+        return self.wrapped\n+\n+    # TODO(kszucs): pass ScanContext\n+    def new_scan(self):\n+        \"\"\"Begin to build a new Scan operation against this Dataset.\"\"\"\n+        cdef shared_ptr[CScannerBuilder] builder\n+        builder = GetResultValue(self.dataset.NewScan())\n+        return ScannerBuilder.wrap(builder)\n+\n+    @property\n+    def sources(self):\n+        cdef vector[shared_ptr[CDataSource]] sources = self.dataset.sources()\n+        return [DataSource.wrap(source) for source in sources]\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.dataset.schema())\n+\n+\n+cdef class ScanOptions:\n+\n+    cdef:\n+        shared_ptr[CScanOptions] wrapped\n+        CScanOptions* options\n+\n+    def __init__(self, Schema schema=None):\n+        self.init(CScanOptions.Defaults())\n+        if schema is not None:\n+            self.schema = schema\n+\n+    cdef init(self, const shared_ptr[CScanOptions]& sp):\n+        self.wrapped = sp\n+        self.options = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CScanOptions]& sp):\n+        cdef ScanOptions self = ScanOptions.__new__(ScanOptions)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanOptions] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        \"\"\"Schema to which record batches will be reconciled\"\"\"\n+        if self.options.schema == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(self.options.schema)\n+\n+    @schema.setter\n+    def schema(self, Schema value):\n+        self.options.schema = pyarrow_unwrap_schema(value)\n+\n+    @property\n+    def use_threads(self):\n+        \"\"\"Make use of the thread pool found in the scan context\"\"\"\n+        return self.options.use_threads\n+\n+    @use_threads.setter\n+    def use_threads(self, bint value):\n+        self.options.use_threads = value\n+\n+\n+cdef class FileScanOptions(ScanOptions):\n+    pass\n+\n+\n+cdef class ParquetScanOptions(FileScanOptions):\n+    pass\n+\n+\n+cdef class WriteOptions:\n+    pass\n+\n+\n+cdef class FileWriteOptions(WriteOptions):\n+    pass\n+\n+\n+cdef class ParquetWriterOptions(FileWriteOptions):\n+    pass\n+\n+\n+cdef class ScanContext:\n+\n+    cdef:\n+        shared_ptr[CScanContext] wrapped\n+        CScanContext *context\n+\n+    def __init__(self, MemoryPool memory_pool=None):\n+        cdef shared_ptr[CScanContext] context = make_shared[CScanContext]()\n+        self.init(context)\n+        if memory_pool is not None:\n+            self.memory_pool = memory_pool\n+\n+    cdef init(self, shared_ptr[CScanContext]& sp):\n+        self.wrapped = sp\n+        self.context = sp.get()\n+\n+    @staticmethod\n+    cdef ScanContext wrap(shared_ptr[CScanContext]& sp):\n+        cdef ScanContext self = ScanContext.__new__(ScanContext)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanContext] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def memory_pool(self):\n+        # TODO(kszucs): there's no function to box/wrap memory pool\n+        cdef:\n+            MemoryPool pool = MemoryPool.__new__(MemoryPool)\n+        # this might be unsafe because of the raw pointers\n+        pool.init(self.context.pool)\n+        return pool\n+\n+    @memory_pool.setter\n+    def memory_pool(self, MemoryPool pool):\n+        self.context.pool = maybe_unbox_memory_pool(pool)\n+\n+\n+cdef class ScanTask:\n+    \"\"\"Read record batches from a range of a single data fragment.\n+\n+    A ScanTask is meant to be a unit of work to be dispatched.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CScanTask] wrapped\n+        CScanTask* task\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__, subclasses_instead=False)\n+\n+    cdef init(self, shared_ptr[CScanTask]& sp):\n+        self.wrapped = sp\n+        self.task = self.wrapped.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CScanTask]& sp):\n+        cdef SimpleScanTask self = SimpleScanTask.__new__(SimpleScanTask)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScanTask] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self):\n+        \"\"\"Iterate through sequence of materialized record batches.\n+\n+        Execution semantics are encapsulated in the particular ScanTask\n+        implementation.\n+\n+        Returns\n+        -------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            CRecordBatchIterator iterator\n+            shared_ptr[CRecordBatch] record_batch\n+\n+        iterator = move(GetResultValue(move(self.task.Scan())))\n+\n+        while True:\n+            iterator.Next(&record_batch)\n+            if record_batch.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield pyarrow_wrap_batch(record_batch)\n+\n+\n+cdef class SimpleScanTask(ScanTask):\n+    \"\"\"A trivial ScanTask that yields the RecordBatch of an array.\"\"\"\n+\n+    cdef:\n+        CSimpleScanTask* simple_task\n+\n+    cdef init(self, shared_ptr[CScanTask]& sp):\n+        ScanTask.init(self, sp)\n+        self.simple_task = <CSimpleScanTask*> sp.get()\n+\n+\n+cdef class ScannerBuilder:\n+    \"\"\"Factory class to construct a Scanner.\n+\n+    It is used to pass information, notably a potential filter expression and a\n+    subset of columns to materialize.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CScannerBuilder] wrapped\n+        CScannerBuilder* builder\n+\n+    def __init__(self, Dataset dataset not None, ScanContext context not None):\n+        cdef shared_ptr[CScannerBuilder] builder\n+        builder = make_shared[CScannerBuilder](\n+            dataset.unwrap(), context.unwrap()\n+        )\n+        self.init(builder)\n+\n+    cdef void init(self, shared_ptr[CScannerBuilder]& sp):\n+        self.wrapped = sp\n+        self.builder = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CScannerBuilder]& sp):\n+        cdef ScannerBuilder self = ScannerBuilder.__new__(ScannerBuilder)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CScannerBuilder] unwrap(self):\n+        return self.wrapped\n+\n+    def project(self, columns):\n+        \"\"\"Set the subset of columns to materialize.\n+\n+        This subset will be passed down to DataSources and corresponding\n+        DataFragments. The goal is to avoid loading, copying, and deserializing\n+        columns that will not be required further down the compute chain.\n+\n+        Raises exception if any column name does not exists in the dataset's\n+        Schema.\n+\n+        Parameters\n+        ----------\n+        columns : list of str\n+            List of columns to project. Order and duplicates will be preserved.\n+\n+        Returns\n+        -------\n+        self : ScannerBuilder\n+        \"\"\"\n+        cdef vector[c_string] cols = [tobytes(c) for c in columns]\n+        check_status(self.builder.Project(cols))\n+        return self\n+\n+    def finish(self):\n+        \"\"\"Return the constructed now-immutable Scanner object\"\"\"\n+        return Scanner.wrap(GetResultValue(self.builder.Finish()))\n+\n+    def filter(self, Expression filter_expression not None):\n+        \"\"\"Set the filter expression to return only rows matching the filter.\n+\n+        The predicate will be passed down to DataSources and corresponding\n+        DataFragments to exploit predicate pushdown if possible using partition\n+        information or DataFragment internal metadata, e.g. Parquet statistics.\n \n Review comment:\n   Add something like \"Otherwise filters the loaded RecordBatches before yielding them\" ?\r\n   \r\n   And the same here about mutating inplace as I mentioned above for project\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T19:42:05.675+0000",
                    "updated": "2019-12-02T19:42:05.675+0000",
                    "started": "2019-12-02T19:42:05.674+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352145",
                    "issueId": "13252760"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/worklog/352146",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on pull request #5237: ARROW-6341: [Python] Implement low-level bindings for Dataset\nURL: https://github.com/apache/arrow/pull/5237#discussion_r352785334\n \n \n\n ##########\n File path: python/pyarrow/_dataset.pyx\n ##########\n @@ -0,0 +1,1381 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+# cython: language_level = 3\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import six\n+\n+from cython.operator cimport dereference as deref\n+\n+from pyarrow.lib cimport *\n+from pyarrow.includes.libarrow_dataset cimport *\n+from pyarrow.compat import frombytes, tobytes\n+from pyarrow._fs cimport FileSystem, FileStats, Selector\n+\n+\n+def _forbid_instantiation(klass, subclasses_instead=True):\n+    msg = '{} is an abstract class thus cannot be initialized.'.format(\n+        klass.__name__\n+    )\n+    if subclasses_instead:\n+        subclasses = [cls.__name__ for cls in klass.__subclasses__]\n+        msg += ' Use one of the subclasses instead: {}'.format(\n+            ', '.join(subclasses)\n+        )\n+    raise TypeError(msg)\n+\n+\n+cdef class FileFormat:\n+\n+    cdef:\n+        shared_ptr[CFileFormat] wrapped\n+        CFileFormat* format\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CFileFormat]& sp):\n+        self.wrapped = sp\n+        self.format = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileFormat]& sp):\n+        cdef FileFormat self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'parquet':\n+            self = ParquetFileFormat.__new__(ParquetFileFormat)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileFormat] unwrap(self):\n+        return self.wrapped\n+\n+    def name(self):\n+        return frombytes(self.format.name())\n+\n+\n+cdef class ParquetFileFormat(FileFormat):\n+\n+    def __init__(self):\n+        self.init(shared_ptr[CFileFormat](new CParquetFileFormat()))\n+\n+\n+cdef class PartitionScheme:\n+\n+    cdef:\n+        shared_ptr[CPartitionScheme] wrapped\n+        CPartitionScheme* scheme\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        self.wrapped = sp\n+        self.scheme = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(const shared_ptr[CPartitionScheme]& sp):\n+        cdef PartitionScheme self\n+\n+        # TODO(kszucs): either choose type() or name() but consistently\n+        typ = frombytes(sp.get().name())\n+        if typ == 'schema_partition_scheme':\n+            self = SchemaPartitionScheme.__new__(SchemaPartitionScheme)\n+        elif typ == 'hive_partition_scheme':\n+            self = HivePartitionScheme.__new__(HivePartitionScheme)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CPartitionScheme] unwrap(self):\n+        return self.wrapped\n+\n+    def parse(self, path):\n+        cdef CResult[shared_ptr[CExpression]] result\n+        result = self.scheme.Parse(tobytes(path))\n+        return Expression.wrap(GetResultValue(result))\n+\n+\n+cdef class SchemaPartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CSchemaPartitionScheme* schema_scheme  # hmmm...\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CSchemaPartitionScheme] scheme\n+        scheme = make_shared[CSchemaPartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.schema_scheme = <CSchemaPartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.schema_scheme.schema())\n+\n+\n+cdef class HivePartitionScheme(PartitionScheme):\n+\n+    cdef:\n+        CHivePartitionScheme* hive_scheme\n+\n+    def __init__(self, Schema schema not None):\n+        cdef shared_ptr[CHivePartitionScheme] scheme\n+        scheme = make_shared[CHivePartitionScheme](\n+            pyarrow_unwrap_schema(schema)\n+        )\n+        self.init(<shared_ptr[CPartitionScheme]> scheme)\n+\n+    cdef init(self, const shared_ptr[CPartitionScheme]& sp):\n+        PartitionScheme.init(self, sp)\n+        self.hive_scheme = <CHivePartitionScheme*> sp.get()\n+\n+    @property\n+    def schema(self):\n+        return pyarrow_wrap_schema(self.hive_scheme.schema())\n+\n+\n+cdef class FileSystemDiscoveryOptions:\n+\n+    cdef:\n+        CFileSystemDiscoveryOptions options\n+\n+    __slots__ = ()  # avoid mistakingly creating attributes\n+\n+    def __init__(self, partition_base_dir=None, exclude_invalid_files=None,\n+                 list ignore_prefixes=None):\n+        if partition_base_dir is not None:\n+            self.partition_base_dir = partition_base_dir\n+        if exclude_invalid_files is not None:\n+            self.exclude_invalid_files = exclude_invalid_files\n+        if ignore_prefixes is not None:\n+            self.ignore_prefixes = ignore_prefixes\n+\n+    cdef inline CFileSystemDiscoveryOptions unwrap(self):\n+        return self.options\n+\n+    @property\n+    def partition_base_dir(self):\n+        return frombytes(self.options.partition_base_dir)\n+\n+    @partition_base_dir.setter\n+    def partition_base_dir(self, value):\n+        self.options.partition_base_dir = tobytes(value)\n+\n+    @property\n+    def exclude_invalid_files(self):\n+        return self.options.exclude_invalid_files\n+\n+    @exclude_invalid_files.setter\n+    def exclude_invalid_files(self, bint value):\n+        self.options.exclude_invalid_files = value\n+\n+    @property\n+    def ignore_prefixes(self):\n+        return [frombytes(p) for p in self.options.ignore_prefixes]\n+\n+    @ignore_prefixes.setter\n+    def ignore_prefixes(self, values):\n+        self.options.ignore_prefixes = [tobytes(v) for v in values]\n+\n+\n+cdef class DataSourceDiscovery:\n+\n+    cdef:\n+        shared_ptr[CDataSourceDiscovery] wrapped\n+        CDataSourceDiscovery* discovery\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        self.wrapped = sp\n+        self.discovery = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSourceDiscovery]& sp):\n+        cdef DataSourceDiscovery self = \\\n+            DataSourceDiscovery.__new__(DataSourceDiscovery)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataSourceDiscovery] unwrap(self):\n+        return self.wrapped\n+\n+    @property\n+    def schema(self):\n+        cdef shared_ptr[CSchema] schema = self.discovery.schema()\n+        if schema.get() == nullptr:\n+            return None\n+        else:\n+            return pyarrow_wrap_schema(schema)\n+\n+    @property\n+    def partition_scheme(self):\n+        cdef shared_ptr[CPartitionScheme] scheme\n+        scheme = self.discovery.partition_scheme()\n+        if scheme.get() == nullptr:\n+            return None\n+        else:\n+            return PartitionScheme.wrap(scheme)\n+\n+    @partition_scheme.setter\n+    def partition_scheme(self, PartitionScheme scheme not None):\n+        check_status(self.discovery.SetPartitionScheme(scheme.unwrap()))\n+\n+    @property\n+    def root_partition(self):\n+        cdef shared_ptr[CExpression] expr = self.discovery.root_partition()\n+        if expr.get() == nullptr:\n+            return None\n+        else:\n+            return Expression.wrap(expr)\n+\n+    def inspect(self):\n+        cdef CResult[shared_ptr[CSchema]] result\n+        with nogil:\n+            result = self.discovery.Inspect()\n+        return pyarrow_wrap_schema(GetResultValue(result))\n+\n+    def finish(self):\n+        cdef CResult[shared_ptr[CDataSource]] result\n+        with nogil:\n+            result = self.discovery.Finish()\n+        return DataSource.wrap(GetResultValue(result))\n+\n+\n+cdef class FileSystemDataSourceDiscovery(DataSourceDiscovery):\n+\n+    cdef:\n+        CFileSystemDataSourceDiscovery* filesystem_discovery\n+\n+    def __init__(self, FileSystem filesystem not None,\n+                 Selector selector not None, FileFormat format not None,\n+                 FileSystemDiscoveryOptions options=None):\n+        cdef CResult[shared_ptr[CDataSourceDiscovery]] result\n+        options = options or FileSystemDiscoveryOptions()\n+        result = CFileSystemDataSourceDiscovery.Make(\n+            filesystem.unwrap(),\n+            selector.unwrap(),\n+            format.unwrap(),\n+            options.unwrap()\n+        )\n+        self.init(GetResultValue(result))\n+\n+    cdef init(self, shared_ptr[CDataSourceDiscovery]& sp):\n+        DataSourceDiscovery.init(self, sp)\n+        self.filesystem_discovery = <CFileSystemDataSourceDiscovery*> sp.get()\n+\n+\n+cdef class FileSource:\n+\n+    cdef:\n+        shared_ptr[CFileSource] wrapped\n+        CFileSource* source\n+\n+    cdef readonly:\n+        FileSystem fs\n+\n+    def __init__(self, path, FileSystem filesystem not None, compression=None):\n+        cdef shared_ptr[CFileSource] source\n+\n+        # need to hold a reference for the filesystem\n+        self.fs = filesystem\n+\n+        source.reset(new CFileSource(\n+            tobytes(path),\n+            self.fs.unwrap().get(),\n+            _get_compression_type(compression)\n+        ))\n+\n+        self.init(source)\n+\n+    cdef init(self, shared_ptr[CFileSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CFileSource]& sp):\n+        cdef FileSource self = FileSource.__new__(FileSource)\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CFileSource] unwrap(self):\n+        return self.wrapped\n+\n+    def equals(self, FileSource other):\n+        return deref(self.source) == deref(other.unwrap())\n+\n+    def __eq__(self, other):\n+        try:\n+            return self.equals(other)\n+        except TypeError:\n+            return NotImplemented\n+\n+    @property\n+    def compression(self):\n+        return self.wrapped.get().compression()\n+\n+    @property\n+    def path(self):\n+        return frombytes(self.wrapped.get().path())\n+\n+    # def open(self):\n+    #     CStatus Open(shared_ptr[CRandomAccessFile]* out)\n+\n+\n+cdef class DataFragment:\n+    \"\"\"A granular piece of a Dataset such as an individual file.\n+\n+    It can be read/scanned separately from other fragments.\n+    A DataFragment yields a collection of RecordBatch, encapsulated in one or\n+    more ScanTasks.\n+    \"\"\"\n+    cdef:\n+        shared_ptr[CDataFragment] wrapped\n+        CDataFragment* fragment\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        self.wrapped = sp\n+        self.fragment = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataFragment]& sp):\n+        cdef DataFragment self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'simple_data_fragment':\n+            self = SimpleDataFragment.__new__(SimpleDataFragment)\n+        elif typ == 'parquet_data_fragment':\n+            self = ParquetDataFragment.__new__(ParquetDataFragment)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef inline shared_ptr[CDataFragment] unwrap(self):\n+        return self.wrapped\n+\n+    def scan(self, ScanContext context=None):\n+        \"\"\"Returns an iterator of ScanTasks\n+\n+        Each of ScanTask yields RecordBatches from this DataFragment.\n+\n+        Parameters\n+        ----------\n+        context : ScanContext, default None\n+\n+        Returns\n+        -------\n+        scan_tasks: Iterator[ScanTask]\n+        \"\"\"\n+        cdef:\n+            CResult[CScanTaskIterator] iterator_result\n+            CScanTaskIterator iterator\n+            CScanTaskPtr task\n+\n+        context = context or ScanContext()\n+        iterator_result = self.fragment.Scan(context.unwrap())\n+        iterator = move(GetResultValue(move(iterator_result)))\n+\n+        while True:\n+            iterator.Next(&task)\n+            if task.get() == nullptr:\n+                raise StopIteration()\n+            else:\n+                yield ScanTask.wrap(task)\n+\n+    @property\n+    def splittable(self):\n+        \"\"\"Get whether the DataFragment supports parallel scanning.\"\"\"\n+        return self.fragment.splittable()\n+\n+    @property\n+    def scan_options(self):\n+        \"\"\"Get the scan options used for scanning the data fragment.\n+\n+        Filtering, schema reconciliation, and partition options. None indicates\n+        that no filtering or schema reconciliation will be performed and all\n+        partitions will be scanned.\n+        \"\"\"\n+        cdef shared_ptr[CScanOptions] options = self.fragment.scan_options()\n+        if options.get() == nullptr:\n+            return None\n+        else:\n+            return ScanOptions.wrap(options)\n+\n+\n+cdef class SimpleDataFragment(DataFragment):\n+    \"\"\"A fragment that yields ScanTasks out of a fixed set of RecordBatches.\"\"\"\n+\n+    cdef:\n+        CSimpleDataFragment* simple_fragment\n+\n+    def __init__(self, record_batches):\n+        \"\"\"Create a simple data fragment from record batches.\n+\n+        Parameters\n+        ----------\n+        record_batches : iterator of RecordBatch\n+        \"\"\"\n+        cdef:\n+            RecordBatch batch\n+            vector[shared_ptr[CRecordBatch]] batches\n+            shared_ptr[CSimpleDataFragment] simple_fragment\n+\n+        for batch in record_batches:\n+            batches.push_back(batch.sp_batch)\n+\n+        simple_fragment = make_shared[CSimpleDataFragment](batches)\n+        self.init(<shared_ptr[CDataFragment]> simple_fragment)\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.simple_fragment = <CSimpleDataFragment*> sp.get()\n+\n+\n+cdef class FileDataFragment(DataFragment):\n+\n+    cdef:\n+        CFileDataFragment* file_fragment\n+\n+    cdef void init(self, const shared_ptr[CDataFragment]& sp):\n+        DataFragment.init(self, sp)\n+        self.file_fragment = <CFileDataFragment*> sp.get()\n+\n+    @property\n+    def source(self):\n+        cdef shared_ptr[CFileSource] source\n+        source = make_shared[CFileSource](self.file_fragment.source())\n+        return FileSource.wrap(source)\n+\n+\n+cdef class ParquetDataFragment(FileDataFragment):\n+\n+    def __init__(self, FileSource source not None,\n+                 ParquetScanOptions scan_options not None):\n+        cdef shared_ptr[CDataFragment] fragment\n+        fragment.reset(new CParquetDataFragment(\n+            deref(source.unwrap()),\n+            scan_options.unwrap()\n+        ))\n+        self.init(fragment)\n+\n+\n+cdef class DataSource:\n+    \"\"\"Basic component of a Dataset which yields zero or more DataFragments.\n+\n+    A DataSource acts as a discovery mechanism of DataFragments and partitions,\n+    e.g. files deeply nested in a directory.\n+    \"\"\"\n+\n+    cdef:\n+        shared_ptr[CDataSource] wrapped\n+        CDataSource* source\n+\n+    def __init__(self):\n+        _forbid_instantiation(self.__class__)\n+\n+    cdef void init(self, const shared_ptr[CDataSource]& sp):\n+        self.wrapped = sp\n+        self.source = sp.get()\n+\n+    @staticmethod\n+    cdef wrap(shared_ptr[CDataSource]& sp):\n+        cdef DataSource self\n+\n+        typ = frombytes(sp.get().type())\n+        if typ == 'tree_data_source':\n+            self = TreeDataSource.__new__(TreeDataSource)\n+        elif typ == 'simple_data_source':\n+            self = SimpleDataSource.__new__(SimpleDataSource)\n+        elif typ == 'filesystem_data_source':\n+            self = FileSystemDataSource.__new__(FileSystemDataSource)\n+        else:\n+            raise TypeError(typ)\n+\n+        self.init(sp)\n+        return self\n+\n+    cdef shared_ptr[CDataSource] unwrap(self):\n+        return self.wrapped\n+\n+    def fragments(self, ScanOptions scan_options=None):\n+        \"\"\"Get the data fragments of this data source.\n+\n+        Parameters\n+        ----------\n+        scan_options : ScanOptions, default None\n+            Controls filtering and schema inference.\n \n Review comment:\n   How does it control that?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-12-02T19:42:05.677+0000",
                    "updated": "2019-12-02T19:42:05.677+0000",
                    "started": "2019-12-02T19:42:05.677+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "352146",
                    "issueId": "13252760"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 56400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@1ade0a63[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@10f9d190[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@ccf2846[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@52e41d1e[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5617b526[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@75c3f32d[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@bb78cb[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@7245992f[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1910bcc4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@73a07e50[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@28063dcf[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@76545501[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 56400,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Dec 13 17:26:47 UTC 2019",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2019-12-13T17:26:46.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-6341/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2019-08-23T21:18:09.000+0000",
        "updated": "2019-12-13T17:26:47.000+0000",
        "timeoriginalestimate": null,
        "description": "The following classes should be accessible from Python:\r\n * class DataSource\r\n * class DataSourceDiscovery\r\n * class Dataset\r\n * class ScanContext, ScanOptions, ScanTask\r\n * class ScannerBuilder\r\n * class Scanner\r\n\r\nThe end result is reading a directory of parquet files as a single stream. One should be able to re-implement\u00a0[https://github.com/apache/arrow/pull/5720]\u00a0in python.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "15h 40m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 56400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python] Implement low-level bindings for Dataset",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13252760/comment/16995776",
                    "id": "16995776",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bkietz",
                        "name": "bkietz",
                        "key": "bkietz",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bkietz&avatarId=37277",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bkietz&avatarId=37277",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bkietz&avatarId=37277",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bkietz&avatarId=37277"
                        },
                        "displayName": "Ben Kietzman",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 5237\n[https://github.com/apache/arrow/pull/5237]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=bkietz",
                        "name": "bkietz",
                        "key": "bkietz",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=bkietz&avatarId=37277",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bkietz&avatarId=37277",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bkietz&avatarId=37277",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bkietz&avatarId=37277"
                        },
                        "displayName": "Ben Kietzman",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-12-13T17:26:47.060+0000",
                    "updated": "2019-12-13T17:26:47.060+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z05zu0:",
        "customfield_12314139": null
    }
}