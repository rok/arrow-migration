{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13376581",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581",
    "key": "ARROW-12650",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350323",
                "id": "12350323",
                "description": "",
                "name": "6.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-10-26"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
            "name": "Minor",
            "id": "4"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=amol-",
            "name": "amol-",
            "key": "amol-",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=amol-&avatarId=46461",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amol-&avatarId=46461",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amol-&avatarId=46461",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amol-&avatarId=46461"
            },
            "displayName": "Alessandro Molina",
            "active": true,
            "timeZone": "Europe/Rome"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332532",
                "id": "12332532",
                "name": "Documentation"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=amol-",
            "name": "amol-",
            "key": "amol-",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=amol-&avatarId=46461",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amol-&avatarId=46461",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amol-&avatarId=46461",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amol-&avatarId=46461"
            },
            "displayName": "Alessandro Molina",
            "active": true,
            "timeZone": "Europe/Rome"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=amol-",
            "name": "amol-",
            "key": "amol-",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=amol-&avatarId=46461",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=amol-&avatarId=46461",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=amol-&avatarId=46461",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=amol-&avatarId=46461"
            },
            "displayName": "Alessandro Molina",
            "active": true,
            "timeZone": "Europe/Rome"
        },
        "aggregateprogress": {
            "progress": 24600,
            "total": 24600,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 24600,
            "total": 24600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-12650/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 41,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/593341",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#issuecomment-834482409\n\n\n   https://issues.apache.org/jira/browse/ARROW-12650\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-07T14:54:59.814+0000",
                    "updated": "2021-05-07T14:54:59.814+0000",
                    "started": "2021-05-07T14:54:59.813+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "593341",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/595139",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r630837422\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n\nReview comment:\n       I know we've started using those `ipython` blocks, but I'm really not fond of them. They seem to make building docs quite slower (especially if the workload is non-trivial).\r\n   \r\n   @jorisvandenbossche What do you think?\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the array can directly reference the data on disk and avoid copying it to memory.\n+In such case the memory consumption is greatly reduced and it's possible to read\n+arrays bigger than the total memory\n\nReview comment:\n       This is rather misleading. The data is loaded back to memory when it is being read. It's just that it's read lazily, so the costs are not paid up front (and the cost is not paid for data that is not accessed).\r\n   \r\n   What memory mapping can avoid is an intermediate copy when reading the data. So it is more performant in that sense.\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the array can directly reference the data on disk and avoid copying it to memory.\n+In such case the memory consumption is greatly reduced and it's possible to read\n+arrays bigger than the total memory\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the memory mapped data without the need to make copies of it\n\nReview comment:\n       This seems a bit misleading again. First, I don't understand the point of writing back data that's read from a memory-mapped file (just copy the file if that's what you want to do?). Second, the fact that writing data doesn't consume additional memory has nothing to do with the fact that the data is memory-mapped, AFAICT.\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n\nReview comment:\n       Always \"Arrow\" capitalized.\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n\nReview comment:\n       It seems this should go into `ipc.rst`. `memory.rst` is about the low-level memory and IO APIs.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-12T08:39:10.781+0000",
                    "updated": "2021-05-12T08:39:10.781+0000",
                    "started": "2021-05-12T08:39:10.781+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "595139",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/596164",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "amol- commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r631867852\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n\nReview comment:\n       I'm open to suggestions about where to best put this. I picked `memory.rst` because the title is \"Memory and IO Interfaces\" and it covers input streams, memory mapped files and reading memory mapped data.\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n\nReview comment:\n       :+1: will change this.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-13T14:48:39.358+0000",
                    "updated": "2021-05-13T14:48:39.358+0000",
                    "started": "2021-05-13T14:48:39.357+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "596164",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/596167",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "amol- commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r631870514\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the array can directly reference the data on disk and avoid copying it to memory.\n+In such case the memory consumption is greatly reduced and it's possible to read\n+arrays bigger than the total memory\n\nReview comment:\n       I see what you mean. What I was trying to say is that Arrow doesn't have to allocate memory itself as it can directly point to the memory mapped buffer which allocation is managed by the system. Also the memory mapped buffer can be paged out more easily by the system without write back cost as it's not flagged as dirty memory, thus allowing to deal with files bigger than memory even in the absence of a swap file.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-13T14:51:46.269+0000",
                    "updated": "2021-05-13T14:51:46.269+0000",
                    "started": "2021-05-13T14:51:46.269+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "596167",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/596169",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "amol- commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r631870514\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the array can directly reference the data on disk and avoid copying it to memory.\n+In such case the memory consumption is greatly reduced and it's possible to read\n+arrays bigger than the total memory\n\nReview comment:\n       I see what you mean. What I was trying to say is that Arrow doesn't have to allocate memory itself as it can directly point to the memory mapped buffer which allocation is managed by the system. Also the memory mapped buffer can be paged out more easily by the system without write back cost as it's not flagged as dirty memory, thus allowing to deal with files bigger than memory even in the absence of a swap file. I'll try to rephrase this in a less misleading way.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-13T14:52:03.411+0000",
                    "updated": "2021-05-13T14:52:03.411+0000",
                    "started": "2021-05-13T14:52:03.411+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "596169",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/596172",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "amol- commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r631872202\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the array can directly reference the data on disk and avoid copying it to memory.\n+In such case the memory consumption is greatly reduced and it's possible to read\n+arrays bigger than the total memory\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the memory mapped data without the need to make copies of it\n\nReview comment:\n       The point was to show that the `total_allocated_bytes` remains zero even if we had to iterate over all data to write it back as we still referenced the memory mapped buffer without having to perform any copy of it.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-13T14:54:03.871+0000",
                    "updated": "2021-05-13T14:54:03.871+0000",
                    "started": "2021-05-13T14:54:03.871+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "596172",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/600350",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "amol- commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r636903715\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the array can directly reference the data on disk and avoid copying it to memory.\n+In such case the memory consumption is greatly reduced and it's possible to read\n+arrays bigger than the total memory\n\nReview comment:\n       I rephrased it to make it more clear that in absolute terms you won't be consuming fewer memory, but the system will be able to more easily page it out.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-21T13:07:26.823+0000",
                    "updated": "2021-05-21T13:07:26.823+0000",
                    "started": "2021-05-21T13:07:26.822+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "600350",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/600351",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "amol- commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r636905643\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n\nReview comment:\n       I don't have a strong opinione about using `ipython` blocks or just code blocks. I used them just for consistency with the rest of the document.\r\n   \r\n   I think that they have a value in at least verifying that the code you provided as an example can actually execute (even thought it might lead to different results) which makes more easy to catch examples that became invalid due to api changes.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-21T13:10:12.723+0000",
                    "updated": "2021-05-21T13:10:12.723+0000",
                    "started": "2021-05-21T13:10:12.723+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "600351",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/600352",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "amol- commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r636905643\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n\nReview comment:\n       I don't have a strong opinion about using `ipython` blocks or just code blocks. I used them just for consistency with the rest of the document.\r\n   \r\n   I think that they have a value in at least verifying that the code you provided as an example can actually execute (even thought it might lead to different results) which makes more easy to catch examples that became invalid due to api changes.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-05-21T13:10:36.129+0000",
                    "updated": "2021-05-21T13:10:36.129+0000",
                    "started": "2021-05-21T13:10:36.129+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "600352",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614033",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r657125159\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n\nReview comment:\n       Sorry for the delay. Basically, reading and writing Arrow arrays uses the IPC layer, so I do think it would be better in `ipc.rst`. `memory.rst` is only dealing with raw bytes data.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-23T13:54:10.317+0000",
                    "updated": "2021-06-23T13:54:10.317+0000",
                    "started": "2021-06-23T13:54:10.317+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614033",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614036",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r657126453\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n\nReview comment:\n       In this case in particular, we're creating and serializing a large amount of data, so that would really seem to add some cost to building the docs.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-23T13:55:38.007+0000",
                    "updated": "2021-06-23T13:55:38.007+0000",
                    "started": "2021-06-23T13:55:38.007+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614036",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614037",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r657127966\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,97 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw Arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the arrow can directly reference the data mapped from disk and avoid having to\n+allocate its own memory.\n+In such case the operating system will be able to page in the mapped memory\n+lazily and page it out without any write back cost when under pressure,\n+allowing to more easily read arrays bigger than the total memory.\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the data without the need to make copies of it\n\nReview comment:\n       While this is true, I'm not sure the use case of writing back identical memory-mapped data is very interesting to talk about. Usually, you would write back data after some amount of processing (e.g. filtering).\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-23T13:57:13.788+0000",
                    "updated": "2021-06-23T13:57:13.788+0000",
                    "started": "2021-06-23T13:57:13.787+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614037",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614039",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r657128479\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,97 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw Arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the arrow can directly reference the data mapped from disk and avoid having to\n+allocate its own memory.\n+In such case the operating system will be able to page in the mapped memory\n+lazily and page it out without any write back cost when under pressure,\n+allowing to more easily read arrays bigger than the total memory.\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the data without the need to make copies of it\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile2.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for chunk in loaded_array[0].iterchunks():\n+                batch = pa.record_batch([chunk], schema)\n+                writer.write(batch)\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Most high level APIs like :meth:`~pyarrow.parquet.read_table` also provide a\n+``memory_map`` option. But in those cases, the memory mapping can't help with\n+reducing resident memory consumption. Because Parquet data needs to be decoded\n+from the parquet format and compression, it can't be directly mapped from disk,\n+thus the ``memory_map`` option might perform better on some systems but won't\n+help much with resident memory consumption.\n\nReview comment:\n       IMHO, it would be better to put the discussion of Parquet options in the Parquet chapter.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-23T13:57:40.200+0000",
                    "updated": "2021-06-23T13:57:40.200+0000",
                    "started": "2021-06-23T13:57:40.200+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614039",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614413",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r657791375\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n\nReview comment:\n       Personally I would prefer to have *some* way to still verify the example, but this doesn't need to be with the IPython directive (which actually only ensures the code runs without error, not that the output is correct). This has come up before as well, so I opened a separate JIRA to discuss this in general: https://issues.apache.org/jira/browse/ARROW-13159\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-24T09:37:46.116+0000",
                    "updated": "2021-06-24T09:37:46.116+0000",
                    "started": "2021-06-24T09:37:46.116+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614413",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614417",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r657797354\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n\nReview comment:\n       For this specific example, we could also make it smaller? (which is still OK for educational purposes I think?)\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-24T09:46:19.866+0000",
                    "updated": "2021-06-24T09:46:19.866+0000",
                    "started": "2021-06-24T09:46:19.866+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614417",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614424",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r657804510\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,97 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw Arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the arrow can directly reference the data mapped from disk and avoid having to\n+allocate its own memory.\n+In such case the operating system will be able to page in the mapped memory\n+lazily and page it out without any write back cost when under pressure,\n+allowing to more easily read arrays bigger than the total memory.\n\nReview comment:\n       I don't know what user we target here, but \"page in\" and \"page out\" is not commonly understood I think (of course, we can't start explaining in detail how memory works here, but I think this section will be typically read by people who might not fully understand what memory mapping is / how it works, but just think they can use it to avoid memory allocation).\r\n   \n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,97 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw Arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the arrow can directly reference the data mapped from disk and avoid having to\n+allocate its own memory.\n+In such case the operating system will be able to page in the mapped memory\n+lazily and page it out without any write back cost when under pressure,\n+allowing to more easily read arrays bigger than the total memory.\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the data without the need to make copies of it\n\nReview comment:\n       I agree it would be nice to show a more \"interesting\" example where memory-mapping can be useful.\r\n   \r\n   It's also not really clear to me (as a memory-mapping noob) what the above exactly means if you do some operation on the memory mapped data (that is not just writing it back unmodified).\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,97 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw Arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the arrow can directly reference the data mapped from disk and avoid having to\n\nReview comment:\n       ```suggestion\r\n   Arrow can directly reference the data mapped from disk and avoid having to\r\n   ```\r\n   \r\n   (or \"pyarrow\")\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-24T09:57:28.241+0000",
                    "updated": "2021-06-24T09:57:28.241+0000",
                    "started": "2021-06-24T09:57:28.241+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614424",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614651",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r658185713\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,95 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the array can directly reference the data on disk and avoid copying it to memory.\n+In such case the memory consumption is greatly reduced and it's possible to read\n+arrays bigger than the total memory\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the memory mapped data without the need to make copies of it\n\nReview comment:\n       If you use the datasets API to scan a dataset (that isn't memory mapped) and then write it back out (repartitioned, filtered, etc.) it shouldn't require more than a couple hundred MB of working RAM (controllable by readahead) regardless of dataset size.\r\n   \r\n   There will be one additional copy (incurred at read time to copy from kernel space to user space) compared to memory mapped files but other than that the two will be similar.  In practice this additional copy happens in parallel with the rest of the work and doesn't make a noticeable runtime difference.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-24T18:25:54.265+0000",
                    "updated": "2021-06-24T18:25:54.265+0000",
                    "started": "2021-06-24T18:25:54.264+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614651",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614654",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r658187770\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,97 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw Arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the arrow can directly reference the data mapped from disk and avoid having to\n+allocate its own memory.\n+In such case the operating system will be able to page in the mapped memory\n+lazily and page it out without any write back cost when under pressure,\n+allowing to more easily read arrays bigger than the total memory.\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the data without the need to make copies of it\n\nReview comment:\n       If you want an example with a clear performance benefit then a partial IPC read is a pretty good one.  The IPC reader today does not column selection into I/O filtering.  In other words, even if you read only a few columns it will still \"read\" the entire file.  Since it doesn't access the memory for the undesired columns you can see a benefit in memory mapping.\r\n   \r\n   One could conceivably implement a smarter IPC reader that does selectively read I/O (with prebuffering) and probably close the gap somewhat although the overhead of figuring out and issuing all the small reads may still leave an advantage to the memory mapped method.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-24T18:29:09.326+0000",
                    "updated": "2021-06-24T18:29:09.326+0000",
                    "started": "2021-06-24T18:29:09.325+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614654",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614655",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r658187770\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,97 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw Arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the arrow can directly reference the data mapped from disk and avoid having to\n+allocate its own memory.\n+In such case the operating system will be able to page in the mapped memory\n+lazily and page it out without any write back cost when under pressure,\n+allowing to more easily read arrays bigger than the total memory.\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the data without the need to make copies of it\n\nReview comment:\n       If you want an example with a clear performance benefit then a partial IPC read is a pretty good one.  The IPC reader today does not push column selection into I/O filtering.  In other words, even if you read only a few columns it will still \"read\" the entire file.  Since it doesn't access the memory for the undesired columns you can see a benefit in memory mapping.\r\n   \r\n   One could conceivably implement a smarter IPC reader that does selectively read I/O (with prebuffering) and probably close the gap somewhat although the overhead of figuring out and issuing all the small reads may still leave an advantage to the memory mapped method.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-24T18:29:22.512+0000",
                    "updated": "2021-06-24T18:29:22.512+0000",
                    "started": "2021-06-24T18:29:22.512+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614655",
                    "issueId": "13376581"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/worklog/614656",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10266:\nURL: https://github.com/apache/arrow/pull/10266#discussion_r658188518\n\n\n\n##########\nFile path: docs/source/python/memory.rst\n##########\n@@ -277,6 +277,97 @@ types than with normal Python file objects.\n    !rm example.dat\n    !rm example2.dat\n \n+Efficiently Writing and Reading Arrow Arrays\n+--------------------------------------------\n+\n+Being optimized for zero copy and memory mapped data, Arrow allows to easily\n+read and write arrays consuming the minimum amount of resident memory.\n+\n+When writing and reading raw Arrow data, we can use the Arrow File Format\n+or the Arrow Streaming Format.\n+\n+To dump an array to file, you can use the :meth:`~pyarrow.ipc.new_file`\n+which will provide a new :class:`~pyarrow.ipc.RecordBatchFileWriter` instance\n+that can be used to write batches of data to that file.\n+\n+For example to write an array of 100M integers, we could write it in 1000 chunks\n+of 100000 entries:\n+\n+.. ipython:: python\n+\n+    BATCH_SIZE = 100000\n+    NUM_BATCHES = 1000\n+\n+    schema = pa.schema([pa.field('nums', pa.int32())])\n+\n+    with pa.OSFile('bigfile.arrow', 'wb') as sink:\n+        with pa.ipc.new_file(sink, schema) as writer:\n+            for row in range(NUM_BATCHES):\n+                batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n+                writer.write(batch)\n+\n+record batches support multiple columns, so in practice we always write the\n+equivalent of a :class:`~pyarrow.Table`.\n+\n+Writing in batches is effective because we in theory need to keep in memory only\n+the current batch we are writing. But when reading back, we can be even more effective\n+by directly mapping the data from disk and avoid allocating any new memory on read.\n+\n+Under normal conditions, reading back our file will consume a few hundred megabytes\n+of memory:\n+\n+.. ipython:: python\n+\n+    with pa.OSFile('bigfile.arrow', 'rb') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+To more efficiently read big data from disk, we can memory map the file, so that\n+the arrow can directly reference the data mapped from disk and avoid having to\n+allocate its own memory.\n+In such case the operating system will be able to page in the mapped memory\n+lazily and page it out without any write back cost when under pressure,\n+allowing to more easily read arrays bigger than the total memory.\n+\n+.. ipython:: python\n+\n+    with pa.memory_map('bigfile.arrow', 'r') as source:\n+        loaded_array = pa.ipc.open_file(source).read_all()\n+    print(\"LEN:\", len(loaded_array))\n+    print(\"RSS: {}MB\".format(pa.total_allocated_bytes() >> 20))\n+\n+Equally we can write back to disk the loaded array without consuming any\n+extra memory thanks to the fact that iterating over the array will just\n+scan through the data without the need to make copies of it\n\nReview comment:\n       I did some experiments here https://github.com/apache/arrow/issues/10138 which demonstrate the effect.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-24T18:30:26.087+0000",
                    "updated": "2021-06-24T18:30:26.087+0000",
                    "started": "2021-06-24T18:30:26.087+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "614656",
                    "issueId": "13376581"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 24600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@1b58ab99[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2712c1e8[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@422452b7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@7388c69a[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@49562514[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@24f3425a[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@44220d99[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@712bbf91[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@92d4a25[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@37f238c8[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6368b56a[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@4f9349a1[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 24600,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Jul 26 15:44:45 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-07-26T15:44:45.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-12650/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-05-04T15:08:42.000+0000",
        "updated": "2021-07-26T15:48:27.000+0000",
        "timeoriginalestimate": null,
        "description": "While one of the Arrow promises is that it makes easy to read/write data bigger than memory, it's not immediately obvious from the pyarrow documentation how to deal with memory mapped files.\r\n\r\nThe doc hints that you can open files as memory mapped ( [https://arrow.apache.org/docs/python/memory.html?highlight=memory_map#on-disk-and-memory-mapped-files]\u00a0) but then it doesn't explain how to read/write Arrow Arrays or Tables from there.\r\n\r\nWhile most high level functions to read/write formats (pqt, feather, ...) have an easy to guess {{memory_map=True}} option, the doc doesn't seem to have any example of how that is meant to work for Arrow format itself. For example how you can do that using\u00a0{{RecordBatchFile*}}.\u00a0\r\n\r\nAn addition to the memory mapping section that makes a more meaningful example that reads/writes actual arrow data (instead of plain bytes) would probably be helpful",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "6h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 24600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Doc][Python] Improve documentation regarding dealing with memory mapped files",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376581/comment/17387434",
                    "id": "17387434",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "Issue resolved by pull request 10266\n[https://github.com/apache/arrow/pull/10266]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2021-07-26T15:44:45.259+0000",
                    "updated": "2021-07-26T15:44:45.259+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0qpog:",
        "customfield_12314139": null
    }
}