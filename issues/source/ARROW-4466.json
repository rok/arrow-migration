{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13213594",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594",
    "key": "ARROW-4466",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12343937",
                "id": "12343937",
                "description": "",
                "name": "0.13.0",
                "archived": false,
                "released": true,
                "releaseDate": "2019-04-01"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12556769",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12556769",
                "type": {
                    "id": "12310000",
                    "name": "Duplicate",
                    "inward": "is duplicated by",
                    "outward": "duplicates",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"
                },
                "inwardIssue": {
                    "id": "13220696",
                    "key": "ARROW-4818",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13220696",
                    "fields": {
                        "summary": "[Rust] [DataFusion] Parquet data source does not support null values",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12556768",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12556768",
                "type": {
                    "id": "12310000",
                    "name": "Duplicate",
                    "inward": "is duplicated by",
                    "outward": "duplicates",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"
                },
                "inwardIssue": {
                    "id": "13221179",
                    "key": "ARROW-4843",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13221179",
                    "fields": {
                        "summary": "[Rust] [DataFusion] Parquet data source should support DATE",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
            "name": "andygrove",
            "key": "andygrove",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
            },
            "displayName": "Andy Grove",
            "active": true,
            "timeZone": "America/Denver"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12333773",
                "id": "12333773",
                "name": "Rust"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12335005",
                "id": "12335005",
                "name": "Rust - DataFusion"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
            "name": "andygrove",
            "key": "andygrove",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
            },
            "displayName": "Andy Grove",
            "active": true,
            "timeZone": "America/Denver"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
            "name": "andygrove",
            "key": "andygrove",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
            },
            "displayName": "Andy Grove",
            "active": true,
            "timeZone": "America/Denver"
        },
        "aggregateprogress": {
            "progress": 39600,
            "total": 39600,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 39600,
            "total": 39600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-4466/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 72,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210584",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source [WIP]\nURL: https://github.com/apache/arrow/pull/3851\n \n \n   I'm sure I'll need some guidance on this one from @sunchao or @liurenjie1024 but I am keen to get parquet support added for primitive types so that I can actually use DataFusion and Arrow in production at some point.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-09T17:41:53.679+0000",
                    "updated": "2019-03-09T17:41:53.679+0000",
                    "started": "2019-03-09T17:41:53.678+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210584",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210589",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on issue #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source [WIP]\nURL: https://github.com/apache/arrow/pull/3851#issuecomment-471208963\n \n \n   @liurenjie1024 should i use the row reader for now ?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-09T18:25:39.059+0000",
                    "updated": "2019-03-09T18:25:39.059+0000",
                    "started": "2019-03-09T18:25:39.059+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210589",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210608",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on issue #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source [WIP]\nURL: https://github.com/apache/arrow/pull/3851#issuecomment-471223857\n \n \n   `row_iter` seems like safest path for now but I don't know how to check for null values\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-09T21:32:18.217+0000",
                    "updated": "2019-03-09T21:32:18.217+0000",
                    "started": "2019-03-09T21:32:18.216+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210608",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210615",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on issue #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source [WIP]\nURL: https://github.com/apache/arrow/pull/3851#issuecomment-471234722\n \n \n   switched back to column readers ... fixed bugs ... ready for first review (this is still WIP)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T00:25:13.824+0000",
                    "updated": "2019-03-10T00:25:13.824+0000",
                    "started": "2019-03-10T00:25:13.823+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210615",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210711",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264057814\n \n \n\n ##########\n File path: rust/arrow/src/datatypes.rs\n ##########\n @@ -751,6 +752,22 @@ impl Schema {\n             \"fields\": self.fields.iter().map(|field| field.to_json()).collect::<Vec<Value>>(),\n         })\n     }\n+\n+    /// Create a new schema by applying a projection to this schema's fields\n+    pub fn projection(&self, projection: &Vec<usize>) -> Result<Arc<Schema>> {\n \n Review comment:\n   nit: can we use `&[usize]` instead of `&Vec<usize>`?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T22:52:14.850+0000",
                    "updated": "2019-03-10T22:52:14.850+0000",
                    "started": "2019-03-10T22:52:14.850+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210711",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210712",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264058026\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n \n Review comment:\n   let's remove these `println`s?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T22:52:14.858+0000",
                    "updated": "2019-03-10T22:52:14.858+0000",
                    "started": "2019-03-10T22:52:14.857+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210712",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210713",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264058068\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n \n Review comment:\n   is it safe to unwrap here? what if the input file doesn't exist?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T22:52:14.874+0000",
                    "updated": "2019-03-10T22:52:14.874+0000",
                    "started": "2019-03-10T22:52:14.873+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210713",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210714",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264063340\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n \n Review comment:\n   We should not ignore definition levels and repetition levels. Otherwise nulls and nested data types may not handled properly.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T22:52:14.925+0000",
                    "updated": "2019-03-10T22:52:14.925+0000",
                    "started": "2019-03-10T22:52:14.925+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210714",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210715",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264058292\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = BooleanBuilder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int32ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i32> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int32Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int64ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i64> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int96ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<Int96> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(Int96::new());\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+\n+                                    for i in 0..count {\n+                                        let v = read_buffer[i].data();\n+                                        let value: u128 = (v[0] as u128) << 64\n+                                            | (v[1] as u128) << 32\n+                                            | (v[2] as u128);\n+                                        let ms: i64 = (value / 1000000) as i64;\n+                                        builder.append_value(ms).unwrap();\n+                                    }\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FloatColumnReader(ref mut r) => {\n+                            let mut builder = Float32Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f32> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::DoubleColumnReader(ref mut r) => {\n+                            let mut builder = Float64Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f64> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FixedLenByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder\n+                                            .append_string(\n+                                                &String::from_utf8(slice.data().to_vec())\n+                                                    .unwrap(),\n+                                            )\n+                                            .unwrap();\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::ByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder\n+                                            .append_string(\n+                                                &String::from_utf8(slice.data().to_vec())\n+                                                    .unwrap(),\n+                                            )\n+                                            .unwrap();\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                    };\n+\n+                    println!(\"Adding array to batch\");\n+                    batch.push(array);\n+                }\n+\n+                println!(\"Loaded batch of {} rows\", row_count);\n+\n+                if row_count == 0 {\n+                    Ok(None)\n+                } else {\n+                    Ok(Some(RecordBatch::try_new(self.schema.clone(), batch)?))\n+                }\n+            }\n+            _ => Ok(None),\n+        }\n+    }\n+}\n+\n+fn to_arrow(t: &Type) -> Result<Field> {\n+    match t {\n+        Type::PrimitiveType {\n+            basic_info,\n+            physical_type,\n+            ..\n+        } => {\n+            let arrow_type = match physical_type {\n \n Review comment:\n   we should look at logical type instead of physical type here.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T22:52:14.985+0000",
                    "updated": "2019-03-10T22:52:14.985+0000",
                    "started": "2019-03-10T22:52:14.984+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210715",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210716",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264058363\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = BooleanBuilder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int32ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i32> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int32Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int64ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i64> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int96ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<Int96> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(Int96::new());\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+\n+                                    for i in 0..count {\n+                                        let v = read_buffer[i].data();\n+                                        let value: u128 = (v[0] as u128) << 64\n+                                            | (v[1] as u128) << 32\n+                                            | (v[2] as u128);\n+                                        let ms: i64 = (value / 1000000) as i64;\n+                                        builder.append_value(ms).unwrap();\n+                                    }\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FloatColumnReader(ref mut r) => {\n+                            let mut builder = Float32Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f32> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::DoubleColumnReader(ref mut r) => {\n+                            let mut builder = Float64Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f64> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FixedLenByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder\n+                                            .append_string(\n+                                                &String::from_utf8(slice.data().to_vec())\n+                                                    .unwrap(),\n+                                            )\n+                                            .unwrap();\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::ByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder\n+                                            .append_string(\n+                                                &String::from_utf8(slice.data().to_vec())\n+                                                    .unwrap(),\n+                                            )\n+                                            .unwrap();\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                    };\n+\n+                    println!(\"Adding array to batch\");\n+                    batch.push(array);\n+                }\n+\n+                println!(\"Loaded batch of {} rows\", row_count);\n+\n+                if row_count == 0 {\n+                    Ok(None)\n+                } else {\n+                    Ok(Some(RecordBatch::try_new(self.schema.clone(), batch)?))\n+                }\n+            }\n+            _ => Ok(None),\n+        }\n+    }\n+}\n+\n+fn to_arrow(t: &Type) -> Result<Field> {\n+    match t {\n+        Type::PrimitiveType {\n+            basic_info,\n+            physical_type,\n+            ..\n+        } => {\n+            let arrow_type = match physical_type {\n+                basic::Type::BOOLEAN => DataType::Boolean,\n+                basic::Type::INT32 => DataType::Int32,\n+                basic::Type::INT64 => DataType::Int64,\n+                basic::Type::INT96 => DataType::Int64,\n+                basic::Type::FLOAT => DataType::Float32,\n+                basic::Type::DOUBLE => DataType::Float64,\n+                basic::Type::BYTE_ARRAY => DataType::Utf8,\n+                basic::Type::FIXED_LEN_BYTE_ARRAY => DataType::Utf8,\n+            };\n+\n+            Ok(Field::new(basic_info.name(), arrow_type, false))\n+        }\n+        Type::GroupType { basic_info, fields } => Ok(Field::new(\n \n Review comment:\n   Group type can also represent list, map, etc. We should not convert them all to struct.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T22:52:14.992+0000",
                    "updated": "2019-03-10T22:52:14.992+0000",
                    "started": "2019-03-10T22:52:14.992+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210716",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210717",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264063112\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n \n Review comment:\n   Maybe we should return a `Result` from this function? also in the `else` branch we should not just `panic`.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T22:52:14.994+0000",
                    "updated": "2019-03-10T22:52:14.994+0000",
                    "started": "2019-03-10T22:52:14.993+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210717",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210719",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264063655\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n \n Review comment:\n   I don't really understand what I need to do here. For now I think we should limit support to simple types so I'm not worried about nesting yet, but I do want to support null values. \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T22:57:35.139+0000",
                    "updated": "2019-03-10T22:57:35.139+0000",
                    "started": "2019-03-10T22:57:35.138+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210719",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210724",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264064669\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = BooleanBuilder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int32ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i32> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int32Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int64ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i64> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int96ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<Int96> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(Int96::new());\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+\n+                                    for i in 0..count {\n+                                        let v = read_buffer[i].data();\n+                                        let value: u128 = (v[0] as u128) << 64\n+                                            | (v[1] as u128) << 32\n+                                            | (v[2] as u128);\n+                                        let ms: i64 = (value / 1000000) as i64;\n+                                        builder.append_value(ms).unwrap();\n+                                    }\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FloatColumnReader(ref mut r) => {\n+                            let mut builder = Float32Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f32> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::DoubleColumnReader(ref mut r) => {\n+                            let mut builder = Float64Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f64> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FixedLenByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder\n+                                            .append_string(\n+                                                &String::from_utf8(slice.data().to_vec())\n+                                                    .unwrap(),\n+                                            )\n+                                            .unwrap();\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::ByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder\n+                                            .append_string(\n+                                                &String::from_utf8(slice.data().to_vec())\n+                                                    .unwrap(),\n+                                            )\n+                                            .unwrap();\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                    };\n+\n+                    println!(\"Adding array to batch\");\n+                    batch.push(array);\n+                }\n+\n+                println!(\"Loaded batch of {} rows\", row_count);\n+\n+                if row_count == 0 {\n+                    Ok(None)\n+                } else {\n+                    Ok(Some(RecordBatch::try_new(self.schema.clone(), batch)?))\n+                }\n+            }\n+            _ => Ok(None),\n+        }\n+    }\n+}\n+\n+fn to_arrow(t: &Type) -> Result<Field> {\n+    match t {\n+        Type::PrimitiveType {\n+            basic_info,\n+            physical_type,\n+            ..\n+        } => {\n+            let arrow_type = match physical_type {\n \n Review comment:\n   Do you mean `basic_info.logical_type()`? If so, the value is `NONE` for every column in the test parquet file\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T23:18:49.535+0000",
                    "updated": "2019-03-10T23:18:49.535+0000",
                    "started": "2019-03-10T23:18:49.534+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210724",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210725",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264064777\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n \n Review comment:\n   In that case, you should only pass in `None` for `def_levels` when you know that the column is required. Otherwise, you should pass in a mutable slice with `batch_size` length, which will be filled up with the `def_levels` for the values. Note that the number of values filled in by this method will always be equal or less than the `batch_size`. \r\n   \r\n   For instance, if `batch_size` is 10, and there are 3 null values, then the `def_levels` will contain 10 entries while `values` will only contain 7 entries (occupy the first 7 slot for the input `values` slice - the rest will just be the default value).\r\n   \r\n   After calling this method, you'll need to inspect the `def_levels` vector to find out the null values. A value is non-null iff the corresponding `def_level` equals to `max_def_level`.\r\n   \r\n   An example can be found [here](https://github.com/apache/arrow/blob/master/rust/parquet/src/record/triplet.rs#L293): \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T23:20:55.833+0000",
                    "updated": "2019-03-10T23:20:55.833+0000",
                    "started": "2019-03-10T23:20:55.832+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210725",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210726",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264064777\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n \n Review comment:\n   In that case, you should only pass in `None` for `def_levels` when you know that the column is required. Otherwise, you should pass in a mutable slice with `batch_size` length, which will be filled up with the `def_levels` for the values. Note that the number of values filled in by this method will always be equal or less than the `batch_size`. \r\n   \r\n   For instance, if `batch_size` is 10, and there are 3 null values, then the `def_levels` will contain 10 entries while `values` will only contain 7 entries (occupy the first 7 slot for the input `values` slice - the rest will just be the default value).\r\n   \r\n   After calling this method, you'll need to inspect the `def_levels` vector to find out the null values. A value is non-null iff the corresponding `def_level` equals to `max_def_level`.\r\n   \r\n   An example can be found [here](https://github.com/apache/arrow/blob/master/rust/parquet/src/record/triplet.rs#L293).\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T23:21:08.031+0000",
                    "updated": "2019-03-10T23:21:08.031+0000",
                    "started": "2019-03-10T23:21:08.030+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210726",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210727",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on issue #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#issuecomment-471364666\n \n \n   @sunchao I fixed the nits but I need some guidance on checking against logical types and how definition/repetition levels work when you have some time. Maybe you can point me to some code I can learn from?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T23:21:27.809+0000",
                    "updated": "2019-03-10T23:21:27.809+0000",
                    "started": "2019-03-10T23:21:27.808+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210727",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210728",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on issue #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#issuecomment-471365008\n \n \n   @andygrove Yes. To convert group type, you can check [here](https://github.com/apache/arrow/blob/master/rust/parquet/src/reader/schema.rs), which actually implements the conversion from Parquet to Arrow schema. For an introduction on definition/repetition levels, you can read [this article](https://blog.twitter.com/engineering/en_us/a/2013/dremel-made-simple-with-parquet.html), which I found pretty helpful.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T23:26:31.498+0000",
                    "updated": "2019-03-10T23:26:31.498+0000",
                    "started": "2019-03-10T23:26:31.497+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210728",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210729",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264065223\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = BooleanBuilder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int32ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i32> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int32Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int64ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i64> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int96ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<Int96> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(Int96::new());\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+\n+                                    for i in 0..count {\n+                                        let v = read_buffer[i].data();\n+                                        let value: u128 = (v[0] as u128) << 64\n+                                            | (v[1] as u128) << 32\n+                                            | (v[2] as u128);\n+                                        let ms: i64 = (value / 1000000) as i64;\n+                                        builder.append_value(ms).unwrap();\n+                                    }\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FloatColumnReader(ref mut r) => {\n+                            let mut builder = Float32Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f32> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::DoubleColumnReader(ref mut r) => {\n+                            let mut builder = Float64Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f64> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count]).unwrap();\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FixedLenByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder\n+                                            .append_string(\n+                                                &String::from_utf8(slice.data().to_vec())\n+                                                    .unwrap(),\n+                                            )\n+                                            .unwrap();\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::ByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder\n+                                            .append_string(\n+                                                &String::from_utf8(slice.data().to_vec())\n+                                                    .unwrap(),\n+                                            )\n+                                            .unwrap();\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                    };\n+\n+                    println!(\"Adding array to batch\");\n+                    batch.push(array);\n+                }\n+\n+                println!(\"Loaded batch of {} rows\", row_count);\n+\n+                if row_count == 0 {\n+                    Ok(None)\n+                } else {\n+                    Ok(Some(RecordBatch::try_new(self.schema.clone(), batch)?))\n+                }\n+            }\n+            _ => Ok(None),\n+        }\n+    }\n+}\n+\n+fn to_arrow(t: &Type) -> Result<Field> {\n+    match t {\n+        Type::PrimitiveType {\n+            basic_info,\n+            physical_type,\n+            ..\n+        } => {\n+            let arrow_type = match physical_type {\n \n Review comment:\n   In the `NONE` case it should be converted to the corresponding type, e.g., `PhysicalType::INT32 -> int`, `PhysicalType::INT64 -> long`, etc. Again you can check [here](https://github.com/apache/arrow/blob/master/rust/parquet/src/reader/schema.rs) or [here](https://github.com/apache/arrow/blob/master/rust/parquet/src/record/api.rs) for reference.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-10T23:32:25.676+0000",
                    "updated": "2019-03-10T23:32:25.676+0000",
                    "started": "2019-03-10T23:32:25.675+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210729",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210736",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "liurenjie1024 commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264068539\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,688 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn new(filename: &str) -> Self {\n+        let file = File::open(filename).unwrap();\n+        let parquet_file = ParquetFile::open(file, None).unwrap();\n+        let schema = parquet_file.schema.clone();\n+        Self {\n+            filename: filename.to_string(),\n+            schema,\n+        }\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone()).unwrap();\n+        let parquet_file = ParquetFile::open(file, projection.clone()).unwrap();\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        println!(\"open()\");\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+                //println!(\"Parquet schema: {:?}\", schema);\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index).unwrap();\n+\n+            self.column_readers = vec![];\n+\n+            for i in &self.projection {\n+                self.column_readers\n+                    .push(reader.get_column_reader(*i).unwrap());\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+        } else {\n+            panic!()\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n \n Review comment:\n   For a simple POC, I think it's better to ignore nulls and nest types. For null elements you need an spaced reader.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-11T00:49:21.177+0000",
                    "updated": "2019-03-11T00:49:21.177+0000",
                    "started": "2019-03-11T00:49:21.176+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210736",
                    "issueId": "13213594"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/worklog/210737",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "liurenjie1024 commented on pull request #3851: ARROW-4466: [Rust] [DataFusion] Add support for Parquet data source\nURL: https://github.com/apache/arrow/pull/3851#discussion_r264068643\n \n \n\n ##########\n File path: rust/datafusion/src/datasource/parquet.rs\n ##########\n @@ -0,0 +1,681 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet Data source\n+\n+use std::fs::File;\n+use std::string::String;\n+use std::sync::{Arc, Mutex};\n+\n+use arrow::array::Array;\n+use arrow::datatypes::{DataType, Field, Schema};\n+use arrow::record_batch::RecordBatch;\n+\n+use parquet::basic;\n+use parquet::column::reader::*;\n+use parquet::data_type::ByteArray;\n+use parquet::file::reader::*;\n+use parquet::schema::types::Type;\n+\n+use crate::datasource::{RecordBatchIterator, ScanResult, Table};\n+use crate::execution::error::{ExecutionError, Result};\n+use arrow::builder::BooleanBuilder;\n+use arrow::builder::Int64Builder;\n+use arrow::builder::{BinaryBuilder, Float32Builder, Float64Builder, Int32Builder};\n+use parquet::data_type::Int96;\n+\n+pub struct ParquetTable {\n+    filename: String,\n+    schema: Arc<Schema>,\n+}\n+\n+impl ParquetTable {\n+    pub fn try_new(filename: &str) -> Result<Self> {\n+        let file = File::open(filename)?;\n+        let parquet_file = ParquetFile::open(file, None)?;\n+        let schema = parquet_file.schema.clone();\n+        Ok(Self {\n+            filename: filename.to_string(),\n+            schema,\n+        })\n+    }\n+}\n+\n+impl Table for ParquetTable {\n+    fn schema(&self) -> &Arc<Schema> {\n+        &self.schema\n+    }\n+\n+    fn scan(\n+        &self,\n+        projection: &Option<Vec<usize>>,\n+        _batch_size: usize,\n+    ) -> Result<Vec<ScanResult>> {\n+        let file = File::open(self.filename.clone())?;\n+        let parquet_file = ParquetFile::open(file, projection.clone())?;\n+        Ok(vec![Arc::new(Mutex::new(parquet_file))])\n+    }\n+}\n+\n+pub struct ParquetFile {\n+    reader: SerializedFileReader<File>,\n+    row_group_index: usize,\n+    /// The schema of the underlying file\n+    schema: Arc<Schema>,\n+    projection: Vec<usize>,\n+    batch_size: usize,\n+    current_row_group: Option<Box<RowGroupReader>>,\n+    column_readers: Vec<ColumnReader>,\n+}\n+\n+impl ParquetFile {\n+    pub fn open(file: File, projection: Option<Vec<usize>>) -> Result<Self> {\n+        let reader = SerializedFileReader::new(file)?;\n+\n+        let metadata = reader.metadata();\n+        let file_type = to_arrow(metadata.file_metadata().schema())?;\n+\n+        match file_type.data_type() {\n+            DataType::Struct(fields) => {\n+                let schema = Schema::new(fields.clone());\n+\n+                let projection = match projection {\n+                    Some(p) => p,\n+                    None => {\n+                        let mut p = Vec::with_capacity(schema.fields().len());\n+                        for i in 0..schema.fields().len() {\n+                            p.push(i);\n+                        }\n+                        p\n+                    }\n+                };\n+\n+                let projected_fields: Vec<Field> = projection\n+                    .iter()\n+                    .map(|i| schema.fields()[*i].clone())\n+                    .collect();\n+\n+                let projected_schema = Arc::new(Schema::new(projected_fields));\n+\n+                Ok(ParquetFile {\n+                    reader: reader,\n+                    row_group_index: 0,\n+                    schema: projected_schema,\n+                    projection,\n+                    batch_size: 64 * 1024,\n+                    current_row_group: None,\n+                    column_readers: vec![],\n+                })\n+            }\n+            _ => Err(ExecutionError::General(\n+                \"Failed to read Parquet schema\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn load_next_row_group(&mut self) -> Result<()> {\n+        if self.row_group_index < self.reader.num_row_groups() {\n+            let reader = self.reader.get_row_group(self.row_group_index)?;\n+\n+            self.column_readers = Vec::with_capacity(self.projection.len());\n+\n+            for i in &self.projection {\n+                self.column_readers.push(reader.get_column_reader(*i)?);\n+            }\n+\n+            self.current_row_group = Some(reader);\n+            self.row_group_index += 1;\n+\n+            Ok(())\n+        } else {\n+            Err(ExecutionError::General(\n+                \"Attempt to read past final row group\".to_string(),\n+            ))\n+        }\n+    }\n+\n+    fn load_batch(&mut self) -> Result<Option<RecordBatch>> {\n+        match &self.current_row_group {\n+            Some(reader) => {\n+                let mut batch: Vec<Arc<Array>> = Vec::with_capacity(reader.num_columns());\n+                let mut row_count = 0;\n+                for i in 0..self.column_readers.len() {\n+                    let array: Arc<Array> = match self.column_readers[i] {\n+                        ColumnReader::BoolColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<bool> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(false);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = BooleanBuilder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count])?;\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int32ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i32> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int32Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count])?;\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int64ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<i64> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0);\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+                                    builder.append_slice(&read_buffer[0..count])?;\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::Int96ColumnReader(ref mut r) => {\n+                            let mut read_buffer: Vec<Int96> =\n+                                Vec::with_capacity(self.batch_size);\n+\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(Int96::new());\n+                            }\n+\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    let mut builder = Int64Builder::new(count);\n+\n+                                    for i in 0..count {\n+                                        let v = read_buffer[i].data();\n+                                        let value: u128 = (v[0] as u128) << 64\n+                                            | (v[1] as u128) << 32\n+                                            | (v[2] as u128);\n+                                        let ms: i64 = (value / 1000000) as i64;\n+                                        builder.append_value(ms)?;\n+                                    }\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FloatColumnReader(ref mut r) => {\n+                            let mut builder = Float32Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f32> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count])?;\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::DoubleColumnReader(ref mut r) => {\n+                            let mut builder = Float64Builder::new(self.batch_size);\n+                            let mut read_buffer: Vec<f64> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                read_buffer.push(0.0);\n+                            }\n+                            match r.read_batch(\n+                                self.batch_size,\n+                                None,\n+                                None,\n+                                &mut read_buffer,\n+                            ) {\n+                                Ok((count, _)) => {\n+                                    builder.append_slice(&read_buffer[0..count])?;\n+                                    row_count = count;\n+                                    Arc::new(builder.finish())\n+                                }\n+                                Err(e) => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {}): {:?}\",\n+                                        i, e\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::FixedLenByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder.append_string(\n+                                            &String::from_utf8(slice.data().to_vec())\n+                                                .unwrap(),\n+                                        )?;\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                        ColumnReader::ByteArrayColumnReader(ref mut r) => {\n+                            let mut b: Vec<ByteArray> =\n+                                Vec::with_capacity(self.batch_size);\n+                            for _ in 0..self.batch_size {\n+                                b.push(ByteArray::default());\n+                            }\n+                            match r.read_batch(self.batch_size, None, None, &mut b) {\n+                                Ok((count, _)) => {\n+                                    row_count = count;\n+                                    let mut builder = BinaryBuilder::new(row_count);\n+                                    for j in 0..row_count {\n+                                        let slice = b[j].slice(0, b[j].len());\n+                                        builder.append_string(\n+                                            &String::from_utf8(slice.data().to_vec())\n+                                                .unwrap(),\n+                                        )?;\n+                                    }\n+                                    Arc::new(builder.finish())\n+                                }\n+                                _ => {\n+                                    return Err(ExecutionError::NotImplemented(format!(\n+                                        \"Error reading parquet batch (column {})\",\n+                                        i\n+                                    )));\n+                                }\n+                            }\n+                        }\n+                    };\n+\n+                    batch.push(array);\n+                }\n+\n+                if row_count == 0 {\n+                    Ok(None)\n+                } else {\n+                    Ok(Some(RecordBatch::try_new(self.schema.clone(), batch)?))\n+                }\n+            }\n+            _ => Ok(None),\n+        }\n+    }\n+}\n+\n+fn to_arrow(t: &Type) -> Result<Field> {\n \n Review comment:\n   Maybe you can use the schema converter in parquet::reader::schema::parquet_to_arrow_schema?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-03-11T00:49:21.261+0000",
                    "updated": "2019-03-11T00:49:21.261+0000",
                    "started": "2019-03-11T00:49:21.260+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "210737",
                    "issueId": "13213594"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 39600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@74a278a5[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@63d28a91[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7df7d094[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@a865bb9[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@39968435[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@1ce55924[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5f55595c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@67b68a9c[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6a0c1d73[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@5c16456f[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@29fc6808[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@7acde5c1[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 39600,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Mar 15 21:45:07 UTC 2019",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2019-03-15T21:45:07.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-4466/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2019-02-03T17:28:55.000+0000",
        "updated": "2019-03-15T21:45:16.000+0000",
        "timeoriginalestimate": null,
        "description": "As a user, I would like to be able to run SQL queries against Parquet files.\r\n\r\nFor the initial implementation we can just support primitive types.\r\n\r\n\u00a0",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "11h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 39600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Rust] [DataFusion] Add support for Parquet data sources",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13213594/comment/16793959",
                    "id": "16793959",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "body": "Issue resolved by pull request 3851\n[https://github.com/apache/arrow/pull/3851]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "created": "2019-03-15T21:45:07.865+0000",
                    "updated": "2019-03-15T21:45:07.865+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|yi0m6o:",
        "customfield_12314139": null
    }
}