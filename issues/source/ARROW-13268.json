{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13388007",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007",
    "key": "ARROW-13268",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350323",
                "id": "12350323",
                "description": "",
                "name": "6.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-10-26"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available",
            "query-engine"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12618703",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12618703",
                "type": {
                    "id": "12310460",
                    "name": "Child-Issue",
                    "inward": "is a child of",
                    "outward": "is a parent of",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310460"
                },
                "inwardIssue": {
                    "id": "13376404",
                    "key": "ARROW-12633",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376404",
                    "fields": {
                        "summary": "[C++] Query engine umbrella issue",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                            "description": "The issue is open and ready for the assignee to start work on it.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                            "name": "Open",
                            "id": "1",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                                "id": 2,
                                "key": "new",
                                "colorName": "blue-gray",
                                "name": "To Do"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            },
            {
                "id": "12623959",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12623959",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13395606",
                    "key": "ARROW-13642",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13395606",
                    "fields": {
                        "summary": "[C++][Compute] Implement many-to-many inner hash join",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
                            "name": "Critical",
                            "id": "2"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            },
            {
                "id": "12622284",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12622284",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "inwardIssue": {
                    "id": "13396837",
                    "key": "ARROW-13706",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13396837",
                    "fields": {
                        "summary": "[C++][Compute] Add Find method to Grouper",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                            "description": "The issue is open and ready for the assignee to start work on it.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                            "name": "Open",
                            "id": "1",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                                "id": 2,
                                "key": "new",
                                "colorName": "blue-gray",
                                "name": "To Do"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=niranda",
            "name": "niranda",
            "key": "niranda",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=39936",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=39936",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=39936",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=39936"
            },
            "displayName": "Niranda Perera",
            "active": true,
            "timeZone": "America/Indiana/Indianapolis"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 36600,
            "total": 36600,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 36600,
            "total": 36600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-13268/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 61,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/628037",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nirandaperera opened a new pull request #10808:\nURL: https://github.com/apache/arrow/pull/10808\n\n\n   Adding hash semi joins\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-26T22:19:38.256+0000",
                    "updated": "2021-07-26T22:19:38.256+0000",
                    "started": "2021-07-26T22:19:38.256+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "628037",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/628038",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10808:\nURL: https://github.com/apache/arrow/pull/10808#issuecomment-887066293\n\n\n   https://issues.apache.org/jira/browse/ARROW-13268\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-26T22:19:52.292+0000",
                    "updated": "2021-07-26T22:19:52.292+0000",
                    "started": "2021-07-26T22:19:52.291+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "628038",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/630706",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nirandaperera closed pull request #10808:\nURL: https://github.com/apache/arrow/pull/10808\n\n\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-28T18:38:55.799+0000",
                    "updated": "2021-07-28T18:38:55.799+0000",
                    "started": "2021-07-28T18:38:55.798+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "630706",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/631868",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nirandaperera opened a new pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845\n\n\n   Adding prelim impl of semi joins\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-30T20:06:49.808+0000",
                    "updated": "2021-07-30T20:06:49.808+0000",
                    "started": "2021-07-30T20:06:49.808+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631868",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/631869",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#issuecomment-890126576\n\n\n   https://issues.apache.org/jira/browse/ARROW-13268\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-30T20:07:03.191+0000",
                    "updated": "2021-07-30T20:07:03.191+0000",
                    "started": "2021-07-30T20:07:03.191+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631869",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/631893",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#issuecomment-890166609\n\n\n   1. The comment about UINT32_MAX probably needs to be updated\r\n   \r\n   2. It seems to me that ConsumeCachedProbeBatches is only called for a single thread index - the one for the thread that reaches completion of build_counter_.\r\n   \r\n   3. Because in StopProducing calls to Cancel() on two AtomicCounters are connected with ||, finished_.MarkFinished() can be called twice (first thread gets true from first counter Cancel() call and some other thread from the second Cancel() call). Also, shouldn't we always Cancel() both counters?\r\n   \r\n   4. I wonder what happens with an empty input on build side?\r\n   \r\n   5. I think we will only have one class for hash join. It should be fine to call it HashJoinNode and throw Status::NotImplemented() for join types outside of semi, anti-semi. JoinType enum could have elements from all other types as well. Also JoinType enum would be better as \"enum class\" although Arrow C++ probably has some policy about enums.\r\n   \r\n   6. I would rename build_side_complete_ to hash_table_built_ or hash_table_build_complete_. Currently I get it confused with build_counter_ checks, where one means all build side input batches consumed by local state hash tables, and the other means hash table merge is complete.\r\n   \r\n   7. Also it would be nice to tie these two conditions above to futures, so that a merge task and tasks to process cache probe side batches could be generated and scheduled to execute once these futures are complete. But the futures are not critical at this point, just something nice to have.\r\n   \r\n   8. Status returned from CacheProbeBatch is always OK()\r\n   \r\n   9. We probably don't support DictionaryArray in key columns in the code as it is right now, we should check and return Status::NotImplemented() when making hash join node (or make sure it works). Also there could be a scenario where one side of the join uses DictionaryArray while the other uses Array with the same underlying type for keys to compare.\r\n   \r\n   10. In BuildSideMerge() ARROW_DCHECK(state->grouper). Perhaps it is a copy-paste from group by node, but it would be good to have a comment about why it is not possible to have states 0 and 2 initialized but not 1. This is not obvious. And maybe it should just be relaxed to skip processing if the local thread state with a given index is not initialized.\r\n   \r\n   11. TotalReached() method added to AtomicCounter is not used anywhere.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-30T21:36:59.096+0000",
                    "updated": "2021-07-30T21:36:59.096+0000",
                    "started": "2021-07-30T21:36:59.095+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631893",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/631894",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa edited a comment on pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#issuecomment-890166609\n\n\n   1. The comment about UINT32_MAX probably needs to be updated\r\n   \r\n   2. It seems to me that ConsumeCachedProbeBatches is only called for a single thread index - the one for the thread that reaches completion of build_counter_.\r\n   \r\n   3. Because in StopProducing calls to Cancel() on two AtomicCounters are connected with ||, finished_.MarkFinished() can be called twice (first thread gets true from first counter Cancel() call and some other thread from the second Cancel() call). Also, shouldn't we always Cancel() both counters?\r\n   \r\n   4. I wonder what happens with an empty input on build side?\r\n   \r\n   5. I think we will only have one class for hash join. It should be fine to call it HashJoinNode and throw Status::NotImplemented() for join types outside of semi, anti-semi. JoinType enum could have elements from all other types as well. Also JoinType enum would be better as \"enum class\" although Arrow C++ probably has some policy about enums.\r\n   \r\n   6. I would rename build_side_complete_ to hash_table_built_ or hash_table_build_complete_. Currently I get it confused with build_counter_ checks, where one means all build side input batches consumed by local state hash tables, and the other means hash table merge is complete.\r\n   \r\n   7. Also it would be nice to tie these two conditions above to futures, so that a merge task and tasks to process cache probe side batches could be generated and scheduled to execute once these futures are complete. But the futures are not critical at this point, just something nice to have.\r\n   \r\n   8. Status returned from CacheProbeBatch is always OK()\r\n   \r\n   9. We probably don't support DictionaryArray in key columns in the code as it is right now, we should check and return Status::NotImplemented() when making hash join node (or make sure it works). Also there could be a scenario where one side of the join uses DictionaryArray while the other uses Array with the same underlying type for keys to compare.\r\n   \r\n   10. In BuildSideMerge() ARROW_DCHECK(state->grouper). Perhaps it is a copy-paste from group by node, but it would be good to have a comment about why it is not possible to have states 0 and 2 initialized but not 1. This is not obvious. And maybe it should just be relaxed to skip processing if the local thread state with a given index is not initialized.\r\n   \r\n   11. TotalReached() method added to AtomicCounter is not used anywhere.\r\n   \r\n   12. There is a problem with null key. I believe in hash join with equality condition it should be that \"null != null\" (and there is usually a separate comparison operator that treats nulls as equal), while in group by \"null==null\" when matching groups. We should have a comment about it and document it for the users (maybe we don't have documentation strings for exec nodes yet). If needed we would have to filter out null keys separately from Grouper.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-30T21:45:52.364+0000",
                    "updated": "2021-07-30T21:45:52.364+0000",
                    "started": "2021-07-30T21:45:52.364+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631894",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/632384",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nirandaperera commented on pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#issuecomment-891068825\n\n\n   >     2. It seems to me that ConsumeCachedProbeBatches is only called for a single thread index - the one for the thread that reaches completion of build_counter_.\r\n   > \r\n   Yes, thanks @michalursa. I missed this! \r\n   \r\n   >     3. Because in StopProducing calls to Cancel() on two AtomicCounters are connected with ||, finished_.MarkFinished() can be called twice (first thread gets true from first counter Cancel() call and some other thread from the second Cancel() call). Also, shouldn't we always Cancel() both counters?\r\n   > \r\n   I see... The `GroupByNode` had this,\r\n   ```c++\r\n       if (input_counter_.Cancel()) {\r\n         finished_.MarkFinished();\r\n       } else if (output_counter_.Cancel()) {\r\n         finished_.MarkFinished();\r\n       }\r\n   ``` \r\n   and I was wondering why both the cases had the same code path. I thought it can be combined in a single statement. \r\n   \r\n   So, do you mean to say that `finished_.MarkFinished()` should be called if `build_counter_.Cancel() && out_counter_.Cancel()`?\r\n   \r\n   >     4. I wonder what happens with an empty input on build side?\r\n   > \r\n   This was my thought process. `build_counter_` has -1 for total initially. So, until the `build_input` signals the `InputFinished` with 0, probe batches will be cached. And when it receives 0, it toggles `build_side_complete_` and probe batches will be queried against an empty hashmap. \r\n   We could actually return a NullArray in the `Grouper::Find` method, prematurely (if the hashmap is empty). WDYT?\r\n   \r\n   >     5. I think we will only have one class for hash join. It should be fine to call it HashJoinNode and throw Status::NotImplemented() for join types outside of semi, anti-semi. JoinType enum could have elements from all other types as well. Also JoinType enum would be better as \"enum class\" although Arrow C++ probably has some policy about enums.\r\n   > \r\n   Sure! \r\n   \r\n   >     6. I would rename build_side_complete_ to hash_table_built_ or hash_table_build_complete_. Currently I get it confused with build_counter_ checks, where one means all build side input batches consumed by local state hash tables, and the other means hash table merge is complete.\r\n   > \r\n   Sure!\r\n   \r\n   >     7. Also it would be nice to tie these two conditions above to futures, so that a merge task and tasks to process cache probe side batches could be generated and scheduled to execute once these futures are complete. But the futures are not critical at this point, just something nice to have.\r\n   > \r\n   I will think about this one! :-)\r\n   \r\n   >     8. Status returned from CacheProbeBatch is always OK()\r\n   > \r\n   I'll make this void!\r\n   \r\n   >     9. We probably don't support DictionaryArray in key columns in the code as it is right now, we should check and return Status::NotImplemented() when making hash join node (or make sure it works). Also there could be a scenario where one side of the join uses DictionaryArray while the other uses Array with the same underlying type for keys to compare.\r\n   > \r\n   Sure!\r\n   \r\n   >     10. In BuildSideMerge() ARROW_DCHECK(state->grouper). Perhaps it is a copy-paste from group by node, but it would be good to have a comment about why it is not possible to have states 0 and 2 initialized but not 1. This is not obvious. And maybe it should just be relaxed to skip processing if the local thread state with a given index is not initialized.\r\n   > \r\n   Yes, it is a copy from the GroupBy impl.\r\n   Ah! Good catch! that is something I didnt think about! Are we talking about a case like this?\r\n   Ex: 4 threads, but only 1 input batch. So, before/while other batches being initialized, thread0 receives the batch and calls `BuildSideMerge()`. Now, other states could have null, and ideally we could `continue` the loop if that is the case (because it is guaranteed that those states wouldn't receive any more batches, because build_counter_ is already completed.)\r\n   \r\n   >     11. TotalReached() method added to AtomicCounter is not used anywhere.\r\n   > \r\n   >     12. There is a problem with null key. I believe in hash join with equality condition it should be that \"null != null\" (and there is usually a separate comparison operator that treats nulls as equal), while in group by \"null==null\" when matching groups. We should have a comment about it and document it for the users (maybe we don't have documentation strings for exec nodes yet). If needed we would have to filter out null keys separately from Grouper.\r\n   > \r\n   I see... but it looks like Pandas holds null/NaN/na as a valid key and if the users want to, they have to explicitly drop na values. \r\n   https://stackoverflow.com/questions/23940181/pandas-merging-with-missing-values\r\n   I started a thread on this in Zulip https://ursalabs.zulipchat.com/#narrow/stream/180245-dev/topic/Null.20values.20as.20keys\r\n   \r\n   \r\n   Thank you very \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-02T14:21:34.239+0000",
                    "updated": "2021-08-02T14:21:34.239+0000",
                    "started": "2021-08-02T14:21:34.239+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "632384",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/632386",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nirandaperera edited a comment on pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#issuecomment-891068825\n\n\n   >     2. It seems to me that ConsumeCachedProbeBatches is only called for a single thread index - the one for the thread that reaches completion of build_counter_.\r\n   > \r\n   Yes, thanks @michalursa. I missed this! \r\n   \r\n   >     3. Because in StopProducing calls to Cancel() on two AtomicCounters are connected with ||, finished_.MarkFinished() can be called twice (first thread gets true from first counter Cancel() call and some other thread from the second Cancel() call). Also, shouldn't we always Cancel() both counters?\r\n   > \r\n   I see... The `GroupByNode` had this,\r\n   ```c++\r\n       if (input_counter_.Cancel()) {\r\n         finished_.MarkFinished();\r\n       } else if (output_counter_.Cancel()) {\r\n         finished_.MarkFinished();\r\n       }\r\n   ``` \r\n   and I was wondering why both the cases had the same code path. I thought it can be combined in a single statement. \r\n   \r\n   So, do you mean to say that `finished_.MarkFinished()` should be called if `build_counter_.Cancel() && out_counter_.Cancel()`?\r\n   \r\n   >     4. I wonder what happens with an empty input on build side?\r\n   > \r\n   This was my thought process. `build_counter_` has -1 for total initially. So, until the `build_input` signals the `InputFinished` with 0, probe batches will be cached. And when it receives 0, it toggles `build_side_complete_` and probe batches will be queried against an empty hashmap. \r\n   We could actually return a NullArray in the `Grouper::Find` method, prematurely (if the hashmap is empty). WDYT?\r\n   \r\n   >     5. I think we will only have one class for hash join. It should be fine to call it HashJoinNode and throw Status::NotImplemented() for join types outside of semi, anti-semi. JoinType enum could have elements from all other types as well. Also JoinType enum would be better as \"enum class\" although Arrow C++ probably has some policy about enums.\r\n   > \r\n   Sure! \r\n   \r\n   >     6. I would rename build_side_complete_ to hash_table_built_ or hash_table_build_complete_. Currently I get it confused with build_counter_ checks, where one means all build side input batches consumed by local state hash tables, and the other means hash table merge is complete.\r\n   > \r\n   Sure!\r\n   \r\n   >     7. Also it would be nice to tie these two conditions above to futures, so that a merge task and tasks to process cache probe side batches could be generated and scheduled to execute once these futures are complete. But the futures are not critical at this point, just something nice to have.\r\n   > \r\n   I will think about this one! :-)\r\n   \r\n   >     8. Status returned from CacheProbeBatch is always OK()\r\n   > \r\n   I'll make this void!\r\n   \r\n   >     9. We probably don't support DictionaryArray in key columns in the code as it is right now, we should check and return Status::NotImplemented() when making hash join node (or make sure it works). Also there could be a scenario where one side of the join uses DictionaryArray while the other uses Array with the same underlying type for keys to compare.\r\n   > \r\n   Sure!\r\n   \r\n   >     10. In BuildSideMerge() ARROW_DCHECK(state->grouper). Perhaps it is a copy-paste from group by node, but it would be good to have a comment about why it is not possible to have states 0 and 2 initialized but not 1. This is not obvious. And maybe it should just be relaxed to skip processing if the local thread state with a given index is not initialized.\r\n   > \r\n   Yes, it is a copy from the GroupBy impl.\r\n   Ah! Good catch! that is something I didnt think about! Are we talking about a case like this?\r\n   Ex: 4 threads, but only 1 input batch. So, before/while other batches being initialized, thread0 receives the batch and calls `BuildSideMerge()`. Now, other states could have null, and ideally we could `continue` the loop if that is the case (because it is guaranteed that those states wouldn't receive any more batches, because build_counter_ is already completed.)\r\n   \r\n   >     11. TotalReached() method added to AtomicCounter is not used anywhere.\r\n   > \r\n   >     12. There is a problem with null key. I believe in hash join with equality condition it should be that \"null != null\" (and there is usually a separate comparison operator that treats nulls as equal), while in group by \"null==null\" when matching groups. We should have a comment about it and document it for the users (maybe we don't have documentation strings for exec nodes yet). If needed we would have to filter out null keys separately from Grouper.\r\n   > \r\n   I see... but it looks like Pandas holds null/NaN/na as a valid key and if the users want to, they have to explicitly drop na values. \r\n   https://stackoverflow.com/questions/23940181/pandas-merging-with-missing-values\r\n   I started a thread on this in Zulip https://ursalabs.zulipchat.com/#narrow/stream/180245-dev/topic/Null.20values.20as.20keys\r\n   \r\n   \r\n   \r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-02T14:23:22.646+0000",
                    "updated": "2021-08-02T14:23:22.646+0000",
                    "started": "2021-08-02T14:23:22.646+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "632386",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/638533",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10948:\nURL: https://github.com/apache/arrow/pull/10948#issuecomment-900024015\n\n\n   https://issues.apache.org/jira/browse/ARROW-13268\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-17T06:17:00.823+0000",
                    "updated": "2021-08-17T06:17:00.823+0000",
                    "started": "2021-08-17T06:17:00.823+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "638533",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/638897",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ianmcook commented on pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#issuecomment-900640812\n\n\n   @aucahuasi could you review this please? thank you!\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-17T21:24:04.404+0000",
                    "updated": "2021-08-17T21:24:04.404+0000",
                    "started": "2021-08-17T21:24:04.403+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "638897",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639091",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#discussion_r691238077\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/options.h\n##########\n@@ -111,5 +111,32 @@ class ARROW_EXPORT SinkNodeOptions : public ExecNodeOptions {\n   std::function<Future<util::optional<ExecBatch>>()>* generator;\n };\n \n+enum JoinType {\n\nReview comment:\n       nit: enum class? (Though the compute functions are already somewhat inconsistent about this)\n\n##########\nFile path: cpp/src/arrow/compute/kernels/hash_aggregate.cc\n##########\n@@ -368,30 +370,43 @@ struct GrouperImpl : Grouper {\n     return std::move(impl);\n   }\n \n-  Result<Datum> Consume(const ExecBatch& batch) override {\n-    std::vector<int32_t> offsets_batch(batch.length + 1);\n+  Status PopulateKeyData(const ExecBatch& batch, std::vector<int32_t>* offsets_batch,\n+                         std::vector<uint8_t>* key_bytes_batch,\n+                         std::vector<uint8_t*>* key_buf_ptrs) const {\n+    offsets_batch->resize(batch.length + 1);\n     for (int i = 0; i < batch.num_values(); ++i) {\n-      encoders_[i]->AddLength(*batch[i].array(), offsets_batch.data());\n+      encoders_[i]->AddLength(*batch[i].array(), offsets_batch->data());\n     }\n \n     int32_t total_length = 0;\n     for (int64_t i = 0; i < batch.length; ++i) {\n       auto total_length_before = total_length;\n-      total_length += offsets_batch[i];\n-      offsets_batch[i] = total_length_before;\n+      total_length += offsets_batch->at(i);\n\nReview comment:\n       note that `at` forces bounds checking, you might prefer `(*offsets_batch)[i]` instead\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n\nReview comment:\n       Won't lambda_status be invalid?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n\nReview comment:\n       I think use of AtomicCounter means that this check is unnecessary/can be a DCHECK.\r\n   \r\n   Though I also think that this might need to be an atomic<bool> to be safe anyways, since I don't think there is any happens-before relationship between the plain bool here and any atomic/mutex.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n+          }));\n+\n+          // if the lambda execution failed internally, return status\n+          RETURN_NOT_OK(lambda_status);\n+        } else {\n+          RETURN_NOT_OK(ConsumeProbeBatch(cached.first, std::move(cached.second)));\n+        }\n+      }\n+      // cached vector will be cleared. exec batches are expected to be moved to the\n+      // lambdas\n+      cached_probe_batches.clear();\n+    }\n+\n+    // set flag\n+    cached_probe_batches_consumed = true;\n+    return Status::OK();\n+  }\n+\n+  Status GenerateOutput(int seq, const ArrayData& group_ids_data, ExecBatch batch) {\n+    if (group_ids_data.GetNullCount() == batch.length) {\n+      // All NULLS! hence, there are no valid outputs!\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" 0\";\n+      outputs_[0]->InputReceived(this, seq, batch.Slice(0, 0));\n+    } else if (group_ids_data.MayHaveNulls()) {  // values need to be filtered\n+      auto filter_arr =\n+          std::make_shared<BooleanArray>(group_ids_data.length, group_ids_data.buffers[0],\n+                                         /*null_bitmap=*/nullptr, /*null_count=*/0,\n+                                         /*offset=*/group_ids_data.offset);\n+      ARROW_ASSIGN_OR_RAISE(auto rec_batch,\n+                            batch.ToRecordBatch(output_schema_, ctx_->memory_pool()));\n+      ARROW_ASSIGN_OR_RAISE(\n+          auto filtered,\n+          Filter(rec_batch, filter_arr,\n+                 /* null_selection = DROP*/ FilterOptions::Defaults(), ctx_));\n+      auto out_batch = ExecBatch(*filtered.record_batch());\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << out_batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(out_batch));\n+    } else {  // all values are valid for output\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(batch));\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a probe batch and increment probe batches count. Probing would query the\n+  // grouper[build_result_index] which have been merged with all others.\n+  Status ConsumeProbeBatch(int seq, ExecBatch batch) {\n+    ARROW_LOG(DEBUG) << \"ConsumeProbeBatch seq:\" << seq;\n+\n+    auto& final_grouper = *local_states_[build_result_index].grouper;\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(probe_index_field_ids_.size());\n+    for (size_t i = 0; i < probe_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[probe_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Query the grouper with key_batch. If no match was found, returning group_ids would\n+    // have null.\n+    ARROW_ASSIGN_OR_RAISE(Datum group_ids, final_grouper.Find(key_batch));\n+    auto group_ids_data = *group_ids.array();\n+\n+    RETURN_NOT_OK(GenerateOutput(seq, group_ids_data, std::move(batch)));\n+\n+    if (out_counter_.Increment()) {\n+      finished_.MarkFinished();\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Attempt to cache a probe batch. If it is not cached, return false.\n+  // if cached_probe_batches_consumed is true, by the time a thread acquires\n+  // cached_probe_batches_mutex, it should no longer be cached! instead, it can be\n+  //  directly consumed!\n+  bool AttemptToCacheProbeBatch(int seq_num, ExecBatch* batch) {\n\nReview comment:\n       just a minor nit, but the current UnionNode doesn't renumber batches so there can be duplicate seq_nums. It sounds like we want to get rid of them anyways so it might be worth getting ahead of that and generating our own indices here.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n+          }));\n+\n+          // if the lambda execution failed internally, return status\n+          RETURN_NOT_OK(lambda_status);\n+        } else {\n+          RETURN_NOT_OK(ConsumeProbeBatch(cached.first, std::move(cached.second)));\n+        }\n+      }\n+      // cached vector will be cleared. exec batches are expected to be moved to the\n+      // lambdas\n+      cached_probe_batches.clear();\n+    }\n+\n+    // set flag\n+    cached_probe_batches_consumed = true;\n+    return Status::OK();\n+  }\n+\n+  Status GenerateOutput(int seq, const ArrayData& group_ids_data, ExecBatch batch) {\n+    if (group_ids_data.GetNullCount() == batch.length) {\n+      // All NULLS! hence, there are no valid outputs!\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" 0\";\n+      outputs_[0]->InputReceived(this, seq, batch.Slice(0, 0));\n+    } else if (group_ids_data.MayHaveNulls()) {  // values need to be filtered\n+      auto filter_arr =\n+          std::make_shared<BooleanArray>(group_ids_data.length, group_ids_data.buffers[0],\n+                                         /*null_bitmap=*/nullptr, /*null_count=*/0,\n+                                         /*offset=*/group_ids_data.offset);\n+      ARROW_ASSIGN_OR_RAISE(auto rec_batch,\n+                            batch.ToRecordBatch(output_schema_, ctx_->memory_pool()));\n+      ARROW_ASSIGN_OR_RAISE(\n+          auto filtered,\n+          Filter(rec_batch, filter_arr,\n+                 /* null_selection = DROP*/ FilterOptions::Defaults(), ctx_));\n+      auto out_batch = ExecBatch(*filtered.record_batch());\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << out_batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(out_batch));\n+    } else {  // all values are valid for output\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(batch));\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a probe batch and increment probe batches count. Probing would query the\n+  // grouper[build_result_index] which have been merged with all others.\n+  Status ConsumeProbeBatch(int seq, ExecBatch batch) {\n+    ARROW_LOG(DEBUG) << \"ConsumeProbeBatch seq:\" << seq;\n+\n+    auto& final_grouper = *local_states_[build_result_index].grouper;\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(probe_index_field_ids_.size());\n+    for (size_t i = 0; i < probe_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[probe_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Query the grouper with key_batch. If no match was found, returning group_ids would\n+    // have null.\n+    ARROW_ASSIGN_OR_RAISE(Datum group_ids, final_grouper.Find(key_batch));\n+    auto group_ids_data = *group_ids.array();\n+\n+    RETURN_NOT_OK(GenerateOutput(seq, group_ids_data, std::move(batch)));\n+\n+    if (out_counter_.Increment()) {\n+      finished_.MarkFinished();\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Attempt to cache a probe batch. If it is not cached, return false.\n+  // if cached_probe_batches_consumed is true, by the time a thread acquires\n+  // cached_probe_batches_mutex, it should no longer be cached! instead, it can be\n+  //  directly consumed!\n+  bool AttemptToCacheProbeBatch(int seq_num, ExecBatch* batch) {\n\nReview comment:\n       Or really, since we're guarding with a mutex already, why not just use a vector?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n\nReview comment:\n       I think this should be ErrorIfNotOk.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T13:53:15.182+0000",
                    "updated": "2021-08-18T13:53:15.182+0000",
                    "started": "2021-08-18T13:53:15.182+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639091",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639171",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nirandaperera commented on a change in pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#discussion_r691390022\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n\nReview comment:\n       Callers of `ConsumeCachedProbeBatches` would pass the invalid status to `ErrorIfNotOk`. So I think it is covered. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T15:55:50.156+0000",
                    "updated": "2021-08-18T15:55:50.156+0000",
                    "started": "2021-08-18T15:55:50.156+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639171",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639172",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nirandaperera commented on a change in pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#discussion_r691391327\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n+          }));\n+\n+          // if the lambda execution failed internally, return status\n+          RETURN_NOT_OK(lambda_status);\n+        } else {\n+          RETURN_NOT_OK(ConsumeProbeBatch(cached.first, std::move(cached.second)));\n+        }\n+      }\n+      // cached vector will be cleared. exec batches are expected to be moved to the\n+      // lambdas\n+      cached_probe_batches.clear();\n+    }\n+\n+    // set flag\n+    cached_probe_batches_consumed = true;\n+    return Status::OK();\n+  }\n+\n+  Status GenerateOutput(int seq, const ArrayData& group_ids_data, ExecBatch batch) {\n+    if (group_ids_data.GetNullCount() == batch.length) {\n+      // All NULLS! hence, there are no valid outputs!\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" 0\";\n+      outputs_[0]->InputReceived(this, seq, batch.Slice(0, 0));\n+    } else if (group_ids_data.MayHaveNulls()) {  // values need to be filtered\n+      auto filter_arr =\n+          std::make_shared<BooleanArray>(group_ids_data.length, group_ids_data.buffers[0],\n+                                         /*null_bitmap=*/nullptr, /*null_count=*/0,\n+                                         /*offset=*/group_ids_data.offset);\n+      ARROW_ASSIGN_OR_RAISE(auto rec_batch,\n+                            batch.ToRecordBatch(output_schema_, ctx_->memory_pool()));\n+      ARROW_ASSIGN_OR_RAISE(\n+          auto filtered,\n+          Filter(rec_batch, filter_arr,\n+                 /* null_selection = DROP*/ FilterOptions::Defaults(), ctx_));\n+      auto out_batch = ExecBatch(*filtered.record_batch());\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << out_batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(out_batch));\n+    } else {  // all values are valid for output\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(batch));\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a probe batch and increment probe batches count. Probing would query the\n+  // grouper[build_result_index] which have been merged with all others.\n+  Status ConsumeProbeBatch(int seq, ExecBatch batch) {\n+    ARROW_LOG(DEBUG) << \"ConsumeProbeBatch seq:\" << seq;\n+\n+    auto& final_grouper = *local_states_[build_result_index].grouper;\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(probe_index_field_ids_.size());\n+    for (size_t i = 0; i < probe_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[probe_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Query the grouper with key_batch. If no match was found, returning group_ids would\n+    // have null.\n+    ARROW_ASSIGN_OR_RAISE(Datum group_ids, final_grouper.Find(key_batch));\n+    auto group_ids_data = *group_ids.array();\n+\n+    RETURN_NOT_OK(GenerateOutput(seq, group_ids_data, std::move(batch)));\n+\n+    if (out_counter_.Increment()) {\n+      finished_.MarkFinished();\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Attempt to cache a probe batch. If it is not cached, return false.\n+  // if cached_probe_batches_consumed is true, by the time a thread acquires\n+  // cached_probe_batches_mutex, it should no longer be cached! instead, it can be\n+  //  directly consumed!\n+  bool AttemptToCacheProbeBatch(int seq_num, ExecBatch* batch) {\n\nReview comment:\n       yes, I'll do that. I was attempting to preserve the seq_num initially. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T15:56:47.747+0000",
                    "updated": "2021-08-18T15:56:47.747+0000",
                    "started": "2021-08-18T15:56:47.747+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639172",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639173",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#discussion_r691392441\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n\nReview comment:\n       This captures a reference to a stack local, but I would think that reference gets invalidated immediately - I think this is unsafe.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T15:57:33.302+0000",
                    "updated": "2021-08-18T15:57:33.302+0000",
                    "started": "2021-08-18T15:57:33.302+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639173",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639174",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#discussion_r691393145\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n+          }));\n+\n+          // if the lambda execution failed internally, return status\n+          RETURN_NOT_OK(lambda_status);\n+        } else {\n+          RETURN_NOT_OK(ConsumeProbeBatch(cached.first, std::move(cached.second)));\n+        }\n+      }\n+      // cached vector will be cleared. exec batches are expected to be moved to the\n+      // lambdas\n+      cached_probe_batches.clear();\n+    }\n+\n+    // set flag\n+    cached_probe_batches_consumed = true;\n+    return Status::OK();\n+  }\n+\n+  Status GenerateOutput(int seq, const ArrayData& group_ids_data, ExecBatch batch) {\n+    if (group_ids_data.GetNullCount() == batch.length) {\n+      // All NULLS! hence, there are no valid outputs!\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" 0\";\n+      outputs_[0]->InputReceived(this, seq, batch.Slice(0, 0));\n+    } else if (group_ids_data.MayHaveNulls()) {  // values need to be filtered\n+      auto filter_arr =\n+          std::make_shared<BooleanArray>(group_ids_data.length, group_ids_data.buffers[0],\n+                                         /*null_bitmap=*/nullptr, /*null_count=*/0,\n+                                         /*offset=*/group_ids_data.offset);\n+      ARROW_ASSIGN_OR_RAISE(auto rec_batch,\n+                            batch.ToRecordBatch(output_schema_, ctx_->memory_pool()));\n+      ARROW_ASSIGN_OR_RAISE(\n+          auto filtered,\n+          Filter(rec_batch, filter_arr,\n+                 /* null_selection = DROP*/ FilterOptions::Defaults(), ctx_));\n+      auto out_batch = ExecBatch(*filtered.record_batch());\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << out_batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(out_batch));\n+    } else {  // all values are valid for output\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(batch));\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a probe batch and increment probe batches count. Probing would query the\n+  // grouper[build_result_index] which have been merged with all others.\n+  Status ConsumeProbeBatch(int seq, ExecBatch batch) {\n+    ARROW_LOG(DEBUG) << \"ConsumeProbeBatch seq:\" << seq;\n+\n+    auto& final_grouper = *local_states_[build_result_index].grouper;\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(probe_index_field_ids_.size());\n+    for (size_t i = 0; i < probe_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[probe_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Query the grouper with key_batch. If no match was found, returning group_ids would\n+    // have null.\n+    ARROW_ASSIGN_OR_RAISE(Datum group_ids, final_grouper.Find(key_batch));\n+    auto group_ids_data = *group_ids.array();\n+\n+    RETURN_NOT_OK(GenerateOutput(seq, group_ids_data, std::move(batch)));\n+\n+    if (out_counter_.Increment()) {\n+      finished_.MarkFinished();\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Attempt to cache a probe batch. If it is not cached, return false.\n+  // if cached_probe_batches_consumed is true, by the time a thread acquires\n+  // cached_probe_batches_mutex, it should no longer be cached! instead, it can be\n+  //  directly consumed!\n+  bool AttemptToCacheProbeBatch(int seq_num, ExecBatch* batch) {\n\nReview comment:\n       In ARROW-13660 we'll get rid of seq_num fortunately\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T15:58:16.671+0000",
                    "updated": "2021-08-18T15:58:16.671+0000",
                    "started": "2021-08-18T15:58:16.671+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639174",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639175",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nirandaperera commented on a change in pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#discussion_r691393873\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/options.h\n##########\n@@ -111,5 +111,32 @@ class ARROW_EXPORT SinkNodeOptions : public ExecNodeOptions {\n   std::function<Future<util::optional<ExecBatch>>()>* generator;\n };\n \n+enum JoinType {\n\nReview comment:\n       I see.. yes, I can do that :-) Do we give preference to enum class over plain enums?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T15:59:00.054+0000",
                    "updated": "2021-08-18T15:59:00.054+0000",
                    "started": "2021-08-18T15:59:00.054+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639175",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639189",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#discussion_r691404976\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/options.h\n##########\n@@ -111,5 +111,32 @@ class ARROW_EXPORT SinkNodeOptions : public ExecNodeOptions {\n   std::function<Future<util::optional<ExecBatch>>()>* generator;\n };\n \n+enum JoinType {\n\nReview comment:\n       Hmm. The C++ style guide doesn't say anything. I would say enum class is generally preferred in modern C++. We semi-intentionally use plain enum in a few places (when nested inside a struct) but I'm not sure why we have that pattern. \r\n   \r\n   This isn't the only place where this happens so it's not a big deal, but I would prefer enum class so that it's not implicitly converted and so that namespacing is explicit.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T16:12:45.793+0000",
                    "updated": "2021-08-18T16:12:45.793+0000",
                    "started": "2021-08-18T16:12:45.792+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639189",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639244",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#discussion_r691238077\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/options.h\n##########\n@@ -111,5 +111,32 @@ class ARROW_EXPORT SinkNodeOptions : public ExecNodeOptions {\n   std::function<Future<util::optional<ExecBatch>>()>* generator;\n };\n \n+enum JoinType {\n\nReview comment:\n       nit: enum class? (Though the compute functions are already somewhat inconsistent about this)\n\n##########\nFile path: cpp/src/arrow/compute/kernels/hash_aggregate.cc\n##########\n@@ -368,30 +370,43 @@ struct GrouperImpl : Grouper {\n     return std::move(impl);\n   }\n \n-  Result<Datum> Consume(const ExecBatch& batch) override {\n-    std::vector<int32_t> offsets_batch(batch.length + 1);\n+  Status PopulateKeyData(const ExecBatch& batch, std::vector<int32_t>* offsets_batch,\n+                         std::vector<uint8_t>* key_bytes_batch,\n+                         std::vector<uint8_t*>* key_buf_ptrs) const {\n+    offsets_batch->resize(batch.length + 1);\n     for (int i = 0; i < batch.num_values(); ++i) {\n-      encoders_[i]->AddLength(*batch[i].array(), offsets_batch.data());\n+      encoders_[i]->AddLength(*batch[i].array(), offsets_batch->data());\n     }\n \n     int32_t total_length = 0;\n     for (int64_t i = 0; i < batch.length; ++i) {\n       auto total_length_before = total_length;\n-      total_length += offsets_batch[i];\n-      offsets_batch[i] = total_length_before;\n+      total_length += offsets_batch->at(i);\n\nReview comment:\n       note that `at` forces bounds checking, you might prefer `(*offsets_batch)[i]` instead\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n\nReview comment:\n       Won't lambda_status be invalid?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n\nReview comment:\n       I think use of AtomicCounter means that this check is unnecessary/can be a DCHECK.\r\n   \r\n   Though I also think that this might need to be an atomic<bool> to be safe anyways, since I don't think there is any happens-before relationship between the plain bool here and any atomic/mutex.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n+          }));\n+\n+          // if the lambda execution failed internally, return status\n+          RETURN_NOT_OK(lambda_status);\n+        } else {\n+          RETURN_NOT_OK(ConsumeProbeBatch(cached.first, std::move(cached.second)));\n+        }\n+      }\n+      // cached vector will be cleared. exec batches are expected to be moved to the\n+      // lambdas\n+      cached_probe_batches.clear();\n+    }\n+\n+    // set flag\n+    cached_probe_batches_consumed = true;\n+    return Status::OK();\n+  }\n+\n+  Status GenerateOutput(int seq, const ArrayData& group_ids_data, ExecBatch batch) {\n+    if (group_ids_data.GetNullCount() == batch.length) {\n+      // All NULLS! hence, there are no valid outputs!\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" 0\";\n+      outputs_[0]->InputReceived(this, seq, batch.Slice(0, 0));\n+    } else if (group_ids_data.MayHaveNulls()) {  // values need to be filtered\n+      auto filter_arr =\n+          std::make_shared<BooleanArray>(group_ids_data.length, group_ids_data.buffers[0],\n+                                         /*null_bitmap=*/nullptr, /*null_count=*/0,\n+                                         /*offset=*/group_ids_data.offset);\n+      ARROW_ASSIGN_OR_RAISE(auto rec_batch,\n+                            batch.ToRecordBatch(output_schema_, ctx_->memory_pool()));\n+      ARROW_ASSIGN_OR_RAISE(\n+          auto filtered,\n+          Filter(rec_batch, filter_arr,\n+                 /* null_selection = DROP*/ FilterOptions::Defaults(), ctx_));\n+      auto out_batch = ExecBatch(*filtered.record_batch());\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << out_batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(out_batch));\n+    } else {  // all values are valid for output\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(batch));\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a probe batch and increment probe batches count. Probing would query the\n+  // grouper[build_result_index] which have been merged with all others.\n+  Status ConsumeProbeBatch(int seq, ExecBatch batch) {\n+    ARROW_LOG(DEBUG) << \"ConsumeProbeBatch seq:\" << seq;\n+\n+    auto& final_grouper = *local_states_[build_result_index].grouper;\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(probe_index_field_ids_.size());\n+    for (size_t i = 0; i < probe_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[probe_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Query the grouper with key_batch. If no match was found, returning group_ids would\n+    // have null.\n+    ARROW_ASSIGN_OR_RAISE(Datum group_ids, final_grouper.Find(key_batch));\n+    auto group_ids_data = *group_ids.array();\n+\n+    RETURN_NOT_OK(GenerateOutput(seq, group_ids_data, std::move(batch)));\n+\n+    if (out_counter_.Increment()) {\n+      finished_.MarkFinished();\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Attempt to cache a probe batch. If it is not cached, return false.\n+  // if cached_probe_batches_consumed is true, by the time a thread acquires\n+  // cached_probe_batches_mutex, it should no longer be cached! instead, it can be\n+  //  directly consumed!\n+  bool AttemptToCacheProbeBatch(int seq_num, ExecBatch* batch) {\n\nReview comment:\n       just a minor nit, but the current UnionNode doesn't renumber batches so there can be duplicate seq_nums. It sounds like we want to get rid of them anyways so it might be worth getting ahead of that and generating our own indices here.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n+          }));\n+\n+          // if the lambda execution failed internally, return status\n+          RETURN_NOT_OK(lambda_status);\n+        } else {\n+          RETURN_NOT_OK(ConsumeProbeBatch(cached.first, std::move(cached.second)));\n+        }\n+      }\n+      // cached vector will be cleared. exec batches are expected to be moved to the\n+      // lambdas\n+      cached_probe_batches.clear();\n+    }\n+\n+    // set flag\n+    cached_probe_batches_consumed = true;\n+    return Status::OK();\n+  }\n+\n+  Status GenerateOutput(int seq, const ArrayData& group_ids_data, ExecBatch batch) {\n+    if (group_ids_data.GetNullCount() == batch.length) {\n+      // All NULLS! hence, there are no valid outputs!\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" 0\";\n+      outputs_[0]->InputReceived(this, seq, batch.Slice(0, 0));\n+    } else if (group_ids_data.MayHaveNulls()) {  // values need to be filtered\n+      auto filter_arr =\n+          std::make_shared<BooleanArray>(group_ids_data.length, group_ids_data.buffers[0],\n+                                         /*null_bitmap=*/nullptr, /*null_count=*/0,\n+                                         /*offset=*/group_ids_data.offset);\n+      ARROW_ASSIGN_OR_RAISE(auto rec_batch,\n+                            batch.ToRecordBatch(output_schema_, ctx_->memory_pool()));\n+      ARROW_ASSIGN_OR_RAISE(\n+          auto filtered,\n+          Filter(rec_batch, filter_arr,\n+                 /* null_selection = DROP*/ FilterOptions::Defaults(), ctx_));\n+      auto out_batch = ExecBatch(*filtered.record_batch());\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << out_batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(out_batch));\n+    } else {  // all values are valid for output\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(batch));\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a probe batch and increment probe batches count. Probing would query the\n+  // grouper[build_result_index] which have been merged with all others.\n+  Status ConsumeProbeBatch(int seq, ExecBatch batch) {\n+    ARROW_LOG(DEBUG) << \"ConsumeProbeBatch seq:\" << seq;\n+\n+    auto& final_grouper = *local_states_[build_result_index].grouper;\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(probe_index_field_ids_.size());\n+    for (size_t i = 0; i < probe_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[probe_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Query the grouper with key_batch. If no match was found, returning group_ids would\n+    // have null.\n+    ARROW_ASSIGN_OR_RAISE(Datum group_ids, final_grouper.Find(key_batch));\n+    auto group_ids_data = *group_ids.array();\n+\n+    RETURN_NOT_OK(GenerateOutput(seq, group_ids_data, std::move(batch)));\n+\n+    if (out_counter_.Increment()) {\n+      finished_.MarkFinished();\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Attempt to cache a probe batch. If it is not cached, return false.\n+  // if cached_probe_batches_consumed is true, by the time a thread acquires\n+  // cached_probe_batches_mutex, it should no longer be cached! instead, it can be\n+  //  directly consumed!\n+  bool AttemptToCacheProbeBatch(int seq_num, ExecBatch* batch) {\n\nReview comment:\n       Or really, since we're guarding with a mutex already, why not just use a vector?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n\nReview comment:\n       I think this should be ErrorIfNotOk.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n\nReview comment:\n       This captures a reference to a stack local, but I would think that reference gets invalidated immediately - I think this is unsafe.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node.cc\n##########\n@@ -0,0 +1,552 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/api.h\"\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/options.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/util/bitmap_ops.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/future.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace compute {\n+\n+namespace {\n+Status ValidateJoinInputs(const std::shared_ptr<Schema>& left_schema,\n+                          const std::shared_ptr<Schema>& right_schema,\n+                          const std::vector<int>& left_keys,\n+                          const std::vector<int>& right_keys) {\n+  if (left_keys.size() != right_keys.size()) {\n+    return Status::Invalid(\"left and right key sizes do not match\");\n+  }\n+\n+  for (size_t i = 0; i < left_keys.size(); i++) {\n+    auto l_type = left_schema->field(left_keys[i])->type();\n+    auto r_type = right_schema->field(right_keys[i])->type();\n+\n+    if (!l_type->Equals(r_type)) {\n+      return Status::Invalid(\"build and probe types do not match: \" + l_type->ToString() +\n+                             \"!=\" + r_type->ToString());\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Result<std::vector<int>> PopulateKeys(const Schema& schema,\n+                                      const std::vector<FieldRef>& keys) {\n+  std::vector<int> key_field_ids(keys.size());\n+  // Find input field indices for left key fields\n+  for (size_t i = 0; i < keys.size(); ++i) {\n+    ARROW_ASSIGN_OR_RAISE(auto match, keys[i].FindOne(schema));\n+    key_field_ids[i] = match[0];\n+  }\n+  return key_field_ids;\n+}\n+}  // namespace\n+\n+template <bool anti_join = false>\n+struct HashSemiJoinNode : ExecNode {\n+  HashSemiJoinNode(ExecNode* build_input, ExecNode* probe_input, ExecContext* ctx,\n+                   const std::vector<int>&& build_index_field_ids,\n+                   const std::vector<int>&& probe_index_field_ids)\n+      : ExecNode(build_input->plan(), {build_input, probe_input},\n+                 {\"hash_join_build\", \"hash_join_probe\"}, probe_input->output_schema(),\n+                 /*num_outputs=*/1),\n+        ctx_(ctx),\n+        build_index_field_ids_(build_index_field_ids),\n+        probe_index_field_ids_(probe_index_field_ids),\n+        build_result_index(-1),\n+        hash_table_built_(false),\n+        cached_probe_batches_consumed(false) {}\n+\n+ private:\n+  struct ThreadLocalState;\n+\n+ public:\n+  const char* kind_name() override { return \"HashSemiJoinNode\"; }\n+\n+  Status InitLocalStateIfNeeded(ThreadLocalState* state) {\n+    ARROW_LOG(DEBUG) << \"init state\";\n+\n+    // Get input schema\n+    auto build_schema = inputs_[0]->output_schema();\n+\n+    if (state->grouper != nullptr) return Status::OK();\n+\n+    // Build vector of key field data types\n+    std::vector<ValueDescr> key_descrs(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      auto build_type = build_schema->field(build_index_field_ids_[i])->type();\n+      key_descrs[i] = ValueDescr(build_type);\n+    }\n+\n+    // Construct grouper\n+    ARROW_ASSIGN_OR_RAISE(state->grouper, internal::Grouper::Make(key_descrs, ctx_));\n+\n+    return Status::OK();\n+  }\n+\n+  // Finds an appropriate index which could accumulate all build indices (i.e. the grouper\n+  // which has the highest # of groups)\n+  void CalculateBuildResultIndex() {\n+    int32_t curr_max = -1;\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); i++) {\n+      auto* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (state->grouper &&\n+          curr_max < static_cast<int32_t>(state->grouper->num_groups())) {\n+        curr_max = static_cast<int32_t>(state->grouper->num_groups());\n+        build_result_index = i;\n+      }\n+    }\n+    ARROW_DCHECK(build_result_index > -1);\n+    ARROW_LOG(DEBUG) << \"build_result_index \" << build_result_index;\n+  }\n+\n+  // Performs the housekeeping work after the build-side is completed.\n+  // Note: this method is not thread safe, and hence should be guaranteed that it is\n+  // not accessed concurrently!\n+  Status BuildSideCompleted() {\n+    ARROW_LOG(DEBUG) << \"build side merge\";\n+\n+    // if the hash table has already been built, return\n+    if (hash_table_built_) return Status::OK();\n+\n+    CalculateBuildResultIndex();\n+\n+    // merge every group into the build_result_index grouper\n+    ThreadLocalState* result_state = &local_states_[build_result_index];\n+    for (int i = 0; i < static_cast<int>(local_states_.size()); ++i) {\n+      ThreadLocalState* state = &local_states_[i];\n+      ARROW_DCHECK(state);\n+      if (i == build_result_index || !state->grouper) {\n+        continue;\n+      }\n+      ARROW_ASSIGN_OR_RAISE(ExecBatch other_keys, state->grouper->GetUniques());\n+\n+      // TODO(niranda) replace with void consume method\n+      ARROW_ASSIGN_OR_RAISE(Datum _, result_state->grouper->Consume(other_keys));\n+      state->grouper.reset();\n+    }\n+\n+    // enable flag that build side is completed\n+    hash_table_built_ = true;\n+\n+    // since the build side is completed, consume cached probe batches\n+    RETURN_NOT_OK(ConsumeCachedProbeBatches());\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a build batch and increments the build_batches count. if the build batches\n+  // total reached at the end of consumption, all the local states will be merged, before\n+  // incrementing the total batches\n+  Status ConsumeBuildBatch(ExecBatch batch) {\n+    size_t thread_index = get_thread_index_();\n+    ARROW_DCHECK(thread_index < local_states_.size());\n+\n+    ARROW_LOG(DEBUG) << \"ConsumeBuildBatch tid:\" << thread_index\n+                     << \" len:\" << batch.length;\n+\n+    auto state = &local_states_[thread_index];\n+    RETURN_NOT_OK(InitLocalStateIfNeeded(state));\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(build_index_field_ids_.size());\n+    for (size_t i = 0; i < build_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[build_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Create a batch with group ids\n+    // TODO(niranda) replace with void consume method\n+    ARROW_ASSIGN_OR_RAISE(Datum _, state->grouper->Consume(key_batch));\n+\n+    if (build_counter_.Increment()) {\n+      // only one thread would get inside this block!\n+      // while incrementing, if the total is reached, call BuildSideCompleted.\n+      RETURN_NOT_OK(BuildSideCompleted());\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes cached probe batches by invoking executor::Spawn.\n+  Status ConsumeCachedProbeBatches() {\n+    ARROW_LOG(DEBUG) << \"ConsumeCachedProbeBatches tid:\" << get_thread_index_()\n+                     << \" len:\" << cached_probe_batches.size();\n+\n+    // acquire the mutex to access cached_probe_batches, because while consuming, other\n+    // batches should not be cached!\n+    std::lock_guard<std::mutex> lck(cached_probe_batches_mutex);\n+\n+    if (!cached_probe_batches_consumed) {\n+      auto executor = ctx_->executor();\n+      for (auto&& cached : cached_probe_batches) {\n+        if (executor) {\n+          Status lambda_status;\n+          RETURN_NOT_OK(executor->Spawn([&] {\n+            lambda_status = ConsumeProbeBatch(cached.first, std::move(cached.second));\n+          }));\n+\n+          // if the lambda execution failed internally, return status\n+          RETURN_NOT_OK(lambda_status);\n+        } else {\n+          RETURN_NOT_OK(ConsumeProbeBatch(cached.first, std::move(cached.second)));\n+        }\n+      }\n+      // cached vector will be cleared. exec batches are expected to be moved to the\n+      // lambdas\n+      cached_probe_batches.clear();\n+    }\n+\n+    // set flag\n+    cached_probe_batches_consumed = true;\n+    return Status::OK();\n+  }\n+\n+  Status GenerateOutput(int seq, const ArrayData& group_ids_data, ExecBatch batch) {\n+    if (group_ids_data.GetNullCount() == batch.length) {\n+      // All NULLS! hence, there are no valid outputs!\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" 0\";\n+      outputs_[0]->InputReceived(this, seq, batch.Slice(0, 0));\n+    } else if (group_ids_data.MayHaveNulls()) {  // values need to be filtered\n+      auto filter_arr =\n+          std::make_shared<BooleanArray>(group_ids_data.length, group_ids_data.buffers[0],\n+                                         /*null_bitmap=*/nullptr, /*null_count=*/0,\n+                                         /*offset=*/group_ids_data.offset);\n+      ARROW_ASSIGN_OR_RAISE(auto rec_batch,\n+                            batch.ToRecordBatch(output_schema_, ctx_->memory_pool()));\n+      ARROW_ASSIGN_OR_RAISE(\n+          auto filtered,\n+          Filter(rec_batch, filter_arr,\n+                 /* null_selection = DROP*/ FilterOptions::Defaults(), ctx_));\n+      auto out_batch = ExecBatch(*filtered.record_batch());\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << out_batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(out_batch));\n+    } else {  // all values are valid for output\n+      ARROW_LOG(DEBUG) << \"output seq:\" << seq << \" \" << batch.length;\n+      outputs_[0]->InputReceived(this, seq, std::move(batch));\n+    }\n+\n+    return Status::OK();\n+  }\n+\n+  // consumes a probe batch and increment probe batches count. Probing would query the\n+  // grouper[build_result_index] which have been merged with all others.\n+  Status ConsumeProbeBatch(int seq, ExecBatch batch) {\n+    ARROW_LOG(DEBUG) << \"ConsumeProbeBatch seq:\" << seq;\n+\n+    auto& final_grouper = *local_states_[build_result_index].grouper;\n+\n+    // Create a batch with key columns\n+    std::vector<Datum> keys(probe_index_field_ids_.size());\n+    for (size_t i = 0; i < probe_index_field_ids_.size(); ++i) {\n+      keys[i] = batch.values[probe_index_field_ids_[i]];\n+    }\n+    ARROW_ASSIGN_OR_RAISE(ExecBatch key_batch, ExecBatch::Make(keys));\n+\n+    // Query the grouper with key_batch. If no match was found, returning group_ids would\n+    // have null.\n+    ARROW_ASSIGN_OR_RAISE(Datum group_ids, final_grouper.Find(key_batch));\n+    auto group_ids_data = *group_ids.array();\n+\n+    RETURN_NOT_OK(GenerateOutput(seq, group_ids_data, std::move(batch)));\n+\n+    if (out_counter_.Increment()) {\n+      finished_.MarkFinished();\n+    }\n+    return Status::OK();\n+  }\n+\n+  // Attempt to cache a probe batch. If it is not cached, return false.\n+  // if cached_probe_batches_consumed is true, by the time a thread acquires\n+  // cached_probe_batches_mutex, it should no longer be cached! instead, it can be\n+  //  directly consumed!\n+  bool AttemptToCacheProbeBatch(int seq_num, ExecBatch* batch) {\n\nReview comment:\n       In ARROW-13660 we'll get rid of seq_num fortunately\n\n##########\nFile path: cpp/src/arrow/compute/exec/options.h\n##########\n@@ -111,5 +111,32 @@ class ARROW_EXPORT SinkNodeOptions : public ExecNodeOptions {\n   std::function<Future<util::optional<ExecBatch>>()>* generator;\n };\n \n+enum JoinType {\n\nReview comment:\n       Hmm. The C++ style guide doesn't say anything. I would say enum class is generally preferred in modern C++. We semi-intentionally use plain enum in a few places (when nested inside a struct) but I'm not sure why we have that pattern. \r\n   \r\n   This isn't the only place where this happens so it's not a big deal, but I would prefer enum class so that it's not implicitly converted and so that namespacing is explicit.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T16:33:57.749+0000",
                    "updated": "2021-08-18T16:33:57.749+0000",
                    "started": "2021-08-18T16:33:57.748+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639244",
                    "issueId": "13388007"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13388007/worklog/639298",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ianmcook commented on pull request #10845:\nURL: https://github.com/apache/arrow/pull/10845#issuecomment-900640812\n\n\n   @aucahuasi could you review this please? thank you!\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-18T16:40:43.613+0000",
                    "updated": "2021-08-18T16:40:43.613+0000",
                    "started": "2021-08-18T16:40:43.613+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "639298",
                    "issueId": "13388007"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 36600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@11965db4[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3d75c14c[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@25fbb585[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@36976fd7[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@55d02aaa[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@4af4f8fb[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@325b2bbd[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@3f3f13f6[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6beefbc1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@6b0f567[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6d5d20cf[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@2aba2e31[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 36600,
        "customfield_12312520": null,
        "customfield_12312521": "2021-07-06 20:18:54.0",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-09-30T19:31:06.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-13268/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2021-07-06T20:18:54.000+0000",
        "updated": "2022-05-04T15:02:52.000+0000",
        "timeoriginalestimate": null,
        "description": "Reuse hash table from group by to implement checking input rows on one side of the join against the set of keys appearing on the other side of the join.\u00a0",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "10h 10m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 36600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Compute] Add ExecNode for semi and anti-semi join",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [],
            "maxResults": 0,
            "total": 0,
            "startAt": 0
        },
        "customfield_12311820": "0|z0so1k:",
        "customfield_12314139": null
    }
}