{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13426020",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020",
    "key": "ARROW-15517",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12351051",
                "id": "12351051",
                "description": "",
                "name": "8.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-05-06"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12638218",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12638218",
                "type": {
                    "id": "12310000",
                    "name": "Duplicate",
                    "inward": "is duplicated by",
                    "outward": "duplicates",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"
                },
                "inwardIssue": {
                    "id": "13405690",
                    "key": "ARROW-14266",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13405690",
                    "fields": {
                        "summary": "[R] Use WriteNode to write queries",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
            "name": "npr",
            "key": "npr",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Neal Richardson",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12333008",
                "id": "12333008",
                "name": "R"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
            "name": "npr",
            "key": "npr",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Neal Richardson",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
            "name": "npr",
            "key": "npr",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Neal Richardson",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 14400,
            "total": 14400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 14400,
            "total": 14400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15517/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 24,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/718996",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson opened a new pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316\n\n\n   I've done the minimal so far to switch write_dataset() to use ExecPlans directly, which should allow streaming writes in more cases. There are a few other cases to test, some refactoring/cleanup that would make sense, and it would be good to verify that this makes a difference e.g. in the case of dataset writing with a join--maybe it doesn't matter today.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-01T21:21:29.514+0000",
                    "updated": "2022-02-01T21:21:29.514+0000",
                    "started": "2022-02-01T21:21:29.513+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "718996",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/718997",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#issuecomment-1027299989\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-01T21:21:52.132+0000",
                    "updated": "2022-02-01T21:21:52.132+0000",
                    "started": "2022-02-01T21:21:52.132+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "718997",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/718998",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] removed a comment on pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#issuecomment-1027300016\n\n\n   :warning: Ticket **has not been started in JIRA**, please click 'Start Progress'.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-01T21:22:24.660+0000",
                    "updated": "2022-02-01T21:22:24.660+0000",
                    "started": "2022-02-01T21:22:24.660+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "718998",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/726584",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806235649\n\n\n\n##########\nFile path: r/R/dataset-write.R\n##########\n@@ -116,25 +116,40 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        stop(\"'dataset' must be a Dataset, RecordBatch, Table, arrow_dplyr_query, or data.frame, not \", deparse(class(dataset)), call. = FALSE)\n+      }\n+    )\n   }\n \n-  scanner <- Scanner$create(dataset)\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  # TODO: warn/error if there is sorting/top_k? or just compute? (this needs test)\n\nReview comment:\n       I think a warning would be good.  Someday we should be able to respect the sort (the dataset writer already has a short serialized path at the front so this should be straightforward).  Dataset writing writes out chunks and those chunks have indices so if the user asked for a sort then the data in chunk-0 should precede chunk-1.  I've created ARROW-15681 to track this on the cpp side.\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       I don't think this method was inside the `#if` block before.  Did you mean to include it?  I might also be reading the git diff incorrectly.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-14T21:05:18.736+0000",
                    "updated": "2022-02-14T21:05:18.736+0000",
                    "started": "2022-02-14T21:05:18.735+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "726584",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/726644",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806275298\n\n\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       The ifdef and the decorations above each function should match, so I moved the endif accordingly. Technically these ExecNode methods compile with just the compute namespace, not dataset. I could move them so they compile even if the arrow C++ library wasn't built with ARROW_DATASET=ON, but that seemed not worth the effort. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-14T22:02:34.174+0000",
                    "updated": "2022-02-14T22:02:34.174+0000",
                    "started": "2022-02-14T22:02:34.174+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "726644",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/726645",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806276486\n\n\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       It should be possible in that case to use a table as the source but I agree it's probably not worth worrying about unless there is a compelling use case.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-14T22:04:36.955+0000",
                    "updated": "2022-02-14T22:04:36.955+0000",
                    "started": "2022-02-14T22:04:36.955+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "726645",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/726647",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806277566\n\n\n\n##########\nFile path: r/R/dataset-write.R\n##########\n@@ -116,25 +116,40 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        stop(\"'dataset' must be a Dataset, RecordBatch, Table, arrow_dplyr_query, or data.frame, not \", deparse(class(dataset)), call. = FALSE)\n+      }\n+    )\n   }\n \n-  scanner <- Scanner$create(dataset)\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  # TODO: warn/error if there is sorting/top_k? or just compute? (this needs test)\n\nReview comment:\n       Cool. TopK is a separate issue--it's another feature only handled in a sink node. I'll handle it here by evaluating the query and then doing a new ExecPlan to write the resulting Table. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-14T22:06:23.774+0000",
                    "updated": "2022-02-14T22:06:23.774+0000",
                    "started": "2022-02-14T22:06:23.774+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "726647",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/726649",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806278437\n\n\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       Yes but we (currently) turn a Table into an InMemoryDataset. I guess we could go through RecordBatchReader, which wouldn't require dataset (I can't remember if I looked into that and found that it didn't work for some reason).\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-14T22:07:51.314+0000",
                    "updated": "2022-02-14T22:07:51.314+0000",
                    "started": "2022-02-14T22:07:51.314+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "726649",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/726650",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806278437\n\n\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       Yes but we (currently) turn a Table into an InMemoryDataset. I guess we could go through RecordBatchReader, which wouldn't require dataset (I can't remember if I looked into that and found that it didn't work for some reason, you'd think it should).\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-14T22:08:03.914+0000",
                    "updated": "2022-02-14T22:08:03.914+0000",
                    "started": "2022-02-14T22:08:03.914+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "726650",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/726652",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806279597\n\n\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       Soon it will be even easier than that: #12267 \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-14T22:09:39.749+0000",
                    "updated": "2022-02-14T22:09:39.749+0000",
                    "started": "2022-02-14T22:09:39.749+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "726652",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/727297",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806235649\n\n\n\n##########\nFile path: r/R/dataset-write.R\n##########\n@@ -116,25 +116,40 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        stop(\"'dataset' must be a Dataset, RecordBatch, Table, arrow_dplyr_query, or data.frame, not \", deparse(class(dataset)), call. = FALSE)\n+      }\n+    )\n   }\n \n-  scanner <- Scanner$create(dataset)\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  # TODO: warn/error if there is sorting/top_k? or just compute? (this needs test)\n\nReview comment:\n       I think a warning would be good.  Someday we should be able to respect the sort (the dataset writer already has a short serialized path at the front so this should be straightforward).  Dataset writing writes out chunks and those chunks have indices so if the user asked for a sort then the data in chunk-0 should precede chunk-1.  I've created ARROW-15681 to track this on the cpp side.\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       I don't think this method was inside the `#if` block before.  Did you mean to include it?  I might also be reading the git diff incorrectly.\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       It should be possible in that case to use a table as the source but I agree it's probably not worth worrying about unless there is a compelling use case.\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       Soon it will be even easier than that: #12267 \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-15T18:38:23.563+0000",
                    "updated": "2022-02-15T18:38:23.563+0000",
                    "started": "2022-02-15T18:38:23.563+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "727297",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/727412",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on a change in pull request #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r806275298\n\n\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       The ifdef and the decorations above each function should match, so I moved the endif accordingly. Technically these ExecNode methods compile with just the compute namespace, not dataset. I could move them so they compile even if the arrow C++ library wasn't built with ARROW_DATASET=ON, but that seemed not worth the effort. \n\n##########\nFile path: r/R/dataset-write.R\n##########\n@@ -116,25 +116,40 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        stop(\"'dataset' must be a Dataset, RecordBatch, Table, arrow_dplyr_query, or data.frame, not \", deparse(class(dataset)), call. = FALSE)\n+      }\n+    )\n   }\n \n-  scanner <- Scanner$create(dataset)\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  # TODO: warn/error if there is sorting/top_k? or just compute? (this needs test)\n\nReview comment:\n       Cool. TopK is a separate issue--it's another feature only handled in a sink node. I'll handle it here by evaluating the query and then doing a new ExecPlan to write the resulting Table. \n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       Yes but we (currently) turn a Table into an InMemoryDataset. I guess we could go through RecordBatchReader, which wouldn't require dataset (I can't remember if I looked into that and found that it didn't work for some reason).\n\n##########\nFile path: r/src/compute-exec.cpp\n##########\n@@ -157,7 +158,33 @@ std::shared_ptr<compute::ExecNode> ExecNode_Scan(\n                             arrow::dataset::ScanNodeOptions{dataset, options});\n }\n \n-#endif\n+// [[dataset::export]]\n+void ExecPlan_Write(const std::shared_ptr<compute::ExecPlan>& plan,\n+                    const std::shared_ptr<compute::ExecNode>& final_node,\n+                    const std::shared_ptr<ds::FileWriteOptions>& file_write_options,\n+                    const std::shared_ptr<fs::FileSystem>& filesystem,\n+                    std::string base_dir,\n+                    const std::shared_ptr<ds::Partitioning>& partitioning,\n+                    std::string basename_template,\n+                    arrow::dataset::ExistingDataBehavior existing_data_behavior,\n+                    int max_partitions) {\n+  ds::FileSystemDatasetWriteOptions opts;\n+  opts.file_write_options = file_write_options;\n+  opts.existing_data_behavior = existing_data_behavior;\n+  opts.filesystem = filesystem;\n+  opts.base_dir = base_dir;\n+  opts.partitioning = partitioning;\n+  opts.basename_template = basename_template;\n+  opts.max_partitions = max_partitions;\n+\n+  MakeExecNodeOrStop(\n+      \"write\", final_node->plan(), {final_node.get()},\n+      ds::WriteNodeOptions{std::move(opts), std::move(final_node->output_schema())});\n+\n+  StopIfNotOk(plan->Validate());\n+  StopIfNotOk(plan->StartProducing());\n+  StopIfNotOk(plan->finished().status());\n+}\n \n // [[dataset::export]]\n std::shared_ptr<compute::ExecNode> ExecNode_Filter(\n\nReview comment:\n       Yes but we (currently) turn a Table into an InMemoryDataset. I guess we could go through RecordBatchReader, which wouldn't require dataset (I can't remember if I looked into that and found that it didn't work for some reason, you'd think it should).\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-02-15T18:49:10.802+0000",
                    "updated": "2022-02-15T18:49:10.802+0000",
                    "started": "2022-02-15T18:49:10.802+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "727412",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/757142",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on code in PR #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r850699081\n\n\n##########\nr/R/dataset-write.R:\n##########\n@@ -136,18 +136,49 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        supported <- c(\n+          \"Dataset\", \"RecordBatch\", \"Table\", \"arrow_dplyr_query\", \"data.frame\"\n+        )\n+        stop(\n+          \"'dataset' must be a \",\n+          oxford_paste(supported, \"or\", quote = FALSE),\n+          \", not \",\n+          deparse(class(dataset)),\n+          call. = FALSE\n+        )\n+      }\n+    )\n+  }\n+\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  if (!is.null(final_node$sort %||% final_node$head %||% final_node$tail)) {\n\nReview Comment:\n   This condition probably still needs a test or two\n\n\n\n",
                    "created": "2022-04-14T18:24:30.166+0000",
                    "updated": "2022-04-14T18:24:30.166+0000",
                    "started": "2022-04-14T18:24:30.166+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757142",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/757387",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on PR #12316:\nURL: https://github.com/apache/arrow/pull/12316#issuecomment-1100072852\n\n   The last failing test was that row-level metadata isn't included when you collect() a dataset, with a warning: https://github.com/apache/arrow/blob/master/r/tests/testthat/test-metadata.R#L274-L277\r\n   \r\n   What was actually happening is that no KeyValueMetadata was being preserved on write in this PR, we just don't have any tests currently around metadata in write_dataset, only this one about row-level metadata (in this PR, neither row-level nor any other kind of of metadata is present). \r\n   \r\n   I pushed a fix that will grab the KVM from the source data object in the query and use that, and that fixes the failing tests, but I'm not sure that's totally correct. If you have a join or aggregation, it won't make sense; we have some logic in do_exec_plan() to handle some of this, should we factor that out and apply it in write_dataset()? Also more than happy to defer handling this since (afaict) this PR is consistent with the status quo, and we don't have any assertions about the behavior (that I can find) of general metadata on dataset write, nor any handling for amalgamating metadata when joining, etc. Your call @jonkeane.\n\n\n",
                    "created": "2022-04-15T12:20:13.995+0000",
                    "updated": "2022-04-15T12:20:13.995+0000",
                    "started": "2022-04-15T12:20:13.994+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757387",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/757886",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jonkeane commented on PR #12316:\nURL: https://github.com/apache/arrow/pull/12316#issuecomment-1101409026\n\n   I took a read through and this looks good this far.\r\n   \r\n   > I pushed a fix that will grab the KVM from the source data object in the query and use that, and that fixes the failing tests, but I'm not sure that's totally correct. If you have a join or aggregation, it won't make sense; we have some logic in do_exec_plan() to handle some of this, should we factor that out and apply it in write_dataset()? Also more than happy to defer handling this since (afaict) this PR is consistent with the status quo, and we don't have any assertions about the behavior (that I can find) of general metadata on dataset write, nor any handling for amalgamating metadata when joining, etc. Your call @jonkeane.\r\n   \r\n   If it's not too much, I would say we should factor out what's in `do_exec_plan` and at least do that much \u2014 we can (and should) defer on what amalgamating when you join (and AFAICT the code in `do_exec_plan` handles the aggregation case). We have had a few issues and twitter threads about metadata + datasets (at least in the cases where it failed with row-level metadata!) but folks expected that would just work. And of course, we should also have tests for that to assert that's what we intend \u2014 though we can do that as a follow on as well.\r\n   \r\n   The only other question I have is: https://github.com/apache/arrow/pull/12316/files#r850699081 did you add tests for these? I didn't see them, but maybe I'm missing something!\n\n\n",
                    "created": "2022-04-18T13:27:53.607+0000",
                    "updated": "2022-04-18T13:27:53.607+0000",
                    "started": "2022-04-18T13:27:53.607+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757886",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/758115",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on PR #12316:\nURL: https://github.com/apache/arrow/pull/12316#issuecomment-1101748535\n\n   I factored out the metadata helper from do_exec_plan. No tests yet that it does what is expected on dataset write, but the existing metadata tests pass. I also have not yet added that other test for the topk or sorted dataset write, did not muster enough brain cells today. Topk seems like a simple test but I'm not sure about sorting, IDK what guarantees there are or should be around sorting in the files that write_dataset produces.\r\n   \n\n\n",
                    "created": "2022-04-18T20:45:20.401+0000",
                    "updated": "2022-04-18T20:45:20.401+0000",
                    "started": "2022-04-18T20:45:20.401+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "758115",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/758126",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jonkeane commented on PR #12316:\nURL: https://github.com/apache/arrow/pull/12316#issuecomment-1101770927\n\n   _nods_ thanks! I can take over adding the tests (tomorrow) if you would like\n\n\n",
                    "created": "2022-04-18T21:10:36.752+0000",
                    "updated": "2022-04-18T21:10:36.752+0000",
                    "started": "2022-04-18T21:10:36.752+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "758126",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/758130",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r852421227\n\n\n##########\nr/R/dataset-write.R:\n##########\n@@ -136,41 +136,84 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        supported <- c(\n+          \"Dataset\", \"RecordBatch\", \"Table\", \"arrow_dplyr_query\", \"data.frame\"\n+        )\n+        stop(\n+          \"'dataset' must be a \",\n+          oxford_paste(supported, \"or\", quote = FALSE),\n+          \", not \",\n+          deparse(class(dataset)),\n+          call. = FALSE\n+        )\n+      }\n+    )\n+  }\n+\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  if (!is.null(final_node$sort %||% final_node$head %||% final_node$tail)) {\n+    # Because sorting and topK are only handled in the SinkNode (or in R!),\n+    # they wouldn't get picked up in the WriteNode. So let's Run this ExecPlan\n+    # to capture those, and then create a new plan for writing\n+    # TODO(ARROW-15681): do sorting in WriteNode in C++\n+    dataset <- as_adq(plan$Run(final_node))\n+    plan <- ExecPlan$create()\n+    final_node <- plan$Build(dataset)\n   }\n \n-  scanner <- Scanner$create(dataset)\n   if (!inherits(partitioning, \"Partitioning\")) {\n-    partition_schema <- scanner$schema[partitioning]\n+    partition_schema <- final_node$schema[partitioning]\n     if (isTRUE(hive_style)) {\n-      partitioning <- HivePartitioning$create(partition_schema, null_fallback = list(...)$null_fallback)\n+      partitioning <- HivePartitioning$create(\n+        partition_schema,\n+        null_fallback = list(...)$null_fallback\n+      )\n     } else {\n       partitioning <- DirectoryPartitioning$create(partition_schema)\n     }\n   }\n \n-  if (!missing(max_rows_per_file) && missing(max_rows_per_group) && max_rows_per_group > max_rows_per_file) {\n-    max_rows_per_group <- max_rows_per_file\n-  }\n-\n   path_and_fs <- get_path_and_filesystem(path)\n-  options <- FileWriteOptions$create(format, table = scanner, ...)\n+  output_schema <- final_node$schema\n+  options <- FileWriteOptions$create(format, table = output_schema, ...)\n\nReview Comment:\n   Why is this arg named `table`?\n\n\n\n##########\nr/R/metadata.R:\n##########\n@@ -208,3 +207,24 @@ arrow_attributes <- function(x, only_top_level = FALSE) {\n     NULL\n   }\n }\n+\n+get_r_metadata_from_old_schema <- function(new_schema,\n+                                           old_schema,\n+                                           drop_attributes = FALSE) {\n+  # TODO: do we care about other (non-R) metadata preservation?\n+  # How would we know if it were meaningful?\n\nReview Comment:\n   I think it depends on the source of `old_schema`.\r\n   \r\n   In the general case, the input is a collection of files and the output is a different set of files (sometimes we explode files and sometimes we merge files).  The idea of writing metadata to the output files in somewhat meaningless.  So, in general, I would say no, you don't care about preservation.\r\n   \r\n   In python, users can create a dataset from a single file, and we do a little bit of work to preserve the metadata on write because we want to feel like it \"round trips\".\r\n   \r\n   When creating or appending to a dataset users might want to specify general information about how the files were created, like \"Origin\": \"Nightly update\" but that is unrelated to the original metadata.\r\n   \r\n   In the future the dataset write may append its own metadata (e.g. dataset statistics, or information about the dataset schema such as which columns are already sorted, etc.)\n\n\n\n##########\nr/R/dataset-write.R:\n##########\n@@ -136,41 +136,84 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        supported <- c(\n+          \"Dataset\", \"RecordBatch\", \"Table\", \"arrow_dplyr_query\", \"data.frame\"\n+        )\n+        stop(\n+          \"'dataset' must be a \",\n+          oxford_paste(supported, \"or\", quote = FALSE),\n+          \", not \",\n+          deparse(class(dataset)),\n+          call. = FALSE\n+        )\n+      }\n+    )\n+  }\n+\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  if (!is.null(final_node$sort %||% final_node$head %||% final_node$tail)) {\n+    # Because sorting and topK are only handled in the SinkNode (or in R!),\n+    # they wouldn't get picked up in the WriteNode. So let's Run this ExecPlan\n+    # to capture those, and then create a new plan for writing\n+    # TODO(ARROW-15681): do sorting in WriteNode in C++\n+    dataset <- as_adq(plan$Run(final_node))\n+    plan <- ExecPlan$create()\n+    final_node <- plan$Build(dataset)\n   }\n \n-  scanner <- Scanner$create(dataset)\n   if (!inherits(partitioning, \"Partitioning\")) {\n-    partition_schema <- scanner$schema[partitioning]\n+    partition_schema <- final_node$schema[partitioning]\n     if (isTRUE(hive_style)) {\n-      partitioning <- HivePartitioning$create(partition_schema, null_fallback = list(...)$null_fallback)\n+      partitioning <- HivePartitioning$create(\n+        partition_schema,\n+        null_fallback = list(...)$null_fallback\n+      )\n     } else {\n       partitioning <- DirectoryPartitioning$create(partition_schema)\n     }\n   }\n \n-  if (!missing(max_rows_per_file) && missing(max_rows_per_group) && max_rows_per_group > max_rows_per_file) {\n-    max_rows_per_group <- max_rows_per_file\n-  }\n-\n   path_and_fs <- get_path_and_filesystem(path)\n-  options <- FileWriteOptions$create(format, table = scanner, ...)\n+  output_schema <- final_node$schema\n+  options <- FileWriteOptions$create(format, table = output_schema, ...)\n \n+  # TODO(ARROW-16200): expose FileSystemDatasetWriteOptions in R\n+  # and encapsulate this logic better\n   existing_data_behavior_opts <- c(\"delete_matching\", \"overwrite\", \"error\")\n   existing_data_behavior <- match(match.arg(existing_data_behavior), existing_data_behavior_opts) - 1L\n \n-  validate_positive_int_value(max_partitions, \"max_partitions must be a positive, non-missing integer\")\n-  validate_positive_int_value(max_open_files, \"max_open_files must be a positive, non-missing integer\")\n-  validate_positive_int_value(min_rows_per_group, \"min_rows_per_group must be a positive, non-missing integer\")\n-  validate_positive_int_value(max_rows_per_group, \"max_rows_per_group must be a positive, non-missing integer\")\n+  if (!missing(max_rows_per_file) && missing(max_rows_per_group) && max_rows_per_group > max_rows_per_file) {\n+    max_rows_per_group <- max_rows_per_file\n+  }\n \n-  dataset___Dataset__Write(\n+  validate_positive_int_value(max_partitions)\n+  validate_positive_int_value(max_open_files)\n+  validate_positive_int_value(min_rows_per_group)\n+  validate_positive_int_value(max_rows_per_group)\n\nReview Comment:\n   The error message says `non-missing` and yet we have defaults for all of these properties (and line 196 seems to tolerate a missing `max_rows_per_group`.  Are they truly required to be non-missing?\n\n\n\n",
                    "created": "2022-04-18T21:37:24.624+0000",
                    "updated": "2022-04-18T21:37:24.624+0000",
                    "started": "2022-04-18T21:37:24.624+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "758130",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/758439",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on code in PR #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r853026794\n\n\n##########\nr/R/dataset-write.R:\n##########\n@@ -136,41 +136,84 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        supported <- c(\n+          \"Dataset\", \"RecordBatch\", \"Table\", \"arrow_dplyr_query\", \"data.frame\"\n+        )\n+        stop(\n+          \"'dataset' must be a \",\n+          oxford_paste(supported, \"or\", quote = FALSE),\n+          \", not \",\n+          deparse(class(dataset)),\n+          call. = FALSE\n+        )\n+      }\n+    )\n+  }\n+\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  if (!is.null(final_node$sort %||% final_node$head %||% final_node$tail)) {\n+    # Because sorting and topK are only handled in the SinkNode (or in R!),\n+    # they wouldn't get picked up in the WriteNode. So let's Run this ExecPlan\n+    # to capture those, and then create a new plan for writing\n+    # TODO(ARROW-15681): do sorting in WriteNode in C++\n+    dataset <- as_adq(plan$Run(final_node))\n+    plan <- ExecPlan$create()\n+    final_node <- plan$Build(dataset)\n   }\n \n-  scanner <- Scanner$create(dataset)\n   if (!inherits(partitioning, \"Partitioning\")) {\n-    partition_schema <- scanner$schema[partitioning]\n+    partition_schema <- final_node$schema[partitioning]\n     if (isTRUE(hive_style)) {\n-      partitioning <- HivePartitioning$create(partition_schema, null_fallback = list(...)$null_fallback)\n+      partitioning <- HivePartitioning$create(\n+        partition_schema,\n+        null_fallback = list(...)$null_fallback\n+      )\n     } else {\n       partitioning <- DirectoryPartitioning$create(partition_schema)\n     }\n   }\n \n-  if (!missing(max_rows_per_file) && missing(max_rows_per_group) && max_rows_per_group > max_rows_per_file) {\n-    max_rows_per_group <- max_rows_per_file\n-  }\n-\n   path_and_fs <- get_path_and_filesystem(path)\n-  options <- FileWriteOptions$create(format, table = scanner, ...)\n+  output_schema <- final_node$schema\n+  options <- FileWriteOptions$create(format, table = output_schema, ...)\n\nReview Comment:\n   Historical reasons/deferred maintenance: the ParquetWriterProperties constructor takes a `table` argument so that it can handle various cases of specifying things like compression and dictionary encoding either as a single value for all or a named list of specific columns to treat that way. Turns out that anything with names works, so schema satisfies the logic. It should get cleaned up at some point.\n\n\n\n",
                    "created": "2022-04-19T12:52:48.150+0000",
                    "updated": "2022-04-19T12:52:48.150+0000",
                    "started": "2022-04-19T12:52:48.150+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "758439",
                    "issueId": "13426020"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/worklog/758440",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on code in PR #12316:\nURL: https://github.com/apache/arrow/pull/12316#discussion_r853029957\n\n\n##########\nr/R/dataset-write.R:\n##########\n@@ -136,41 +136,84 @@ write_dataset <- function(dataset,\n   if (inherits(dataset, \"arrow_dplyr_query\")) {\n     # partitioning vars need to be in the `select` schema\n     dataset <- ensure_group_vars(dataset)\n-  } else if (inherits(dataset, \"grouped_df\")) {\n-    force(partitioning)\n-    # Drop the grouping metadata before writing; we've already consumed it\n-    # now to construct `partitioning` and don't want it in the metadata$r\n-    dataset <- dplyr::ungroup(dataset)\n+  } else {\n+    if (inherits(dataset, \"grouped_df\")) {\n+      force(partitioning)\n+      # Drop the grouping metadata before writing; we've already consumed it\n+      # now to construct `partitioning` and don't want it in the metadata$r\n+      dataset <- dplyr::ungroup(dataset)\n+    }\n+    dataset <- tryCatch(\n+      as_adq(dataset),\n+      error = function(e) {\n+        supported <- c(\n+          \"Dataset\", \"RecordBatch\", \"Table\", \"arrow_dplyr_query\", \"data.frame\"\n+        )\n+        stop(\n+          \"'dataset' must be a \",\n+          oxford_paste(supported, \"or\", quote = FALSE),\n+          \", not \",\n+          deparse(class(dataset)),\n+          call. = FALSE\n+        )\n+      }\n+    )\n+  }\n+\n+  plan <- ExecPlan$create()\n+  final_node <- plan$Build(dataset)\n+  if (!is.null(final_node$sort %||% final_node$head %||% final_node$tail)) {\n+    # Because sorting and topK are only handled in the SinkNode (or in R!),\n+    # they wouldn't get picked up in the WriteNode. So let's Run this ExecPlan\n+    # to capture those, and then create a new plan for writing\n+    # TODO(ARROW-15681): do sorting in WriteNode in C++\n+    dataset <- as_adq(plan$Run(final_node))\n+    plan <- ExecPlan$create()\n+    final_node <- plan$Build(dataset)\n   }\n \n-  scanner <- Scanner$create(dataset)\n   if (!inherits(partitioning, \"Partitioning\")) {\n-    partition_schema <- scanner$schema[partitioning]\n+    partition_schema <- final_node$schema[partitioning]\n     if (isTRUE(hive_style)) {\n-      partitioning <- HivePartitioning$create(partition_schema, null_fallback = list(...)$null_fallback)\n+      partitioning <- HivePartitioning$create(\n+        partition_schema,\n+        null_fallback = list(...)$null_fallback\n+      )\n     } else {\n       partitioning <- DirectoryPartitioning$create(partition_schema)\n     }\n   }\n \n-  if (!missing(max_rows_per_file) && missing(max_rows_per_group) && max_rows_per_group > max_rows_per_file) {\n-    max_rows_per_group <- max_rows_per_file\n-  }\n-\n   path_and_fs <- get_path_and_filesystem(path)\n-  options <- FileWriteOptions$create(format, table = scanner, ...)\n+  output_schema <- final_node$schema\n+  options <- FileWriteOptions$create(format, table = output_schema, ...)\n \n+  # TODO(ARROW-16200): expose FileSystemDatasetWriteOptions in R\n+  # and encapsulate this logic better\n   existing_data_behavior_opts <- c(\"delete_matching\", \"overwrite\", \"error\")\n   existing_data_behavior <- match(match.arg(existing_data_behavior), existing_data_behavior_opts) - 1L\n \n-  validate_positive_int_value(max_partitions, \"max_partitions must be a positive, non-missing integer\")\n-  validate_positive_int_value(max_open_files, \"max_open_files must be a positive, non-missing integer\")\n-  validate_positive_int_value(min_rows_per_group, \"min_rows_per_group must be a positive, non-missing integer\")\n-  validate_positive_int_value(max_rows_per_group, \"max_rows_per_group must be a positive, non-missing integer\")\n+  if (!missing(max_rows_per_file) && missing(max_rows_per_group) && max_rows_per_group > max_rows_per_file) {\n+    max_rows_per_group <- max_rows_per_file\n+  }\n \n-  dataset___Dataset__Write(\n+  validate_positive_int_value(max_partitions)\n+  validate_positive_int_value(max_open_files)\n+  validate_positive_int_value(min_rows_per_group)\n+  validate_positive_int_value(max_rows_per_group)\n\nReview Comment:\n   I believe it means \"not `NA`\" rather than \"omitted\", judging from the actual validation it is doing\n\n\n\n",
                    "created": "2022-04-19T12:55:08.240+0000",
                    "updated": "2022-04-19T12:55:08.240+0000",
                    "started": "2022-04-19T12:55:08.240+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "758440",
                    "issueId": "13426020"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 14400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@5e9dbfe[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5d638082[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6177cc37[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@2051270b[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6facacf4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@13ffc2fe[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4cb6a2c2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@65f6bd20[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6f377666[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@232a14c[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7b6ba37a[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@2ba40e8a[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 14400,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Apr 19 21:41:09 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-04-19T21:41:09.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15517/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2022-02-01T17:20:16.000+0000",
        "updated": "2022-04-21T00:51:44.000+0000",
        "timeoriginalestimate": null,
        "description": "Currently, write_dataset uses the Scanner interface, which can't handle everything that the ExecPlan does. So if your arrow_dplyr_query contains things like aggregations or (more importantly) joins, you have to materialize the Table in memory before you can write to disk. The WriteNode added in ARROW-13542 is a special sink node that can be put at the end of an ExecPlan, so data should be able to stream to disk in more cases, and will benefit from future improvements to ExecPlan memory usage and spillover.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "4h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 14400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[R] Use WriteNode in write_dataset()",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13426020/comment/17524599",
                    "id": "17524599",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jonkeane",
                        "name": "jonkeane",
                        "key": "jonkeane",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34057",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34057",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34057",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34057"
                        },
                        "displayName": "Jonathan Keane",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Issue resolved by pull request 12316\n[https://github.com/apache/arrow/pull/12316]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jonkeane",
                        "name": "jonkeane",
                        "key": "jonkeane",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34057",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34057",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34057",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34057"
                        },
                        "displayName": "Jonathan Keane",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2022-04-19T21:41:09.589+0000",
                    "updated": "2022-04-19T21:41:09.589+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0z5og:",
        "customfield_12314139": null
    }
}