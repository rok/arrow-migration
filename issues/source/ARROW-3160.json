{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13182524",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524",
    "key": "ARROW-3160",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12343066",
                "id": "12343066",
                "description": "",
                "name": "0.11.0",
                "archived": false,
                "released": true,
                "releaseDate": "2018-10-08"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
            "name": "kszucs",
            "key": "kszucs",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Krisztian Szucs",
            "active": true,
            "timeZone": "Europe/Budapest"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
            "name": "kszucs",
            "key": "kszucs",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Krisztian Szucs",
            "active": true,
            "timeZone": "Europe/Budapest"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
            "name": "kszucs",
            "key": "kszucs",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Krisztian Szucs",
            "active": true,
            "timeZone": "Europe/Budapest"
        },
        "aggregateprogress": {
            "progress": 5400,
            "total": 5400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 5400,
            "total": 5400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-3160/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 9,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/140799",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "xhochy commented on issue #2506: ARROW-3160: [Python] ParquetManifest and ParquetDatasetPiece accept pathlib.Path\nURL: https://github.com/apache/arrow/pull/2506#issuecomment-418289088\n \n \n   This PR still has test failures.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-04T08:44:14.586+0000",
                    "updated": "2018-09-04T08:44:14.586+0000",
                    "started": "2018-09-04T08:44:14.584+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "140799",
                    "issueId": "13182524"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/140801",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs commented on issue #2506: ARROW-3160: [Python] ParquetManifest and ParquetDatasetPiece accept pathlib.Path\nURL: https://github.com/apache/arrow/pull/2506#issuecomment-418290767\n \n \n   Sorry, forgot to prepend with WIP\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-04T08:50:02.395+0000",
                    "updated": "2018-09-04T08:50:02.395+0000",
                    "started": "2018-09-04T08:50:02.395+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "140801",
                    "issueId": "13182524"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/140802",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs edited a comment on issue #2506: ARROW-3160: [Python] ParquetManifest and ParquetDatasetPiece accept pathlib.Path\nURL: https://github.com/apache/arrow/pull/2506#issuecomment-418290767\n \n \n   Sorry, forgot to prepend with WIP. I'll request your review when it's done.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-04T08:50:18.664+0000",
                    "updated": "2018-09-04T08:50:18.664+0000",
                    "started": "2018-09-04T08:50:18.659+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "140802",
                    "issueId": "13182524"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/140887",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs commented on issue #2506: ARROW-3160: [Python] Improve pathlib.Path support in parquet and filesystem modules\nURL: https://github.com/apache/arrow/pull/2506#issuecomment-418349161\n \n \n   @xhochy the tests should pass (however appveyor produces strange things lately)\r\n   \r\n   What do You think about using pathlib in the implementation too?\r\n   \r\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-04T12:30:50.204+0000",
                    "updated": "2018-09-04T12:30:50.204+0000",
                    "started": "2018-09-04T12:30:50.203+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "140887",
                    "issueId": "13182524"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/140904",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs edited a comment on issue #2506: ARROW-3160: [Python] Improve pathlib.Path support in parquet and filesystem modules\nURL: https://github.com/apache/arrow/pull/2506#issuecomment-418349161\n \n \n   @xhochy What do You think about using pathlib in the implementation too?\r\n   \r\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-04T13:52:50.923+0000",
                    "updated": "2018-09-04T13:52:50.923+0000",
                    "started": "2018-09-04T13:52:50.922+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "140904",
                    "issueId": "13182524"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/141172",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io commented on issue #2506: ARROW-3160: [Python] Improve pathlib.Path support in parquet and filesystem modules\nURL: https://github.com/apache/arrow/pull/2506#issuecomment-418670602\n \n \n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/2506?src=pr&el=h1) Report\n   > Merging [#2506](https://codecov.io/gh/apache/arrow/pull/2506?src=pr&el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/ad7e6c3d5422eb8742fc198905ca2460dce685c6?src=pr&el=desc) will **increase** coverage by `1.25%`.\n   > The diff coverage is `88.65%`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/2506/graphs/tree.svg?width=650&token=LpTCFbqVT1&height=150&src=pr)](https://codecov.io/gh/apache/arrow/pull/2506?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #2506      +/-   ##\n   ==========================================\n   + Coverage   86.24%   87.49%   +1.25%     \n   ==========================================\n     Files         308      246      -62     \n     Lines       47094    43441    -3653     \n   ==========================================\n   - Hits        40615    38010    -2605     \n   + Misses       6405     5431     -974     \n   + Partials       74        0      -74\n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/2506?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [python/pyarrow/io.pxi](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvaW8ucHhp) | `60.52% <0%> (-0.41%)` | :arrow_down: |\n   | [python/pyarrow/tests/test\\_orc.py](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvdGVzdHMvdGVzdF9vcmMucHk=) | `100% <100%> (\u00f8)` | :arrow_up: |\n   | [python/pyarrow/parquet.py](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvcGFycXVldC5weQ==) | `93.56% <100%> (+2.22%)` | :arrow_up: |\n   | [python/pyarrow/tests/test\\_parquet.py](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvdGVzdHMvdGVzdF9wYXJxdWV0LnB5) | `97.41% <100%> (-0.02%)` | :arrow_down: |\n   | [python/pyarrow/filesystem.py](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvZmlsZXN5c3RlbS5weQ==) | `50.27% <50%> (-1.2%)` | :arrow_down: |\n   | [python/pyarrow/util.py](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvdXRpbC5weQ==) | `75% <85%> (+16.66%)` | :arrow_up: |\n   | [python/pyarrow/tests/conftest.py](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cHl0aG9uL3B5YXJyb3cvdGVzdHMvY29uZnRlc3QucHk=) | `77.04% <91.66%> (+3.46%)` | :arrow_up: |\n   | [rust/src/record\\_batch.rs](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cnVzdC9zcmMvcmVjb3JkX2JhdGNoLnJz) | | |\n   | [go/arrow/datatype\\_nested.go](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-Z28vYXJyb3cvZGF0YXR5cGVfbmVzdGVkLmdv) | | |\n   | [rust/src/array.rs](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree#diff-cnVzdC9zcmMvYXJyYXkucnM=) | | |\n   | ... and [59 more](https://codecov.io/gh/apache/arrow/pull/2506/diff?src=pr&el=tree-more) | |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2506?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/2506?src=pr&el=footer). Last update [ad7e6c3...bf1fe1f](https://codecov.io/gh/apache/arrow/pull/2506?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-05T09:49:25.068+0000",
                    "updated": "2018-09-05T09:49:25.068+0000",
                    "started": "2018-09-05T09:49:25.067+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "141172",
                    "issueId": "13182524"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/141229",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs commented on issue #2506: ARROW-3160: [Python] Improve pathlib.Path support in parquet and filesystem modules\nURL: https://github.com/apache/arrow/pull/2506#issuecomment-418723401\n \n \n   @xhochy green\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-05T13:06:40.596+0000",
                    "updated": "2018-09-05T13:06:40.596+0000",
                    "started": "2018-09-05T13:06:40.593+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "141229",
                    "issueId": "13182524"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/141422",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on a change in pull request #2506: ARROW-3160: [Python] Improve pathlib.Path support in parquet and filesystem modules\nURL: https://github.com/apache/arrow/pull/2506#discussion_r214552829\n \n \n\n ##########\n File path: python/pyarrow/tests/conftest.py\n ##########\n @@ -115,4 +120,15 @@ def pytest_runtest_setup(item):\n                 skip_item = False\n \n         if skip_item:\n-            skip('Only running some groups with only flags')\n+            pytest.skip('Only running some groups with only flags')\n+\n+\n+@pytest.fixture\n+def tempdir(tmpdir):\n+    # convert pytest's LocalPath to pathlib.Path\n+    return pathlib.Path(tmpdir)\n+\n+\n+@pytest.fixture(scope='session')\n+def datadir():\n+    return pathlib.Path(__file__).parent / 'data'\n \n Review comment:\n   I'd be in favor of moving all the test data files to https://github.com/apache/arrow-testing. It would take less stress (for me at least) about adding binary files in the future. There's relatively little need to ship the files (or the unit tests for that matter) with the library \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-05T16:44:22.082+0000",
                    "updated": "2018-09-05T16:44:22.082+0000",
                    "started": "2018-09-05T16:44:22.081+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "141422",
                    "issueId": "13182524"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/worklog/141425",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm closed pull request #2506: ARROW-3160: [Python] Improve pathlib.Path support in parquet and filesystem modules\nURL: https://github.com/apache/arrow/pull/2506\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/python/pyarrow/filesystem.py b/python/pyarrow/filesystem.py\nindex ff78095756..7dd94a8c08 100644\n--- a/python/pyarrow/filesystem.py\n+++ b/python/pyarrow/filesystem.py\n@@ -15,11 +15,15 @@\n # specific language governing permissions and limitations\n # under the License.\n \n-from os.path import join as pjoin\n import os\n+import inspect\n import posixpath\n \n-from pyarrow.util import implements\n+from os.path import join as pjoin\n+from six.moves.urllib.parse import urlparse\n+\n+import pyarrow as pa\n+from pyarrow.util import implements, _stringify_path\n \n \n class FileSystem(object):\n@@ -68,6 +72,7 @@ def disk_usage(self, path):\n         -------\n         usage : int\n         \"\"\"\n+        path = _stringify_path(path)\n         path_info = self.stat(path)\n         if path_info['kind'] == 'file':\n             return path_info['size']\n@@ -199,10 +204,12 @@ def get_instance(cls):\n \n     @implements(FileSystem.ls)\n     def ls(self, path):\n+        path = _stringify_path(path)\n         return sorted(pjoin(path, x) for x in os.listdir(path))\n \n     @implements(FileSystem.mkdir)\n     def mkdir(self, path, create_parents=True):\n+        path = _stringify_path(path)\n         if create_parents:\n             os.makedirs(path)\n         else:\n@@ -210,10 +217,12 @@ def mkdir(self, path, create_parents=True):\n \n     @implements(FileSystem.isdir)\n     def isdir(self, path):\n+        path = _stringify_path(path)\n         return os.path.isdir(path)\n \n     @implements(FileSystem.isfile)\n     def isfile(self, path):\n+        path = _stringify_path(path)\n         return os.path.isfile(path)\n \n     @implements(FileSystem._isfilestore)\n@@ -222,6 +231,7 @@ def _isfilestore(self):\n \n     @implements(FileSystem.exists)\n     def exists(self, path):\n+        path = _stringify_path(path)\n         return os.path.exists(path)\n \n     @implements(FileSystem.open)\n@@ -229,17 +239,19 @@ def open(self, path, mode='rb'):\n         \"\"\"\n         Open file for reading or writing\n         \"\"\"\n+        path = _stringify_path(path)\n         return open(path, mode=mode)\n \n     @property\n     def pathsep(self):\n         return os.path.sep\n \n-    def walk(self, top_dir):\n+    def walk(self, path):\n         \"\"\"\n         Directory tree generator, see os.walk\n         \"\"\"\n-        return os.walk(top_dir)\n+        path = _stringify_path(path)\n+        return os.walk(path)\n \n \n class DaskFileSystem(FileSystem):\n@@ -268,14 +280,17 @@ def _isfilestore(self):\n \n     @implements(FileSystem.delete)\n     def delete(self, path, recursive=False):\n+        path = _stringify_path(path)\n         return self.fs.rm(path, recursive=recursive)\n \n     @implements(FileSystem.exists)\n     def exists(self, path):\n+        path = _stringify_path(path)\n         return self.fs.exists(path)\n \n     @implements(FileSystem.mkdir)\n     def mkdir(self, path, create_parents=True):\n+        path = _stringify_path(path)\n         if create_parents:\n             return self.fs.mkdirs(path)\n         else:\n@@ -286,22 +301,26 @@ def open(self, path, mode='rb'):\n         \"\"\"\n         Open file for reading or writing\n         \"\"\"\n+        path = _stringify_path(path)\n         return self.fs.open(path, mode=mode)\n \n     def ls(self, path, detail=False):\n+        path = _stringify_path(path)\n         return self.fs.ls(path, detail=detail)\n \n-    def walk(self, top_path):\n+    def walk(self, path):\n         \"\"\"\n         Directory tree generator, like os.walk\n         \"\"\"\n-        return self.fs.walk(top_path)\n+        path = _stringify_path(path)\n+        return self.fs.walk(path)\n \n \n class S3FSWrapper(DaskFileSystem):\n \n     @implements(FileSystem.isdir)\n     def isdir(self, path):\n+        path = _stringify_path(path)\n         try:\n             contents = self.fs.ls(path)\n             if len(contents) == 1 and contents[0] == path:\n@@ -313,6 +332,7 @@ def isdir(self, path):\n \n     @implements(FileSystem.isfile)\n     def isfile(self, path):\n+        path = _stringify_path(path)\n         try:\n             contents = self.fs.ls(path)\n             return len(contents) == 1 and contents[0] == path\n@@ -326,7 +346,7 @@ def walk(self, path, refresh=False):\n         Generator version of what is in s3fs, which yields a flattened list of\n         files\n         \"\"\"\n-        path = path.replace('s3://', '')\n+        path = _stringify_path(path).replace('s3://', '')\n         directories = set()\n         files = set()\n \n@@ -350,3 +370,46 @@ def walk(self, path, refresh=False):\n         for directory in directories:\n             for tup in self.walk(directory, refresh=refresh):\n                 yield tup\n+\n+\n+def _ensure_filesystem(fs):\n+    fs_type = type(fs)\n+\n+    # If the arrow filesystem was subclassed, assume it supports the full\n+    # interface and return it\n+    if not issubclass(fs_type, FileSystem):\n+        for mro in inspect.getmro(fs_type):\n+            if mro.__name__ is 'S3FileSystem':\n+                return S3FSWrapper(fs)\n+            # In case its a simple LocalFileSystem (e.g. dask) use native arrow\n+            # FS\n+            elif mro.__name__ is 'LocalFileSystem':\n+                return LocalFileSystem.get_instance()\n+\n+        raise IOError('Unrecognized filesystem: {0}'.format(fs_type))\n+    else:\n+        return fs\n+\n+\n+def _get_fs_from_path(path):\n+    \"\"\"\n+    return filesystem from path which could be an HDFS URI\n+    \"\"\"\n+    # input can be hdfs URI such as hdfs://host:port/myfile.parquet\n+    path = _stringify_path(path)\n+    # if _has_pathlib and isinstance(path, pathlib.Path):\n+    #     path = str(path)\n+    parsed_uri = urlparse(path)\n+    if parsed_uri.scheme == 'hdfs':\n+        netloc_split = parsed_uri.netloc.split(':')\n+        host = netloc_split[0]\n+        if host == '':\n+            host = 'default'\n+        port = 0\n+        if len(netloc_split) == 2 and netloc_split[1].isnumeric():\n+            port = int(netloc_split[1])\n+        fs = pa.hdfs.connect(host=host, port=port)\n+    else:\n+        fs = LocalFileSystem.get_instance()\n+\n+    return fs\ndiff --git a/python/pyarrow/io.pxi b/python/pyarrow/io.pxi\nindex 5a4d16441c..e240b239e4 100644\n--- a/python/pyarrow/io.pxi\n+++ b/python/pyarrow/io.pxi\n@@ -19,8 +19,6 @@\n # arrow::ipc\n \n from libc.stdlib cimport malloc, free\n-from pyarrow.compat import builtin_pickle, frombytes, tobytes, encode_file_path\n-from io import BufferedIOBase, UnsupportedOperation\n \n import re\n import six\n@@ -28,6 +26,10 @@ import sys\n import threading\n import time\n import warnings\n+from io import BufferedIOBase, UnsupportedOperation\n+\n+from pyarrow.util import _stringify_path\n+from pyarrow.compat import builtin_pickle, frombytes, tobytes, encode_file_path\n \n \n # 64K\n@@ -40,18 +42,6 @@ cdef extern from \"Python.h\":\n         char *v, Py_ssize_t len) except NULL\n \n \n-def _stringify_path(path):\n-    \"\"\"\n-    Convert *path* to a string or unicode path if possible.\n-    \"\"\"\n-    if isinstance(path, six.string_types):\n-        return path\n-    try:\n-        return path.__fspath__()\n-    except AttributeError:\n-        raise TypeError(\"not a path-like object\")\n-\n-\n cdef class NativeFile:\n \n     def __cinit__(self):\ndiff --git a/python/pyarrow/parquet.py b/python/pyarrow/parquet.py\nindex 343758ab5a..d56a67fd68 100644\n--- a/python/pyarrow/parquet.py\n+++ b/python/pyarrow/parquet.py\n@@ -18,28 +18,21 @@\n from collections import defaultdict\n from concurrent import futures\n import os\n-import inspect\n import json\n import re\n-import six\n-from six.moves.urllib.parse import urlparse\n-# pathlib might not be available in Python 2\n-try:\n-    import pathlib\n-    _has_pathlib = True\n-except ImportError:\n-    _has_pathlib = False\n \n import numpy as np\n \n-from pyarrow.filesystem import FileSystem, LocalFileSystem, S3FSWrapper\n+import pyarrow as pa\n+import pyarrow._parquet as _parquet\n+import pyarrow.lib as lib\n from pyarrow._parquet import (ParquetReader, RowGroupStatistics,  # noqa\n                               FileMetaData, RowGroupMetaData,\n                               ColumnChunkMetaData,\n                               ParquetSchema, ColumnSchema)\n-import pyarrow._parquet as _parquet\n-import pyarrow.lib as lib\n-import pyarrow as pa\n+from pyarrow.filesystem import (LocalFileSystem, _ensure_filesystem,\n+                                _get_fs_from_path)\n+from pyarrow.util import _is_path_like, _stringify_path\n \n \n # ----------------------------------------------------------------------\n@@ -52,7 +45,7 @@ class ParquetFile(object):\n \n     Parameters\n     ----------\n-    source : str, pyarrow.NativeFile, or file-like object\n+    source : str, pathlib.Path, pyarrow.NativeFile, or file-like object\n         Readable source. For passing bytes or buffer-like file containing a\n         Parquet file, use pyarorw.BufferReader\n     metadata : ParquetFileMetadata, default None\n@@ -296,7 +289,7 @@ def __init__(self, where, schema, flavor=None,\n         # to be closed\n         self.file_handle = None\n \n-        if is_path(where):\n+        if _is_path_like(where):\n             fs = _get_fs_from_path(where)\n             sink = self.file_handle = fs.open(where, 'wb')\n         else:\n@@ -362,7 +355,7 @@ class ParquetDatasetPiece(object):\n \n     Parameters\n     ----------\n-    path : str\n+    path : str or pathlib.Path\n         Path to file in the file system where this piece is located\n     partition_keys : list of tuples\n       [(column name, ordinal index)]\n@@ -371,7 +364,7 @@ class ParquetDatasetPiece(object):\n     \"\"\"\n \n     def __init__(self, path, row_group=None, partition_keys=None):\n-        self.path = path\n+        self.path = _stringify_path(path)\n         self.row_group = row_group\n         self.partition_keys = partition_keys or []\n \n@@ -634,11 +627,6 @@ def filter_accepts_partition(self, part_key, filter, level):\n                              filter[1])\n \n \n-def is_path(x):\n-    return (isinstance(x, six.string_types)\n-            or (_has_pathlib and isinstance(x, pathlib.Path)))\n-\n-\n class ParquetManifest(object):\n     \"\"\"\n \n@@ -647,7 +635,7 @@ def __init__(self, dirpath, filesystem=None, pathsep='/',\n                  partition_scheme='hive', metadata_nthreads=1):\n         self.filesystem = filesystem or _get_fs_from_path(dirpath)\n         self.pathsep = pathsep\n-        self.dirpath = dirpath\n+        self.dirpath = _stringify_path(dirpath)\n         self.partition_scheme = partition_scheme\n         self.partitions = ParquetPartitions()\n         self.pieces = []\n@@ -957,25 +945,6 @@ def all_filters_accept(piece):\n         self.pieces = [p for p in self.pieces if all_filters_accept(p)]\n \n \n-def _ensure_filesystem(fs):\n-    fs_type = type(fs)\n-\n-    # If the arrow filesystem was subclassed, assume it supports the full\n-    # interface and return it\n-    if not issubclass(fs_type, FileSystem):\n-        for mro in inspect.getmro(fs_type):\n-            if mro.__name__ is 'S3FileSystem':\n-                return S3FSWrapper(fs)\n-            # In case its a simple LocalFileSystem (e.g. dask) use native arrow\n-            # FS\n-            elif mro.__name__ is 'LocalFileSystem':\n-                return LocalFileSystem.get_instance()\n-\n-        raise IOError('Unrecognized filesystem: {0}'.format(fs_type))\n-    else:\n-        return fs\n-\n-\n def _make_manifest(path_or_paths, fs, pathsep='/', metadata_nthreads=1):\n     partitions = None\n     common_metadata_path = None\n@@ -985,7 +954,7 @@ def _make_manifest(path_or_paths, fs, pathsep='/', metadata_nthreads=1):\n         # Dask passes a directory as a list of length 1\n         path_or_paths = path_or_paths[0]\n \n-    if is_path(path_or_paths) and fs.isdir(path_or_paths):\n+    if _is_path_like(path_or_paths) and fs.isdir(path_or_paths):\n         manifest = ParquetManifest(path_or_paths, filesystem=fs,\n                                    pathsep=fs.pathsep,\n                                    metadata_nthreads=metadata_nthreads)\n@@ -1040,7 +1009,7 @@ def _make_manifest(path_or_paths, fs, pathsep='/', metadata_nthreads=1):\n \n def read_table(source, columns=None, nthreads=1, metadata=None,\n                use_pandas_metadata=False):\n-    if is_path(source):\n+    if _is_path_like(source):\n         fs = _get_fs_from_path(source)\n         return fs.read_parquet(source, columns=columns, metadata=metadata,\n                                use_pandas_metadata=use_pandas_metadata)\n@@ -1092,9 +1061,9 @@ def write_table(table, where, row_group_size=None, version='1.0',\n                 **kwargs) as writer:\n             writer.write_table(table, row_group_size=row_group_size)\n     except Exception:\n-        if is_path(where):\n+        if _is_path_like(where):\n             try:\n-                os.remove(where)\n+                os.remove(_stringify_path(where))\n             except os.error:\n                 pass\n         raise\n@@ -1258,26 +1227,3 @@ def read_schema(where):\n     schema : pyarrow.Schema\n     \"\"\"\n     return ParquetFile(where).schema.to_arrow_schema()\n-\n-\n-def _get_fs_from_path(path):\n-    \"\"\"\n-    return filesystem from path which could be an HDFS URI\n-    \"\"\"\n-    # input can be hdfs URI such as hdfs://host:port/myfile.parquet\n-    if _has_pathlib and isinstance(path, pathlib.Path):\n-        path = str(path)\n-    parsed_uri = urlparse(path)\n-    if parsed_uri.scheme == 'hdfs':\n-        netloc_split = parsed_uri.netloc.split(':')\n-        host = netloc_split[0]\n-        if host == '':\n-            host = 'default'\n-        port = 0\n-        if len(netloc_split) == 2 and netloc_split[1].isnumeric():\n-            port = int(netloc_split[1])\n-        fs = pa.hdfs.connect(host=host, port=port)\n-    else:\n-        fs = LocalFileSystem.get_instance()\n-\n-    return fs\ndiff --git a/python/pyarrow/tests/conftest.py b/python/pyarrow/tests/conftest.py\nindex e67aac1dcb..fff0ca4186 100644\n--- a/python/pyarrow/tests/conftest.py\n+++ b/python/pyarrow/tests/conftest.py\n@@ -15,7 +15,12 @@\n # specific language governing permissions and limitations\n # under the License.\n \n-from pytest import skip, mark\n+import pytest\n+\n+try:\n+    import pathlib\n+except ImportError:\n+    import pathlib2 as pathlib  # py2 compat\n \n \n groups = [\n@@ -84,7 +89,7 @@ def pytest_addoption(parser):\n \n def pytest_collection_modifyitems(config, items):\n     if not config.getoption('--runslow'):\n-        skip_slow = mark.skip(reason='need --runslow option to run')\n+        skip_slow = pytest.mark.skip(reason='need --runslow option to run')\n \n         for item in items:\n             if 'slow' in item.keywords:\n@@ -104,7 +109,7 @@ def pytest_runtest_setup(item):\n         elif getattr(item.obj, group, None):\n             if (item.config.getoption(disable_flag) or\n                     not item.config.getoption(flag)):\n-                skip('{0} NOT enabled'.format(flag))\n+                pytest.skip('{0} NOT enabled'.format(flag))\n \n     if only_set:\n         skip_item = True\n@@ -115,4 +120,15 @@ def pytest_runtest_setup(item):\n                 skip_item = False\n \n         if skip_item:\n-            skip('Only running some groups with only flags')\n+            pytest.skip('Only running some groups with only flags')\n+\n+\n+@pytest.fixture\n+def tempdir(tmpdir):\n+    # convert pytest's LocalPath to pathlib.Path\n+    return pathlib.Path(tmpdir.strpath)\n+\n+\n+@pytest.fixture(scope='session')\n+def datadir():\n+    return pathlib.Path(__file__).parent / 'data'\ndiff --git a/python/pyarrow/tests/data/v0.7.1.all-named-index.parquet b/python/pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet\nsimilarity index 100%\nrename from python/pyarrow/tests/data/v0.7.1.all-named-index.parquet\nrename to python/pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet\ndiff --git a/python/pyarrow/tests/data/v0.7.1.column-metadata-handling.parquet b/python/pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet\nsimilarity index 100%\nrename from python/pyarrow/tests/data/v0.7.1.column-metadata-handling.parquet\nrename to python/pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet\ndiff --git a/python/pyarrow/tests/data/v0.7.1.parquet b/python/pyarrow/tests/data/parquet/v0.7.1.parquet\nsimilarity index 100%\nrename from python/pyarrow/tests/data/v0.7.1.parquet\nrename to python/pyarrow/tests/data/parquet/v0.7.1.parquet\ndiff --git a/python/pyarrow/tests/data/v0.7.1.some-named-index.parquet b/python/pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet\nsimilarity index 100%\nrename from python/pyarrow/tests/data/v0.7.1.some-named-index.parquet\nrename to python/pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet\ndiff --git a/python/pyarrow/tests/test_orc.py b/python/pyarrow/tests/test_orc.py\nindex e2fcc93c34..ca0406f700 100644\n--- a/python/pyarrow/tests/test_orc.py\n+++ b/python/pyarrow/tests/test_orc.py\n@@ -15,32 +15,23 @@\n # specific language governing permissions and limitations\n # under the License.\n \n-import datetime\n+import pytest\n import decimal\n-import os\n+import datetime\n \n-from pandas.util.testing import assert_frame_equal\n import pandas as pd\n-import pytest\n-\n import pyarrow as pa\n \n+from pandas.util.testing import assert_frame_equal\n \n # Marks all of the tests in this module\n # Ignore these with pytest ... -m 'not orc'\n pytestmark = pytest.mark.orc\n \n \n-here = os.path.abspath(os.path.dirname(__file__))\n-orc_data_dir = os.path.join(here, 'data', 'orc')\n-\n-\n-def path_for_orc_example(name):\n-    return os.path.join(orc_data_dir, '%s.orc' % name)\n-\n-\n-def path_for_json_example(name):\n-    return os.path.join(orc_data_dir, '%s.jsn.gz' % name)\n+@pytest.fixture(scope='module')\n+def datadir(datadir):\n+    return datadir / 'orc'\n \n \n def fix_example_values(actual_cols, expected_cols):\n@@ -119,31 +110,29 @@ def check_example_file(orc_path, expected_df, need_fix=False):\n     assert json_pos == orc_file.nrows\n \n \n-@pytest.mark.parametrize('example_name', [\n-    'TestOrcFile.test1',\n-    'TestOrcFile.testDate1900',\n-    'decimal'\n+@pytest.mark.parametrize('filename', [\n+    'TestOrcFile.test1.orc',\n+    'TestOrcFile.testDate1900.orc',\n+    'decimal.orc'\n ])\n-def test_example_using_json(example_name):\n+def test_example_using_json(filename, datadir):\n     \"\"\"\n     Check a ORC file example against the equivalent JSON file, as given\n     in the Apache ORC repository (the JSON file has one JSON object per\n     line, corresponding to one row in the ORC file).\n     \"\"\"\n     # Read JSON file\n-    json_path = path_for_json_example(example_name)\n-    table = pd.read_json(json_path, lines=True)\n+    path = datadir / filename\n+    table = pd.read_json(str(path.with_suffix('.jsn.gz')), lines=True)\n+    check_example_file(path, table, need_fix=True)\n \n-    check_example_file(path_for_orc_example(example_name), table,\n-                       need_fix=True)\n \n-\n-def test_orcfile_empty():\n+def test_orcfile_empty(datadir):\n     from pyarrow import orc\n-    f = orc.ORCFile(path_for_orc_example('TestOrcFile.emptyFile'))\n-    table = f.read()\n+\n+    table = orc.ORCFile(datadir / 'TestOrcFile.emptyFile.orc').read()\n     assert table.num_rows == 0\n-    schema = table.schema\n+\n     expected_schema = pa.schema([\n         ('boolean1', pa.bool_()),\n         ('byte1', pa.int8()),\n@@ -172,4 +161,4 @@ def test_orcfile_empty():\n                 ])),\n             ]))),\n         ])\n-    assert schema == expected_schema\n+    assert table.schema == expected_schema\ndiff --git a/python/pyarrow/tests/test_parquet.py b/python/pyarrow/tests/test_parquet.py\nindex 324ae16e6c..b40294a355 100644\n--- a/python/pyarrow/tests/test_parquet.py\n+++ b/python/pyarrow/tests/test_parquet.py\n@@ -15,32 +15,34 @@\n # specific language governing permissions and limitations\n # under the License.\n \n-from os.path import join as pjoin\n-\n import datetime\n import decimal\n import io\n import json\n import os\n-import sys\n-\n import pytest\n \n+import numpy as np\n+import pandas as pd\n+import pandas.util.testing as tm\n+\n+import pyarrow as pa\n from pyarrow.compat import guid, u, BytesIO, unichar\n from pyarrow.tests import util\n from pyarrow.filesystem import LocalFileSystem\n from .pandas_examples import dataframe_with_arrays, dataframe_with_lists\n \n-import numpy as np\n-import pandas as pd\n-import pyarrow as pa\n-import pandas.util.testing as tm\n \n # Marks all of the tests in this module\n # Ignore these with pytest ... -m 'not parquet'\n pytestmark = pytest.mark.parquet\n \n \n+@pytest.fixture(scope='module')\n+def datadir(datadir):\n+    return datadir / 'parquet'\n+\n+\n def _write_table(table, path, **kwargs):\n     import pyarrow.parquet as pq\n \n@@ -88,13 +90,12 @@ def _roundtrip_pandas_dataframe(df, write_kwargs):\n \n \n @pytest.mark.parametrize('dtype', [int, float])\n-def test_single_pylist_column_roundtrip(tmpdir, dtype):\n-    filename = tmpdir.join('single_{}_column.parquet'\n-                           .format(dtype.__name__))\n+def test_single_pylist_column_roundtrip(tempdir, dtype):\n+    filename = tempdir / 'single_{}_column.parquet'.format(dtype.__name__)\n     data = [pa.array(list(map(dtype, range(5))))]\n     table = pa.Table.from_arrays(data, names=['a'])\n-    _write_table(table, filename.strpath)\n-    table_read = _read_table(filename.strpath)\n+    _write_table(table, filename)\n+    table_read = _read_table(filename)\n     for col_written, col_read in zip(table.itercolumns(),\n                                      table_read.itercolumns()):\n         assert col_written.name == col_read.name\n@@ -134,18 +135,18 @@ def alltypes_sample(size=10000, seed=0, categorical=False):\n \n \n @pytest.mark.parametrize('chunk_size', [None, 1000])\n-def test_pandas_parquet_2_0_rountrip(tmpdir, chunk_size):\n+def test_pandas_parquet_2_0_rountrip(tempdir, chunk_size):\n     import pyarrow.parquet as pq\n \n     df = alltypes_sample(size=10000, categorical=True)\n \n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df)\n     assert b'pandas' in arrow_table.schema.metadata\n \n-    _write_table(arrow_table, filename.strpath, version=\"2.0\",\n+    _write_table(arrow_table, filename, version=\"2.0\",\n                  coerce_timestamps='ms', chunk_size=chunk_size)\n-    table_read = pq.read_pandas(filename.strpath)\n+    table_read = pq.read_pandas(filename)\n     assert b'pandas' in table_read.schema.metadata\n \n     assert arrow_table.schema.metadata == table_read.schema.metadata\n@@ -220,26 +221,25 @@ def test_pandas_parquet_datetime_tz():\n     tm.assert_frame_equal(df, df_read)\n \n \n-def test_pandas_parquet_custom_metadata(tmpdir):\n+def test_pandas_parquet_custom_metadata(tempdir):\n     import pyarrow.parquet as pq\n \n     df = alltypes_sample(size=10000)\n \n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df)\n     assert b'pandas' in arrow_table.schema.metadata\n \n-    _write_table(arrow_table, filename.strpath, version=\"2.0\",\n-                 coerce_timestamps='ms')\n+    _write_table(arrow_table, filename, version='2.0', coerce_timestamps='ms')\n \n-    md = pq.read_metadata(filename.strpath).metadata\n-    assert b'pandas' in md\n+    metadata = pq.read_metadata(filename).metadata\n+    assert b'pandas' in metadata\n \n-    js = json.loads(md[b'pandas'].decode('utf8'))\n+    js = json.loads(metadata[b'pandas'].decode('utf8'))\n     assert js['index_columns'] == ['__index_level_0__']\n \n \n-def test_pandas_parquet_column_multiindex(tmpdir):\n+def test_pandas_parquet_column_multiindex(tempdir):\n     import pyarrow.parquet as pq\n \n     df = alltypes_sample(size=10)\n@@ -248,24 +248,23 @@ def test_pandas_parquet_column_multiindex(tmpdir):\n         names=['level_1', 'level_2']\n     )\n \n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df)\n     assert b'pandas' in arrow_table.schema.metadata\n \n-    _write_table(arrow_table, filename.strpath, version=\"2.0\",\n-                 coerce_timestamps='ms')\n+    _write_table(arrow_table, filename, version='2.0', coerce_timestamps='ms')\n \n-    table_read = pq.read_pandas(filename.strpath)\n+    table_read = pq.read_pandas(filename)\n     df_read = table_read.to_pandas()\n     tm.assert_frame_equal(df, df_read)\n \n \n-def test_pandas_parquet_2_0_rountrip_read_pandas_no_index_written(tmpdir):\n+def test_pandas_parquet_2_0_rountrip_read_pandas_no_index_written(tempdir):\n     import pyarrow.parquet as pq\n \n     df = alltypes_sample(size=10000)\n \n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n     js = json.loads(arrow_table.schema.metadata[b'pandas'].decode('utf8'))\n     assert not js['index_columns']\n@@ -273,9 +272,8 @@ def test_pandas_parquet_2_0_rountrip_read_pandas_no_index_written(tmpdir):\n     # While index_columns should be empty, columns needs to be filled still.\n     assert js['columns']\n \n-    _write_table(arrow_table, filename.strpath, version=\"2.0\",\n-                 coerce_timestamps='ms')\n-    table_read = pq.read_pandas(filename.strpath)\n+    _write_table(arrow_table, filename, version='2.0', coerce_timestamps='ms')\n+    table_read = pq.read_pandas(filename)\n \n     js = json.loads(table_read.schema.metadata[b'pandas'].decode('utf8'))\n     assert not js['index_columns']\n@@ -286,7 +284,7 @@ def test_pandas_parquet_2_0_rountrip_read_pandas_no_index_written(tmpdir):\n     tm.assert_frame_equal(df, df_read)\n \n \n-def test_pandas_parquet_1_0_rountrip(tmpdir):\n+def test_pandas_parquet_1_0_rountrip(tempdir):\n     size = 10000\n     np.random.seed(0)\n     df = pd.DataFrame({\n@@ -305,10 +303,10 @@ def test_pandas_parquet_1_0_rountrip(tmpdir):\n         'str_with_nulls': [None] + [str(x) for x in range(size - 2)] + [None],\n         'empty_str': [''] * size\n     })\n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df)\n-    _write_table(arrow_table, filename.strpath, version=\"1.0\")\n-    table_read = _read_table(filename.strpath)\n+    _write_table(arrow_table, filename, version='1.0')\n+    table_read = _read_table(filename)\n     df_read = table_read.to_pandas()\n \n     # We pass uint32_t as int64_t if we write Parquet version 1.0\n@@ -317,29 +315,35 @@ def test_pandas_parquet_1_0_rountrip(tmpdir):\n     tm.assert_frame_equal(df, df_read)\n \n \n-@pytest.mark.skipif(sys.version_info < (3, 6), reason=\"need Python 3.6\")\n-def test_path_objects(tmpdir):\n+def test_multiple_path_types(tempdir):\n     # Test compatibility with PEP 519 path-like objects\n-    import pathlib\n-    p = pathlib.Path(tmpdir) / 'zzz.parquet'\n+    path = tempdir / 'zzz.parquet'\n     df = pd.DataFrame({'x': np.arange(10, dtype=np.int64)})\n-    _write_table(df, p)\n-    table_read = _read_table(p)\n+    _write_table(df, path)\n+    table_read = _read_table(path)\n     df_read = table_read.to_pandas()\n     tm.assert_frame_equal(df, df_read)\n \n+    # Test compatibility with plain string paths\n+    path = str(tempdir) + 'zzz.parquet'\n+    df = pd.DataFrame({'x': np.arange(10, dtype=np.int64)})\n+    _write_table(df, path)\n+    table_read = _read_table(path)\n+    df_read = table_read.to_pandas()\n+    tm.assert_frame_equal(df, df_read)\n \n-def test_pandas_column_selection(tmpdir):\n+\n+def test_pandas_column_selection(tempdir):\n     size = 10000\n     np.random.seed(0)\n     df = pd.DataFrame({\n         'uint8': np.arange(size, dtype=np.uint8),\n         'uint16': np.arange(size, dtype=np.uint16)\n     })\n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df)\n-    _write_table(arrow_table, filename.strpath)\n-    table_read = _read_table(filename.strpath, columns=['uint8'])\n+    _write_table(arrow_table, filename)\n+    table_read = _read_table(filename, columns=['uint8'])\n     df_read = table_read.to_pandas()\n \n     tm.assert_frame_equal(df[['uint8']], df_read)\n@@ -377,7 +381,7 @@ def _test_dataframe(size=10000, seed=0):\n     return df\n \n \n-def test_pandas_parquet_native_file_roundtrip(tmpdir):\n+def test_pandas_parquet_native_file_roundtrip(tempdir):\n     df = _test_dataframe(10000)\n     arrow_table = pa.Table.from_pandas(df)\n     imos = pa.BufferOutputStream()\n@@ -388,7 +392,7 @@ def test_pandas_parquet_native_file_roundtrip(tmpdir):\n     tm.assert_frame_equal(df, df_read)\n \n \n-def test_parquet_incremental_file_build(tmpdir):\n+def test_parquet_incremental_file_build(tempdir):\n     import pyarrow.parquet as pq\n \n     df = _test_dataframe(100)\n@@ -416,7 +420,7 @@ def test_parquet_incremental_file_build(tmpdir):\n     tm.assert_frame_equal(result.to_pandas(), expected)\n \n \n-def test_read_pandas_column_subset(tmpdir):\n+def test_read_pandas_column_subset(tempdir):\n     import pyarrow.parquet as pq\n \n     df = _test_dataframe(10000)\n@@ -429,7 +433,7 @@ def test_read_pandas_column_subset(tmpdir):\n     tm.assert_frame_equal(df[['strings', 'uint8']], df_read)\n \n \n-def test_pandas_parquet_empty_roundtrip(tmpdir):\n+def test_pandas_parquet_empty_roundtrip(tempdir):\n     df = _test_dataframe(0)\n     arrow_table = pa.Table.from_pandas(df)\n     imos = pa.BufferOutputStream()\n@@ -440,8 +444,8 @@ def test_pandas_parquet_empty_roundtrip(tmpdir):\n     tm.assert_frame_equal(df, df_read)\n \n \n-def test_pandas_parquet_pyfile_roundtrip(tmpdir):\n-    filename = tmpdir.join('pandas_pyfile_roundtrip.parquet').strpath\n+def test_pandas_parquet_pyfile_roundtrip(tempdir):\n+    filename = tempdir / 'pandas_pyfile_roundtrip.parquet'\n     size = 5\n     df = pd.DataFrame({\n         'int64': np.arange(size, dtype=np.int64),\n@@ -453,17 +457,17 @@ def test_pandas_parquet_pyfile_roundtrip(tmpdir):\n \n     arrow_table = pa.Table.from_pandas(df)\n \n-    with open(filename, 'wb') as f:\n+    with filename.open('wb') as f:\n         _write_table(arrow_table, f, version=\"1.0\")\n \n-    data = io.BytesIO(open(filename, 'rb').read())\n+    data = io.BytesIO(filename.read_bytes())\n \n     table_read = _read_table(data)\n     df_read = table_read.to_pandas()\n     tm.assert_frame_equal(df, df_read)\n \n \n-def test_pandas_parquet_configuration_options(tmpdir):\n+def test_pandas_parquet_configuration_options(tempdir):\n     size = 10000\n     np.random.seed(0)\n     df = pd.DataFrame({\n@@ -479,22 +483,20 @@ def test_pandas_parquet_configuration_options(tmpdir):\n         'float64': np.arange(size, dtype=np.float64),\n         'bool': np.random.randn(size) > 0\n     })\n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df)\n \n     for use_dictionary in [True, False]:\n-        _write_table(arrow_table, filename.strpath,\n-                     version=\"2.0\",\n+        _write_table(arrow_table, filename, version='2.0',\n                      use_dictionary=use_dictionary)\n-        table_read = _read_table(filename.strpath)\n+        table_read = _read_table(filename)\n         df_read = table_read.to_pandas()\n         tm.assert_frame_equal(df, df_read)\n \n     for compression in ['NONE', 'SNAPPY', 'GZIP', 'LZ4', 'ZSTD']:\n-        _write_table(arrow_table, filename.strpath,\n-                     version=\"2.0\",\n+        _write_table(arrow_table, filename, version='2.0',\n                      compression=compression)\n-        table_read = _read_table(filename.strpath)\n+        table_read = _read_table(filename)\n         df_read = table_read.to_pandas()\n         tm.assert_frame_equal(df, df_read)\n \n@@ -689,7 +691,7 @@ def test_compare_schemas():\n     assert fileh.schema[0] != 'arbitrary object'\n \n \n-def test_validate_schema_write_table(tmpdir):\n+def test_validate_schema_write_table(tempdir):\n     # ARROW-2926\n     import pyarrow.parquet as pq\n \n@@ -704,7 +706,7 @@ def test_validate_schema_write_table(tmpdir):\n     simple_from_array = [pa.array([1]), pa.array(['bla'])]\n     simple_table = pa.Table.from_arrays(simple_from_array, ['POS', 'desc'])\n \n-    path = tmpdir.join('simple_validate_schema.parquet').strpath\n+    path = tempdir / 'simple_validate_schema.parquet'\n \n     with pq.ParquetWriter(path, simple_schema,\n                           version='2.0',\n@@ -713,19 +715,18 @@ def test_validate_schema_write_table(tmpdir):\n             w.write_table(simple_table)\n \n \n-def test_column_of_arrays(tmpdir):\n+def test_column_of_arrays(tempdir):\n     df, schema = dataframe_with_arrays()\n \n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df, schema=schema)\n-    _write_table(arrow_table, filename.strpath, version=\"2.0\",\n-                 coerce_timestamps='ms')\n-    table_read = _read_table(filename.strpath)\n+    _write_table(arrow_table, filename, version=\"2.0\", coerce_timestamps='ms')\n+    table_read = _read_table(filename)\n     df_read = table_read.to_pandas()\n     tm.assert_frame_equal(df, df_read)\n \n \n-def test_coerce_timestamps(tmpdir):\n+def test_coerce_timestamps(tempdir):\n     from collections import OrderedDict\n     # ARROW-622\n     arrays = OrderedDict()\n@@ -747,12 +748,11 @@ def test_coerce_timestamps(tmpdir):\n     df = pd.DataFrame(arrays)\n     schema = pa.schema(fields)\n \n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df, schema=schema)\n \n-    _write_table(arrow_table, filename.strpath, version=\"2.0\",\n-                 coerce_timestamps='us')\n-    table_read = _read_table(filename.strpath)\n+    _write_table(arrow_table, filename, version=\"2.0\", coerce_timestamps='us')\n+    table_read = _read_table(filename)\n     df_read = table_read.to_pandas()\n \n     df_expected = df.copy()\n@@ -763,18 +763,18 @@ def test_coerce_timestamps(tmpdir):\n     tm.assert_frame_equal(df_expected, df_read)\n \n     with pytest.raises(ValueError):\n-        _write_table(arrow_table, filename.strpath, version=\"2.0\",\n+        _write_table(arrow_table, filename, version='2.0',\n                      coerce_timestamps='unknown')\n \n \n-def test_column_of_lists(tmpdir):\n+def test_column_of_lists(tempdir):\n     df, schema = dataframe_with_lists()\n \n-    filename = tmpdir.join('pandas_rountrip.parquet')\n+    filename = tempdir / 'pandas_rountrip.parquet'\n     arrow_table = pa.Table.from_pandas(df, schema=schema)\n-    _write_table(arrow_table, filename.strpath, version=\"2.0\",\n+    _write_table(arrow_table, filename, version='2.0',\n                  coerce_timestamps='ms')\n-    table_read = _read_table(filename.strpath)\n+    table_read = _read_table(filename)\n     df_read = table_read.to_pandas()\n     tm.assert_frame_equal(df, df_read)\n \n@@ -1028,13 +1028,13 @@ def test_scan_contents():\n     assert pf.scan_contents(df.columns[:4]) == 10000\n \n \n-def test_parquet_piece_read(tmpdir):\n+def test_parquet_piece_read(tempdir):\n     import pyarrow.parquet as pq\n \n     df = _test_dataframe(1000)\n     table = pa.Table.from_pandas(df)\n \n-    path = tmpdir.join('parquet_piece_read.parquet').strpath\n+    path = tempdir / 'parquet_piece_read.parquet'\n     _write_table(table, path, version='2.0')\n \n     piece1 = pq.ParquetDatasetPiece(path)\n@@ -1077,18 +1077,16 @@ def test_partition_set_dictionary_type():\n         set3.dictionary\n \n \n-def test_read_partitioned_directory(tmpdir):\n+def test_read_partitioned_directory(tempdir):\n     fs = LocalFileSystem.get_instance()\n-    base_path = str(tmpdir)\n-\n-    _partition_test_for_filesystem(fs, base_path)\n+    _partition_test_for_filesystem(fs, tempdir)\n \n \n-def test_create_parquet_dataset_multi_threaded(tmpdir):\n+def test_create_parquet_dataset_multi_threaded(tempdir):\n     import pyarrow.parquet as pq\n \n     fs = LocalFileSystem.get_instance()\n-    base_path = str(tmpdir)\n+    base_path = tempdir\n \n     _partition_test_for_filesystem(fs, base_path)\n \n@@ -1102,11 +1100,11 @@ def test_create_parquet_dataset_multi_threaded(tmpdir):\n     assert len(partitions.levels) == len(manifest.partitions.levels)\n \n \n-def test_equivalency(tmpdir):\n+def test_equivalency(tempdir):\n     import pyarrow.parquet as pq\n \n     fs = LocalFileSystem.get_instance()\n-    base_path = str(tmpdir)\n+    base_path = tempdir\n \n     integer_keys = [0, 1]\n     string_keys = ['a', 'b', 'c']\n@@ -1139,11 +1137,11 @@ def test_equivalency(tmpdir):\n     assert False not in result_df['boolean'].values\n \n \n-def test_cutoff_exclusive_integer(tmpdir):\n+def test_cutoff_exclusive_integer(tempdir):\n     import pyarrow.parquet as pq\n \n     fs = LocalFileSystem.get_instance()\n-    base_path = str(tmpdir)\n+    base_path = tempdir\n \n     integer_keys = [0, 1, 2, 3, 4]\n     partition_spec = [\n@@ -1178,11 +1176,11 @@ def test_cutoff_exclusive_integer(tmpdir):\n     raises=TypeError,\n     reason='Loss of type information in creation of categoricals.'\n )\n-def test_cutoff_exclusive_datetime(tmpdir):\n+def test_cutoff_exclusive_datetime(tempdir):\n     import pyarrow.parquet as pq\n \n     fs = LocalFileSystem.get_instance()\n-    base_path = str(tmpdir)\n+    base_path = tempdir\n \n     date_keys = [\n         datetime.date(2018, 4, 9),\n@@ -1222,11 +1220,11 @@ def test_cutoff_exclusive_datetime(tmpdir):\n     assert result_df['dates'].values == expected\n \n \n-def test_inclusive_integer(tmpdir):\n+def test_inclusive_integer(tempdir):\n     import pyarrow.parquet as pq\n \n     fs = LocalFileSystem.get_instance()\n-    base_path = str(tmpdir)\n+    base_path = tempdir\n \n     integer_keys = [0, 1, 2, 3, 4]\n     partition_spec = [\n@@ -1257,11 +1255,11 @@ def test_inclusive_integer(tmpdir):\n     assert result_list == [2, 3]\n \n \n-def test_inclusive_set(tmpdir):\n+def test_inclusive_set(tempdir):\n     import pyarrow.parquet as pq\n \n     fs = LocalFileSystem.get_instance()\n-    base_path = str(tmpdir)\n+    base_path = tempdir\n \n     integer_keys = [0, 1]\n     string_keys = ['a', 'b', 'c']\n@@ -1294,11 +1292,11 @@ def test_inclusive_set(tmpdir):\n     assert False not in result_df['boolean'].values\n \n \n-def test_invalid_pred_op(tmpdir):\n+def test_invalid_pred_op(tempdir):\n     import pyarrow.parquet as pq\n \n     fs = LocalFileSystem.get_instance()\n-    base_path = str(tmpdir)\n+    base_path = tempdir\n \n     integer_keys = [0, 1, 2, 3, 4]\n     partition_spec = [\n@@ -1414,12 +1412,12 @@ def _visit_level(base_dir, level, part_keys):\n         for value in values:\n             this_part_keys = part_keys + [(name, value)]\n \n-            level_dir = pjoin(base_dir, '{0}={1}'.format(name, value))\n+            level_dir = base_dir / '{0}={1}'.format(name, value)\n             fs.mkdir(level_dir)\n \n             if level == DEPTH - 1:\n                 # Generate example data\n-                file_path = pjoin(level_dir, guid())\n+                file_path = level_dir / guid()\n \n                 filtered_df = _filter_partition(df, this_part_keys)\n                 part_table = pa.Table.from_pandas(filtered_df)\n@@ -1427,10 +1425,10 @@ def _visit_level(base_dir, level, part_keys):\n                     _write_table(part_table, f)\n                 assert fs.exists(file_path)\n \n-                _touch(pjoin(level_dir, '_SUCCESS'))\n+                (level_dir / '_SUCCESS').touch()\n             else:\n                 _visit_level(level_dir, level + 1, this_part_keys)\n-                _touch(pjoin(level_dir, '_SUCCESS'))\n+                (level_dir / '_SUCCESS').touch()\n \n     _visit_level(base_dir, 0, [])\n \n@@ -1444,19 +1442,19 @@ def _test_read_common_metadata_files(fs, base_path):\n         'values': np.random.randn(N)\n     }, columns=['index', 'values'])\n \n-    data_path = pjoin(base_path, 'data.parquet')\n+    data_path = base_path / 'data.parquet'\n \n     table = pa.Table.from_pandas(df)\n \n     with fs.open(data_path, 'wb') as f:\n         _write_table(table, f)\n \n-    metadata_path = pjoin(base_path, '_common_metadata')\n+    metadata_path = base_path / '_common_metadata'\n     with fs.open(metadata_path, 'wb') as f:\n         pq.write_metadata(table.schema, f)\n \n     dataset = pq.ParquetDataset(base_path, filesystem=fs)\n-    assert dataset.common_metadata_path == metadata_path\n+    assert dataset.common_metadata_path == str(metadata_path)\n \n     with fs.open(data_path) as f:\n         common_schema = pq.read_metadata(f).schema\n@@ -1467,47 +1465,42 @@ def _test_read_common_metadata_files(fs, base_path):\n     assert dataset2.schema.equals(dataset.schema)\n \n \n-def test_read_common_metadata_files(tmpdir):\n-    base_path = str(tmpdir)\n+def test_read_common_metadata_files(tempdir):\n     fs = LocalFileSystem.get_instance()\n-    _test_read_common_metadata_files(fs, base_path)\n+    _test_read_common_metadata_files(fs, tempdir)\n \n \n-def _test_read_metadata_files(fs, base_path):\n+def test_read_metadata_files(tempdir):\n     import pyarrow.parquet as pq\n \n+    fs = LocalFileSystem.get_instance()\n+\n     N = 100\n     df = pd.DataFrame({\n         'index': np.arange(N),\n         'values': np.random.randn(N)\n     }, columns=['index', 'values'])\n \n-    data_path = pjoin(base_path, 'data.parquet')\n+    data_path = tempdir / 'data.parquet'\n \n     table = pa.Table.from_pandas(df)\n \n     with fs.open(data_path, 'wb') as f:\n         _write_table(table, f)\n \n-    metadata_path = pjoin(base_path, '_metadata')\n+    metadata_path = tempdir / '_metadata'\n     with fs.open(metadata_path, 'wb') as f:\n         pq.write_metadata(table.schema, f)\n \n-    dataset = pq.ParquetDataset(base_path, filesystem=fs)\n-    assert dataset.metadata_path == metadata_path\n+    dataset = pq.ParquetDataset(tempdir, filesystem=fs)\n+    assert dataset.metadata_path == str(metadata_path)\n \n     with fs.open(data_path) as f:\n         metadata_schema = pq.read_metadata(f).schema\n     assert dataset.schema.equals(metadata_schema)\n \n \n-def test_read_metadata_files(tmpdir):\n-    base_path = str(tmpdir)\n-    fs = LocalFileSystem.get_instance()\n-    _test_read_metadata_files(fs, base_path)\n-\n-\n-def test_read_schema(tmpdir):\n+def test_read_schema(tempdir):\n     import pyarrow.parquet as pq\n \n     N = 100\n@@ -1516,7 +1509,7 @@ def test_read_schema(tmpdir):\n         'values': np.random.randn(N)\n     }, columns=['index', 'values'])\n \n-    data_path = pjoin(str(tmpdir), 'test.parquet')\n+    data_path = tempdir / 'test.parquet'\n \n     table = pa.Table.from_pandas(df)\n     _write_table(table, data_path)\n@@ -1535,19 +1528,14 @@ def _filter_partition(df, part_keys):\n     return df[predicate].drop(to_drop, axis=1)\n \n \n-def _touch(path):\n-    with open(path, 'wb'):\n-        pass\n-\n-\n-def test_read_multiple_files(tmpdir):\n+def test_read_multiple_files(tempdir):\n     import pyarrow.parquet as pq\n \n     nfiles = 10\n     size = 5\n \n-    dirpath = tmpdir.join(guid()).strpath\n-    os.mkdir(dirpath)\n+    dirpath = tempdir / guid()\n+    dirpath.mkdir()\n \n     test_data = []\n     paths = []\n@@ -1557,7 +1545,7 @@ def test_read_multiple_files(tmpdir):\n         # Hack so that we don't have a dtype cast in v1 files\n         df['uint32'] = df['uint32'].astype(np.int64)\n \n-        path = pjoin(dirpath, '{0}.parquet'.format(i))\n+        path = dirpath / '{}.parquet'.format(i)\n \n         table = pa.Table.from_pandas(df)\n         _write_table(table, path)\n@@ -1566,7 +1554,7 @@ def test_read_multiple_files(tmpdir):\n         paths.append(path)\n \n     # Write a _SUCCESS.crc file\n-    _touch(pjoin(dirpath, '_SUCCESS.crc'))\n+    (dirpath / '_SUCCESS.crc').touch()\n \n     def read_multiple_files(paths, columns=None, nthreads=None, **kwargs):\n         dataset = pq.ParquetDataset(paths, **kwargs)\n@@ -1599,7 +1587,7 @@ def read_multiple_files(paths, columns=None, nthreads=None, **kwargs):\n \n     # Test failure modes with non-uniform metadata\n     bad_apple = _test_dataframe(size, seed=i).iloc[:, :4]\n-    bad_apple_path = tmpdir.join('{0}.parquet'.format(guid())).strpath\n+    bad_apple_path = tempdir / '{}.parquet'.format(guid())\n \n     t = pa.Table.from_pandas(bad_apple)\n     _write_table(t, bad_apple_path)\n@@ -1621,14 +1609,14 @@ def read_multiple_files(paths, columns=None, nthreads=None, **kwargs):\n         read_multiple_files(mixed_paths)\n \n \n-def test_dataset_read_pandas(tmpdir):\n+def test_dataset_read_pandas(tempdir):\n     import pyarrow.parquet as pq\n \n     nfiles = 5\n     size = 5\n \n-    dirpath = tmpdir.join(guid()).strpath\n-    os.mkdir(dirpath)\n+    dirpath = tempdir / guid()\n+    dirpath.mkdir()\n \n     test_data = []\n     frames = []\n@@ -1638,7 +1626,7 @@ def test_dataset_read_pandas(tmpdir):\n         df.index = np.arange(i * size, (i + 1) * size)\n         df.index.name = 'index'\n \n-        path = pjoin(dirpath, '{0}.parquet'.format(i))\n+        path = dirpath / '{}.parquet'.format(i)\n \n         table = pa.Table.from_pandas(df)\n         _write_table(table, path)\n@@ -1655,15 +1643,15 @@ def test_dataset_read_pandas(tmpdir):\n \n \n @pytest.mark.parametrize('preserve_index', [True, False])\n-def test_dataset_read_pandas_common_metadata(tmpdir, preserve_index):\n+def test_dataset_read_pandas_common_metadata(tempdir, preserve_index):\n     import pyarrow.parquet as pq\n \n     # ARROW-1103\n     nfiles = 5\n     size = 5\n \n-    dirpath = tmpdir.join(guid()).strpath\n-    os.mkdir(dirpath)\n+    dirpath = tempdir / guid()\n+    dirpath.mkdir()\n \n     test_data = []\n     frames = []\n@@ -1672,7 +1660,7 @@ def test_dataset_read_pandas_common_metadata(tmpdir, preserve_index):\n         df = _test_dataframe(size, seed=i)\n         df.index = pd.Index(np.arange(i * size, (i + 1) * size), name='index')\n \n-        path = pjoin(dirpath, '{:d}.parquet'.format(i))\n+        path = dirpath / '{}.parquet'.format(i)\n \n         table = pa.Table.from_pandas(df, preserve_index=preserve_index)\n \n@@ -1689,8 +1677,7 @@ def test_dataset_read_pandas_common_metadata(tmpdir, preserve_index):\n     table_for_metadata = pa.Table.from_pandas(\n         df, preserve_index=preserve_index\n     )\n-    pq.write_metadata(table_for_metadata.schema,\n-                      pjoin(dirpath, '_metadata'))\n+    pq.write_metadata(table_for_metadata.schema, dirpath / '_metadata')\n \n     dataset = pq.ParquetDataset(dirpath)\n     columns = ['uint8', 'strings']\n@@ -1705,49 +1692,49 @@ def _make_example_multifile_dataset(base_path, nfiles=10, file_nrows=5):\n     paths = []\n     for i in range(nfiles):\n         df = _test_dataframe(file_nrows, seed=i)\n-        path = pjoin(base_path, '{0}.parquet'.format(i))\n+        path = base_path / '{}.parquet'.format(i)\n \n         test_data.append(_write_table(df, path))\n         paths.append(path)\n     return paths\n \n \n-def test_ignore_private_directories(tmpdir):\n+def test_ignore_private_directories(tempdir):\n     import pyarrow.parquet as pq\n \n-    dirpath = tmpdir.join(guid()).strpath\n-    os.mkdir(dirpath)\n+    dirpath = tempdir / guid()\n+    dirpath.mkdir()\n \n     paths = _make_example_multifile_dataset(dirpath, nfiles=10,\n                                             file_nrows=5)\n \n     # private directory\n-    os.mkdir(pjoin(dirpath, '_impala_staging'))\n+    (dirpath / '_impala_staging').mkdir()\n \n     dataset = pq.ParquetDataset(dirpath)\n-    assert set(paths) == set(x.path for x in dataset.pieces)\n+    assert set(map(str, paths)) == set(x.path for x in dataset.pieces)\n \n \n-def test_ignore_hidden_files(tmpdir):\n+def test_ignore_hidden_files(tempdir):\n     import pyarrow.parquet as pq\n \n-    dirpath = tmpdir.join(guid()).strpath\n-    os.mkdir(dirpath)\n+    dirpath = tempdir / guid()\n+    dirpath.mkdir()\n \n     paths = _make_example_multifile_dataset(dirpath, nfiles=10,\n                                             file_nrows=5)\n \n-    with open(pjoin(dirpath, '.DS_Store'), 'wb') as f:\n+    with (dirpath / '.DS_Store').open('wb') as f:\n         f.write(b'gibberish')\n \n-    with open(pjoin(dirpath, '.private'), 'wb') as f:\n+    with (dirpath / '.private').open('wb') as f:\n         f.write(b'gibberish')\n \n     dataset = pq.ParquetDataset(dirpath)\n-    assert set(paths) == set(x.path for x in dataset.pieces)\n+    assert set(map(str, paths)) == set(x.path for x in dataset.pieces)\n \n \n-def test_multiindex_duplicate_values(tmpdir):\n+def test_multiindex_duplicate_values(tempdir):\n     num_rows = 3\n     numbers = list(range(num_rows))\n     index = pd.MultiIndex.from_arrays(\n@@ -1758,7 +1745,7 @@ def test_multiindex_duplicate_values(tmpdir):\n     df = pd.DataFrame({'numbers': numbers}, index=index)\n     table = pa.Table.from_pandas(df)\n \n-    filename = tmpdir.join('dup_multi_index_levels.parquet').strpath\n+    filename = tempdir / 'dup_multi_index_levels.parquet'\n \n     _write_table(table, filename)\n     result_table = _read_table(filename)\n@@ -1768,7 +1755,7 @@ def test_multiindex_duplicate_values(tmpdir):\n     tm.assert_frame_equal(result_df, df)\n \n \n-def test_write_error_deletes_incomplete_file(tmpdir):\n+def test_write_error_deletes_incomplete_file(tempdir):\n     # ARROW-1285\n     df = pd.DataFrame({'a': list('abc'),\n                        'b': list(range(1, 4)),\n@@ -1783,16 +1770,16 @@ def test_write_error_deletes_incomplete_file(tmpdir):\n \n     pdf = pa.Table.from_pandas(df)\n \n-    filename = tmpdir.join('tmp_file').strpath\n+    filename = tempdir / 'tmp_file'\n     try:\n         _write_table(pdf, filename)\n     except pa.ArrowException:\n         pass\n \n-    assert not os.path.exists(filename)\n+    assert not filename.exists()\n \n \n-def test_read_non_existent_file(tmpdir):\n+def test_read_non_existent_file(tempdir):\n     import pyarrow.parquet as pq\n \n     path = 'non-existent-file.parquet'\n@@ -1802,13 +1789,11 @@ def test_read_non_existent_file(tmpdir):\n         assert path in e.args[0]\n \n \n-def test_read_table_doesnt_warn():\n+def test_read_table_doesnt_warn(datadir):\n     import pyarrow.parquet as pq\n \n-    path = os.path.join(os.path.dirname(__file__), 'data', 'v0.7.1.parquet')\n-\n     with pytest.warns(None) as record:\n-        pq.read_table(path)\n+        pq.read_table(datadir / 'v0.7.1.parquet')\n \n     assert len(record) == 0\n \n@@ -1897,21 +1882,21 @@ def _test_write_to_dataset_no_partitions(base_path, filesystem=None):\n     assert output_df.equals(input_df)\n \n \n-def test_write_to_dataset_with_partitions(tmpdir):\n-    _test_write_to_dataset_with_partitions(str(tmpdir))\n+def test_write_to_dataset_with_partitions(tempdir):\n+    _test_write_to_dataset_with_partitions(str(tempdir))\n \n \n-def test_write_to_dataset_with_partitions_and_schema(tmpdir):\n+def test_write_to_dataset_with_partitions_and_schema(tempdir):\n     schema = pa.schema([pa.field('group1', type=pa.string()),\n                         pa.field('group2', type=pa.string()),\n                         pa.field('num', type=pa.int64()),\n                         pa.field('nan', type=pa.int32()),\n                         pa.field('date', type=pa.timestamp(unit='us'))])\n-    _test_write_to_dataset_with_partitions(str(tmpdir), schema=schema)\n+    _test_write_to_dataset_with_partitions(str(tempdir), schema=schema)\n \n \n-def test_write_to_dataset_no_partitions(tmpdir):\n-    _test_write_to_dataset_no_partitions(str(tmpdir))\n+def test_write_to_dataset_no_partitions(tempdir):\n+    _test_write_to_dataset_no_partitions(str(tempdir))\n \n \n @pytest.mark.large_memory\n@@ -1927,7 +1912,7 @@ def test_large_table_int32_overflow():\n     _write_table(table, f)\n \n \n-def test_index_column_name_duplicate(tmpdir):\n+def test_index_column_name_duplicate(tempdir):\n     data = {\n         'close': {\n             pd.Timestamp('2017-06-30 01:31:00'): 154.99958999999998,\n@@ -1942,7 +1927,7 @@ def test_index_column_name_duplicate(tmpdir):\n             ),\n         }\n     }\n-    path = str(tmpdir / 'data.parquet')\n+    path = str(tempdir / 'data.parquet')\n     dfx = pd.DataFrame(data).set_index('time', drop=False)\n     tdfx = pa.Table.from_pandas(dfx)\n     _write_table(tdfx, path)\n@@ -1951,7 +1936,7 @@ def test_index_column_name_duplicate(tmpdir):\n     tm.assert_frame_equal(result_df, dfx)\n \n \n-def test_parquet_nested_convenience(tmpdir):\n+def test_parquet_nested_convenience(tempdir):\n     import pyarrow.parquet as pq\n \n     # ARROW-1684\n@@ -1960,7 +1945,7 @@ def test_parquet_nested_convenience(tmpdir):\n         'b': [[1.], None, None, [6., 7.]],\n     })\n \n-    path = str(tmpdir / 'nested_convenience.parquet')\n+    path = str(tempdir / 'nested_convenience.parquet')\n \n     table = pa.Table.from_pandas(df, preserve_index=False)\n     _write_table(table, path)\n@@ -1972,7 +1957,7 @@ def test_parquet_nested_convenience(tmpdir):\n     tm.assert_frame_equal(read.to_pandas(), df)\n \n \n-def test_backwards_compatible_index_naming():\n+def test_backwards_compatible_index_naming(datadir):\n     expected_string = b\"\"\"\\\n carat        cut  color  clarity  depth  table  price     x     y     z\n  0.23      Ideal      E      SI2   61.5   55.0    326  3.95  3.98  2.43\n@@ -1988,13 +1973,12 @@ def test_backwards_compatible_index_naming():\n     expected = pd.read_csv(\n         io.BytesIO(expected_string), sep=r'\\s{2,}', index_col=None, header=0\n     )\n-    path = os.path.join(os.path.dirname(__file__), 'data', 'v0.7.1.parquet')\n-    t = _read_table(path)\n-    result = t.to_pandas()\n+    table = _read_table(datadir / 'v0.7.1.parquet')\n+    result = table.to_pandas()\n     tm.assert_frame_equal(result, expected)\n \n \n-def test_backwards_compatible_index_multi_level_named():\n+def test_backwards_compatible_index_multi_level_named(datadir):\n     expected_string = b\"\"\"\\\n carat        cut  color  clarity  depth  table  price     x     y     z\n  0.23      Ideal      E      SI2   61.5   55.0    326  3.95  3.98  2.43\n@@ -2011,15 +1995,13 @@ def test_backwards_compatible_index_multi_level_named():\n         io.BytesIO(expected_string),\n         sep=r'\\s{2,}', index_col=['cut', 'color', 'clarity'], header=0\n     ).sort_index()\n-    path = os.path.join(\n-        os.path.dirname(__file__), 'data', 'v0.7.1.all-named-index.parquet'\n-    )\n-    t = _read_table(path)\n-    result = t.to_pandas()\n+\n+    table = _read_table(datadir / 'v0.7.1.all-named-index.parquet')\n+    result = table.to_pandas()\n     tm.assert_frame_equal(result, expected)\n \n \n-def test_backwards_compatible_index_multi_level_some_named():\n+def test_backwards_compatible_index_multi_level_some_named(datadir):\n     expected_string = b\"\"\"\\\n carat        cut  color  clarity  depth  table  price     x     y     z\n  0.23      Ideal      E      SI2   61.5   55.0    326  3.95  3.98  2.43\n@@ -2037,15 +2019,13 @@ def test_backwards_compatible_index_multi_level_some_named():\n         sep=r'\\s{2,}', index_col=['cut', 'color', 'clarity'], header=0\n     ).sort_index()\n     expected.index = expected.index.set_names(['cut', None, 'clarity'])\n-    path = os.path.join(\n-        os.path.dirname(__file__), 'data', 'v0.7.1.some-named-index.parquet'\n-    )\n-    t = _read_table(path)\n-    result = t.to_pandas()\n+\n+    table = _read_table(datadir / 'v0.7.1.some-named-index.parquet')\n+    result = table.to_pandas()\n     tm.assert_frame_equal(result, expected)\n \n \n-def test_backwards_compatible_column_metadata_handling():\n+def test_backwards_compatible_column_metadata_handling(datadir):\n     expected = pd.DataFrame(\n         {'a': [1, 2, 3], 'b': [.1, .2, .3],\n          'c': pd.date_range(\"2017-01-01\", periods=3, tz='Europe/Brussels')})\n@@ -2054,24 +2034,20 @@ def test_backwards_compatible_column_metadata_handling():\n          pd.date_range(\"2017-01-01\", periods=3, tz='Europe/Brussels')],\n         names=['index', None])\n \n-    path = os.path.join(\n-        os.path.dirname(__file__), 'data',\n-        'v0.7.1.column-metadata-handling.parquet'\n-    )\n-    t = _read_table(path)\n-    result = t.to_pandas()\n+    path = datadir / 'v0.7.1.column-metadata-handling.parquet'\n+    table = _read_table(path)\n+    result = table.to_pandas()\n     tm.assert_frame_equal(result, expected)\n \n-    t = _read_table(path, columns=['a'])\n-    result = t.to_pandas()\n+    table = _read_table(path, columns=['a'])\n+    result = table.to_pandas()\n     tm.assert_frame_equal(result, expected[['a']].reset_index(drop=True))\n \n \n-def test_decimal_roundtrip(tmpdir):\n+def test_decimal_roundtrip(tempdir):\n     num_values = 10\n \n     columns = {}\n-\n     for precision in range(1, 39):\n         for scale in range(0, precision + 1):\n             with util.random_seed(0):\n@@ -2084,10 +2060,10 @@ def test_decimal_roundtrip(tmpdir):\n             columns[column_name] = random_decimal_values\n \n     expected = pd.DataFrame(columns)\n-    filename = tmpdir.join('decimals.parquet')\n+    filename = tempdir / 'decimals.parquet'\n     string_filename = str(filename)\n-    t = pa.Table.from_pandas(expected)\n-    _write_table(t, string_filename)\n+    table = pa.Table.from_pandas(expected)\n+    _write_table(table, string_filename)\n     result_table = _read_table(string_filename)\n     result = result_table.to_pandas()\n     tm.assert_frame_equal(result, expected)\n@@ -2096,9 +2072,9 @@ def test_decimal_roundtrip(tmpdir):\n @pytest.mark.xfail(\n     raises=pa.ArrowException, reason='Parquet does not support negative scale'\n )\n-def test_decimal_roundtrip_negative_scale(tmpdir):\n+def test_decimal_roundtrip_negative_scale(tempdir):\n     expected = pd.DataFrame({'decimal_num': [decimal.Decimal('1.23E4')]})\n-    filename = tmpdir.join('decimals.parquet')\n+    filename = tempdir / 'decimals.parquet'\n     string_filename = str(filename)\n     t = pa.Table.from_pandas(expected)\n     _write_table(t, string_filename)\n@@ -2107,7 +2083,7 @@ def test_decimal_roundtrip_negative_scale(tmpdir):\n     tm.assert_frame_equal(result, expected)\n \n \n-def test_parquet_writer_context_obj(tmpdir):\n+def test_parquet_writer_context_obj(tempdir):\n     import pyarrow.parquet as pq\n \n     df = _test_dataframe(100)\n@@ -2133,7 +2109,7 @@ def test_parquet_writer_context_obj(tmpdir):\n     tm.assert_frame_equal(result.to_pandas(), expected)\n \n \n-def test_parquet_writer_context_obj_with_exception(tmpdir):\n+def test_parquet_writer_context_obj_with_exception(tempdir):\n     import pyarrow.parquet as pq\n \n     df = _test_dataframe(100)\ndiff --git a/python/pyarrow/util.py b/python/pyarrow/util.py\nindex b8825658d8..12064e6232 100644\n--- a/python/pyarrow/util.py\n+++ b/python/pyarrow/util.py\n@@ -15,9 +15,20 @@\n # specific language governing permissions and limitations\n # under the License.\n \n+# Miscellaneous utility code\n+\n+import six\n import warnings\n \n-# Miscellaneous utility code\n+try:\n+    # pathlib might not be available\n+    try:\n+        import pathlib\n+    except ImportError:\n+        import pathlib2 as pathlib  # python 2 backport\n+    _has_pathlib = True\n+except ImportError:\n+    _has_pathlib = False\n \n \n def implements(f):\n@@ -35,3 +46,29 @@ def wrapper(*args, **kwargs):\n         warnings.warn(msg, FutureWarning)\n         return api(*args)\n     return wrapper\n+\n+\n+def _is_path_like(path):\n+    # PEP519 filesystem path protocol is available from python 3.6, so pathlib\n+    # doesn't implement __fspath__ for earlier versions\n+    return (isinstance(path, six.string_types) or\n+            hasattr(path, '__fspath__') or\n+            (_has_pathlib and isinstance(path, pathlib.Path)))\n+\n+\n+def _stringify_path(path):\n+    \"\"\"\n+    Convert *path* to a string or unicode path if possible.\n+    \"\"\"\n+    if isinstance(path, six.string_types):\n+        return path\n+\n+    # checking whether path implements the filesystem protocol\n+    try:\n+        return path.__fspath__()  # new in python 3.6\n+    except AttributeError:\n+        # fallback pathlib ckeck for earlier python versions than 3.6\n+        if _has_pathlib and isinstance(path, pathlib.Path):\n+            return str(path)\n+\n+    raise TypeError(\"not a path-like object\")\ndiff --git a/python/requirements.txt b/python/requirements.txt\nindex 7a435ac02c..f17386de42 100644\n--- a/python/requirements.txt\n+++ b/python/requirements.txt\n@@ -1,5 +1,6 @@\n+six\n pytest\n cloudpickle>=0.4.0\n numpy>=1.10.0\n-six\n-futures ; python_version < \"3\"\n+futures; python_version < \"3\"\n+pathlib2; python_version < \"3.4\"\ndiff --git a/python/setup.py b/python/setup.py\nindex c8cdb97590..f554fb6419 100644\n--- a/python/setup.py\n+++ b/python/setup.py\n@@ -557,7 +557,7 @@ def has_ext_modules(foo):\n                      },\n     setup_requires=['setuptools_scm', 'cython >= 0.27'] + setup_requires,\n     install_requires=install_requires,\n-    tests_require=['pytest', 'pandas'],\n+    tests_require=['pytest', 'pandas', 'pathlib2; python_version < \"3.4\"'],\n     description=\"Python library for Apache Arrow\",\n     long_description=long_description,\n     long_description_content_type=\"text/markdown\",\n@@ -567,7 +567,7 @@ def has_ext_modules(foo):\n         'Programming Language :: Python :: 3.5',\n         'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n-        ],\n+    ],\n     license='Apache License, Version 2.0',\n     maintainer=\"Apache Arrow Developers\",\n     maintainer_email=\"dev@arrow.apache.org\",\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-09-05T16:44:57.375+0000",
                    "updated": "2018-09-05T16:44:57.375+0000",
                    "started": "2018-09-05T16:44:57.374+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "141425",
                    "issueId": "13182524"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 5400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@2a482500[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3126019d[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@785364bb[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@79f8b8c5[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7e38f367[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@2c2d304e[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4f38efcc[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@73ba587e[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4aa03ea5[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@fcac8ff[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7840045[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@265cd163[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 5400,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Sep 05 16:44:50 UTC 2018",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2018-09-05T16:44:50.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-3160/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2018-09-02T11:10:44.000+0000",
        "updated": "2018-09-05T16:44:57.000+0000",
        "timeoriginalestimate": null,
        "description": null,
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "1.5h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 5400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python] Improve pathlib.Path support in parquet and filesystem modules",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13182524/comment/16604655",
                    "id": "16604655",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 2506\n[https://github.com/apache/arrow/pull/2506]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-09-05T16:44:50.858+0000",
                    "updated": "2018-09-05T16:44:50.858+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|i3xnp3:",
        "customfield_12314139": null
    }
}