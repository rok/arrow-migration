{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13381544",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544",
    "key": "ARROW-12921",
    "fields": {
        "fixVersions": [],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/10101",
            "id": "10101",
            "description": "Issues which have gone without any activity for an extended period of time",
            "name": "Abandoned"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12642766",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12642766",
                "type": {
                    "id": "12310051",
                    "name": "Supercedes",
                    "inward": "is superceded by",
                    "outward": "supercedes",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310051"
                },
                "outwardIssue": {
                    "id": "13339805",
                    "key": "ARROW-10549",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13339805",
                    "fields": {
                        "summary": "[C++][Dataset] RADOS dataset",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            },
            {
                "id": "12642765",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12642765",
                "type": {
                    "id": "12310051",
                    "name": "Supercedes",
                    "inward": "is superceded by",
                    "outward": "supercedes",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310051"
                },
                "inwardIssue": {
                    "id": "13394689",
                    "key": "ARROW-13607",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13394689",
                    "fields": {
                        "summary": "[C++] Add Skyhook to Arrow",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                            "id": "4",
                            "description": "An improvement or enhancement to an existing feature or task.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                            "name": "Improvement",
                            "subtask": false,
                            "avatarId": 21140
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=Jayjeet",
            "name": "Jayjeet",
            "key": "jayjeet",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Jayjeet Chakraborty",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
            "name": "Closed",
            "id": "6",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12333244",
                "id": "12333244",
                "name": "Continuous Integration"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332532",
                "id": "12332532",
                "name": "Documentation"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=Jayjeet",
            "name": "Jayjeet",
            "key": "jayjeet",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Jayjeet Chakraborty",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=Jayjeet",
            "name": "Jayjeet",
            "key": "jayjeet",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Jayjeet Chakraborty",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 23400,
            "total": 23400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 23400,
            "total": 23400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-12921/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 39,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/604726",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub opened a new pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431\n\n\n   The implementation includes a new `RadosParquetFileFormat` class that inherits from the `ParquetFileFormat` class to defer the evaluation of scan operations on a Parquet dataset to a RADOS storage backend. This new file format plugs into the `FileSystemDataset` API, converts filenames to object IDs using FS metadata and uses the [librados](https://docs.ceph.com/en/latest/rados/api/librados-intro/) C++ library to execute storage side functions that scan the files on the [Ceph](https://ceph.io) storage nodes (OSDs) using Arrow libraries. We ship unit and integration tests with our implementation where the tests are run against a single-node Ceph cluster.\r\n   \r\n   The storage-side code is implemented as a RADOS CLS (object storage class) using [Ceph's Object Class SDK](https://docs.ceph.com/en/latest/rados/api/objclass-sdk/#:~:text=Ceph%20can%20be%20extended%20by,object%20classes%20within%20the%20tree.). The code lives in `cpp/src/arrow/adapters/arrow-rados-cls`, and is expected to be deployed on the storage nodes (Ceph's OSDs) prior to operating on tables via the `RadosParquetFileFormat` implementation. This PR includes a CMake configuration for building this library if desired (`ARROW_CLS` CMake option). We have also added Python bindings for our C++ implementations and added integration tests for them.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-01T19:31:50.479+0000",
                    "updated": "2021-06-01T19:31:50.479+0000",
                    "started": "2021-06-01T19:31:50.479+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "604726",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/604738",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#issuecomment-852396357\n\n\n   https://issues.apache.org/jira/browse/ARROW-12921\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-06-01T19:40:38.439+0000",
                    "updated": "2021-06-01T19:40:38.439+0000",
                    "started": "2021-06-01T19:40:38.439+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "604738",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/628822",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r677894587\n\n\n\n##########\nFile path: ci/scripts/cpp_build.sh\n##########\n@@ -85,6 +85,8 @@ cmake -G \"${CMAKE_GENERATOR:-Ninja}\" \\\n       -DARROW_PYTHON=${ARROW_PYTHON:-OFF} \\\n       -DARROW_RUNTIME_SIMD_LEVEL=${ARROW_RUNTIME_SIMD_LEVEL:-MAX} \\\n       -DARROW_S3=${ARROW_S3:-OFF} \\\n+      -DARROW_RADOS=${ARROW_RADOS:-OFF} \\\n+      -DARROW_CLS=${ARROW_CLS:-OFF} \\\n\nReview comment:\n       Can we name this `ARROW_CEPH_CLS` or `ARROW_CEPH`?\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/README.md\n##########\n@@ -0,0 +1,78 @@\n+<!---\n+  Licensed to the Apache Software Foundation (ASF) under one\n+  or more contributor license agreements.  See the NOTICE file\n+  distributed with this work for additional information\n+  regarding copyright ownership.  The ASF licenses this file\n+  to you under the Apache License, Version 2.0 (the\n+  \"License\"); you may not use this file except in compliance\n+  with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing,\n+  software distributed under the License is distributed on an\n+  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  KIND, either express or implied.  See the License for the\n+  specific language governing permissions and limitations\n+  under the License.\n+-->\n+# <img src=\"https://iris-hep.org/assets/logos/skyhookdmLogoJeff.png\" width=\"64\" valign=\"middle\" alt=\"Skyhook\"/> SkyhookDM-Arrow\n+\n+Apache Arrow provides a [`Dataset`](https://arrow.apache.org/docs/cpp/api/dataset.html) API, which acts as an abstraction over a collection of files in different storage backends like S3 and HDFS. It supports different file formats like CSV and Parquet through the [`FileFormat`](https://arrow.apache.org/docs/cpp/api/dataset.html#_CPPv4N5arrow7dataset10FileFormatE) API. In SkyhookDM, we create a new file format called `RadosParquetFileFormat` on top of `ParquetFileFormat`, which besides providing all the features of Parquet allows offloading file fragment scan operations into the storage backend. Offloading scan operations increases the query performance many folds, provides better scalability, and results in less network traffic. The architecture of SkyhookDM is described [here](./docs/architecture.md).\n\nReview comment:\n       I think we should probably avoid references to Skyhook from within Arrow.  I think that will raise confusion.  Could we describe the architecture with something more like \"Arrow Ceph object class\" terminology?\n\n##########\nFile path: cpp/src/arrow/dataset/rados.cc\n##########\n@@ -0,0 +1,75 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       This file (and its header) should maybe be in `src/arrow/util` or `src/arrow/filesystem`.\n\n##########\nFile path: format/ScanRequest.fbs\n##########\n@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       This directory includes files that are meant to reflect the column format (e.g. types that all implementations are standardized on).  I'm not sure if your intention is to make the scan request a part of the specification or not.\r\n   \r\n   However, I think this was probably placed here so it would get picked up by thrift.  I'm not sure what a good approach to take would be and I'll defer to future reviewers here.\n\n##########\nFile path: cpp/src/arrow/dataset/file_rados_parquet.h\n##########\n@@ -0,0 +1,182 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// This API is EXPERIMENTAL.\n+#define _FILE_OFFSET_BITS 64\n+\n+#pragma once\n+\n+#include <sys/stat.h>\n+#include <sys/types.h>\n+#include <unistd.h>\n+\n+#include <functional>\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/discovery.h\"\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/rados.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/type_fwd.h\"\n+#include \"arrow/dataset/visibility.h\"\n+#include \"arrow/filesystem/api.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/iterator.h\"\n+#include \"arrow/util/macros.h\"\n+\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/exception.h\"\n+\n+namespace arrow {\n+namespace dataset {\n+\n+class ARROW_DS_EXPORT RadosCluster {\n+ public:\n+  struct RadosConnectionCtx {\n+    std::string ceph_config_path;\n+    std::string data_pool;\n+    std::string user_name;\n+    std::string cluster_name;\n+    std::string cls_name;\n+  };\n+  explicit RadosCluster(RadosConnectionCtx& ctx)\n+      : ctx(ctx), rados(new RadosWrapper()), ioCtx(new IoCtxWrapper()) {}\n+\n+  ~RadosCluster() { Shutdown(); }\n+\n+  Status Connect() {\n+    if (rados->init2(ctx.user_name.c_str(), ctx.cluster_name.c_str(), 0))\n+      return Status::Invalid(\"librados::init2 returned non-zero exit code.\");\n\nReview comment:\n       It might be simpler if `RadosInterface` returned `Status` instead of `int`.  For example, the compression_xyz.cc files convert from libxyz to Status.  You could also follow the pattern used in hdfs where `hdfs_internal.h` returns int error codes but then it is hidden by `hdfs.h` which exposes everything with `Status`/`Result`.\n\n##########\nFile path: cpp/src/arrow/dataset/rados.h\n##########\n@@ -0,0 +1,132 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <rados/librados.hpp>\n\nReview comment:\n       Ideally this would be included from the `.cc` file any any instance to `ceph::bufferlist` would be replaced by Arrow buffers or arrays.  This helps keep rados out of the Arrow public API (even if the Arrow dist was built with RADOS support).  For example, this is done with our compression utilities and filesystem utilities.  Internal headers can be used if you need templates (though I don't see any here).\n\n##########\nFile path: python/pyarrow/_rados.pxd\n##########\n@@ -0,0 +1,38 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       I wonder if we need a brand new module here or not.  For example, it could be in the dataset module and then it could be an error at use time if someone tries to use it.  Currently the parquet file format and csv file format live in the dataset module (even though there is already a parquet module and csv module).  However, those two are generally guaranteed to be installed.  I'm not sure which pattern to follow for rados.\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/scripts/splitter.py\n##########\n@@ -0,0 +1,32 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       I think this is more of a \"skyhook\" utility and less of a \"Arrow utility\".  It maybe shouldn't be included in this PR.\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/docs/architecture.md\n##########\n@@ -0,0 +1,41 @@\n+<!---\n\nReview comment:\n       This document is very good for helping to understand the intent of the PR but I'm not sure if we want this, as written, to be merged in.  I don't think your goal is to embed the Skyhook project inside of Arrow correct?  I think the goal is only to include the Arrow-Rados integration (and the Arrow Ceph object class)?  I may be misunderstanding things too.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-28T01:42:20.478+0000",
                    "updated": "2021-07-28T01:42:20.478+0000",
                    "started": "2021-07-28T01:42:20.478+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "628822",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/630479",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#issuecomment-888255960\n\n\n   @westonpace Thanks a lot for taking the time to review the PR. I wanted to let you know that I have just updated the Pull request branch with the latest changes we had in our development branch (in our fork). The main change that you would see now as compared to the last time you looked at the PR, is that we changed the `RadosParquetFileFormat` name to `SkyhookFileFormat`, which would act as an abstraction to offload into Ceph any real file format supported by Arrow. Currently, `SkyhookFileFormat` supports LZ4 compressed IPC (feather) and Parquet. \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-28T12:34:34.538+0000",
                    "updated": "2021-07-28T12:34:34.538+0000",
                    "started": "2021-07-28T12:34:34.538+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "630479",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/630480",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub edited a comment on pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#issuecomment-888255960\n\n\n   @westonpace Thanks a lot for taking the time to review the PR. I wanted to let you know that I have just updated the Pull request branch with the latest changes we had in our development branch (in our fork). The main change that you would see now as compared to the last time you looked at the PR, is that we changed the `RadosParquetFileFormat` name to `SkyhookFileFormat`, which would act as an abstraction to offload into Ceph any real file format supported by Arrow. Currently, `SkyhookFileFormat` supports LZ4 compressed IPC (feather) and Parquet. \r\n   \r\n   > The approach of extending ParquetFileFormat seems like it might lead to duplicate code since you'd need a RadosXyzFileFormat for every fragment. Instead would it be possible to have RadosFragment which takes in (via the constructor) a shared_ptr to a FileFormat? The FileFormat could then be serialized (which format to use and read options) and sent as part of the rados scan request.\r\n   \r\n   I think the `SkyhookFileFormat` addresses this problem probably.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-28T12:34:37.813+0000",
                    "updated": "2021-07-28T12:34:37.813+0000",
                    "started": "2021-07-28T12:34:37.813+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "630480",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/630707",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "carlosmalt commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r678561604\n\n\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/docs/architecture.md\n##########\n@@ -0,0 +1,41 @@\n+<!---\n\nReview comment:\n       @westonpace Thank you very much for your review. Since the beginning of this year the Skyhook project _is_ the Arrow-RADOS integration. They have become the same because we discovered that Arrow had already all the parts we needed. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-28T18:40:29.235+0000",
                    "updated": "2021-07-28T18:40:29.235+0000",
                    "started": "2021-07-28T18:40:29.234+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "630707",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631121",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r679120482\n\n\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/scripts/splitter.py\n##########\n@@ -0,0 +1,32 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       Fixed\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-29T12:51:33.923+0000",
                    "updated": "2021-07-29T12:51:33.923+0000",
                    "started": "2021-07-29T12:51:33.923+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631121",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631164",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "carlosmalt commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r679203620\n\n\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/docs/architecture.md\n##########\n@@ -0,0 +1,41 @@\n+<!---\n\nReview comment:\n       @westonpace After discussing this with @JayjeetAtGithub, I confirm that the goal is to only include the Arrow-RADOS integration (including the Arrow Ceph object class). Jayjeet will update [the architecture document](https://github.com/uccross/skyhookdm-arrow/blob/rados-parquet-pr/cpp/src/arrow/adapters/arrow-rados-cls/docs/architecture.md) to highlight the parts that will be part of this pull request. The current plan is to put all other parts into a separate repository that will have Arrow as a submodule.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-29T14:28:01.047+0000",
                    "updated": "2021-07-29T14:28:01.047+0000",
                    "started": "2021-07-29T14:28:01.047+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631164",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631234",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r679350853\n\n\n\n##########\nFile path: ci/scripts/cpp_build.sh\n##########\n@@ -85,6 +85,8 @@ cmake -G \"${CMAKE_GENERATOR:-Ninja}\" \\\n       -DARROW_PYTHON=${ARROW_PYTHON:-OFF} \\\n       -DARROW_RUNTIME_SIMD_LEVEL=${ARROW_RUNTIME_SIMD_LEVEL:-MAX} \\\n       -DARROW_S3=${ARROW_S3:-OFF} \\\n+      -DARROW_RADOS=${ARROW_RADOS:-OFF} \\\n+      -DARROW_CLS=${ARROW_CLS:-OFF} \\\n\nReview comment:\n       @westonpace Since the API's we added are named as Skyhook**, can we name the flag as `ARROW_SKYHOOK` ?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-29T17:29:43.736+0000",
                    "updated": "2021-07-29T17:29:43.736+0000",
                    "started": "2021-07-29T17:29:43.735+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631234",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631235",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r679352904\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/rados.cc\n##########\n@@ -0,0 +1,75 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       Maybe we can put this in an internal file like `file_skyhook_internal.h` which would contain this along with the RadosConnect interface ?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-29T17:32:41.481+0000",
                    "updated": "2021-07-29T17:32:41.481+0000",
                    "started": "2021-07-29T17:32:41.481+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631235",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631236",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r679354107\n\n\n\n##########\nFile path: format/ScanRequest.fbs\n##########\n@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       Okay, let's wait. In general, the intention is to add a generic flatbuffer schema that can serialize some scan request parameters to be sent through the network. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-29T17:34:26.911+0000",
                    "updated": "2021-07-29T17:34:26.911+0000",
                    "started": "2021-07-29T17:34:26.910+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631236",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631345",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r679476587\n\n\n\n##########\nFile path: ci/scripts/cpp_build.sh\n##########\n@@ -85,6 +85,8 @@ cmake -G \"${CMAKE_GENERATOR:-Ninja}\" \\\n       -DARROW_PYTHON=${ARROW_PYTHON:-OFF} \\\n       -DARROW_RUNTIME_SIMD_LEVEL=${ARROW_RUNTIME_SIMD_LEVEL:-MAX} \\\n       -DARROW_S3=${ARROW_S3:-OFF} \\\n+      -DARROW_RADOS=${ARROW_RADOS:-OFF} \\\n+      -DARROW_CLS=${ARROW_CLS:-OFF} \\\n\nReview comment:\n       I'm pretty sure I understand where you are coming from.  Thinking on this more I think it boils down to the idea of hosting a separate tool (Skyhook) in Arrow.  I'm going to raise this on the mailing list for clarification as I'm not sure we have a clear precedent.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-29T20:54:49.894+0000",
                    "updated": "2021-07-29T20:54:49.894+0000",
                    "started": "2021-07-29T20:54:49.894+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631345",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631927",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r680278216\n\n\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n\nReview comment:\n       return `Status::NotImplemented`?\n\n##########\nFile path: ci/scripts/cpp_build.sh\n##########\n@@ -85,6 +85,8 @@ cmake -G \"${CMAKE_GENERATOR:-Ninja}\" \\\n       -DARROW_PYTHON=${ARROW_PYTHON:-OFF} \\\n       -DARROW_RUNTIME_SIMD_LEVEL=${ARROW_RUNTIME_SIMD_LEVEL:-MAX} \\\n       -DARROW_S3=${ARROW_S3:-OFF} \\\n+      -DARROW_RADOS=${ARROW_RADOS:-OFF} \\\n+      -DARROW_CLS=${ARROW_CLS:-OFF} \\\n\nReview comment:\n       Ok, coming back to this now that there has been a bit of discussion.  Yes, I think you can name it `ARROW_SKYHOOK`.  In the future it might be extracted into its own build file.\n\n##########\nFile path: .travis.yml\n##########\n@@ -79,6 +79,44 @@ jobs:\n         ARCH: arm64v8\n         ARROW_CI_MODULES: \"GO\"\n         DOCKER_IMAGE_ID: debian-go\n+    \n\nReview comment:\n       Ideally we would just be able to add ARROW_RADOS=ON to the existing Travis build.  It appears you have identified two issues with the current build (S3 and no-cache leading to failing / timed out Travis builds).  I don't think these are unique to this PR.  It might be a good idea (and very helpful use of your good work) to raise these issues through a separate JIRA and get them resolved on the main repo and then there is no need for a separate build job here.\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n+    }\n+    return std::make_shared<arrow::Buffer>(\"\");\n+  }\n+\n+  /// Read a specified number of bytes from the current position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> Read(int64_t nbytes) {\n+    ARROW_ASSIGN_OR_RAISE(auto buffer, ReadAt(pos_, nbytes));\n+    pos_ += buffer->size();\n+    return std::move(buffer);\n+  }\n+\n+  /// Read a specified number of bytes from the current position into an output stream.\n+  arrow::Result<int64_t> Read(int64_t nbytes, void* out) {\n+    ARROW_ASSIGN_OR_RAISE(int64_t bytes_read, ReadAt(pos_, nbytes, out));\n+    pos_ += bytes_read;\n+    return bytes_read;\n+  }\n+\n+  /// Return the size of the file.\n+  arrow::Result<int64_t> GetSize() {\n+    RETURN_NOT_OK(CheckClosed());\n+    return content_length_;\n+  }\n+\n+  /// Sets the file-pointer offset, measured from the beginning of the\n+  /// file, at which the next read or write occurs.\n+  arrow::Status Seek(int64_t position) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Returns the file-pointer offset.\n+  arrow::Result<int64_t> Tell() const {\n+    RETURN_NOT_OK(CheckClosed());\n+    return pos_;\n+  }\n+\n+  /// Closes the file stream and deletes the chunks and releases the memory\n+  /// used by the chunks.\n+  arrow::Status Close() {\n+    closed_ = true;\n+    for (auto chunk : chunks_) {\n+      delete chunk;\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  bool closed() const { return closed_; }\n+\n+ protected:\n+  cls_method_context_t hctx_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+  std::vector<ceph::bufferlist*> chunks_;\n+};\n+\n+/// \\brief Scan RADOS objects containing Arrow IPC data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanIpcObject(cls_method_context_t hctx,\n+                                   arrow::compute::Expression filter,\n+                                   arrow::compute::Expression partition_expression,\n+                                   std::shared_ptr<arrow::Schema> projection_schema,\n+                                   std::shared_ptr<arrow::Schema> dataset_schema,\n+                                   std::shared_ptr<arrow::Table>& result_table,\n+                                   int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file, arrow::Compression::LZ4_FRAME);\n+\n+  auto format = std::make_shared<arrow::dataset::IpcFileFormat>();\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief Scan RADOS objects containing Parquet binary data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanParquetObject(cls_method_context_t hctx,\n+                                       arrow::compute::Expression filter,\n+                                       arrow::compute::Expression partition_expression,\n+                                       std::shared_ptr<arrow::Schema> projection_schema,\n+                                       std::shared_ptr<arrow::Schema> dataset_schema,\n+                                       std::shared_ptr<arrow::Table>& result_table,\n\nReview comment:\n       No mutable references.\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n+    }\n+    return std::make_shared<arrow::Buffer>(\"\");\n+  }\n+\n+  /// Read a specified number of bytes from the current position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> Read(int64_t nbytes) {\n+    ARROW_ASSIGN_OR_RAISE(auto buffer, ReadAt(pos_, nbytes));\n+    pos_ += buffer->size();\n+    return std::move(buffer);\n+  }\n+\n+  /// Read a specified number of bytes from the current position into an output stream.\n+  arrow::Result<int64_t> Read(int64_t nbytes, void* out) {\n+    ARROW_ASSIGN_OR_RAISE(int64_t bytes_read, ReadAt(pos_, nbytes, out));\n+    pos_ += bytes_read;\n+    return bytes_read;\n+  }\n+\n+  /// Return the size of the file.\n+  arrow::Result<int64_t> GetSize() {\n+    RETURN_NOT_OK(CheckClosed());\n+    return content_length_;\n+  }\n+\n+  /// Sets the file-pointer offset, measured from the beginning of the\n+  /// file, at which the next read or write occurs.\n+  arrow::Status Seek(int64_t position) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Returns the file-pointer offset.\n+  arrow::Result<int64_t> Tell() const {\n+    RETURN_NOT_OK(CheckClosed());\n+    return pos_;\n+  }\n+\n+  /// Closes the file stream and deletes the chunks and releases the memory\n+  /// used by the chunks.\n+  arrow::Status Close() {\n+    closed_ = true;\n+    for (auto chunk : chunks_) {\n+      delete chunk;\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  bool closed() const { return closed_; }\n+\n+ protected:\n+  cls_method_context_t hctx_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+  std::vector<ceph::bufferlist*> chunks_;\n+};\n+\n+/// \\brief Scan RADOS objects containing Arrow IPC data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanIpcObject(cls_method_context_t hctx,\n+                                   arrow::compute::Expression filter,\n+                                   arrow::compute::Expression partition_expression,\n+                                   std::shared_ptr<arrow::Schema> projection_schema,\n+                                   std::shared_ptr<arrow::Schema> dataset_schema,\n+                                   std::shared_ptr<arrow::Table>& result_table,\n\nReview comment:\n       Style guide prohibits mutable references.  Use `std::shared_ptr<arrow::Table>* result_table` instead.\n\n##########\nFile path: .github/workflows/docs.yml\n##########\n@@ -0,0 +1,57 @@\n+# # Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       Just a reminder to revert this at some point\n\n##########\nFile path: ci/scripts/integration_ceph.sh\n##########\n@@ -0,0 +1,129 @@\n+#!/usr/bin/env bash\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+set -e\n+set -x\n+set -u\n+\n+ARROW_BUILD_DIR=${1}/cpp\n+DIR=/tmp/\n+\n+# reset\n+pkill ceph || true\n+rm -rf ${DIR}/*\n+LOG_DIR=${DIR}/log\n+MON_DATA=${DIR}/mon\n+MDS_DATA=${DIR}/mds\n+MOUNTPT=${MDS_DATA}/mnt\n+OSD_DATA=${DIR}/osd\n+mkdir ${LOG_DIR} ${MON_DATA} ${OSD_DATA} ${MDS_DATA} ${MOUNTPT}\n+MDS_NAME=\"Z\"\n+MON_NAME=\"a\"\n+MGR_NAME=\"x\"\n+MIRROR_ID=\"m\"\n+\n+# cluster wide parameters\n+cat >> ${DIR}/ceph.conf <<EOF\n+[global]\n+fsid = $(uuidgen)\n+osd crush chooseleaf type = 0\n+run dir = ${DIR}/run\n+auth cluster required = none\n+auth service required = none\n+auth client required = none\n+osd pool default size = 1\n+mon host = ${HOSTNAME}\n+[mds.${MDS_NAME}]\n+host = ${HOSTNAME}\n+[mon.${MON_NAME}]\n+log file = ${LOG_DIR}/mon.log\n+chdir = \"\"\n+mon cluster log file = ${LOG_DIR}/mon-cluster.log\n+mon data = ${MON_DATA}\n+mon data avail crit = 0\n+mon addr = ${HOSTNAME}\n+mon allow pool delete = true\n+[osd.0]\n+log file = ${LOG_DIR}/osd.log\n+chdir = \"\"\n+osd data = ${OSD_DATA}\n+osd journal = ${OSD_DATA}.journal\n+osd journal size = 100\n+osd objectstore = memstore\n+osd class load list = *\n+osd class default list = *\n+EOF\n+\n+export CEPH_CONF=${DIR}/ceph.conf\n+cp $CEPH_CONF /etc/ceph/ceph.conf\n+\n+# start an osd\n+ceph-mon --id ${MON_NAME} --mkfs --keyring /dev/null\n+touch ${MON_DATA}/keyring\n+ceph-mon --id ${MON_NAME}\n+\n+# start an osd\n+OSD_ID=$(ceph osd create)\n+ceph osd crush add osd.${OSD_ID} 1 root=default\n+ceph-osd --id ${OSD_ID} --mkjournal --mkfs\n+ceph-osd --id ${OSD_ID} || ceph-osd --id ${OSD_ID} || ceph-osd --id ${OSD_ID}\n+\n+# start an mds for cephfs\n+ceph auth get-or-create mds.${MDS_NAME} mon 'profile mds' mgr 'profile mds' mds 'allow *' osd 'allow *' > ${MDS_DATA}/keyring\n+ceph osd pool create cephfs_data 8\n+ceph osd pool create cephfs_metadata 8\n+ceph fs new cephfs cephfs_metadata cephfs_data\n+ceph fs ls\n+ceph-mds -i ${MDS_NAME}\n+ceph status\n+while [[ ! $(ceph mds stat | grep \"up:active\") ]]; do sleep 1; done\n+\n+# start a manager\n+ceph-mgr --id ${MGR_NAME}\n+\n+# test the setup\n+ceph --version\n+ceph status\n+\n+apt update\n+apt install -y ceph-fuse attr\n+\n+pushd ${ARROW_BUILD_DIR}\n+    # create the rados-classes, if not there already\n+    mkdir -p /usr/lib/x86_64-linux-gnu/rados-classes/\n+    cp debug/libcls_arrow* /usr/lib/x86_64-linux-gnu/rados-classes/\n+\n+    # mount a ceph filesystem to /mnt/cephfs in the user-space using ceph-fuse\n+    mkdir -p /mnt/cephfs\n+    ceph-fuse /mnt/cephfs\n+    sleep 5\n+\n+    # download an example dataset and copy into the mounted dir\n+    rm -rf nyc*\n+    wget https://raw.githubusercontent.com/JayjeetAtGithub/zips/main/nyc.zip\n\nReview comment:\n       We'll probably want to find a better place for this data to live.  I've started a topic on Zulip (https://ursalabs.zulipchat.com/#narrow/stream/180245-dev/topic/Data.20storage.20for.20CI/near/247775557) to ask where the best place to store such test data might be.\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n+    }\n+    return std::make_shared<arrow::Buffer>(\"\");\n+  }\n+\n+  /// Read a specified number of bytes from the current position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> Read(int64_t nbytes) {\n+    ARROW_ASSIGN_OR_RAISE(auto buffer, ReadAt(pos_, nbytes));\n+    pos_ += buffer->size();\n+    return std::move(buffer);\n+  }\n+\n+  /// Read a specified number of bytes from the current position into an output stream.\n+  arrow::Result<int64_t> Read(int64_t nbytes, void* out) {\n+    ARROW_ASSIGN_OR_RAISE(int64_t bytes_read, ReadAt(pos_, nbytes, out));\n+    pos_ += bytes_read;\n+    return bytes_read;\n+  }\n+\n+  /// Return the size of the file.\n+  arrow::Result<int64_t> GetSize() {\n+    RETURN_NOT_OK(CheckClosed());\n+    return content_length_;\n+  }\n+\n+  /// Sets the file-pointer offset, measured from the beginning of the\n+  /// file, at which the next read or write occurs.\n+  arrow::Status Seek(int64_t position) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Returns the file-pointer offset.\n+  arrow::Result<int64_t> Tell() const {\n+    RETURN_NOT_OK(CheckClosed());\n+    return pos_;\n+  }\n+\n+  /// Closes the file stream and deletes the chunks and releases the memory\n+  /// used by the chunks.\n+  arrow::Status Close() {\n+    closed_ = true;\n+    for (auto chunk : chunks_) {\n+      delete chunk;\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  bool closed() const { return closed_; }\n+\n+ protected:\n+  cls_method_context_t hctx_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+  std::vector<ceph::bufferlist*> chunks_;\n+};\n+\n+/// \\brief Scan RADOS objects containing Arrow IPC data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanIpcObject(cls_method_context_t hctx,\n+                                   arrow::compute::Expression filter,\n+                                   arrow::compute::Expression partition_expression,\n+                                   std::shared_ptr<arrow::Schema> projection_schema,\n+                                   std::shared_ptr<arrow::Schema> dataset_schema,\n+                                   std::shared_ptr<arrow::Table>& result_table,\n+                                   int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file, arrow::Compression::LZ4_FRAME);\n+\n+  auto format = std::make_shared<arrow::dataset::IpcFileFormat>();\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief Scan RADOS objects containing Parquet binary data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanParquetObject(cls_method_context_t hctx,\n+                                       arrow::compute::Expression filter,\n+                                       arrow::compute::Expression partition_expression,\n+                                       std::shared_ptr<arrow::Schema> projection_schema,\n+                                       std::shared_ptr<arrow::Schema> dataset_schema,\n+                                       std::shared_ptr<arrow::Table>& result_table,\n+                                       int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file);\n+\n+  auto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n+\n+  auto fragment_scan_options =\n+      std::make_shared<arrow::dataset::ParquetFragmentScanOptions>();\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n\nReview comment:\n       Why set `UseThreads` to `false`?\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n\nReview comment:\n       ```suggestion\r\n     /// Read a specified number of bytes from a specified position.\r\n   ```\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n+    }\n+    return std::make_shared<arrow::Buffer>(\"\");\n+  }\n+\n+  /// Read a specified number of bytes from the current position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> Read(int64_t nbytes) {\n+    ARROW_ASSIGN_OR_RAISE(auto buffer, ReadAt(pos_, nbytes));\n+    pos_ += buffer->size();\n+    return std::move(buffer);\n+  }\n+\n+  /// Read a specified number of bytes from the current position into an output stream.\n+  arrow::Result<int64_t> Read(int64_t nbytes, void* out) {\n+    ARROW_ASSIGN_OR_RAISE(int64_t bytes_read, ReadAt(pos_, nbytes, out));\n+    pos_ += bytes_read;\n+    return bytes_read;\n+  }\n+\n+  /// Return the size of the file.\n+  arrow::Result<int64_t> GetSize() {\n+    RETURN_NOT_OK(CheckClosed());\n+    return content_length_;\n+  }\n+\n+  /// Sets the file-pointer offset, measured from the beginning of the\n+  /// file, at which the next read or write occurs.\n+  arrow::Status Seek(int64_t position) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Returns the file-pointer offset.\n+  arrow::Result<int64_t> Tell() const {\n+    RETURN_NOT_OK(CheckClosed());\n+    return pos_;\n+  }\n+\n+  /// Closes the file stream and deletes the chunks and releases the memory\n+  /// used by the chunks.\n+  arrow::Status Close() {\n+    closed_ = true;\n+    for (auto chunk : chunks_) {\n+      delete chunk;\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  bool closed() const { return closed_; }\n+\n+ protected:\n+  cls_method_context_t hctx_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+  std::vector<ceph::bufferlist*> chunks_;\n+};\n+\n+/// \\brief Scan RADOS objects containing Arrow IPC data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanIpcObject(cls_method_context_t hctx,\n+                                   arrow::compute::Expression filter,\n+                                   arrow::compute::Expression partition_expression,\n+                                   std::shared_ptr<arrow::Schema> projection_schema,\n+                                   std::shared_ptr<arrow::Schema> dataset_schema,\n+                                   std::shared_ptr<arrow::Table>& result_table,\n+                                   int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file, arrow::Compression::LZ4_FRAME);\n+\n+  auto format = std::make_shared<arrow::dataset::IpcFileFormat>();\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief Scan RADOS objects containing Parquet binary data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanParquetObject(cls_method_context_t hctx,\n\nReview comment:\n       Seems like there is some overlap between `ScanParquetObject` and `ScanIpcObject` you could lift into a common helper method.\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n\nReview comment:\n       You should ensure you `Close` the file in `~RandomAccessObject` for safety and maintainability.\n\n##########\nFile path: ci/scripts/integration_ceph.sh\n##########\n@@ -0,0 +1,129 @@\n+#!/usr/bin/env bash\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+set -e\n+set -x\n+set -u\n+\n+ARROW_BUILD_DIR=${1}/cpp\n+DIR=/tmp/\n\nReview comment:\n       Given the below `rm -rf ${DIR}/*` command can we maybe use `/tmp/integration-ceph` or something?\n\n##########\nFile path: format/ScanRequest.fbs\n##########\n@@ -0,0 +1,33 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       So here is my current understanding.  Let me know if this seems off.  There are two pieces to this.\r\n   \r\n   There is a ceph object class (called Skyhook?) which processes scan tasks and lives in a \"contrib\" directory.\r\n   \r\n   There is a fragment / file format for Arrow that understands how to send scan requests to a ceph storage server in the skyhook format.\r\n   \r\n   These two components aren't tightly coupled.  The only source of agreement is the Arrow columnar format and this flatbuffers file.  So for example (these are thought exercises, not things that will necessarily ever happen):\r\n   \r\n   * Ceph could be running an older version of Skyhook built with Arrow version X and the dataset client could be running a newer version of Arrow version X+N.\r\n   * Skyhook could switch to some other library entirely in the future and as long as it continued to respect the flatbuffers format it would continue to work.\r\n   * A different non-arrow library (or an Arrow implementation in a different language) could decide to start sending requests to Skyhook and as long as they agreed upon the flatbuffers and arrow columnar format everything would continue to work.\r\n   \r\n   Given the above I think the proper place for this flatbuffers file to live is in the same directory as the ceph object class.  This flatbuffers file is the API for skyhook.\r\n   \r\n   Then, for building everything, the make files for that directory could produce two artifacts: A ceph object class and a small C++ \"client library\" which is just the output of the flatbuffers compiler.\r\n   \r\n   Or you could skip the \"client library\" step and add an extra build step for the datasets module which runs the flatbuffers compiler.\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n+    }\n+    return std::make_shared<arrow::Buffer>(\"\");\n+  }\n+\n+  /// Read a specified number of bytes from the current position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> Read(int64_t nbytes) {\n+    ARROW_ASSIGN_OR_RAISE(auto buffer, ReadAt(pos_, nbytes));\n+    pos_ += buffer->size();\n+    return std::move(buffer);\n+  }\n+\n+  /// Read a specified number of bytes from the current position into an output stream.\n+  arrow::Result<int64_t> Read(int64_t nbytes, void* out) {\n+    ARROW_ASSIGN_OR_RAISE(int64_t bytes_read, ReadAt(pos_, nbytes, out));\n+    pos_ += bytes_read;\n+    return bytes_read;\n+  }\n+\n+  /// Return the size of the file.\n+  arrow::Result<int64_t> GetSize() {\n+    RETURN_NOT_OK(CheckClosed());\n+    return content_length_;\n+  }\n+\n+  /// Sets the file-pointer offset, measured from the beginning of the\n+  /// file, at which the next read or write occurs.\n+  arrow::Status Seek(int64_t position) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Returns the file-pointer offset.\n+  arrow::Result<int64_t> Tell() const {\n+    RETURN_NOT_OK(CheckClosed());\n+    return pos_;\n+  }\n+\n+  /// Closes the file stream and deletes the chunks and releases the memory\n+  /// used by the chunks.\n+  arrow::Status Close() {\n+    closed_ = true;\n+    for (auto chunk : chunks_) {\n+      delete chunk;\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  bool closed() const { return closed_; }\n+\n+ protected:\n+  cls_method_context_t hctx_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+  std::vector<ceph::bufferlist*> chunks_;\n+};\n+\n+/// \\brief Scan RADOS objects containing Arrow IPC data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanIpcObject(cls_method_context_t hctx,\n+                                   arrow::compute::Expression filter,\n+                                   arrow::compute::Expression partition_expression,\n+                                   std::shared_ptr<arrow::Schema> projection_schema,\n+                                   std::shared_ptr<arrow::Schema> dataset_schema,\n+                                   std::shared_ptr<arrow::Table>& result_table,\n+                                   int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file, arrow::Compression::LZ4_FRAME);\n+\n+  auto format = std::make_shared<arrow::dataset::IpcFileFormat>();\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief Scan RADOS objects containing Parquet binary data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanParquetObject(cls_method_context_t hctx,\n+                                       arrow::compute::Expression filter,\n+                                       arrow::compute::Expression partition_expression,\n+                                       std::shared_ptr<arrow::Schema> projection_schema,\n+                                       std::shared_ptr<arrow::Schema> dataset_schema,\n+                                       std::shared_ptr<arrow::Table>& result_table,\n+                                       int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file);\n+\n+  auto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n+\n+  auto fragment_scan_options =\n+      std::make_shared<arrow::dataset::ParquetFragmentScanOptions>();\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+  ARROW_RETURN_NOT_OK(builder->FragmentScanOptions(fragment_scan_options));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief The scanning operation to register on Ceph nodes. The request is\n+/// deserialized, the object is scanned, and the resulting table is serialized\n+/// and sent to the client.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] in Input bufferlist.\n+/// \\param[out] out Output bufferlist.\n+/// \\return Status code.\n+static int scan_op(cls_method_context_t hctx, ceph::bufferlist* in,\n+                   ceph::bufferlist* out) {\n+  // Components required to construct a File fragment.\n+  arrow::Status s;\n+  arrow::compute::Expression filter;\n+  arrow::compute::Expression partition_expression;\n+  std::shared_ptr<arrow::Schema> projection_schema;\n+  std::shared_ptr<arrow::Schema> dataset_schema;\n+  int64_t file_size;\n+  int file_format = 0;  // 0 = Parquet, 1 = Ipc\n+\n+  // Deserialize the scan request\n+  if (!(s = arrow::dataset::DeserializeScanRequest(&filter, &partition_expression,\n\nReview comment:\n       I'm not sure you should need anything called a projection schema.  I think you just need to load the materialized fields (from scan options).\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n+    }\n+    return std::make_shared<arrow::Buffer>(\"\");\n+  }\n+\n+  /// Read a specified number of bytes from the current position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> Read(int64_t nbytes) {\n+    ARROW_ASSIGN_OR_RAISE(auto buffer, ReadAt(pos_, nbytes));\n+    pos_ += buffer->size();\n+    return std::move(buffer);\n+  }\n+\n+  /// Read a specified number of bytes from the current position into an output stream.\n+  arrow::Result<int64_t> Read(int64_t nbytes, void* out) {\n+    ARROW_ASSIGN_OR_RAISE(int64_t bytes_read, ReadAt(pos_, nbytes, out));\n+    pos_ += bytes_read;\n+    return bytes_read;\n+  }\n+\n+  /// Return the size of the file.\n+  arrow::Result<int64_t> GetSize() {\n+    RETURN_NOT_OK(CheckClosed());\n+    return content_length_;\n+  }\n+\n+  /// Sets the file-pointer offset, measured from the beginning of the\n+  /// file, at which the next read or write occurs.\n+  arrow::Status Seek(int64_t position) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Returns the file-pointer offset.\n+  arrow::Result<int64_t> Tell() const {\n+    RETURN_NOT_OK(CheckClosed());\n+    return pos_;\n+  }\n+\n+  /// Closes the file stream and deletes the chunks and releases the memory\n+  /// used by the chunks.\n+  arrow::Status Close() {\n+    closed_ = true;\n+    for (auto chunk : chunks_) {\n+      delete chunk;\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  bool closed() const { return closed_; }\n+\n+ protected:\n+  cls_method_context_t hctx_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+  std::vector<ceph::bufferlist*> chunks_;\n+};\n+\n+/// \\brief Scan RADOS objects containing Arrow IPC data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanIpcObject(cls_method_context_t hctx,\n+                                   arrow::compute::Expression filter,\n+                                   arrow::compute::Expression partition_expression,\n+                                   std::shared_ptr<arrow::Schema> projection_schema,\n+                                   std::shared_ptr<arrow::Schema> dataset_schema,\n+                                   std::shared_ptr<arrow::Table>& result_table,\n+                                   int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file, arrow::Compression::LZ4_FRAME);\n+\n+  auto format = std::make_shared<arrow::dataset::IpcFileFormat>();\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief Scan RADOS objects containing Parquet binary data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanParquetObject(cls_method_context_t hctx,\n+                                       arrow::compute::Expression filter,\n+                                       arrow::compute::Expression partition_expression,\n+                                       std::shared_ptr<arrow::Schema> projection_schema,\n+                                       std::shared_ptr<arrow::Schema> dataset_schema,\n+                                       std::shared_ptr<arrow::Table>& result_table,\n+                                       int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file);\n+\n+  auto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n+\n+  auto fragment_scan_options =\n+      std::make_shared<arrow::dataset::ParquetFragmentScanOptions>();\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+  ARROW_RETURN_NOT_OK(builder->FragmentScanOptions(fragment_scan_options));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief The scanning operation to register on Ceph nodes. The request is\n+/// deserialized, the object is scanned, and the resulting table is serialized\n+/// and sent to the client.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] in Input bufferlist.\n+/// \\param[out] out Output bufferlist.\n+/// \\return Status code.\n+static int scan_op(cls_method_context_t hctx, ceph::bufferlist* in,\n+                   ceph::bufferlist* out) {\n+  // Components required to construct a File fragment.\n+  arrow::Status s;\n+  arrow::compute::Expression filter;\n+  arrow::compute::Expression partition_expression;\n+  std::shared_ptr<arrow::Schema> projection_schema;\n+  std::shared_ptr<arrow::Schema> dataset_schema;\n+  int64_t file_size;\n+  int file_format = 0;  // 0 = Parquet, 1 = Ipc\n+\n+  // Deserialize the scan request\n+  if (!(s = arrow::dataset::DeserializeScanRequest(&filter, &partition_expression,\n+                                                   &projection_schema, &dataset_schema,\n+                                                   file_size, file_format, *in))\n+           .ok()) {\n+    CLS_LOG(0, \"error: %s\", s.message().c_str());\n\nReview comment:\n       Maybe extract this into a helper method `LogArrowError` or something?\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n\nReview comment:\n       I think the approach used elsewhere is to subclass buffer and delete the memory in the constructor.  For example, see `arrow::io::MemoryMappedFile::MemoryMap::Region` in `src/arrow/io/file.cc`.  It seems odd to me that the data would be deleted when the object is closed.\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n+    }\n+    return std::make_shared<arrow::Buffer>(\"\");\n+  }\n+\n+  /// Read a specified number of bytes from the current position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> Read(int64_t nbytes) {\n+    ARROW_ASSIGN_OR_RAISE(auto buffer, ReadAt(pos_, nbytes));\n+    pos_ += buffer->size();\n+    return std::move(buffer);\n+  }\n+\n+  /// Read a specified number of bytes from the current position into an output stream.\n+  arrow::Result<int64_t> Read(int64_t nbytes, void* out) {\n+    ARROW_ASSIGN_OR_RAISE(int64_t bytes_read, ReadAt(pos_, nbytes, out));\n+    pos_ += bytes_read;\n+    return bytes_read;\n+  }\n+\n+  /// Return the size of the file.\n+  arrow::Result<int64_t> GetSize() {\n+    RETURN_NOT_OK(CheckClosed());\n+    return content_length_;\n+  }\n+\n+  /// Sets the file-pointer offset, measured from the beginning of the\n+  /// file, at which the next read or write occurs.\n+  arrow::Status Seek(int64_t position) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Returns the file-pointer offset.\n+  arrow::Result<int64_t> Tell() const {\n+    RETURN_NOT_OK(CheckClosed());\n+    return pos_;\n+  }\n+\n+  /// Closes the file stream and deletes the chunks and releases the memory\n+  /// used by the chunks.\n+  arrow::Status Close() {\n+    closed_ = true;\n+    for (auto chunk : chunks_) {\n+      delete chunk;\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  bool closed() const { return closed_; }\n+\n+ protected:\n+  cls_method_context_t hctx_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+  std::vector<ceph::bufferlist*> chunks_;\n+};\n+\n+/// \\brief Scan RADOS objects containing Arrow IPC data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanIpcObject(cls_method_context_t hctx,\n+                                   arrow::compute::Expression filter,\n+                                   arrow::compute::Expression partition_expression,\n+                                   std::shared_ptr<arrow::Schema> projection_schema,\n+                                   std::shared_ptr<arrow::Schema> dataset_schema,\n+                                   std::shared_ptr<arrow::Table>& result_table,\n+                                   int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file, arrow::Compression::LZ4_FRAME);\n+\n+  auto format = std::make_shared<arrow::dataset::IpcFileFormat>();\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief Scan RADOS objects containing Parquet binary data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanParquetObject(cls_method_context_t hctx,\n+                                       arrow::compute::Expression filter,\n+                                       arrow::compute::Expression partition_expression,\n+                                       std::shared_ptr<arrow::Schema> projection_schema,\n+                                       std::shared_ptr<arrow::Schema> dataset_schema,\n+                                       std::shared_ptr<arrow::Table>& result_table,\n+                                       int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file);\n+\n+  auto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n+\n+  auto fragment_scan_options =\n+      std::make_shared<arrow::dataset::ParquetFragmentScanOptions>();\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+  ARROW_RETURN_NOT_OK(builder->FragmentScanOptions(fragment_scan_options));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief The scanning operation to register on Ceph nodes. The request is\n+/// deserialized, the object is scanned, and the resulting table is serialized\n+/// and sent to the client.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] in Input bufferlist.\n+/// \\param[out] out Output bufferlist.\n+/// \\return Status code.\n+static int scan_op(cls_method_context_t hctx, ceph::bufferlist* in,\n+                   ceph::bufferlist* out) {\n+  // Components required to construct a File fragment.\n+  arrow::Status s;\n+  arrow::compute::Expression filter;\n+  arrow::compute::Expression partition_expression;\n+  std::shared_ptr<arrow::Schema> projection_schema;\n+  std::shared_ptr<arrow::Schema> dataset_schema;\n+  int64_t file_size;\n+  int file_format = 0;  // 0 = Parquet, 1 = Ipc\n+\n+  // Deserialize the scan request\n+  if (!(s = arrow::dataset::DeserializeScanRequest(&filter, &partition_expression,\n+                                                   &projection_schema, &dataset_schema,\n+                                                   file_size, file_format, *in))\n+           .ok()) {\n+    CLS_LOG(0, \"error: %s\", s.message().c_str());\n+    return SCAN_REQ_DESER_ERR_CODE;\n+  }\n+\n+  // Scan the object\n+  std::shared_ptr<arrow::Table> table;\n+  if (file_format == 0) {\n\nReview comment:\n       You might want to consider an enum here instead of magic numbers.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-31T01:33:16.032+0000",
                    "updated": "2021-07-31T01:33:16.032+0000",
                    "started": "2021-07-31T01:33:16.032+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631927",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631935",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r680304011\n\n\n\n##########\nFile path: .github/workflows/docs.yml\n##########\n@@ -0,0 +1,57 @@\n+# # Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       Done\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-31T04:27:22.597+0000",
                    "updated": "2021-07-31T04:27:22.597+0000",
                    "started": "2021-07-31T04:27:22.597+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631935",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631936",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#issuecomment-890289092\n\n\n   > I hope to finish this first pass on Monday but here is a new batch of comments in the meantime.\r\n   \r\n   Thanks a lot\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-31T04:28:11.776+0000",
                    "updated": "2021-07-31T04:28:11.776+0000",
                    "started": "2021-07-31T04:28:11.776+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631936",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/631964",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r680350236\n\n\n\n##########\nFile path: .travis.yml\n##########\n@@ -79,6 +79,44 @@ jobs:\n         ARCH: arm64v8\n         ARROW_CI_MODULES: \"GO\"\n         DOCKER_IMAGE_ID: debian-go\n+    \n\nReview comment:\n       Hi @westonpace, The issues (S3 and no-cache) were a problem in an earlier version of Arrow and the comments are also from an old version of arrow. I believe the changes somehow got carried forward in my PR. I believe they have been already addressed. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-31T11:55:08.029+0000",
                    "updated": "2021-07-31T11:55:08.029+0000",
                    "started": "2021-07-31T11:55:08.028+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "631964",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/632586",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r681240368\n\n\n\n##########\nFile path: cpp/src/arrow/adapters/arrow-rados-cls/cls_arrow.cc\n##########\n@@ -0,0 +1,292 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#define _FILE_OFFSET_BITS 64\n+#include <rados/objclass.h>\n+#include <memory>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"parquet/api/reader.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_RES_SER_ERR_CODE 27\n+\n+CLS_VER(1, 0)\n+CLS_NAME(arrow)\n+\n+cls_handle_t h_class;\n+cls_method_handle_t h_scan_op;\n+\n+/// \\class RandomAccessObject\n+/// \\brief An interface to provide a file-like view over RADOS objects.\n+class RandomAccessObject : public arrow::io::RandomAccessFile {\n+ public:\n+  explicit RandomAccessObject(cls_method_context_t hctx, int64_t file_size) {\n+    hctx_ = hctx;\n+    content_length_ = file_size;\n+    chunks_ = std::vector<ceph::bufferlist*>();\n+  }\n+\n+  /// Check if the file stream is closed.\n+  arrow::Status CheckClosed() const {\n+    if (closed_) {\n+      return arrow::Status::Invalid(\"Operation on closed stream\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Check if the position of the object is valid.\n+  arrow::Status CheckPosition(int64_t position, const char* action) const {\n+    if (position < 0) {\n+      return arrow::Status::Invalid(\"Cannot \", action, \" from negative position\");\n+    }\n+    if (position > content_length_) {\n+      return arrow::Status::IOError(\"Cannot \", action, \" past end of file\");\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  arrow::Result<int64_t> ReadAt(int64_t position, int64_t nbytes, void* out) { return 0; }\n+\n+  /// Read at a specified number of bytes from a specified position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> ReadAt(int64_t position, int64_t nbytes) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"read\"));\n+\n+    // No need to allocate more than the remaining number of bytes\n+    nbytes = std::min(nbytes, content_length_ - position);\n+\n+    if (nbytes > 0) {\n+      ceph::bufferlist* bl = new ceph::bufferlist();\n+      cls_cxx_read(hctx_, position, nbytes, bl);\n+      chunks_.push_back(bl);\n+      return std::make_shared<arrow::Buffer>((uint8_t*)bl->c_str(), bl->length());\n+    }\n+    return std::make_shared<arrow::Buffer>(\"\");\n+  }\n+\n+  /// Read a specified number of bytes from the current position.\n+  arrow::Result<std::shared_ptr<arrow::Buffer>> Read(int64_t nbytes) {\n+    ARROW_ASSIGN_OR_RAISE(auto buffer, ReadAt(pos_, nbytes));\n+    pos_ += buffer->size();\n+    return std::move(buffer);\n+  }\n+\n+  /// Read a specified number of bytes from the current position into an output stream.\n+  arrow::Result<int64_t> Read(int64_t nbytes, void* out) {\n+    ARROW_ASSIGN_OR_RAISE(int64_t bytes_read, ReadAt(pos_, nbytes, out));\n+    pos_ += bytes_read;\n+    return bytes_read;\n+  }\n+\n+  /// Return the size of the file.\n+  arrow::Result<int64_t> GetSize() {\n+    RETURN_NOT_OK(CheckClosed());\n+    return content_length_;\n+  }\n+\n+  /// Sets the file-pointer offset, measured from the beginning of the\n+  /// file, at which the next read or write occurs.\n+  arrow::Status Seek(int64_t position) {\n+    RETURN_NOT_OK(CheckClosed());\n+    RETURN_NOT_OK(CheckPosition(position, \"seek\"));\n+\n+    pos_ = position;\n+    return arrow::Status::OK();\n+  }\n+\n+  /// Returns the file-pointer offset.\n+  arrow::Result<int64_t> Tell() const {\n+    RETURN_NOT_OK(CheckClosed());\n+    return pos_;\n+  }\n+\n+  /// Closes the file stream and deletes the chunks and releases the memory\n+  /// used by the chunks.\n+  arrow::Status Close() {\n+    closed_ = true;\n+    for (auto chunk : chunks_) {\n+      delete chunk;\n+    }\n+    return arrow::Status::OK();\n+  }\n+\n+  bool closed() const { return closed_; }\n+\n+ protected:\n+  cls_method_context_t hctx_;\n+  bool closed_ = false;\n+  int64_t pos_ = 0;\n+  int64_t content_length_ = -1;\n+  std::vector<ceph::bufferlist*> chunks_;\n+};\n+\n+/// \\brief Scan RADOS objects containing Arrow IPC data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanIpcObject(cls_method_context_t hctx,\n+                                   arrow::compute::Expression filter,\n+                                   arrow::compute::Expression partition_expression,\n+                                   std::shared_ptr<arrow::Schema> projection_schema,\n+                                   std::shared_ptr<arrow::Schema> dataset_schema,\n+                                   std::shared_ptr<arrow::Table>& result_table,\n+                                   int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file, arrow::Compression::LZ4_FRAME);\n+\n+  auto format = std::make_shared<arrow::dataset::IpcFileFormat>();\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n+\n+  ARROW_ASSIGN_OR_RAISE(auto scanner, builder->Finish());\n+  ARROW_ASSIGN_OR_RAISE(auto table, scanner->ToTable());\n+\n+  result_table = table;\n+\n+  ARROW_RETURN_NOT_OK(file->Close());\n+  return arrow::Status::OK();\n+}\n+\n+/// \\brief Scan RADOS objects containing Parquet binary data.\n+/// \\param[in] hctx RADOS object context.\n+/// \\param[in] filter The filter expression to apply.\n+/// \\param[in] partition_expression The partition expression to use.\n+/// \\param[in] projection_schema The projection schema.\n+/// \\param[in] dataset_schema The dataset schema.\n+/// \\param[out] result_table Table to store the resultant data.\n+/// \\param[in] object_size The size of the object.\n+/// \\return Status.\n+static arrow::Status ScanParquetObject(cls_method_context_t hctx,\n+                                       arrow::compute::Expression filter,\n+                                       arrow::compute::Expression partition_expression,\n+                                       std::shared_ptr<arrow::Schema> projection_schema,\n+                                       std::shared_ptr<arrow::Schema> dataset_schema,\n+                                       std::shared_ptr<arrow::Table>& result_table,\n+                                       int64_t object_size) {\n+  auto file = std::make_shared<RandomAccessObject>(hctx, object_size);\n+  arrow::dataset::FileSource source(file);\n+\n+  auto format = std::make_shared<arrow::dataset::ParquetFileFormat>();\n+\n+  auto fragment_scan_options =\n+      std::make_shared<arrow::dataset::ParquetFragmentScanOptions>();\n+\n+  ARROW_ASSIGN_OR_RAISE(auto fragment,\n+                        format->MakeFragment(source, partition_expression));\n+  auto options = std::make_shared<arrow::dataset::ScanOptions>();\n+  auto builder =\n+      std::make_shared<arrow::dataset::ScannerBuilder>(dataset_schema, fragment, options);\n+\n+  ARROW_RETURN_NOT_OK(builder->Filter(filter));\n+  ARROW_RETURN_NOT_OK(builder->Project(projection_schema->field_names()));\n+  ARROW_RETURN_NOT_OK(builder->UseThreads(false));\n\nReview comment:\n       We can use threads. That boosts up performance using by more CPU.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-02T20:10:50.367+0000",
                    "updated": "2021-08-02T20:10:50.367+0000",
                    "started": "2021-08-02T20:10:50.367+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "632586",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/632630",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r681318901\n\n\n\n##########\nFile path: cpp/src/arrow/dataset/file_skyhook.h\n##########\n@@ -0,0 +1,275 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// This API is EXPERIMENTAL.\n+#define _FILE_OFFSET_BITS 64\n+\n+#pragma once\n+\n+#include <sys/stat.h>\n+#include <sys/types.h>\n+#include <unistd.h>\n+\n+#include <functional>\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/discovery.h\"\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/rados.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/type_fwd.h\"\n+#include \"arrow/dataset/visibility.h\"\n+#include \"arrow/filesystem/api.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/iterator.h\"\n+#include \"arrow/util/macros.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/exception.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_ERR_MSG \"failed to scan file fragment\"\n+\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_REQ_DESER_ERR_MSG \"failed to deserialize scan request\"\n+\n+#define SCAN_RES_SER_ERR_CODE 27\n+#define SCAN_RES_SER_ERR_MSG \"failed to serialize result table\"\n+\n+namespace arrow {\n+namespace dataset {\n+\n+enum SkyhookFileType { PARQUET, IPC };\n+\n+/// \\addtogroup dataset-file-formats\n+///\n+/// @{\n+\n+namespace connection {\n+/// \\brief An interface for general connections.\n+class ARROW_DS_EXPORT Connection {\n\nReview comment:\n       I'm not sure this is adding much at the moment.  Is the only implementation `RadosConnection`?  If you plan on having multiple implementations then I think the interface could be added back in when that happens.  Otherwise, it is not clear from the code what the future goal is for this type.\n\n##########\nFile path: cpp/src/arrow/dataset/file_skyhook.cc\n##########\n@@ -0,0 +1,280 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#include \"arrow/dataset/file_skyhook.h\"\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset_internal.h\"\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/util_internal.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"arrow/util/iterator.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#include <flatbuffers/flatbuffers.h>\n+\n+#include \"generated/ScanRequest_generated.h\"\n+\n+namespace arrow {\n+\n+namespace flatbuf = org::apache::arrow::flatbuf;\n+\n+namespace dataset {\n+\n+namespace connection {\n+\n+std::mutex connection_mutex;\n+\n+RadosConnection::~RadosConnection() { Shutdown(); }\n+\n+Status RadosConnection::Connect() {\n+  if (connected) {\n+    return Status::OK();\n+  }\n+\n+  // Locks the mutex. Only one thread can pass here at a time.\n+  // Another thread handled the connection already.\n+  std::unique_lock<std::mutex> lock(connection_mutex);\n+  if (connected) {\n+    return Status::OK();\n+  }\n+  connected = true;\n+\n+  if (rados->init2(ctx.user_name.c_str(), ctx.cluster_name.c_str(), 0))\n+    return Status::Invalid(\"librados::init2 returned non-zero exit code.\");\n+\n+  if (rados->conf_read_file(ctx.ceph_config_path.c_str()))\n+    return Status::Invalid(\"librados::conf_read_file returned non-zero exit code.\");\n+\n+  if (rados->connect())\n+    return Status::Invalid(\"librados::connect returned non-zero exit code.\");\n+\n+  if (rados->ioctx_create(ctx.data_pool.c_str(), ioCtx))\n+    return Status::Invalid(\"librados::ioctx_create returned non-zero exit code.\");\n+\n+  return Status::OK();\n+}\n+\n+Status RadosConnection::Shutdown() {\n+  rados->shutdown();\n+  return Status::OK();\n+}\n+\n+}  // namespace connection\n+\n+/// \\brief A ScanTask to scan a file fragment in Skyhook format.\n+class SkyhookScanTask : public ScanTask {\n+ public:\n+  SkyhookScanTask(std::shared_ptr<ScanOptions> options,\n+                  std::shared_ptr<Fragment> fragment, FileSource source,\n+                  std::shared_ptr<SkyhookDirectObjectAccess> doa, int fragment_format)\n+      : ScanTask(std::move(options), std::move(fragment)),\n+        source_(std::move(source)),\n+        doa_(std::move(doa)),\n+        fragment_format_(fragment_format) {}\n+\n+  Result<RecordBatchIterator> Execute() override {\n+    struct stat st {};\n+    ARROW_RETURN_NOT_OK(doa_->Stat(source_.path(), st));\n+\n+    ceph::bufferlist request;\n+    ARROW_RETURN_NOT_OK(\n+        SerializeScanRequest(options_, fragment_format_, st.st_size, request));\n+\n+    ceph::bufferlist result;\n+    ARROW_RETURN_NOT_OK(doa_->Exec(st.st_ino, \"scan_op\", request, result));\n+\n+    RecordBatchVector batches;\n+    ARROW_RETURN_NOT_OK(DeserializeTable(batches, result, !options_->use_threads));\n\nReview comment:\n       Why `!options_->use_threads`?\n\n##########\nFile path: cpp/src/arrow/dataset/scanner_internal.h\n##########\n@@ -185,6 +185,12 @@ inline Result<ScanTaskIterator> GetScanTaskIterator(\n   auto fn = [options](std::shared_ptr<Fragment> fragment) -> Result<ScanTaskIterator> {\n     ARROW_ASSIGN_OR_RAISE(auto scan_task_it, fragment->Scan(options));\n \n+    if (fragment->type_name() == \"skyhook\") {\n\nReview comment:\n       Is Skyhook actually doing projection?  I think filtering is fine but if projection is done early at the format level then I believe that may cause issues when executing datasets that come from a variety of sources (e.g. unions and joins).\n\n##########\nFile path: cpp/src/arrow/dataset/file_skyhook.cc\n##########\n@@ -0,0 +1,280 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+#include \"arrow/dataset/file_skyhook.h\"\n+\n+#include <mutex>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset_internal.h\"\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/file_ipc.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/filesystem/filesystem.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/filesystem/util_internal.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/compression.h\"\n+#include \"arrow/util/iterator.h\"\n+#include \"arrow/util/logging.h\"\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/file_reader.h\"\n+\n+#include <flatbuffers/flatbuffers.h>\n+\n+#include \"generated/ScanRequest_generated.h\"\n+\n+namespace arrow {\n+\n+namespace flatbuf = org::apache::arrow::flatbuf;\n+\n+namespace dataset {\n+\n+namespace connection {\n+\n+std::mutex connection_mutex;\n+\n+RadosConnection::~RadosConnection() { Shutdown(); }\n+\n+Status RadosConnection::Connect() {\n+  if (connected) {\n+    return Status::OK();\n+  }\n+\n+  // Locks the mutex. Only one thread can pass here at a time.\n+  // Another thread handled the connection already.\n+  std::unique_lock<std::mutex> lock(connection_mutex);\n+  if (connected) {\n+    return Status::OK();\n+  }\n+  connected = true;\n+\n+  if (rados->init2(ctx.user_name.c_str(), ctx.cluster_name.c_str(), 0))\n+    return Status::Invalid(\"librados::init2 returned non-zero exit code.\");\n+\n+  if (rados->conf_read_file(ctx.ceph_config_path.c_str()))\n+    return Status::Invalid(\"librados::conf_read_file returned non-zero exit code.\");\n+\n+  if (rados->connect())\n+    return Status::Invalid(\"librados::connect returned non-zero exit code.\");\n+\n+  if (rados->ioctx_create(ctx.data_pool.c_str(), ioCtx))\n+    return Status::Invalid(\"librados::ioctx_create returned non-zero exit code.\");\n+\n+  return Status::OK();\n+}\n+\n+Status RadosConnection::Shutdown() {\n+  rados->shutdown();\n+  return Status::OK();\n+}\n+\n+}  // namespace connection\n+\n+/// \\brief A ScanTask to scan a file fragment in Skyhook format.\n+class SkyhookScanTask : public ScanTask {\n+ public:\n+  SkyhookScanTask(std::shared_ptr<ScanOptions> options,\n+                  std::shared_ptr<Fragment> fragment, FileSource source,\n+                  std::shared_ptr<SkyhookDirectObjectAccess> doa, int fragment_format)\n+      : ScanTask(std::move(options), std::move(fragment)),\n+        source_(std::move(source)),\n+        doa_(std::move(doa)),\n+        fragment_format_(fragment_format) {}\n+\n+  Result<RecordBatchIterator> Execute() override {\n+    struct stat st {};\n+    ARROW_RETURN_NOT_OK(doa_->Stat(source_.path(), st));\n+\n+    ceph::bufferlist request;\n+    ARROW_RETURN_NOT_OK(\n+        SerializeScanRequest(options_, fragment_format_, st.st_size, request));\n+\n+    ceph::bufferlist result;\n+    ARROW_RETURN_NOT_OK(doa_->Exec(st.st_ino, \"scan_op\", request, result));\n+\n+    RecordBatchVector batches;\n+    ARROW_RETURN_NOT_OK(DeserializeTable(batches, result, !options_->use_threads));\n+    return MakeVectorIterator(batches);\n+  }\n+\n+ protected:\n+  FileSource source_;\n+  std::shared_ptr<SkyhookDirectObjectAccess> doa_;\n+  int fragment_format_;\n+};\n+\n+SkyhookFileFormat::SkyhookFileFormat(const std::string& fragment_format,\n+                                     const std::string& ceph_config_path,\n+                                     const std::string& data_pool,\n+                                     const std::string& user_name,\n+                                     const std::string& cluster_name,\n+                                     const std::string& cls_name)\n+    : SkyhookFileFormat(std::make_shared<connection::RadosConnection>(\n+          connection::RadosConnection::RadosConnectionCtx(\n+              ceph_config_path, data_pool, user_name, cluster_name, cls_name))) {\n+  fragment_format_ = fragment_format;\n+}\n+\n+SkyhookFileFormat::SkyhookFileFormat(\n+    const std::shared_ptr<connection::RadosConnection>& connection) {\n+  connection->Connect();\n\nReview comment:\n       I'm not sure what resources are consumed when calling `Connect` but generally I wouldn't expect any connections to be opened just be creating a `SkyhookFileFormat` instance.  It might make more sense to establish the connection on `Inspect` or `ScanFile`.\n\n##########\nFile path: cpp/src/arrow/dataset/file_skyhook.h\n##########\n@@ -0,0 +1,275 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// This API is EXPERIMENTAL.\n+#define _FILE_OFFSET_BITS 64\n+\n+#pragma once\n+\n+#include <sys/stat.h>\n+#include <sys/types.h>\n+#include <unistd.h>\n+\n+#include <functional>\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/discovery.h\"\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/rados.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/type_fwd.h\"\n+#include \"arrow/dataset/visibility.h\"\n+#include \"arrow/filesystem/api.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/iterator.h\"\n+#include \"arrow/util/macros.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/exception.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_ERR_MSG \"failed to scan file fragment\"\n+\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_REQ_DESER_ERR_MSG \"failed to deserialize scan request\"\n+\n+#define SCAN_RES_SER_ERR_CODE 27\n+#define SCAN_RES_SER_ERR_MSG \"failed to serialize result table\"\n+\n+namespace arrow {\n+namespace dataset {\n+\n+enum SkyhookFileType { PARQUET, IPC };\n+\n+/// \\addtogroup dataset-file-formats\n+///\n+/// @{\n+\n+namespace connection {\n+/// \\brief An interface for general connections.\n+class ARROW_DS_EXPORT Connection {\n+ public:\n+  virtual Status Connect() = 0;\n+\n+  Connection() = default;\n+  virtual ~Connection() = default;\n+};\n+\n+/// \\class RadosConnection\n+/// \\brief An interface to connect to a Rados cluster and hold the connection\n+/// information for usage in later stages.\n+class ARROW_DS_EXPORT RadosConnection : public Connection {\n+ public:\n+  struct RadosConnectionCtx {\n+    std::string ceph_config_path;\n+    std::string data_pool;\n+    std::string user_name;\n+    std::string cluster_name;\n+    std::string cls_name;\n+\n+    RadosConnectionCtx(const std::string& ceph_config_path, const std::string& data_pool,\n+                       const std::string& user_name, const std::string& cluster_name,\n+                       const std::string& cls_name)\n+        : ceph_config_path(ceph_config_path),\n+          data_pool(data_pool),\n+          user_name(user_name),\n+          cluster_name(cluster_name),\n+          cls_name(cls_name) {}\n+  };\n+  explicit RadosConnection(const RadosConnectionCtx& ctx)\n+      : Connection(),\n+        ctx(ctx),\n+        rados(new RadosWrapper()),\n+        ioCtx(new IoCtxWrapper()),\n+        connected(false) {}\n+\n+  ~RadosConnection();\n+\n+  /// \\brief Connect to the Rados cluster.\n+  /// \\return Status.\n+  Status Connect();\n+\n+  /// \\brief Shutdown the connection to the Rados cluster.\n+  /// \\return Status.\n+  Status Shutdown();\n+\n+  RadosConnectionCtx ctx;\n+  RadosInterface* rados;\n+  IoCtxInterface* ioCtx;\n+  bool connected;\n+};\n+\n+}  // namespace connection\n+\n+/// \\class SkyhookDirectObjectAccess\n+/// \\brief Interface for translating the name of a file in CephFS to its\n+/// corresponding object ID in RADOS assuming 1:1 mapping between a file\n+/// and its underlying object.\n+class ARROW_DS_EXPORT SkyhookDirectObjectAccess {\n+ public:\n+  explicit SkyhookDirectObjectAccess(\n+      const std::shared_ptr<connection::RadosConnection>& connection)\n+      : connection_(std::move(connection)) {}\n+\n+  /// \\brief Executes the POSIX stat call on a file.\n+  /// \\param[in] path Path of the file.\n+  /// \\param[out] st Refernce to the struct object to store the result.\n+  /// \\return Status.\n+  Status Stat(const std::string& path, struct stat& st) {\n\nReview comment:\n       Since there are no templates involved I think the implementation here should go in a .cc file (preferred) or these functions will need to be marked inline.  Another advantage of moving the implementation to .cc files is that some of the includes (e.g. sstream) might be able to be removed from the header.\n\n##########\nFile path: cpp/src/arrow/dataset/file_skyhook_test.cc\n##########\n@@ -0,0 +1,95 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dataset/file_skyhook.h\"\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/test_util.h\"\n+\n+#define ABORT_ON_FAILURE(expr)                     \\\n\nReview comment:\n       I think we have `ASSERT_OK` (for tests),  `ABORT_NOT_OK` (for scripts and places where error checking isn't allowed) and `DCHECK` (for debug assertions).  I'm not sure we need a new macro.\n\n##########\nFile path: cpp/src/arrow/dataset/discovery.cc\n##########\n@@ -270,7 +270,14 @@ Result<std::shared_ptr<Dataset>> FileSystemDatasetFactory::Finish(FinishOptions\n   for (const auto& info : files_) {\n     auto fixed_path = StripPrefixAndFilename(info.path(), options_.partition_base_dir);\n     ARROW_ASSIGN_OR_RAISE(auto partition, partitioning->Parse(fixed_path));\n-    ARROW_ASSIGN_OR_RAISE(auto fragment, format_->MakeFragment({info, fs_}, partition));\n+    std::shared_ptr<FileFragment> fragment;\n+    if (format_->type_name() == \"skyhook\") {\n\nReview comment:\n       If skyhook has a different implementation it should be handled in the `MakeFragment`'s polymorphism.  This `if` statement shouldn't be needed.\n\n##########\nFile path: cpp/src/arrow/dataset/file_base.h\n##########\n@@ -199,18 +203,23 @@ class ARROW_DS_EXPORT FileFragment : public Fragment {\n   const FileSource& source() const { return source_; }\n   const std::shared_ptr<FileFormat>& format() const { return format_; }\n \n+  const std::shared_ptr<Schema>& dataset_schema() const { return dataset_schema_; }\n+\n  protected:\n   FileFragment(FileSource source, std::shared_ptr<FileFormat> format,\n                compute::Expression partition_expression,\n-               std::shared_ptr<Schema> physical_schema)\n+               std::shared_ptr<Schema> physical_schema,\n+               std::shared_ptr<Schema> dataset_schema)\n       : Fragment(std::move(partition_expression), std::move(physical_schema)),\n         source_(std::move(source)),\n-        format_(std::move(format)) {}\n+        format_(std::move(format)),\n+        dataset_schema_(std::move(dataset_schema)) {}\n \n   Result<std::shared_ptr<Schema>> ReadPhysicalSchemaImpl() override;\n \n   FileSource source_;\n   std::shared_ptr<FileFormat> format_;\n+  std::shared_ptr<Schema> dataset_schema_;\n\nReview comment:\n       As best I can tell the changes in `file_base.h` and `file_base.cc` seem to be around populating this `dataset_schema_` which is later used when creating a scan task.  However, when creating a scan task, the dataset schema should be accessible via the `ScanOptions`.  Can you help me understand why these changes are needed?\n\n##########\nFile path: cpp/src/arrow/dataset/file_skyhook.h\n##########\n@@ -0,0 +1,275 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// This API is EXPERIMENTAL.\n+#define _FILE_OFFSET_BITS 64\n+\n+#pragma once\n+\n+#include <sys/stat.h>\n+#include <sys/types.h>\n+#include <unistd.h>\n+\n+#include <functional>\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <utility>\n+#include <vector>\n+\n+#include \"arrow/api.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/dataset.h\"\n+#include \"arrow/dataset/discovery.h\"\n+#include \"arrow/dataset/file_base.h\"\n+#include \"arrow/dataset/file_parquet.h\"\n+#include \"arrow/dataset/rados.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/dataset/type_fwd.h\"\n+#include \"arrow/dataset/visibility.h\"\n+#include \"arrow/filesystem/api.h\"\n+#include \"arrow/filesystem/path_util.h\"\n+#include \"arrow/io/api.h\"\n+#include \"arrow/ipc/api.h\"\n+#include \"arrow/util/iterator.h\"\n+#include \"arrow/util/macros.h\"\n+#include \"parquet/arrow/writer.h\"\n+#include \"parquet/exception.h\"\n+\n+#define SCAN_ERR_CODE 25\n+#define SCAN_ERR_MSG \"failed to scan file fragment\"\n+\n+#define SCAN_REQ_DESER_ERR_CODE 26\n+#define SCAN_REQ_DESER_ERR_MSG \"failed to deserialize scan request\"\n+\n+#define SCAN_RES_SER_ERR_CODE 27\n+#define SCAN_RES_SER_ERR_MSG \"failed to serialize result table\"\n+\n+namespace arrow {\n+namespace dataset {\n+\n+enum SkyhookFileType { PARQUET, IPC };\n+\n+/// \\addtogroup dataset-file-formats\n+///\n+/// @{\n+\n+namespace connection {\n+/// \\brief An interface for general connections.\n+class ARROW_DS_EXPORT Connection {\n+ public:\n+  virtual Status Connect() = 0;\n+\n+  Connection() = default;\n+  virtual ~Connection() = default;\n+};\n+\n+/// \\class RadosConnection\n+/// \\brief An interface to connect to a Rados cluster and hold the connection\n+/// information for usage in later stages.\n+class ARROW_DS_EXPORT RadosConnection : public Connection {\n+ public:\n+  struct RadosConnectionCtx {\n+    std::string ceph_config_path;\n+    std::string data_pool;\n+    std::string user_name;\n+    std::string cluster_name;\n+    std::string cls_name;\n+\n+    RadosConnectionCtx(const std::string& ceph_config_path, const std::string& data_pool,\n+                       const std::string& user_name, const std::string& cluster_name,\n+                       const std::string& cls_name)\n+        : ceph_config_path(ceph_config_path),\n+          data_pool(data_pool),\n+          user_name(user_name),\n+          cluster_name(cluster_name),\n+          cls_name(cls_name) {}\n+  };\n+  explicit RadosConnection(const RadosConnectionCtx& ctx)\n+      : Connection(),\n+        ctx(ctx),\n+        rados(new RadosWrapper()),\n+        ioCtx(new IoCtxWrapper()),\n+        connected(false) {}\n+\n+  ~RadosConnection();\n+\n+  /// \\brief Connect to the Rados cluster.\n+  /// \\return Status.\n+  Status Connect();\n+\n+  /// \\brief Shutdown the connection to the Rados cluster.\n+  /// \\return Status.\n+  Status Shutdown();\n+\n+  RadosConnectionCtx ctx;\n+  RadosInterface* rados;\n+  IoCtxInterface* ioCtx;\n+  bool connected;\n+};\n+\n+}  // namespace connection\n+\n+/// \\class SkyhookDirectObjectAccess\n+/// \\brief Interface for translating the name of a file in CephFS to its\n+/// corresponding object ID in RADOS assuming 1:1 mapping between a file\n+/// and its underlying object.\n+class ARROW_DS_EXPORT SkyhookDirectObjectAccess {\n+ public:\n+  explicit SkyhookDirectObjectAccess(\n+      const std::shared_ptr<connection::RadosConnection>& connection)\n+      : connection_(std::move(connection)) {}\n+\n+  /// \\brief Executes the POSIX stat call on a file.\n+  /// \\param[in] path Path of the file.\n+  /// \\param[out] st Refernce to the struct object to store the result.\n+  /// \\return Status.\n+  Status Stat(const std::string& path, struct stat& st) {\n+    struct stat file_st;\n+    if (stat(path.c_str(), &file_st) < 0)\n+      return Status::Invalid(\"stat returned non-zero exit code.\");\n+    st = file_st;\n+    return Status::OK();\n+  }\n+\n+  // Helper function to convert Inode to ObjectID because Rados calls work with\n+  // ObjectIDs.\n+  std::string ConvertFileInodeToObjectID(uint64_t inode) {\n+    std::stringstream ss;\n+    ss << std::hex << inode;\n+    std::string oid(ss.str() + \".00000000\");\n\nReview comment:\n       Maybe add a comment or some explanation behind this magic string?\n\n##########\nFile path: cpp/src/arrow/dataset/rados.cc\n##########\n@@ -0,0 +1,75 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n\nReview comment:\n       That could work although it wouldn't be my first conclusion.  Elsewhere the file_xyz files are about bridging the gap between a specific format (which has readers and writers in an `arrow/parquet` or `arrow/csv` or `arrow/ipc` directory and these formats know nothing of datasets) and the generic datasets API (e.g. file_base, dataset.h, etc.  These common files know nothing about any specific format).\r\n   \r\n   So my thinking was more that you might not have enough in just one file to create an `arrow/rados` directory but the format-specific files should go somewhere other than `arrow/dataset`.  At a quick inventory we have:\r\n   \r\n   Types that help when interacting with rados as a client (in rados.cpp/rados.h, these are analogous to our hdfs or s3 client libraries which live in `src/arrow/filesystem`.  I think `RadosConnection` and `SkyhookDirectObjectAccess` belong here as well (this might have been your point).  These types can stand on their own as Ceph integration types even if skyhook doesn't exist.\r\n   \r\n   Types that define the \"skyhook protocol\" (the flatbuffer files, the global/static methods `SerializeScanRequest`, `DeserializeScanRequest`, `SerializeTable`, and `DeserializeTable`).  Personally, I think these might be best to live in the same directory as the cls.\r\n   \r\n   Types that bridge the dataset API and the skyhook API (this would be `SkyhookFileFormat` and it uses types from both of the above categories).\r\n   \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-02T23:33:39.955+0000",
                    "updated": "2021-08-02T23:33:39.955+0000",
                    "started": "2021-08-02T23:33:39.954+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "632630",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/632700",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#issuecomment-891525546\n\n\n   Thanks @westonpace for sharing your thoughts.\r\n   \r\n   > So here is my current understanding. Let me know if this seems off. There are two pieces to this.\r\n   > \r\n   > There is a ceph object class (called Skyhook?) which processes scan tasks and lives in a \"contrib\" directory.\r\n   > \r\n   > There is a fragment / file format for Arrow that understands how to send scan requests to a ceph storage server in the skyhook format.\r\n   > \r\n   That is correct.\r\n   \r\n   > These two components aren't tightly coupled. The only source of agreement is the Arrow columnar format and this flatbuffers file. So for example (these are thought exercises, not things that will necessarily ever happen):\r\n   > \r\n   > * Ceph could be running an older version of Skyhook built with Arrow version X and the dataset client could be running a newer version of Arrow version X+N.\r\n   \r\n   Yeah, this could happen. In this case, we need to ensure that the storage side understand the `ScanRequest` language in which the client sends requests and can also the serialize tables in a buffer format understandable/supported by the client.\r\n   \r\n   > * Skyhook could switch to some other library entirely in the future and as long as it continued to respect the flatbuffers format it would continue to work.\r\n   \r\n   Similar as above I guess.\r\n   \r\n   > * A different non-arrow library (or an Arrow implementation in a different language) could decide to start sending requests to Skyhook and as long as they agreed upon the flatbuffers and arrow columnar format everything would continue to work.\r\n   \r\n   Yes, as along as both the client and server agrees upon the same send and receive protocol, they should work fine.\r\n   \r\n   > Given the above I think the proper place for this flatbuffers file to live is in the same directory as the ceph object class. This flatbuffers file is the API for skyhook.\r\n   \r\n   I agree. This is the send API for skyhook.\r\n   \r\n   > Then, for building everything, the make files for that directory could produce two artifacts: A ceph object class and a small C++ \"client library\" which is just the output of the flatbuffers compiler.\r\n   > Or you could skip the \"client library\" step and add an extra build step for the datasets module which runs the flatbuffers compiler.\r\n   \r\n   Could you please explain this part a little more?\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-03T04:58:20.324+0000",
                    "updated": "2021-08-03T04:58:20.324+0000",
                    "started": "2021-08-03T04:58:20.324+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "632700",
                    "issueId": "13381544"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/worklog/632754",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #10431:\nURL: https://github.com/apache/arrow/pull/10431#discussion_r681554778\n\n\n\n##########\nFile path: ci/scripts/integration_ceph.sh\n##########\n@@ -0,0 +1,129 @@\n+#!/usr/bin/env bash\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+set -e\n+set -x\n+set -u\n+\n+ARROW_BUILD_DIR=${1}/cpp\n+DIR=/tmp/\n+\n+# reset\n+pkill ceph || true\n+rm -rf ${DIR}/*\n+LOG_DIR=${DIR}/log\n+MON_DATA=${DIR}/mon\n+MDS_DATA=${DIR}/mds\n+MOUNTPT=${MDS_DATA}/mnt\n+OSD_DATA=${DIR}/osd\n+mkdir ${LOG_DIR} ${MON_DATA} ${OSD_DATA} ${MDS_DATA} ${MOUNTPT}\n+MDS_NAME=\"Z\"\n+MON_NAME=\"a\"\n+MGR_NAME=\"x\"\n+MIRROR_ID=\"m\"\n+\n+# cluster wide parameters\n+cat >> ${DIR}/ceph.conf <<EOF\n+[global]\n+fsid = $(uuidgen)\n+osd crush chooseleaf type = 0\n+run dir = ${DIR}/run\n+auth cluster required = none\n+auth service required = none\n+auth client required = none\n+osd pool default size = 1\n+mon host = ${HOSTNAME}\n+[mds.${MDS_NAME}]\n+host = ${HOSTNAME}\n+[mon.${MON_NAME}]\n+log file = ${LOG_DIR}/mon.log\n+chdir = \"\"\n+mon cluster log file = ${LOG_DIR}/mon-cluster.log\n+mon data = ${MON_DATA}\n+mon data avail crit = 0\n+mon addr = ${HOSTNAME}\n+mon allow pool delete = true\n+[osd.0]\n+log file = ${LOG_DIR}/osd.log\n+chdir = \"\"\n+osd data = ${OSD_DATA}\n+osd journal = ${OSD_DATA}.journal\n+osd journal size = 100\n+osd objectstore = memstore\n+osd class load list = *\n+osd class default list = *\n+EOF\n+\n+export CEPH_CONF=${DIR}/ceph.conf\n+cp $CEPH_CONF /etc/ceph/ceph.conf\n+\n+# start an osd\n+ceph-mon --id ${MON_NAME} --mkfs --keyring /dev/null\n+touch ${MON_DATA}/keyring\n+ceph-mon --id ${MON_NAME}\n+\n+# start an osd\n+OSD_ID=$(ceph osd create)\n+ceph osd crush add osd.${OSD_ID} 1 root=default\n+ceph-osd --id ${OSD_ID} --mkjournal --mkfs\n+ceph-osd --id ${OSD_ID} || ceph-osd --id ${OSD_ID} || ceph-osd --id ${OSD_ID}\n+\n+# start an mds for cephfs\n+ceph auth get-or-create mds.${MDS_NAME} mon 'profile mds' mgr 'profile mds' mds 'allow *' osd 'allow *' > ${MDS_DATA}/keyring\n+ceph osd pool create cephfs_data 8\n+ceph osd pool create cephfs_metadata 8\n+ceph fs new cephfs cephfs_metadata cephfs_data\n+ceph fs ls\n+ceph-mds -i ${MDS_NAME}\n+ceph status\n+while [[ ! $(ceph mds stat | grep \"up:active\") ]]; do sleep 1; done\n+\n+# start a manager\n+ceph-mgr --id ${MGR_NAME}\n+\n+# test the setup\n+ceph --version\n+ceph status\n+\n+apt update\n+apt install -y ceph-fuse attr\n+\n+pushd ${ARROW_BUILD_DIR}\n+    # create the rados-classes, if not there already\n+    mkdir -p /usr/lib/x86_64-linux-gnu/rados-classes/\n+    cp debug/libcls_arrow* /usr/lib/x86_64-linux-gnu/rados-classes/\n+\n+    # mount a ceph filesystem to /mnt/cephfs in the user-space using ceph-fuse\n+    mkdir -p /mnt/cephfs\n+    ceph-fuse /mnt/cephfs\n+    sleep 5\n+\n+    # download an example dataset and copy into the mounted dir\n+    rm -rf nyc*\n+    wget https://raw.githubusercontent.com/JayjeetAtGithub/zips/main/nyc.zip\n\nReview comment:\n       Can we use a python script to generate a random dataset of parquet files instead of this `wget` here ?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-08-03T08:37:23.333+0000",
                    "updated": "2021-08-03T08:37:23.333+0000",
                    "started": "2021-08-03T08:37:23.333+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "632754",
                    "issueId": "13381544"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 23400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@221d4817[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@73044e59[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@9ee4fb0[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@58c9be52[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@226b4e98[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@3cddd897[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@47358373[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@521b9ee9[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2a7c05bc[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@78b77904[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1372537a[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@1d106b2b[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 23400,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Jun 28 20:13:56 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-06-28T20:13:56.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-12921/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-06-01T19:19:50.000+0000",
        "updated": "2022-06-28T20:13:56.000+0000",
        "timeoriginalestimate": null,
        "description": "Implement a RadosParquetFileFormat class to defer the evaluation of scan operations on a Parquet dataset to a RADOS storage backend. This can be done by using the librados C++ library to execute storage side functions that scan the files on the Ceph storage nodes (OSDs) using Arrow libraries. This issue is an upgrade to the previous story of ARROW-10549. See corresponding [mailing list|https://lists.apache.org/thread.html/r2a5a693967213b7c6bb49015194ca16afc4d20047805d0e069c2e45c%40%3Cdev.arrow.apache.org%3E] discussion.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "6.5h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 23400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Dataset] Add RadosParquetFileFormat to Dataset API",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/comment/17364502",
                    "id": "17364502",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=Jayjeet",
                        "name": "Jayjeet",
                        "key": "jayjeet",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Jayjeet Chakraborty",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Hello all, Just wanted to follow up regarding this issue if anyone has got the chance to look at the PR. It would be great if any reviewer can be assigned to the Pull request and we would like to work with them on this.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=Jayjeet",
                        "name": "Jayjeet",
                        "key": "jayjeet",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Jayjeet Chakraborty",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2021-06-16T19:55:47.974+0000",
                    "updated": "2021-06-16T19:55:47.974+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/comment/17559931",
                    "id": "17559931",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=octalene",
                        "name": "octalene",
                        "key": "octalene",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=octalene&avatarId=51083",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=octalene&avatarId=51083",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=octalene&avatarId=51083",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=octalene&avatarId=51083"
                        },
                        "displayName": "Aldrin Montana",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "I believe this issue is superceded by ARROW-13607 and thus can be closed?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=octalene",
                        "name": "octalene",
                        "key": "octalene",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=octalene&avatarId=51083",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=octalene&avatarId=51083",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=octalene&avatarId=51083",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=octalene&avatarId=51083"
                        },
                        "displayName": "Aldrin Montana",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2022-06-28T19:35:12.092+0000",
                    "updated": "2022-06-28T19:35:12.092+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13381544/comment/17559945",
                    "id": "17559945",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Yup, thanks for pointing that out Aldrin!",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2022-06-28T20:13:56.564+0000",
                    "updated": "2022-06-28T20:13:56.564+0000"
                }
            ],
            "maxResults": 3,
            "total": 3,
            "startAt": 0
        },
        "customfield_12311820": "0|z0rk80:",
        "customfield_12314139": null
    }
}