{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13325786",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786",
    "key": "ARROW-9901",
    "fields": {
        "parent": {
            "id": "13107102",
            "key": "ARROW-1644",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13107102",
            "fields": {
                "summary": "[C++][Parquet] Read and write nested Parquet data with a mix of struct and list nesting levels",
                "status": {
                    "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                    "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                    "name": "Resolved",
                    "id": "5",
                    "statusCategory": {
                        "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                        "id": 3,
                        "key": "done",
                        "colorName": "green",
                        "name": "Done"
                    }
                },
                "priority": {
                    "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                    "name": "Major",
                    "id": "3"
                },
                "issuetype": {
                    "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                    "id": "2",
                    "description": "A new feature of the product, which has yet to be developed.",
                    "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                    "name": "New Feature",
                    "subtask": false,
                    "avatarId": 21141
                }
            }
        },
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12345977",
                "id": "12345977",
                "description": "",
                "name": "2.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2020-10-19"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
            "name": "apitrou",
            "key": "pitrou",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
            },
            "displayName": "Antoine Pitrou",
            "active": true,
            "timeZone": "Europe/Paris"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
            "name": "apitrou",
            "key": "pitrou",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
            },
            "displayName": "Antoine Pitrou",
            "active": true,
            "timeZone": "Europe/Paris"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
            "name": "apitrou",
            "key": "pitrou",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
            },
            "displayName": "Antoine Pitrou",
            "active": true,
            "timeZone": "Europe/Paris"
        },
        "aggregateprogress": {
            "progress": 6600,
            "total": 6600,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 6600,
            "total": 6600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-9901/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 11,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/478010",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou opened a new pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100\n\n\n   Exercise various cases of nested Parquet schemas and def / rep levels values.\r\n   \r\n   Some of those tests currently fail.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-02T17:57:04.935+0000",
                    "updated": "2020-09-02T17:57:04.935+0000",
                    "started": "2020-09-02T17:57:04.935+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "478010",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/478012",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#issuecomment-685901101\n\n\n   @emkornfield WIP (some more tests need to be written), but this is a start. Feel free to give any feedback.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-02T17:58:03.565+0000",
                    "updated": "2020-09-02T17:58:03.565+0000",
                    "started": "2020-09-02T17:58:03.565+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "478012",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/478027",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#issuecomment-685910227\n\n\n   https://issues.apache.org/jira/browse/ARROW-9901\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-02T18:15:12.230+0000",
                    "updated": "2020-09-02T18:15:12.230+0000",
                    "started": "2020-09-02T18:15:12.230+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "478027",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/478616",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#discussion_r483029266\n\n\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n\nReview comment:\n       This macro easily enables / disables all failing tests. @emkornfield \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-03T14:37:54.969+0000",
                    "updated": "2020-09-03T14:37:54.969+0000",
                    "started": "2020-09-03T14:37:54.969+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "478616",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/478617",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#issuecomment-686538502\n\n\n   This is now ready for review. @emkornfield It would be nice if you could take a look at the failing / disabled tests, and see if they are not mistaken. Especially, there are failures in some nested struct tests.\r\n   \r\n   There are probably more possibilities that could be tested (such as list-in-struct-in-list, or struct-in-list-in-struct), but this is a good start IMHO.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-03T14:39:54.021+0000",
                    "updated": "2020-09-03T14:39:54.021+0000",
                    "started": "2020-09-03T14:39:54.021+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "478617",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/478932",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#discussion_r483382102\n\n\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n\nReview comment:\n       nit: /*column_index=*/ make this easier to read (at least for me)\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-04T04:40:27.918+0000",
                    "updated": "2020-09-04T04:40:27.918+0000",
                    "started": "2020-09-04T04:40:27.918+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "478932",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/478941",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "emkornfield commented on a change in pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#discussion_r483382421\n\n\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n\nReview comment:\n       this is a bit of a mess right now I think, since by default we right out non-conforming values and rely on that fact.\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  return list(field(\"element\", type, nullable));\n+}\n+\n+//\n+// Primitive columns with no intermediate group node\n+//\n+\n+TEST_F(TestReconstructColumn, PrimitiveOptional) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::OPTIONAL, ParquetType::INT32));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, null, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, PrimitiveRequired) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REQUIRED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(PrimitiveRepeated)) {\n+  // Arrow schema: list(int32 not null) not null\n+  this->SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REPEATED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(list(field(\"node_name\", int32(), /*nullable=*/false)),\n+                                \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequired) {\n+  // Arrow schema: struct(a: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[[4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequired)) {\n\nReview comment:\n       this I think should also pass based on existing code (unless there re bugs)?\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  return list(field(\"element\", type, nullable));\n+}\n+\n+//\n+// Primitive columns with no intermediate group node\n+//\n+\n+TEST_F(TestReconstructColumn, PrimitiveOptional) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::OPTIONAL, ParquetType::INT32));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, null, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, PrimitiveRequired) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REQUIRED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(PrimitiveRepeated)) {\n+  // Arrow schema: list(int32 not null) not null\n+  this->SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REPEATED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(list(field(\"node_name\", int32(), /*nullable=*/false)),\n+                                \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequired) {\n+  // Arrow schema: struct(a: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[[4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequired)) {\n+  // Arrow schema: struct(a: int32 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[null, [4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedRequiredOptional) {\n+  // Arrow schema: struct(a: int32) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32()), \"[[null], [4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedOptionalOptional) {\n+  // Arrow schema: struct(a: int32)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 2, 2};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32()), \"[null, [null], [4], [5]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Nested struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequiredRequired) {\n+  // Arrow schema: struct(a: struct(b: int32 not null) not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+          \"a\", Repetition::REQUIRED,\n+          {PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32(), false), false),\n+                    \"[[[4]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedRequiredOptionalRequired) {\n+  // Arrow schema: struct(a: struct(b: int32 not null)) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+          \"a\", Repetition::OPTIONAL,\n+          {PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32(), false)),\n+                                \"[[[4]], [[null]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequiredOptional)) {\n+  // Arrow schema: struct(a: struct(b: int32) not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+          \"a\", Repetition::REQUIRED,\n+          {PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 2, 0, 2, 2};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32()), false),\n+                                \"[[[null]], [[4]], null, [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedOptionalOptionalOptional) {\n+  // Arrow schema: struct(a: struct(b: int32) not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+          \"a\", Repetition::OPTIONAL,\n+          {PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 2, 0, 3, 3, 3};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32())),\n+                                \"[[null], [[null]], null, [[4]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (two fields)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedTwoFields1) {\n+  // Arrow schema: struct(a: int32 not null, b: int64 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{4, 5, 6}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{}, RepLevels{}, Int64Vector{7, 8, 9}));\n+\n+  auto type = struct_(\n+      {field(\"a\", int32(), /*nullable=*/false), field(\"b\", int64(), /*nullable=*/false)});\n+  auto expected = ArrayFromJSON(type, \"[[4, 7], [5, 8], [6, 9]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedTwoFields2) {\n+  // Arrow schema: struct(a: int32 not null, b: int64) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{4, 5, 6}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 1}, RepLevels{}, Int64Vector{7, 8}));\n+\n+  auto type = struct_({field(\"a\", int32(), /*nullable=*/false), field(\"b\", int64())});\n+  auto expected = ArrayFromJSON(type, \"[[4, null], [5, 7], [6, 8]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedTwoFields3)) {\n\nReview comment:\n       again surpised this fails, it would seem to indicate a bug in existing code (I thought we should accept structs).\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  return list(field(\"element\", type, nullable));\n+}\n+\n+//\n+// Primitive columns with no intermediate group node\n+//\n+\n+TEST_F(TestReconstructColumn, PrimitiveOptional) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::OPTIONAL, ParquetType::INT32));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, null, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, PrimitiveRequired) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REQUIRED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(PrimitiveRepeated)) {\n+  // Arrow schema: list(int32 not null) not null\n+  this->SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REPEATED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(list(field(\"node_name\", int32(), /*nullable=*/false)),\n+                                \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequired) {\n+  // Arrow schema: struct(a: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[[4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequired)) {\n+  // Arrow schema: struct(a: int32 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[null, [4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedRequiredOptional) {\n+  // Arrow schema: struct(a: int32) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32()), \"[[null], [4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedOptionalOptional) {\n+  // Arrow schema: struct(a: int32)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 2, 2};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32()), \"[null, [null], [4], [5]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Nested struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequiredRequired) {\n+  // Arrow schema: struct(a: struct(b: int32 not null) not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+          \"a\", Repetition::REQUIRED,\n+          {PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32(), false), false),\n+                    \"[[[4]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedRequiredOptionalRequired) {\n+  // Arrow schema: struct(a: struct(b: int32 not null)) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+          \"a\", Repetition::OPTIONAL,\n+          {PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32(), false)),\n+                                \"[[[4]], [[null]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequiredOptional)) {\n+  // Arrow schema: struct(a: struct(b: int32) not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+          \"a\", Repetition::REQUIRED,\n+          {PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 2, 0, 2, 2};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32()), false),\n+                                \"[[[null]], [[4]], null, [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedOptionalOptionalOptional) {\n+  // Arrow schema: struct(a: struct(b: int32) not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+          \"a\", Repetition::OPTIONAL,\n+          {PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 2, 0, 3, 3, 3};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32())),\n+                                \"[[null], [[null]], null, [[4]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (two fields)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedTwoFields1) {\n+  // Arrow schema: struct(a: int32 not null, b: int64 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{4, 5, 6}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{}, RepLevels{}, Int64Vector{7, 8, 9}));\n+\n+  auto type = struct_(\n+      {field(\"a\", int32(), /*nullable=*/false), field(\"b\", int64(), /*nullable=*/false)});\n+  auto expected = ArrayFromJSON(type, \"[[4, 7], [5, 8], [6, 9]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedTwoFields2) {\n+  // Arrow schema: struct(a: int32 not null, b: int64) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{4, 5, 6}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 1}, RepLevels{}, Int64Vector{7, 8}));\n+\n+  auto type = struct_({field(\"a\", int32(), /*nullable=*/false), field(\"b\", int64())});\n+  auto expected = ArrayFromJSON(type, \"[[4, null], [5, 7], [6, 8]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedTwoFields3)) {\n+  // Arrow schema: struct(a: int32 not null, b: int64 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{0, 1, 1}, RepLevels{}, Int32Vector{4, 5}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 1}, RepLevels{}, Int64Vector{7, 8}));\n+\n+  auto type = struct_(\n+      {field(\"a\", int32(), /*nullable=*/false), field(\"b\", int64(), /*nullable=*/false)});\n+  auto expected = ArrayFromJSON(type, \"[null, [4, 7], [5, 8]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedTwoFields4) {\n+  // Arrow schema: struct(a: int32, b: int64 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::OPTIONAL, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{0, 1, 2}, RepLevels{}, Int32Vector{4}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 1}, RepLevels{}, Int64Vector{7, 8}));\n+\n+  auto type = struct_({field(\"a\", int32()), field(\"b\", int64(), /*nullable=*/false)});\n+  auto expected = ArrayFromJSON(type, \"[null, [null, 7], [4, 8]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedTwoFields5) {\n+  // Arrow schema: struct(a: int32, b: int64)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::OPTIONAL, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{0, 1, 2}, RepLevels{}, Int32Vector{4}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 2, 1}, RepLevels{}, Int64Vector{7}));\n+\n+  auto type = struct_({field(\"a\", int32()), field(\"b\", int64())});\n+  auto expected = ArrayFromJSON(type, \"[null, [null, 7], [4, null]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+//\n+// Nested struct encodings (two fields)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedNestedTwoFields1) {\n+  // Arrow schema: struct(a: struct(aa: int32 not null,\n+  //                                ab: int64 not null) not null,\n+  //                      b: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+           \"a\", Repetition::REQUIRED,\n+           {PrimitiveNode::Make(\"aa\", Repetition::REQUIRED, ParquetType::INT32),\n+            PrimitiveNode::Make(\"ab\", Repetition::REQUIRED, ParquetType::INT64)}),\n+       PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  // aa\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{4, 5, 6}));\n+  // ab\n+  ASSERT_OK(WriteInt64Column(DefLevels{}, RepLevels{}, Int64Vector{7, 8, 9}));\n+  // b\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{10, 11, 12}));\n+\n+  auto type = struct_({field(\"a\",\n+                             struct_({field(\"aa\", int32(), /*nullable=*/false),\n+                                      field(\"ab\", int64(), /*nullable=*/false)}),\n+                             /*nullable=*/false),\n+                       field(\"b\", int32(), /*nullable=*/false)});\n+  auto expected = ArrayFromJSON(type, \"[[[4, 7], 10], [[5, 8], 11], [[6, 9], 12]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedNestedTwoFields2) {\n+  // Arrow schema: struct(a: struct(aa: int32,\n+  //                                ab: int64 not null) not null,\n+  //                      b: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+           \"a\", Repetition::REQUIRED,\n+           {PrimitiveNode::Make(\"aa\", Repetition::OPTIONAL, ParquetType::INT32),\n+            PrimitiveNode::Make(\"ab\", Repetition::REQUIRED, ParquetType::INT64)}),\n+       PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  // aa\n+  ASSERT_OK(WriteInt32Column(DefLevels{1, 0, 1}, RepLevels{}, Int32Vector{4, 5}));\n+  // ab\n+  ASSERT_OK(WriteInt64Column(DefLevels{}, RepLevels{}, Int64Vector{7, 8, 9}));\n+  // b\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{10, 11, 12}));\n+\n+  auto type = struct_(\n+      {field(\"a\",\n+             struct_({field(\"aa\", int32()), field(\"ab\", int64(), /*nullable=*/false)}),\n+             /*nullable=*/false),\n+       field(\"b\", int32(), /*nullable=*/false)});\n+  auto expected = ArrayFromJSON(type, \"[[[4, 7], 10], [[null, 8], 11], [[5, 9], 12]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedNestedTwoFields3) {\n+  // Arrow schema: struct(a: struct(aa: int32 not null,\n+  //                                ab: int64) not null,\n+  //                      b: int32) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+           \"a\", Repetition::REQUIRED,\n+           {PrimitiveNode::Make(\"aa\", Repetition::REQUIRED, ParquetType::INT32),\n+            PrimitiveNode::Make(\"ab\", Repetition::OPTIONAL, ParquetType::INT64)}),\n+       PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  // aa\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{4, 5, 6}));\n+  // ab\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 1}, RepLevels{}, Int64Vector{7, 8}));\n+  // b\n+  ASSERT_OK(WriteInt32Column(DefLevels{1, 0, 1}, RepLevels{}, Int32Vector{10, 11}));\n+\n+  auto type = struct_(\n+      {field(\"a\",\n+             struct_({field(\"aa\", int32(), /*nullable=*/false), field(\"ab\", int64())}),\n+             /*nullable=*/false),\n+       field(\"b\", int32())});\n+  auto expected = ArrayFromJSON(type, \"[[[4, null], 10], [[5, 7], null], [[6, 8], 11]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedNestedTwoFields4) {\n+  // Arrow schema: struct(a: struct(aa: int32 not null,\n+  //                                ab: int64),\n+  //                      b: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+           \"a\", Repetition::OPTIONAL,\n+           {PrimitiveNode::Make(\"aa\", Repetition::REQUIRED, ParquetType::INT32),\n+            PrimitiveNode::Make(\"ab\", Repetition::OPTIONAL, ParquetType::INT64)}),\n+       PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  // aa\n+  ASSERT_OK(WriteInt32Column(DefLevels{0, 1, 1}, RepLevels{}, Int32Vector{4, 5}));\n+  // ab\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 2}, RepLevels{}, Int64Vector{7}));\n+  // b\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{10, 11, 12}));\n+\n+  auto type = struct_({field(\"a\", struct_({field(\"aa\", int32(), /*nullable=*/false),\n+                                           field(\"ab\", int64())})),\n+                       field(\"b\", int32(), /*nullable=*/false)});\n+  auto expected = ArrayFromJSON(type, \"[[null, 10], [[4, null], 11], [[5, 7], 12]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedNestedTwoFields5) {\n+  // Arrow schema: struct(a: struct(aa: int32 not null,\n+  //                                ab: int64) not null,\n+  //                      b: int32)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+           \"a\", Repetition::REQUIRED,\n+           {PrimitiveNode::Make(\"aa\", Repetition::REQUIRED, ParquetType::INT32),\n+            PrimitiveNode::Make(\"ab\", Repetition::OPTIONAL, ParquetType::INT64)}),\n+       PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  // aa\n+  ASSERT_OK(WriteInt32Column(DefLevels{0, 1, 1}, RepLevels{}, Int32Vector{4, 5}));\n+  // ab\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 2}, RepLevels{}, Int64Vector{7}));\n+  // b\n+  ASSERT_OK(WriteInt32Column(DefLevels{0, 2, 1}, RepLevels{}, Int32Vector{10}));\n+\n+  auto type = struct_(\n+      {field(\"a\",\n+             struct_({field(\"aa\", int32(), /*nullable=*/false), field(\"ab\", int64())}),\n+             /*nullable=*/false),\n+       field(\"b\", int32())});\n+  auto expected = ArrayFromJSON(type, \"[null, [[4, null], 10], [[5, 7], null]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedNestedTwoFields6) {\n+  // Arrow schema: struct(a: struct(aa: int32 not null,\n+  //                                ab: int64),\n+  //                      b: int32)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+           \"a\", Repetition::OPTIONAL,\n+           {PrimitiveNode::Make(\"aa\", Repetition::REQUIRED, ParquetType::INT32),\n+            PrimitiveNode::Make(\"ab\", Repetition::OPTIONAL, ParquetType::INT64)}),\n+       PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  // aa\n+  ASSERT_OK(WriteInt32Column(DefLevels{0, 1, 2, 2}, RepLevels{}, Int32Vector{4, 5}));\n+  // ab\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 2, 3}, RepLevels{}, Int64Vector{7}));\n+  // b\n+  ASSERT_OK(WriteInt32Column(DefLevels{0, 2, 1, 2}, RepLevels{}, Int32Vector{10, 11}));\n+\n+  auto type = struct_({field(\"a\", struct_({field(\"aa\", int32(), /*nullable=*/false),\n+                                           field(\"ab\", int64())})),\n+                       field(\"b\", int32())});\n+  auto expected =\n+      ArrayFromJSON(type, \"[null, [null, 10], [[4, null], null], [[5, 7], 11]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+//\n+// Three-level list encodings\n+//\n+\n+TEST_F(TestReconstructColumn, ThreeLevelListRequiredRequired) {\n+  // Arrow schema: list(int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+          \"list\", Repetition::REPEATED,\n+          {PrimitiveNode::Make(\"element\", Repetition::REQUIRED, ParquetType::INT32)})},\n+      LogicalType::List()));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  auto expected = ArrayFromJSON(List(int32(), /*nullable=*/false), \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, ThreeLevelListOptionalRequired) {\n+  // Arrow schema: list(int32 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+          \"list\", Repetition::REPEATED,\n+          {PrimitiveNode::Make(\"element\", Repetition::REQUIRED, ParquetType::INT32)})},\n+      LogicalType::List()));\n+\n+  LevelVector def_levels = {0, 1, 2, 2, 2};\n+  LevelVector rep_levels = {0, 0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(List(int32(), /*nullable=*/false), \"[null, [], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, ThreeLevelListRequiredOptional) {\n+  // Arrow schema: list(int32) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+          \"list\", Repetition::REPEATED,\n+          {PrimitiveNode::Make(\"element\", Repetition::OPTIONAL, ParquetType::INT32)})},\n+      LogicalType::List()));\n+\n+  LevelVector def_levels = {0, 1, 2, 2, 2};\n+  LevelVector rep_levels = {0, 0, 1, 0, 1};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(List(int32()), \"[[], [null, 4], [5, 6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, ThreeLevelListOptionalOptional) {\n+  // Arrow schema: list(int32)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+          \"list\", Repetition::REPEATED,\n+          {PrimitiveNode::Make(\"element\", Repetition::OPTIONAL, ParquetType::INT32)})},\n+      LogicalType::List()));\n+\n+  LevelVector def_levels = {0, 1, 2, 3, 3, 3};\n+  LevelVector rep_levels = {0, 0, 0, 1, 0, 1};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(List(int32()), \"[null, [], [null, 4], [5, 6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Legacy list encodings\n+//\n+\n+TEST_F(TestReconstructColumn, FAILING(TwoLevelListRequired)) {\n+  // Arrow schema: list(int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"element\", Repetition::REPEATED, ParquetType::INT32)},\n+      LogicalType::List()));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  auto expected = ArrayFromJSON(List(int32()), \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(TwoLevelListOptional)) {\n\nReview comment:\n       surprised on this one also, the code should support it.\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  return list(field(\"element\", type, nullable));\n+}\n+\n+//\n+// Primitive columns with no intermediate group node\n+//\n+\n+TEST_F(TestReconstructColumn, PrimitiveOptional) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::OPTIONAL, ParquetType::INT32));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, null, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, PrimitiveRequired) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REQUIRED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(PrimitiveRepeated)) {\n+  // Arrow schema: list(int32 not null) not null\n+  this->SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REPEATED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(list(field(\"node_name\", int32(), /*nullable=*/false)),\n+                                \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequired) {\n+  // Arrow schema: struct(a: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[[4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequired)) {\n+  // Arrow schema: struct(a: int32 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[null, [4], [5], [6]]\");\n\nReview comment:\n       is the JSON right here, I would have through expected the need for '{}' entries like: https://github.com/apache/arrow/blob/b0902ab32f26681c9e99a0b61a5ab5d6d03a20df/cpp/src/parquet/arrow/path_internal_test.cc#L450\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  return list(field(\"element\", type, nullable));\n+}\n+\n+//\n+// Primitive columns with no intermediate group node\n+//\n+\n+TEST_F(TestReconstructColumn, PrimitiveOptional) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::OPTIONAL, ParquetType::INT32));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, null, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, PrimitiveRequired) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REQUIRED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(PrimitiveRepeated)) {\n\nReview comment:\n       I'm surprised this fails.  I thought this case should be handled today.  The def/rep levels look right to be.\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  return list(field(\"element\", type, nullable));\n+}\n+\n+//\n+// Primitive columns with no intermediate group node\n+//\n+\n+TEST_F(TestReconstructColumn, PrimitiveOptional) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::OPTIONAL, ParquetType::INT32));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, null, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, PrimitiveRequired) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REQUIRED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(PrimitiveRepeated)) {\n+  // Arrow schema: list(int32 not null) not null\n+  this->SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REPEATED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(list(field(\"node_name\", int32(), /*nullable=*/false)),\n+                                \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequired) {\n+  // Arrow schema: struct(a: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[[4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequired)) {\n+  // Arrow schema: struct(a: int32 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[null, [4], [5], [6]]\");\n\nReview comment:\n       I guess this is acceptable short-hand.  Good to know.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-04T05:17:32.415+0000",
                    "updated": "2020-09-04T05:17:32.415+0000",
                    "started": "2020-09-04T05:17:32.415+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "478941",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/479623",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#discussion_r484405009\n\n\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  return list(field(\"element\", type, nullable));\n+}\n+\n+//\n+// Primitive columns with no intermediate group node\n+//\n+\n+TEST_F(TestReconstructColumn, PrimitiveOptional) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::OPTIONAL, ParquetType::INT32));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, null, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, PrimitiveRequired) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REQUIRED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(PrimitiveRepeated)) {\n+  // Arrow schema: list(int32 not null) not null\n+  this->SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REPEATED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(list(field(\"node_name\", int32(), /*nullable=*/false)),\n+                                \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequired) {\n+  // Arrow schema: struct(a: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[[4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequired)) {\n+  // Arrow schema: struct(a: int32 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[null, [4], [5], [6]]\");\n\nReview comment:\n       I'm not sure what you mean. `null` is a null struct entry. `{}` would be a non-null struct entry with null child member.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-07T12:34:34.408+0000",
                    "updated": "2020-09-07T12:34:34.408+0000",
                    "started": "2020-09-07T12:34:34.408+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "479623",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/479625",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#issuecomment-688298194\n\n\n   I think I've addressed your concerns. Using object notation actually helped me fix a test... which made it fail. So one more bug to fix :-)\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-07T12:39:10.336+0000",
                    "updated": "2020-09-07T12:39:10.336+0000",
                    "started": "2020-09-07T12:39:10.336+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "479625",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/479627",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100#discussion_r484410655\n\n\n\n##########\nFile path: cpp/src/parquet/arrow/reconstruct_internal_test.cc\n##########\n@@ -0,0 +1,1528 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"parquet/arrow/path_internal.h\"\n+\n+#include <algorithm>\n+#include <memory>\n+#include <utility>\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/chunked_array.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type_fwd.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"parquet/arrow/reader.h\"\n+#include \"parquet/arrow/schema.h\"\n+#include \"parquet/column_writer.h\"\n+#include \"parquet/file_writer.h\"\n+#include \"parquet/properties.h\"\n+\n+// Set to 1 to see failures in failing tests\n+#define RUN_FAILING_TESTS 0\n+\n+#if RUN_FAILING_TESTS\n+#define FAILING(test_name) test_name\n+#else\n+#define FAILING(test_name) DISABLED_##test_name\n+#endif\n+\n+using arrow::Array;\n+using arrow::ArrayFromJSON;\n+using arrow::AssertArraysEqual;\n+using arrow::ChunkedArray;\n+using arrow::DataType;\n+using arrow::field;\n+using arrow::int32;\n+using arrow::int64;\n+using arrow::list;\n+using arrow::MemoryPool;\n+using arrow::Result;\n+using arrow::Status;\n+using arrow::struct_;\n+using arrow::internal::checked_cast;\n+using arrow::internal::checked_pointer_cast;\n+using arrow::io::BufferOutputStream;\n+using arrow::io::BufferReader;\n+\n+using testing::ElementsAre;\n+using testing::ElementsAreArray;\n+using testing::Eq;\n+using testing::NotNull;\n+using testing::SizeIs;\n+\n+namespace parquet {\n+namespace arrow {\n+\n+using parquet::schema::GroupNode;\n+using parquet::schema::NodePtr;\n+using parquet::schema::PrimitiveNode;\n+\n+using ParquetType = parquet::Type::type;\n+template <ParquetType T>\n+using ParquetTraits = parquet::type_traits<T>;\n+\n+using LevelVector = std::vector<int16_t>;\n+// For readability\n+using DefLevels = LevelVector;\n+using RepLevels = LevelVector;\n+using Int32Vector = std::vector<int32_t>;\n+using Int64Vector = std::vector<int64_t>;\n+\n+// A Parquet file builder that allows writing values one leaf column at a time\n+class FileBuilder {\n+ public:\n+  static Result<std::shared_ptr<FileBuilder>> Make(const NodePtr& group_node,\n+                                                   int num_columns) {\n+    auto self = std::make_shared<FileBuilder>();\n+    RETURN_NOT_OK(self->Open(group_node, num_columns));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Buffer>> Finish() {\n+    DCHECK_EQ(column_index_, num_columns_);\n+    row_group_writer_->Close();\n+    file_writer_->Close();\n+    return stream_->Finish();\n+  }\n+\n+  // Write a leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    auto column_writer = row_group_writer_->NextColumn();\n+    auto column_descr = column_writer->descr();\n+    const int16_t max_def_level = column_descr->max_definition_level();\n+    const int16_t max_rep_level = column_descr->max_repetition_level();\n+    CheckTestedLevels(def_levels, max_def_level);\n+    CheckTestedLevels(rep_levels, max_rep_level);\n+\n+    auto typed_writer =\n+        checked_cast<TypedColumnWriter<PhysicalType<TYPE>>*>(column_writer);\n+\n+    const int64_t num_values = static_cast<int64_t>(\n+        (max_def_level > 0) ? def_levels.size()\n+                            : (max_rep_level > 0) ? rep_levels.size() : values.size());\n+    const int64_t values_written = typed_writer->WriteBatch(\n+        num_values, LevelPointerOrNull(def_levels, max_def_level),\n+        LevelPointerOrNull(rep_levels, max_rep_level), values.data());\n+    DCHECK_EQ(values_written, static_cast<int64_t>(values.size()));  // Sanity check\n+\n+    column_writer->Close();\n+    ++column_index_;\n+    return Status::OK();\n+  }\n+\n+ protected:\n+  Status Open(const NodePtr& group_node, int num_columns) {\n+    ARROW_ASSIGN_OR_RAISE(stream_, BufferOutputStream::Create());\n+    file_writer_ =\n+        ParquetFileWriter::Open(stream_, checked_pointer_cast<GroupNode>(group_node));\n+    row_group_writer_ = file_writer_->AppendRowGroup();\n+    num_columns_ = num_columns;\n+    column_index_ = 0;\n+    return Status::OK();\n+  }\n+\n+  void CheckTestedLevels(const LevelVector& levels, int16_t max_level) {\n+    // Tests are expected to exercise all possible levels in [0, max_level]\n+    if (!levels.empty()) {\n+      const int16_t max_seen_level = *std::max_element(levels.begin(), levels.end());\n+      DCHECK_EQ(max_seen_level, max_level);\n+    }\n+  }\n+\n+  const int16_t* LevelPointerOrNull(const LevelVector& levels, int16_t max_level) {\n+    if (max_level > 0) {\n+      DCHECK_GT(levels.size(), 0);\n+      return levels.data();\n+    } else {\n+      DCHECK_EQ(levels.size(), 0);\n+      return nullptr;\n+    }\n+  }\n+\n+  std::shared_ptr<BufferOutputStream> stream_;\n+  std::unique_ptr<ParquetFileWriter> file_writer_;\n+  RowGroupWriter* row_group_writer_;\n+  int num_columns_;\n+  int column_index_;\n+};\n+\n+// A Parquet file tester that allows reading Arrow columns, corresponding to\n+// children of the top-level group node.\n+class FileTester {\n+ public:\n+  static Result<std::shared_ptr<FileTester>> Make(std::shared_ptr<Buffer> buffer,\n+                                                  MemoryPool* pool) {\n+    auto self = std::make_shared<FileTester>();\n+    RETURN_NOT_OK(self->Open(buffer, pool));\n+    return self;\n+  }\n+\n+  Result<std::shared_ptr<Array>> ReadColumn(int column_index) {\n+    std::shared_ptr<ChunkedArray> column;\n+    RETURN_NOT_OK(file_reader_->ReadColumn(column_index, &column));\n+    return ::arrow::Concatenate(column->chunks(), pool_);\n+  }\n+\n+  void CheckColumn(int column_index, const Array& expected) {\n+    ASSERT_OK_AND_ASSIGN(const auto actual, ReadColumn(column_index));\n+    ASSERT_OK(actual->ValidateFull());\n+    AssertArraysEqual(expected, *actual, /*verbose=*/true);\n+  }\n+\n+ protected:\n+  Status Open(std::shared_ptr<Buffer> buffer, MemoryPool* pool) {\n+    pool_ = pool;\n+    return OpenFile(std::make_shared<BufferReader>(buffer), pool_, &file_reader_);\n+  }\n+\n+  MemoryPool* pool_;\n+  std::unique_ptr<FileReader> file_reader_;\n+};\n+\n+class TestReconstructColumn : public testing::Test {\n+ public:\n+  void SetUp() override { pool_ = ::arrow::default_memory_pool(); }\n+\n+  // Write the next leaf (primitive) column\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  Status WriteColumn(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                     const std::vector<C_TYPE>& values) {\n+    if (!builder_) {\n+      ARROW_ASSIGN_OR_RAISE(builder_,\n+                            FileBuilder::Make(group_node_, descriptor_->num_columns()));\n+    }\n+    return builder_->WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt32Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT32>(def_levels, rep_levels, values);\n+  }\n+\n+  template <typename C_TYPE>\n+  Status WriteInt64Column(const LevelVector& def_levels, const LevelVector& rep_levels,\n+                          const std::vector<C_TYPE>& values) {\n+    return WriteColumn<ParquetType::INT64>(def_levels, rep_levels, values);\n+  }\n+\n+  // Read a Arrow column and check its values\n+  void CheckColumn(int column_index, const Array& expected) {\n+    if (!tester_) {\n+      ASSERT_OK_AND_ASSIGN(auto buffer, builder_->Finish());\n+      ASSERT_OK_AND_ASSIGN(tester_, FileTester::Make(buffer, pool_));\n+    }\n+    tester_->CheckColumn(column_index, expected);\n+  }\n+\n+  void CheckColumn(const Array& expected) { CheckColumn(0, expected); }\n+\n+  // One-column shortcut\n+  template <ParquetType TYPE, typename C_TYPE = typename ParquetTraits<TYPE>::value_type>\n+  void AssertReconstruct(const Array& expected, const LevelVector& def_levels,\n+                         const LevelVector& rep_levels,\n+                         const std::vector<C_TYPE>& values) {\n+    ASSERT_OK((WriteColumn<TYPE, C_TYPE>(def_levels, rep_levels, values)));\n+    CheckColumn(0, expected);\n+  }\n+\n+  ::arrow::Status MaybeSetParquetSchema(const NodePtr& column) {\n+    descriptor_.reset(new SchemaDescriptor());\n+    manifest_.reset(new SchemaManifest());\n+    group_node_ = GroupNode::Make(\"root\", Repetition::REQUIRED, {column});\n+    descriptor_->Init(group_node_);\n+    return SchemaManifest::Make(descriptor_.get(),\n+                                std::shared_ptr<const ::arrow::KeyValueMetadata>(),\n+                                ArrowReaderProperties(), manifest_.get());\n+  }\n+\n+  void SetParquetSchema(const NodePtr& column) {\n+    ASSERT_OK(MaybeSetParquetSchema(column));\n+  }\n+\n+ protected:\n+  MemoryPool* pool_;\n+  NodePtr group_node_;\n+  std::unique_ptr<SchemaDescriptor> descriptor_;\n+  std::unique_ptr<SchemaManifest> manifest_;\n+\n+  std::shared_ptr<FileBuilder> builder_;\n+  std::shared_ptr<FileTester> tester_;\n+};\n+\n+static std::shared_ptr<DataType> OneFieldStruct(const std::string& name,\n+                                                std::shared_ptr<DataType> type,\n+                                                bool nullable = true) {\n+  return struct_({field(name, type, nullable)});\n+}\n+\n+static std::shared_ptr<DataType> List(std::shared_ptr<DataType> type,\n+                                      bool nullable = true) {\n+  // TODO should field name \"element\" (Parquet convention for List nodes)\n+  // be changed to \"item\" (Arrow convention for List types)?\n+  return list(field(\"element\", type, nullable));\n+}\n+\n+//\n+// Primitive columns with no intermediate group node\n+//\n+\n+TEST_F(TestReconstructColumn, PrimitiveOptional) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::OPTIONAL, ParquetType::INT32));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, null, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, PrimitiveRequired) {\n+  SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REQUIRED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(int32(), \"[4, 5, 6]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(PrimitiveRepeated)) {\n+  // Arrow schema: list(int32 not null) not null\n+  this->SetParquetSchema(\n+      PrimitiveNode::Make(\"node_name\", Repetition::REPEATED, ParquetType::INT32));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {0, 0, 1, 0};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(list(field(\"node_name\", int32(), /*nullable=*/false)),\n+                                \"[[], [4, 5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequired) {\n+  // Arrow schema: struct(a: int32 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[[4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequired)) {\n+  // Arrow schema: struct(a: int32 not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", int32(), false), \"[null, [4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedRequiredOptional) {\n+  // Arrow schema: struct(a: int32) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32()), \"[[null], [4], [5], [6]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedOptionalOptional) {\n+  // Arrow schema: struct(a: int32)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {PrimitiveNode::Make(\"a\", Repetition::OPTIONAL, ParquetType::INT32)}));\n+\n+  LevelVector def_levels = {0, 1, 2, 2};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", int32()), \"[null, [null], [4], [5]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Nested struct encodings (one field each)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedRequiredRequiredRequired) {\n+  // Arrow schema: struct(a: struct(b: int32 not null) not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+          \"a\", Repetition::REQUIRED,\n+          {PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected =\n+      ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32(), false), false),\n+                    \"[[[4]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedRequiredOptionalRequired) {\n+  // Arrow schema: struct(a: struct(b: int32 not null)) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {GroupNode::Make(\n+          \"a\", Repetition::OPTIONAL,\n+          {PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 0, 1, 1};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32(), false)),\n+                                \"[[[4]], [[null]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedOptionalRequiredOptional)) {\n+  // Arrow schema: struct(a: struct(b: int32) not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+          \"a\", Repetition::REQUIRED,\n+          {PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 2, 0, 2, 2};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32()), false),\n+                                \"[[[null]], [[4]], null, [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedOptionalOptionalOptional) {\n+  // Arrow schema: struct(a: struct(b: int32) not null)\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::OPTIONAL,\n+      {GroupNode::Make(\n+          \"a\", Repetition::OPTIONAL,\n+          {PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT32)})}));\n+\n+  LevelVector def_levels = {1, 2, 0, 3, 3, 3};\n+  LevelVector rep_levels = {};\n+  std::vector<int32_t> values = {4, 5, 6};\n+\n+  auto expected = ArrayFromJSON(OneFieldStruct(\"a\", OneFieldStruct(\"b\", int32())),\n+                                \"[[null], [[null]], null, [[4]], [[5]], [[6]]]\");\n+  AssertReconstruct<ParquetType::INT32>(*expected, def_levels, rep_levels, values);\n+}\n+\n+//\n+// Struct encodings (two fields)\n+//\n+\n+TEST_F(TestReconstructColumn, NestedTwoFields1) {\n+  // Arrow schema: struct(a: int32 not null, b: int64 not null) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::REQUIRED, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{4, 5, 6}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{}, RepLevels{}, Int64Vector{7, 8, 9}));\n+\n+  auto type = struct_(\n+      {field(\"a\", int32(), /*nullable=*/false), field(\"b\", int64(), /*nullable=*/false)});\n+  auto expected = ArrayFromJSON(type, \"[[4, 7], [5, 8], [6, 9]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, NestedTwoFields2) {\n+  // Arrow schema: struct(a: int32 not null, b: int64) not null\n+  SetParquetSchema(GroupNode::Make(\n+      \"parent\", Repetition::REQUIRED,\n+      {PrimitiveNode::Make(\"a\", Repetition::REQUIRED, ParquetType::INT32),\n+       PrimitiveNode::Make(\"b\", Repetition::OPTIONAL, ParquetType::INT64)}));\n+\n+  ASSERT_OK(WriteInt32Column(DefLevels{}, RepLevels{}, Int32Vector{4, 5, 6}));\n+  ASSERT_OK(WriteInt64Column(DefLevels{0, 1, 1}, RepLevels{}, Int64Vector{7, 8}));\n+\n+  auto type = struct_({field(\"a\", int32(), /*nullable=*/false), field(\"b\", int64())});\n+  auto expected = ArrayFromJSON(type, \"[[4, null], [5, 7], [6, 8]]\");\n+\n+  CheckColumn(0, *expected);\n+}\n+\n+TEST_F(TestReconstructColumn, FAILING(NestedTwoFields3)) {\n\nReview comment:\n       Structs are accepted but in some cases nulls are not written at the right nesting level.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-07T12:45:35.777+0000",
                    "updated": "2020-09-07T12:45:35.777+0000",
                    "started": "2020-09-07T12:45:35.776+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "479627",
                    "issueId": "13325786"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/worklog/479642",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou closed pull request #8100:\nURL: https://github.com/apache/arrow/pull/8100\n\n\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-09-07T13:35:54.989+0000",
                    "updated": "2020-09-07T13:35:54.989+0000",
                    "started": "2020-09-07T13:35:54.988+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "479642",
                    "issueId": "13325786"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
            "id": "7",
            "description": "The sub-task of the issue",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
            "name": "Sub-task",
            "subtask": true,
            "avatarId": 21146
        },
        "timespent": 6600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@42b32dbe[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@762e30c6[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3aad8a57[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@3d0235c4[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1d39095f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@381c87f4[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6e354d92[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@4235e985[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1713636d[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@4bec655c[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@ff1f86e[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@626551d4[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 6600,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Sep 07 13:36:54 UTC 2020",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2020-09-07T13:36:54.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-9901/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2020-09-02T17:54:32.000+0000",
        "updated": "2020-09-07T13:36:54.000+0000",
        "timeoriginalestimate": null,
        "description": "We should write tests where definition and repetition levels are explicitly written out for a particular Parquet schema, then read as a Arrow column.\r\n\r\nSketch here:\r\nhttps://gist.github.com/pitrou/282dd790cac0eb2c1b59e8c9ab1941d8",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "1h 50m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 6600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Add hand-crafted Parquet to Arrow reconstruction test for nested reading",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13325786/comment/17191710",
                    "id": "17191710",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "Issue resolved by pull request 8100\n[https://github.com/apache/arrow/pull/8100]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2020-09-07T13:36:54.785+0000",
                    "updated": "2020-09-07T13:36:54.785+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0iaqw:",
        "customfield_12314139": null
    }
}