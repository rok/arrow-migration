{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13472954",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954",
    "key": "ARROW-17188",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12351550",
                "id": "12351550",
                "name": "9.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-08-03"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12351550",
                "id": "12351550",
                "name": "9.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-08-03"
            }
        ],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=willjones127",
            "name": "willjones127",
            "key": "willjones127",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"
            },
            "displayName": "Will Jones",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12333008",
                "id": "12333008",
                "name": "R"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=willjones127",
            "name": "willjones127",
            "key": "willjones127",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"
            },
            "displayName": "Will Jones",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=willjones127",
            "name": "willjones127",
            "key": "willjones127",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"
            },
            "displayName": "Will Jones",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 7800,
            "total": 7800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 7800,
            "total": 7800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-17188/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 13,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795725",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "dragosmg commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931211294\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and Tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and Writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n   `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)]\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+* Removed `read_arrow()` and `write_arrow()` functions. They have been deprecated for several versions in favor of corresponding \"ipc\" and \"feather\" functions. (ARROW-16268)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query`. (ARROW-15016)\n\nReview Comment:\n   Do we want to add here that both `dplyr::show_query()` and `dplyr::explain()` also work?\n\n\n\n",
                    "created": "2022-07-27T15:33:07.697+0000",
                    "updated": "2022-07-27T15:33:07.697+0000",
                    "started": "2022-07-27T15:33:07.696+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795725",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795726",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "raulcd commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931213113\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n\nReview Comment:\n   Shouldn't this be 9.0.0?\r\n   ```suggestion\r\n   # arrow 9.0.0\r\n   ```\n\n\n\n",
                    "created": "2022-07-27T15:35:13.157+0000",
                    "updated": "2022-07-27T15:35:13.157+0000",
                    "started": "2022-07-27T15:35:13.156+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795726",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795727",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "dragosmg commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931211294\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and Tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and Writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n   `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)]\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+* Removed `read_arrow()` and `write_arrow()` functions. They have been deprecated for several versions in favor of corresponding \"ipc\" and \"feather\" functions. (ARROW-16268)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query`. (ARROW-15016)\n\nReview Comment:\n   Do we want to add here that both `dplyr::show_query()` and `dplyr::explain()` also work?\r\n   \r\n   We now support namespacing (`pkg::` prefixing) => something like the chunk below works.\r\n   ```r\r\n   mtcars %>% \r\n     mutate(make_model = rownames(.)) %>% \r\n     arrow_table() %>% \r\n     mutate(name_length = stringr::str_length(make_model)) %>% \r\n     collect()\r\n   ```\n\n\n\n",
                    "created": "2022-07-27T15:39:28.210+0000",
                    "updated": "2022-07-27T15:39:28.210+0000",
                    "started": "2022-07-27T15:39:28.210+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795727",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795728",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "raulcd commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931218589\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n\nReview Comment:\n   ok, I see this is supposed to be done on the `utils-prepare.sh` script as with the other versions.\n\n\n\n",
                    "created": "2022-07-27T15:39:48.268+0000",
                    "updated": "2022-07-27T15:39:48.268+0000",
                    "started": "2022-07-27T15:39:48.268+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795728",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795777",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931353783\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n\nReview Comment:\n   Correct, we don't make this change manually\n\n\n\n",
                    "created": "2022-07-27T18:04:14.732+0000",
                    "updated": "2022-07-27T18:04:14.732+0000",
                    "started": "2022-07-27T18:04:14.731+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795777",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795784",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931357047\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n\nReview Comment:\n   Maybe lead with this one? We should sort the section based on relevance/priority\n\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n\nReview Comment:\n   This was already sorta the case for csv and json, but there were some bugs. But parquet and feather don't automatically do anything with the file path\n\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n\nReview Comment:\n   Let's reorder: First dplyr, then reading/writing, then this (or general assorted bugfixes), then packaging\n\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query()`. `dplyr::show_query()` and `dplyr::explain()` also work in Arrow dplyr pipelines. (ARROW-15016)\n+* Functions can be called with package namespace prefixes (e.g. `stringr::`, `lubridate::`) within queries. For example, `stringr::str_length` will now dispatch to the same kernel as `str_length`. (ARROW-14575)\n+* User-defined functions are supported in queries. Use `register_scalar_function()` to create them. (ARROW-16444)\n+* `lubridate::parse_date_time()` datetime parser: (ARROW-14848, ARROW-16407)\n+  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n\nReview Comment:\n   Are some orders not supported?\n\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query()`. `dplyr::show_query()` and `dplyr::explain()` also work in Arrow dplyr pipelines. (ARROW-15016)\n+* Functions can be called with package namespace prefixes (e.g. `stringr::`, `lubridate::`) within queries. For example, `stringr::str_length` will now dispatch to the same kernel as `str_length`. (ARROW-14575)\n+* User-defined functions are supported in queries. Use `register_scalar_function()` to create them. (ARROW-16444)\n\nReview Comment:\n   This should go higher up. Also should discuss `map_batches()` alongside this since they're both kinds of UDF\n\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n\nReview Comment:\n   Likewise let's lead with major new features (new dplyr verbs, then new functions) and put bug fixes at the end\n\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query()`. `dplyr::show_query()` and `dplyr::explain()` also work in Arrow dplyr pipelines. (ARROW-15016)\n+* Functions can be called with package namespace prefixes (e.g. `stringr::`, `lubridate::`) within queries. For example, `stringr::str_length` will now dispatch to the same kernel as `str_length`. (ARROW-14575)\n\nReview Comment:\n   This is also significant\n\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query()`. `dplyr::show_query()` and `dplyr::explain()` also work in Arrow dplyr pipelines. (ARROW-15016)\n+* Functions can be called with package namespace prefixes (e.g. `stringr::`, `lubridate::`) within queries. For example, `stringr::str_length` will now dispatch to the same kernel as `str_length`. (ARROW-14575)\n+* User-defined functions are supported in queries. Use `register_scalar_function()` to create them. (ARROW-16444)\n+* `lubridate::parse_date_time()` datetime parser: (ARROW-14848, ARROW-16407)\n+  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n+  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+* `lubridate::ymd()` and related string date parsers supported. (ARROW-16394). Month (`ym`, `my`) and quarter (`yq`) resolution parsers are also added. (ARROW-16516)\n+* lubridate family of `ymd_hms` datetime parsing functions are supported. (ARROW-16395)\n+* `lubridate::fast_strptime()` supported. (ARROW-16439)\n\nReview Comment:\n   I'm not sure we need a separate bullet point for every function that just says \"supported\". We can group them as is relevant, and we don't need to include all of the JIRA issue ids.\n\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n\nReview Comment:\n   Can we say more than just \"supported\"?\n\n\n\n",
                    "created": "2022-07-27T18:11:55.293+0000",
                    "updated": "2022-07-27T18:11:55.293+0000",
                    "started": "2022-07-27T18:11:55.293+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795784",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795786",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "dragosmg commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931409676\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query()`. `dplyr::show_query()` and `dplyr::explain()` also work in Arrow dplyr pipelines. (ARROW-15016)\n+* Functions can be called with package namespace prefixes (e.g. `stringr::`, `lubridate::`) within queries. For example, `stringr::str_length` will now dispatch to the same kernel as `str_length`. (ARROW-14575)\n+* User-defined functions are supported in queries. Use `register_scalar_function()` to create them. (ARROW-16444)\n+* `lubridate::parse_date_time()` datetime parser: (ARROW-14848, ARROW-16407)\n+  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n\nReview Comment:\n   Yes, #13506 adds the remaining formats/ orders. So far the focus was on supporting the orders that would enable the higher level parsers (e.g. `ymd_hms()`).\n\n\n\n",
                    "created": "2022-07-27T18:20:31.138+0000",
                    "updated": "2022-07-27T18:20:31.138+0000",
                    "started": "2022-07-27T18:20:31.137+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795786",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795787",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "dragosmg commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931412125\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query()`. `dplyr::show_query()` and `dplyr::explain()` also work in Arrow dplyr pipelines. (ARROW-15016)\n+* Functions can be called with package namespace prefixes (e.g. `stringr::`, `lubridate::`) within queries. For example, `stringr::str_length` will now dispatch to the same kernel as `str_length`. (ARROW-14575)\n+* User-defined functions are supported in queries. Use `register_scalar_function()` to create them. (ARROW-16444)\n+* `lubridate::parse_date_time()` datetime parser: (ARROW-14848, ARROW-16407)\n+  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n+  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+* `lubridate::ymd()` and related string date parsers supported. (ARROW-16394). Month (`ym`, `my`) and quarter (`yq`) resolution parsers are also added. (ARROW-16516)\n+* lubridate family of `ymd_hms` datetime parsing functions are supported. (ARROW-16395)\n+* `lubridate::fast_strptime()` supported. (ARROW-16439)\n\nReview Comment:\n   I second \u261d\ud83c\udffb that. \n\n\n\n",
                    "created": "2022-07-27T18:21:00.967+0000",
                    "updated": "2022-07-27T18:21:00.967+0000",
                    "started": "2022-07-27T18:21:00.967+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795787",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795796",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#issuecomment-1197227572\n\n   https://issues.apache.org/jira/browse/ARROW-17188\n\n\n",
                    "created": "2022-07-27T18:47:00.503+0000",
                    "updated": "2022-07-27T18:47:00.503+0000",
                    "started": "2022-07-27T18:47:00.503+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795796",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795800",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931480008\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n\nReview Comment:\n   Sure, though I was putting these at the top because they seemed like critical bugfixes.\n\n\n\n",
                    "created": "2022-07-27T18:54:18.692+0000",
                    "updated": "2022-07-27T18:54:18.692+0000",
                    "started": "2022-07-27T18:54:18.691+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795800",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795802",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on code in PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#discussion_r931485167\n\n\n##########\nr/NEWS.md:\n##########\n@@ -19,19 +19,54 @@\n \n # arrow 8.0.0.9000\n \n-* The `arrow.dev_repo` for nightly builds of the R package and prebuilt\n-  libarrow binaries is now https://nightlies.apache.org/arrow/r/.\n-* `lubridate::parse_date_time()` datetime parser:\n-  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n-  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+## Arrays and tables\n+\n+* Table and RecordBatch `$num_rows()` method returns a double (previously integer), avoiding integer overflow on larger tables. (ARROW-14989, ARROW-16977)\n+\n+## Reading and writing\n+\n * New functions `read_ipc_file()` and `write_ipc_file()` are added.\n   These functions are almost the same as `read_feather()` and `write_feather()`,\n   but differ in that they only target IPC files (Feather V2 files), not Feather V1 files.\n * `read_arrow()` and `write_arrow()`, deprecated since 1.0.0 (July 2020), have been removed.\n   Instead of these, use the `read_ipc_file()` and `write_ipc_file()` for IPC files, or,\n-  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams.\n-* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps.\n+  `read_ipc_stream()` and `write_ipc_stream()` for IPC streams. (ARROW-16268)\n+* `write_parquet()` now defaults to writing Parquet format version 2.4 (was 1.0). Previously deprecated arguments `properties` and `arrow_properties` have been removed; if you need to deal with these lower-level properties objects directly, use `ParquetFileWriter`, which `write_parquet()` wraps. (ARROW-16715)\n+* UnionDatasets can unify schemas of multiple InMemoryDatasets with varying\n+  schemas. (ARROW-16085)\n+* `write_dataset()` preserves all schema metadata again. In 8.0.0, it would drop most metadata, breaking packages such as sfarrow. (ARROW-16511)\n+* Reading and writing functions (such as `write_csv_arrow()`) will automatically (de-)compress data if the file path contains a compression extension (e.g. `\"data.csv.gz\"`). This works locally as well as on remote filesystems like S3 and GCS. (ARROW-16144)\n+* `FileSystemFactoryOptions` can be provided to `open_dataset()`, allowing you to pass options such as which file prefixes to ignore. (ARROW-15280)\n+* By default, `S3FileSystem` will not create or delete buckets. To enable that, pass the configuration option `allow_bucket_creation` or `allow_bucket_deletion`. (ARROW-15906)\n+* `GcsFileSystem` and `gs_bucket()` allow connecting to Google Cloud Storage. (ARROW-13404, ARROW-16887)\n+\n+## Arrow dplyr queries\n+\n+* Bugfixes:\n+  * Count distinct now gives correct result across multiple row groups. (ARROW-16807)\n+  * Aggregations over partition columns return correct results. (ARROW-16700)\n+* `dplyr::union` and `dplyr::union_all` are supported. (ARROW-15622)\n+* `dplyr::glimpse` is supported. (ARROW-16776)\n+* `show_exec_plan()` can be added to the end of a dplyr pipeline to show the underlying plan, similar to `dplyr::show_query()`. `dplyr::show_query()` and `dplyr::explain()` also work in Arrow dplyr pipelines. (ARROW-15016)\n+* Functions can be called with package namespace prefixes (e.g. `stringr::`, `lubridate::`) within queries. For example, `stringr::str_length` will now dispatch to the same kernel as `str_length`. (ARROW-14575)\n+* User-defined functions are supported in queries. Use `register_scalar_function()` to create them. (ARROW-16444)\n+* `lubridate::parse_date_time()` datetime parser: (ARROW-14848, ARROW-16407)\n+  * `orders` with year, month, day, hours, minutes, and seconds components are supported.\n+  * the `orders` argument in the Arrow binding works as follows: `orders` are transformed into `formats` which subsequently get applied in turn. There is no `select_formats` parameter and no inference takes place (like is the case in `lubridate::parse_date_time()`).\n+* `lubridate::ymd()` and related string date parsers supported. (ARROW-16394). Month (`ym`, `my`) and quarter (`yq`) resolution parsers are also added. (ARROW-16516)\n+* lubridate family of `ymd_hms` datetime parsing functions are supported. (ARROW-16395)\n+* `lubridate::fast_strptime()` supported. (ARROW-16439)\n\nReview Comment:\n   Yeah I'll consolidate. Wrote those bullets in a hurry :)\r\n   \r\n   I include the JIRA IDs because it makes it *way* easier for me to revise the news. We can strip them at the end if people don't want them shown publicly.\n\n\n\n",
                    "created": "2022-07-27T19:00:24.204+0000",
                    "updated": "2022-07-27T19:00:24.204+0000",
                    "started": "2022-07-27T19:00:24.203+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795802",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795851",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "kszucs merged PR #13726:\nURL: https://github.com/apache/arrow/pull/13726\n\n\n",
                    "created": "2022-07-28T00:07:58.572+0000",
                    "updated": "2022-07-28T00:07:58.572+0000",
                    "started": "2022-07-28T00:07:58.571+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795851",
                    "issueId": "13472954"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/worklog/795946",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot commented on PR #13726:\nURL: https://github.com/apache/arrow/pull/13726#issuecomment-1197729042\n\n   Benchmark runs are scheduled for baseline = 71ccff9c12657070c1c39e66b9779896f698b280 and contender = a5f0c56af89a99e581ee77e764d9fe0944d2bd6c. a5f0c56af89a99e581ee77e764d9fe0944d2bd6c is a master commit associated with this PR. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Failed :arrow_down:0.0% :arrow_up:0.0%] [ec2-t3-xlarge-us-east-2](https://conbench.ursa.dev/compare/runs/1841e8d2903c46a6a3deb6af8684b8a8...3033e0219ca643bd983b4ab3fc48c807/)\n   [Failed :arrow_down:0.24% :arrow_up:0.0%] [test-mac-arm](https://conbench.ursa.dev/compare/runs/57f8a09ffd484474a182d21ff3e52622...138a72b1fa9e41d481e40107368c2ee5/)\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ursa-i9-9960x](https://conbench.ursa.dev/compare/runs/92e14d1f700343d3bded2dd734d1be91...7226da8d7f2d46d4b232dcf91e496f1e/)\n   [Finished :arrow_down:0.21% :arrow_up:0.07%] [ursa-thinkcentre-m75q](https://conbench.ursa.dev/compare/runs/bbca2733db82495bbef1fe08e28fa45f...81b807013e5b42fbb9f8d19a49e510f6/)\n   Buildkite builds:\n   [Failed] [`a5f0c56a` ec2-t3-xlarge-us-east-2](https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-ec2-t3-xlarge-us-east-2/builds/1211)\n   [Failed] [`a5f0c56a` test-mac-arm](https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-test-mac-arm/builds/1223)\n   [Finished] [`a5f0c56a` ursa-i9-9960x](https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-ursa-i9-9960x/builds/1205)\n   [Finished] [`a5f0c56a` ursa-thinkcentre-m75q](https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-ursa-thinkcentre-m75q/builds/1225)\n   [Failed] [`71ccff9c` ec2-t3-xlarge-us-east-2](https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-ec2-t3-xlarge-us-east-2/builds/1210)\n   [Finished] [`71ccff9c` test-mac-arm](https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-test-mac-arm/builds/1222)\n   [Finished] [`71ccff9c` ursa-i9-9960x](https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-ursa-i9-9960x/builds/1204)\n   [Finished] [`71ccff9c` ursa-thinkcentre-m75q](https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-ursa-thinkcentre-m75q/builds/1224)\n   Supported benchmarks:\n   ec2-t3-xlarge-us-east-2: Supported benchmark langs: Python, R. Runs only benchmarks with cloud = True\n   test-mac-arm: Supported benchmark langs: C++, Python, R\n   ursa-i9-9960x: Supported benchmark langs: Python, R, JavaScript\n   ursa-thinkcentre-m75q: Supported benchmark langs: C++, Java\n   \n\n\n",
                    "created": "2022-07-28T06:41:23.303+0000",
                    "updated": "2022-07-28T06:41:23.303+0000",
                    "started": "2022-07-28T06:41:23.303+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "795946",
                    "issueId": "13472954"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 7800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@5e887f73[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@790fa68f[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5d5a3123[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@5965cfd9[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@6dfa66a8[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@8469793[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4997c91[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@737570dc[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4d84973c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@71c26b10[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3ef0fc4d[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@66db88df[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 7800,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Jul 28 00:07:59 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-07-28T00:07:59.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-17188/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2022-07-22T17:14:32.000+0000",
        "updated": "2022-07-28T06:41:23.000+0000",
        "timeoriginalestimate": null,
        "description": null,
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "2h 10m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 7800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[R] Update news for 9.0.0",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/comment/17571401",
                    "id": "17571401",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=raulcd",
                        "name": "raulcd",
                        "key": "JIRAUSER287560",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
                        },
                        "displayName": "Ra\u00fal Cumplido",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "[~willjones127] is this required to be included on the Release or can be merged after the Release? Just to understand whether this should block the Release or not",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=raulcd",
                        "name": "raulcd",
                        "key": "JIRAUSER287560",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34050",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34050",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34050",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34050"
                        },
                        "displayName": "Ra\u00fal Cumplido",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2022-07-26T12:56:15.240+0000",
                    "updated": "2022-07-26T12:56:15.240+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13472954/comment/17572170",
                    "id": "17572170",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "body": "Issue resolved by pull request 13726\n[https://github.com/apache/arrow/pull/13726]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "created": "2022-07-28T00:07:59.591+0000",
                    "updated": "2022-07-28T00:07:59.591+0000"
                }
            ],
            "maxResults": 2,
            "total": 2,
            "startAt": 0
        },
        "customfield_12311820": "0|z173j4:",
        "customfield_12314139": null
    }
}