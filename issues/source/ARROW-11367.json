{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13354225",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225",
    "key": "ARROW-11367",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12349493",
                "id": "12349493",
                "description": "",
                "name": "4.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-04-26"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=yibocai",
            "name": "yibocai",
            "key": "yibo",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=yibo&avatarId=47542",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yibo&avatarId=47542",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yibo&avatarId=47542",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yibo&avatarId=47542"
            },
            "displayName": "Yibo Cai",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=yibocai",
            "name": "yibocai",
            "key": "yibo",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=yibo&avatarId=47542",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yibo&avatarId=47542",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yibo&avatarId=47542",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yibo&avatarId=47542"
            },
            "displayName": "Yibo Cai",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=yibocai",
            "name": "yibocai",
            "key": "yibo",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=yibo&avatarId=47542",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yibo&avatarId=47542",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yibo&avatarId=47542",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yibo&avatarId=47542"
            },
            "displayName": "Yibo Cai",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 25800,
            "total": 25800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 25800,
            "total": 25800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11367/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 43,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/540812",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 opened a new pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310\n\n\n   t-Digest is a data structure to approximate accurate quantiles of\r\n   arbitrary length dataset using constant space.\r\n   This utility will be used in implementing approximate quantile kernel\r\n   and latency estimation in flightrpc benchmark.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-25T02:36:01.073+0000",
                    "updated": "2021-01-25T02:36:01.073+0000",
                    "started": "2021-01-25T02:36:01.073+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "540812",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/540813",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#issuecomment-766501335\n\n\n   https://issues.apache.org/jira/browse/ARROW-11367\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-25T02:36:24.710+0000",
                    "updated": "2021-01-25T02:36:24.710+0000",
                    "started": "2021-01-25T02:36:24.710+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "540813",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/540882",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#issuecomment-766570951\n\n\n   mingw64 ci failure is not relevant\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-25T06:13:06.731+0000",
                    "updated": "2021-01-25T06:13:06.731+0000",
                    "started": "2021-01-25T06:13:06.731+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "540882",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/541558",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#issuecomment-766501335\n\n\n   https://issues.apache.org/jira/browse/ARROW-11367\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-26T03:58:37.570+0000",
                    "updated": "2021-01-26T03:58:37.570+0000",
                    "started": "2021-01-26T03:58:37.569+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "541558",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/541561",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#issuecomment-766570951\n\n\n   mingw64 ci failure is not relevant\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-26T03:59:08.896+0000",
                    "updated": "2021-01-26T03:59:08.896+0000",
                    "started": "2021-01-26T03:59:08.895+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "541561",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/541675",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 opened a new pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310\n\n\n   t-Digest is a data structure to approximate accurate quantiles of\r\n   arbitrary length dataset using constant space.\r\n   This utility will be used in implementing approximate quantile kernel\r\n   and latency estimation in flightrpc benchmark.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-26T04:09:09.039+0000",
                    "updated": "2021-01-26T04:09:09.039+0000",
                    "started": "2021-01-26T04:09:09.039+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "541675",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542046",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 removed a comment on pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#issuecomment-766570951\n\n\n   mingw64 ci failure is not relevant\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-26T07:08:28.105+0000",
                    "updated": "2021-01-26T07:08:28.105+0000",
                    "started": "2021-01-26T07:08:28.104+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542046",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542296",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r564696684\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n\nReview comment:\n       `td.back().Merge(centroid)`?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n\nReview comment:\n       Why is everything below exposed in this `.h` file?\r\n   \r\n   It seems that only the `TDigest` class needs to be exposed, and only `TDigest::Add(value)` needs to be inlined. All the rest can be hidden in a `.cc` file, reducing inclusions and achieving better modularity.\r\n   \n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n\nReview comment:\n       Hmm... if you want to minimize errors, perhaps you should have:\r\n   ```c++\r\n   const double q = q_prev + td[i].weight;\r\n   const double k = this->K(q / total_weight);\r\n   ```\r\n   ?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n+    MergeInput();\n+    // check weight, centroid order\n+    double total_weight = 0, prev_mean = std::numeric_limits<double>::lowest();\n+    for (const auto& centroid : *tdigest_) {\n+      if (std::isnan(centroid.mean) || std::isnan(centroid.weight)) {\n+        ARROW_LOG(ERROR) << \"NAN found in tdigest\";\n+        return false;\n+      }\n+      if (centroid.mean < prev_mean) {\n+        ARROW_LOG(ERROR) << \"centroid mean decreases\";\n+        return false;\n+      }\n+      if (centroid.weight < 1) {\n+        ARROW_LOG(ERROR) << \"invalid centroid weight\";\n+        return false;\n+      }\n+      prev_mean = centroid.mean;\n+      total_weight += centroid.weight;\n+    }\n+    if (total_weight != total_weight_) {\n+      ARROW_LOG(ERROR) << \"tdigest total weight mismatch\";\n+      return false;\n+    }\n+    // check if buffer expanded\n+    if (tdigest_->capacity() > delta_) {\n+      ARROW_LOG(ERROR) << \"oversized tdigest buffer\";\n+      return false;\n+    }\n+    // check k-size\n+    return merger_.Verify(tdigest_, total_weight_);\n+  }\n+\n+  // dump internal data, only for debug\n+  void Dump() {\n+    MergeInput();\n+    const auto& td = *tdigest_;\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      std::cout << i << \": mean = \" << td[i].mean << \", weight = \" << td[i].weight\n\nReview comment:\n       `std::cerr` would be better. \n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n\nReview comment:\n       Also, should this be `const`?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n\nReview comment:\n       Return a `Status`?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n\nReview comment:\n       This doesn't seem used anywhere. Is there a reason to keep it?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n\nReview comment:\n       I suppose this formula is meant to avoid accumulating errors?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n\nReview comment:\n       I don't think we want to log this, especially as Arrow is a library.\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n+    MergeInput();\n+    // check weight, centroid order\n+    double total_weight = 0, prev_mean = std::numeric_limits<double>::lowest();\n+    for (const auto& centroid : *tdigest_) {\n+      if (std::isnan(centroid.mean) || std::isnan(centroid.weight)) {\n+        ARROW_LOG(ERROR) << \"NAN found in tdigest\";\n+        return false;\n+      }\n+      if (centroid.mean < prev_mean) {\n+        ARROW_LOG(ERROR) << \"centroid mean decreases\";\n+        return false;\n+      }\n+      if (centroid.weight < 1) {\n+        ARROW_LOG(ERROR) << \"invalid centroid weight\";\n+        return false;\n+      }\n+      prev_mean = centroid.mean;\n+      total_weight += centroid.weight;\n+    }\n+    if (total_weight != total_weight_) {\n\nReview comment:\n       Is this equality really supposed to hold exactly?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n+    MergeInput();\n+    // check weight, centroid order\n+    double total_weight = 0, prev_mean = std::numeric_limits<double>::lowest();\n+    for (const auto& centroid : *tdigest_) {\n+      if (std::isnan(centroid.mean) || std::isnan(centroid.weight)) {\n+        ARROW_LOG(ERROR) << \"NAN found in tdigest\";\n+        return false;\n+      }\n+      if (centroid.mean < prev_mean) {\n+        ARROW_LOG(ERROR) << \"centroid mean decreases\";\n+        return false;\n+      }\n+      if (centroid.weight < 1) {\n+        ARROW_LOG(ERROR) << \"invalid centroid weight\";\n+        return false;\n+      }\n+      prev_mean = centroid.mean;\n+      total_weight += centroid.weight;\n+    }\n+    if (total_weight != total_weight_) {\n+      ARROW_LOG(ERROR) << \"tdigest total weight mismatch\";\n+      return false;\n+    }\n+    // check if buffer expanded\n+    if (tdigest_->capacity() > delta_) {\n+      ARROW_LOG(ERROR) << \"oversized tdigest buffer\";\n+      return false;\n+    }\n+    // check k-size\n+    return merger_.Verify(tdigest_, total_weight_);\n+  }\n+\n+  // dump internal data, only for debug\n+  void Dump() {\n+    MergeInput();\n+    const auto& td = *tdigest_;\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      std::cout << i << \": mean = \" << td[i].mean << \", weight = \" << td[i].weight\n+                << std::endl;\n+    }\n+    std::cout << \"min = \" << min_ << \", max = \" << max_ << std::endl;\n+    const size_t total_memory =\n+        2 * td.capacity() * sizeof(detail::Centroid) + input_.capacity() * sizeof(double);\n+    std::cout << \"total memory = \" << total_memory << \" bytes\" << std::endl;\n+  }\n+\n+  // buffer a single data point, consume internal buffer if full\n+  // this function is intensively called and performance critical\n+  void Add(double value) {\n+    DCHECK(!std::isnan(value)) << \"cannot add NAN\";\n+    if (ARROW_PREDICT_FALSE(input_.size() == input_.capacity())) {\n+      MergeInput();\n+    }\n+    input_.push_back(value);\n+  }\n+\n+  // merge with other t-digests, called infrequently\n+  void Merge(std::vector<TDigest*>& tdigests) {\n+    // current and end iterator\n+    using CentroidIter = std::vector<detail::Centroid>::const_iterator;\n+    using CentroidIterPair = std::pair<CentroidIter, CentroidIter>;\n+    // use a min-heap to find next minimal centroid from all tdigests\n+    auto centroid_gt = [](const CentroidIterPair& lhs, const CentroidIterPair& rhs) {\n+      return lhs.first->mean > rhs.first->mean;\n+    };\n+    using CentroidQueue =\n+        std::priority_queue<CentroidIterPair, std::vector<CentroidIterPair>,\n+                            decltype(centroid_gt)>;\n+\n+    // trivial dynamic memory allocated at runtime\n+    std::vector<CentroidIterPair> queue_buffer;\n+    queue_buffer.reserve(tdigests.size() + 1);\n+    CentroidQueue queue(std::move(centroid_gt), std::move(queue_buffer));\n+\n+    MergeInput();\n+    if (tdigest_->size() > 0) {\n+      queue.emplace(tdigest_->cbegin(), tdigest_->cend());\n+    }\n+    for (TDigest* td : tdigests) {\n+      td->MergeInput();\n+      if (td->tdigest_->size() > 0) {\n+        queue.emplace(td->tdigest_->cbegin(), td->tdigest_->cend());\n+        total_weight_ += td->total_weight_;\n+        min_ = std::min(min_, td->min_);\n+        max_ = std::max(max_, td->max_);\n+      }\n+    }\n+\n+    merger_.Reset(total_weight_, tdigest_next_);\n+\n+    CentroidIter current_iter, end_iter;\n+    // do k-way merge till one buffer left\n+    while (queue.size() > 1) {\n+      std::tie(current_iter, end_iter) = queue.top();\n+      merger_.Add(*current_iter);\n+      queue.pop();\n+      if (++current_iter != end_iter) {\n+        queue.emplace(current_iter, end_iter);\n+      }\n+    }\n+    // merge last buffer\n+    if (!queue.empty()) {\n+      std::tie(current_iter, end_iter) = queue.top();\n+      while (current_iter != end_iter) {\n+        merger_.Add(*current_iter++);\n+      }\n+    }\n+\n+    std::swap(tdigest_, tdigest_next_);\n+  }\n+\n+  // an ugly helper\n+  void Merge(std::vector<std::unique_ptr<TDigest>>& ptrs) {\n+    std::vector<TDigest*> tdigests;\n+    tdigests.reserve(ptrs.size());\n+    for (auto& ptr : ptrs) {\n+      tdigests.push_back(ptr.get());\n+    }\n+    Merge(tdigests);\n+  }\n+\n+  // calculate quantile\n+  double Quantile(double q) {\n+    MergeInput();\n+\n+    const auto& td = *tdigest_;\n+\n+    if (q < 0 || q > 1) {\n+      ARROW_LOG(ERROR) << \"quantile must be between 0 and 1\";\n+      return NAN;\n+    }\n+    if (td.size() == 0) {\n+      ARROW_LOG(WARNING) << \"empty tdigest\";\n\nReview comment:\n       We don't want to warn if it's ok to have an empty digest.\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n\nReview comment:\n       Return a `Status` instead, so as to return the error message?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n+    MergeInput();\n+    // check weight, centroid order\n+    double total_weight = 0, prev_mean = std::numeric_limits<double>::lowest();\n+    for (const auto& centroid : *tdigest_) {\n+      if (std::isnan(centroid.mean) || std::isnan(centroid.weight)) {\n+        ARROW_LOG(ERROR) << \"NAN found in tdigest\";\n+        return false;\n+      }\n+      if (centroid.mean < prev_mean) {\n+        ARROW_LOG(ERROR) << \"centroid mean decreases\";\n+        return false;\n+      }\n+      if (centroid.weight < 1) {\n+        ARROW_LOG(ERROR) << \"invalid centroid weight\";\n+        return false;\n+      }\n+      prev_mean = centroid.mean;\n+      total_weight += centroid.weight;\n+    }\n+    if (total_weight != total_weight_) {\n+      ARROW_LOG(ERROR) << \"tdigest total weight mismatch\";\n+      return false;\n+    }\n+    // check if buffer expanded\n+    if (tdigest_->capacity() > delta_) {\n+      ARROW_LOG(ERROR) << \"oversized tdigest buffer\";\n+      return false;\n+    }\n+    // check k-size\n+    return merger_.Verify(tdigest_, total_weight_);\n+  }\n+\n+  // dump internal data, only for debug\n+  void Dump() {\n+    MergeInput();\n+    const auto& td = *tdigest_;\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      std::cout << i << \": mean = \" << td[i].mean << \", weight = \" << td[i].weight\n+                << std::endl;\n+    }\n+    std::cout << \"min = \" << min_ << \", max = \" << max_ << std::endl;\n+    const size_t total_memory =\n+        2 * td.capacity() * sizeof(detail::Centroid) + input_.capacity() * sizeof(double);\n+    std::cout << \"total memory = \" << total_memory << \" bytes\" << std::endl;\n+  }\n+\n+  // buffer a single data point, consume internal buffer if full\n+  // this function is intensively called and performance critical\n+  void Add(double value) {\n+    DCHECK(!std::isnan(value)) << \"cannot add NAN\";\n+    if (ARROW_PREDICT_FALSE(input_.size() == input_.capacity())) {\n+      MergeInput();\n+    }\n+    input_.push_back(value);\n+  }\n+\n+  // merge with other t-digests, called infrequently\n+  void Merge(std::vector<TDigest*>& tdigests) {\n+    // current and end iterator\n+    using CentroidIter = std::vector<detail::Centroid>::const_iterator;\n+    using CentroidIterPair = std::pair<CentroidIter, CentroidIter>;\n+    // use a min-heap to find next minimal centroid from all tdigests\n+    auto centroid_gt = [](const CentroidIterPair& lhs, const CentroidIterPair& rhs) {\n+      return lhs.first->mean > rhs.first->mean;\n+    };\n+    using CentroidQueue =\n+        std::priority_queue<CentroidIterPair, std::vector<CentroidIterPair>,\n+                            decltype(centroid_gt)>;\n+\n+    // trivial dynamic memory allocated at runtime\n+    std::vector<CentroidIterPair> queue_buffer;\n+    queue_buffer.reserve(tdigests.size() + 1);\n+    CentroidQueue queue(std::move(centroid_gt), std::move(queue_buffer));\n+\n+    MergeInput();\n+    if (tdigest_->size() > 0) {\n+      queue.emplace(tdigest_->cbegin(), tdigest_->cend());\n+    }\n+    for (TDigest* td : tdigests) {\n+      td->MergeInput();\n+      if (td->tdigest_->size() > 0) {\n+        queue.emplace(td->tdigest_->cbegin(), td->tdigest_->cend());\n+        total_weight_ += td->total_weight_;\n+        min_ = std::min(min_, td->min_);\n+        max_ = std::max(max_, td->max_);\n+      }\n+    }\n+\n+    merger_.Reset(total_weight_, tdigest_next_);\n+\n+    CentroidIter current_iter, end_iter;\n+    // do k-way merge till one buffer left\n+    while (queue.size() > 1) {\n+      std::tie(current_iter, end_iter) = queue.top();\n+      merger_.Add(*current_iter);\n+      queue.pop();\n+      if (++current_iter != end_iter) {\n+        queue.emplace(current_iter, end_iter);\n+      }\n+    }\n+    // merge last buffer\n+    if (!queue.empty()) {\n+      std::tie(current_iter, end_iter) = queue.top();\n+      while (current_iter != end_iter) {\n+        merger_.Add(*current_iter++);\n+      }\n+    }\n+\n+    std::swap(tdigest_, tdigest_next_);\n+  }\n+\n+  // an ugly helper\n+  void Merge(std::vector<std::unique_ptr<TDigest>>& ptrs) {\n\nReview comment:\n       Why do you need both `Merge(vector<TDigest*>&)` and `Merge(vector<std::unique_ptr<TDigest>>&)`?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n+    MergeInput();\n+    // check weight, centroid order\n+    double total_weight = 0, prev_mean = std::numeric_limits<double>::lowest();\n+    for (const auto& centroid : *tdigest_) {\n+      if (std::isnan(centroid.mean) || std::isnan(centroid.weight)) {\n+        ARROW_LOG(ERROR) << \"NAN found in tdigest\";\n+        return false;\n+      }\n+      if (centroid.mean < prev_mean) {\n+        ARROW_LOG(ERROR) << \"centroid mean decreases\";\n+        return false;\n+      }\n+      if (centroid.weight < 1) {\n+        ARROW_LOG(ERROR) << \"invalid centroid weight\";\n+        return false;\n+      }\n+      prev_mean = centroid.mean;\n+      total_weight += centroid.weight;\n+    }\n+    if (total_weight != total_weight_) {\n+      ARROW_LOG(ERROR) << \"tdigest total weight mismatch\";\n+      return false;\n+    }\n+    // check if buffer expanded\n+    if (tdigest_->capacity() > delta_) {\n+      ARROW_LOG(ERROR) << \"oversized tdigest buffer\";\n+      return false;\n+    }\n+    // check k-size\n+    return merger_.Verify(tdigest_, total_weight_);\n+  }\n+\n+  // dump internal data, only for debug\n+  void Dump() {\n+    MergeInput();\n+    const auto& td = *tdigest_;\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      std::cout << i << \": mean = \" << td[i].mean << \", weight = \" << td[i].weight\n+                << std::endl;\n+    }\n+    std::cout << \"min = \" << min_ << \", max = \" << max_ << std::endl;\n+    const size_t total_memory =\n+        2 * td.capacity() * sizeof(detail::Centroid) + input_.capacity() * sizeof(double);\n+    std::cout << \"total memory = \" << total_memory << \" bytes\" << std::endl;\n+  }\n+\n+  // buffer a single data point, consume internal buffer if full\n+  // this function is intensively called and performance critical\n+  void Add(double value) {\n+    DCHECK(!std::isnan(value)) << \"cannot add NAN\";\n+    if (ARROW_PREDICT_FALSE(input_.size() == input_.capacity())) {\n+      MergeInput();\n+    }\n+    input_.push_back(value);\n+  }\n+\n+  // merge with other t-digests, called infrequently\n+  void Merge(std::vector<TDigest*>& tdigests) {\n+    // current and end iterator\n+    using CentroidIter = std::vector<detail::Centroid>::const_iterator;\n+    using CentroidIterPair = std::pair<CentroidIter, CentroidIter>;\n+    // use a min-heap to find next minimal centroid from all tdigests\n+    auto centroid_gt = [](const CentroidIterPair& lhs, const CentroidIterPair& rhs) {\n+      return lhs.first->mean > rhs.first->mean;\n+    };\n+    using CentroidQueue =\n+        std::priority_queue<CentroidIterPair, std::vector<CentroidIterPair>,\n+                            decltype(centroid_gt)>;\n+\n+    // trivial dynamic memory allocated at runtime\n+    std::vector<CentroidIterPair> queue_buffer;\n+    queue_buffer.reserve(tdigests.size() + 1);\n+    CentroidQueue queue(std::move(centroid_gt), std::move(queue_buffer));\n+\n+    MergeInput();\n+    if (tdigest_->size() > 0) {\n+      queue.emplace(tdigest_->cbegin(), tdigest_->cend());\n+    }\n+    for (TDigest* td : tdigests) {\n+      td->MergeInput();\n+      if (td->tdigest_->size() > 0) {\n+        queue.emplace(td->tdigest_->cbegin(), td->tdigest_->cend());\n+        total_weight_ += td->total_weight_;\n+        min_ = std::min(min_, td->min_);\n+        max_ = std::max(max_, td->max_);\n+      }\n+    }\n+\n+    merger_.Reset(total_weight_, tdigest_next_);\n+\n+    CentroidIter current_iter, end_iter;\n+    // do k-way merge till one buffer left\n+    while (queue.size() > 1) {\n+      std::tie(current_iter, end_iter) = queue.top();\n+      merger_.Add(*current_iter);\n+      queue.pop();\n+      if (++current_iter != end_iter) {\n+        queue.emplace(current_iter, end_iter);\n+      }\n+    }\n+    // merge last buffer\n+    if (!queue.empty()) {\n+      std::tie(current_iter, end_iter) = queue.top();\n+      while (current_iter != end_iter) {\n+        merger_.Add(*current_iter++);\n+      }\n+    }\n+\n+    std::swap(tdigest_, tdigest_next_);\n+  }\n+\n+  // an ugly helper\n+  void Merge(std::vector<std::unique_ptr<TDigest>>& ptrs) {\n+    std::vector<TDigest*> tdigests;\n+    tdigests.reserve(ptrs.size());\n+    for (auto& ptr : ptrs) {\n+      tdigests.push_back(ptr.get());\n+    }\n+    Merge(tdigests);\n+  }\n+\n+  // calculate quantile\n+  double Quantile(double q) {\n+    MergeInput();\n+\n+    const auto& td = *tdigest_;\n+\n+    if (q < 0 || q > 1) {\n+      ARROW_LOG(ERROR) << \"quantile must be between 0 and 1\";\n\nReview comment:\n       It seems like either this function should return `Status<double>`, or parameters should be checked by the caller.\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n\nReview comment:\n       Note you could use indices instead of pointers.\n\n##########\nFile path: cpp/src/arrow/util/tdigest_test.cc\n##########\n@@ -0,0 +1,266 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// XXX: There's no rigid error bound available. The accuracy is to some degree\n+// *random*, which depends on input data and quantiles to be calculated. I also\n+// find small gaps among linux/windows/macos.\n+// In below tests, most quantiles are within 1% deviation from exact values,\n+// while the worst test case is about 10% drift.\n+// To make test result stable, I relaxed error bound to be *good enough*.\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/testing/random.h\"\n+#include \"arrow/testing/util.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tdigest.h\"\n+\n+namespace arrow {\n+namespace internal {\n+\n+TEST(TDigestTest, SingleValue) {\n+  const double value = 0.12345678;\n+\n+  TDigest td;\n+  td.Add(value);\n+  EXPECT_TRUE(td.Verify());\n+  // all quantiles equal to same single vaue\n+  for (double q = 0; q <= 1; q += 0.1) {\n+    EXPECT_EQ(td.Quantile(q), value);\n+  }\n+}\n+\n+TEST(TDigestTest, FewValues) {\n+  // exact quantile at 0.1 intervanl, test sorted and unsorted input\n+  std::vector<std::vector<double>> values_vector = {\n+      {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n+      {4, 1, 9, 0, 3, 2, 5, 6, 8, 7, 10},\n+  };\n+\n+  for (auto& values : values_vector) {\n\nReview comment:\n       `const auto&`\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n\nReview comment:\n       This doesn't seem really useful, is it?\n\n##########\nFile path: cpp/src/arrow/util/tdigest_test.cc\n##########\n@@ -0,0 +1,266 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// XXX: There's no rigid error bound available. The accuracy is to some degree\n+// *random*, which depends on input data and quantiles to be calculated. I also\n+// find small gaps among linux/windows/macos.\n+// In below tests, most quantiles are within 1% deviation from exact values,\n+// while the worst test case is about 10% drift.\n+// To make test result stable, I relaxed error bound to be *good enough*.\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/testing/random.h\"\n+#include \"arrow/testing/util.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tdigest.h\"\n+\n+namespace arrow {\n+namespace internal {\n+\n+TEST(TDigestTest, SingleValue) {\n+  const double value = 0.12345678;\n+\n+  TDigest td;\n+  td.Add(value);\n+  EXPECT_TRUE(td.Verify());\n+  // all quantiles equal to same single vaue\n+  for (double q = 0; q <= 1; q += 0.1) {\n+    EXPECT_EQ(td.Quantile(q), value);\n+  }\n+}\n+\n+TEST(TDigestTest, FewValues) {\n+  // exact quantile at 0.1 intervanl, test sorted and unsorted input\n+  std::vector<std::vector<double>> values_vector = {\n+      {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n+      {4, 1, 9, 0, 3, 2, 5, 6, 8, 7, 10},\n+  };\n+\n+  for (auto& values : values_vector) {\n+    TDigest td;\n+    for (double v : values) {\n+      td.Add(v);\n+    }\n+    EXPECT_TRUE(td.Verify());\n+\n+    double q = 0;\n+    for (size_t i = 0; i < values.size(); ++i) {\n+      double expected = static_cast<double>(i);\n+      EXPECT_EQ(td.Quantile(q), expected);\n+      q += 0.1;\n+    }\n+  }\n+}\n+\n+// Calculate exact quantile as truth\n+std::vector<double> ExactQuantile(std::vector<double> values,\n+                                  const std::vector<double> quantiles) {\n\nReview comment:\n       `const std::vector<double>& quantiles`?\n\n##########\nFile path: cpp/src/arrow/util/tdigest_test.cc\n##########\n@@ -0,0 +1,266 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// XXX: There's no rigid error bound available. The accuracy is to some degree\n+// *random*, which depends on input data and quantiles to be calculated. I also\n+// find small gaps among linux/windows/macos.\n+// In below tests, most quantiles are within 1% deviation from exact values,\n+// while the worst test case is about 10% drift.\n+// To make test result stable, I relaxed error bound to be *good enough*.\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/testing/random.h\"\n+#include \"arrow/testing/util.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tdigest.h\"\n+\n+namespace arrow {\n+namespace internal {\n+\n+TEST(TDigestTest, SingleValue) {\n+  const double value = 0.12345678;\n+\n+  TDigest td;\n+  td.Add(value);\n+  EXPECT_TRUE(td.Verify());\n+  // all quantiles equal to same single vaue\n+  for (double q = 0; q <= 1; q += 0.1) {\n+    EXPECT_EQ(td.Quantile(q), value);\n+  }\n+}\n+\n+TEST(TDigestTest, FewValues) {\n+  // exact quantile at 0.1 intervanl, test sorted and unsorted input\n+  std::vector<std::vector<double>> values_vector = {\n+      {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n+      {4, 1, 9, 0, 3, 2, 5, 6, 8, 7, 10},\n+  };\n+\n+  for (auto& values : values_vector) {\n+    TDigest td;\n+    for (double v : values) {\n+      td.Add(v);\n+    }\n+    EXPECT_TRUE(td.Verify());\n+\n+    double q = 0;\n+    for (size_t i = 0; i < values.size(); ++i) {\n+      double expected = static_cast<double>(i);\n+      EXPECT_EQ(td.Quantile(q), expected);\n+      q += 0.1;\n+    }\n+  }\n+}\n+\n+// Calculate exact quantile as truth\n+std::vector<double> ExactQuantile(std::vector<double> values,\n+                                  const std::vector<double> quantiles) {\n+  std::sort(values.begin(), values.end());\n+\n+  std::vector<double> output;\n+  for (double q : quantiles) {\n+    const double index = (values.size() - 1) * q;\n+    const int64_t lower_index = static_cast<int64_t>(index);\n+    const double fraction = index - lower_index;\n+    if (fraction == 0) {\n+      output.push_back(values[lower_index]);\n+    } else {\n+      const double lerp =\n+          fraction * values[lower_index + 1] + (1 - fraction) * values[lower_index];\n+      output.push_back(lerp);\n+    }\n+  }\n+  return output;\n+}\n+\n+void TestRandom(size_t size) {\n+  const std::vector<double> fixed_quantiles = {0, 0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 0.99, 1};\n+\n+  // append random quantiles to test\n+  std::vector<double> quantiles;\n+  random_real(50, 0x11223344, 0.0, 1.0, &quantiles);\n+  quantiles.insert(quantiles.end(), fixed_quantiles.cbegin(), fixed_quantiles.cend());\n+\n+  // generate random test values\n+  const double min = 1e3, max = 1e10;\n+  std::vector<double> values;\n+  random_real(size, 0x11223344, min, max, &values);\n+\n+  TDigest td(200);\n+  for (double value : values) {\n+    td.Add(value);\n+  }\n+  EXPECT_TRUE(td.Verify());\n+\n+  std::vector<double> expected = ExactQuantile(values, quantiles);\n+  std::vector<double> approximated;\n+  for (auto q : quantiles) {\n+    approximated.push_back(td.Quantile(q));\n+  }\n+\n+  // r-square of expected and approximated quantiles should be greater than 0.999\n+  const double expected_mean =\n+      std::accumulate(expected.begin(), expected.end(), 0.0) / expected.size();\n+  double rss = 0, tss = 0;\n+  for (size_t i = 0; i < quantiles.size(); ++i) {\n+    rss += (expected[i] - approximated[i]) * (expected[i] - approximated[i]);\n+    tss += (expected[i] - expected_mean) * (expected[i] - expected_mean);\n+  }\n+  const double r2 = 1 - rss / tss;\n+  EXPECT_GT(r2, 0.999);\n+\n+  // make sure no quantile drifts more than 5% from truth\n+  for (size_t i = 0; i < quantiles.size(); ++i) {\n+    const double tolerance = std::fabs(expected[i]) * 0.05;\n+    EXPECT_NEAR(approximated[i], expected[i], tolerance) << quantiles[i];\n+  }\n+}\n+\n+TEST(TDigestTest, RandomValues) { TestRandom(100000); }\n+\n+// too heavy to run in ci\n+TEST(TDigestTest, DISABLED_HugeVolume) { TestRandom(1U << 30); }\n+\n+void TestMerge(const std::vector<std::vector<double>>& values_vector, uint32_t delta,\n+               double error_ratio) {\n+  const std::vector<double> quantiles = {0,   0.01, 0.1, 0.2, 0.3,  0.4, 0.5,\n+                                         0.6, 0.7,  0.8, 0.9, 0.99, 1};\n+\n+  std::vector<std::unique_ptr<TDigest>> tds;\n+  for (const auto& values : values_vector) {\n+    auto td = make_unique<TDigest>(delta);\n+    for (double value : values) {\n+      td->Add(value);\n+    }\n+    EXPECT_TRUE(td->Verify());\n+    tds.push_back(std::move(td));\n+  }\n+\n+  std::vector<double> values_combined;\n+  for (const auto& values : values_vector) {\n+    values_combined.insert(values_combined.end(), values.begin(), values.end());\n+  }\n+  std::vector<double> expected = ExactQuantile(values_combined, quantiles);\n+\n+  // merge into an empty tdigest\n+  {\n+    TDigest td(delta);\n+    td.Merge(tds);\n+    td.Verify();\n+    for (size_t i = 0; i < quantiles.size(); ++i) {\n+      const double tolerance = std::max(std::fabs(expected[i]) * error_ratio, 0.1);\n+      EXPECT_NEAR(td.Quantile(quantiles[i]), expected[i], tolerance) << quantiles[i];\n+    }\n+  }\n+\n+  // merge into a non empty tdigest\n+  {\n+    std::unique_ptr<TDigest> td = std::move(tds[0]);\n+    tds.erase(tds.begin(), tds.begin() + 1);\n+    td->Merge(tds);\n+    td->Verify();\n+    for (size_t i = 0; i < quantiles.size(); ++i) {\n+      const double tolerance = std::max(std::fabs(expected[i]) * error_ratio, 0.1);\n+      EXPECT_NEAR(td->Quantile(quantiles[i]), expected[i], tolerance) << quantiles[i];\n+    }\n+  }\n+}\n+\n+// merge tdigests with same distribution\n+TEST(TDigestTest, MergeUniform) {\n+  const std::vector<size_t> sizes = {20000, 3000, 1500, 18000, 9999, 6666};\n+  std::vector<std::vector<double>> values_vector;\n+  for (auto size : sizes) {\n+    std::vector<double> values;\n+    random_real(size, 0x11223344, -123456789.0, 987654321.0, &values);\n+    values_vector.push_back(std::move(values));\n+  }\n+\n+  TestMerge(values_vector, /*delta=*/200, /*error_ratio=*/0.05);\n+}\n+\n+// merge tdigests with different distributions\n+TEST(TDigestTest, MergeNonUniform) {\n+  const std::vector<std::vector<double>> configs = {\n\nReview comment:\n       Instead of `std::vector<double>`, use either a `std::tuple<>` or a dedicated `struct`?\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n+    MergeInput();\n+    // check weight, centroid order\n+    double total_weight = 0, prev_mean = std::numeric_limits<double>::lowest();\n+    for (const auto& centroid : *tdigest_) {\n+      if (std::isnan(centroid.mean) || std::isnan(centroid.weight)) {\n+        ARROW_LOG(ERROR) << \"NAN found in tdigest\";\n+        return false;\n+      }\n+      if (centroid.mean < prev_mean) {\n+        ARROW_LOG(ERROR) << \"centroid mean decreases\";\n+        return false;\n+      }\n+      if (centroid.weight < 1) {\n+        ARROW_LOG(ERROR) << \"invalid centroid weight\";\n+        return false;\n+      }\n+      prev_mean = centroid.mean;\n+      total_weight += centroid.weight;\n+    }\n+    if (total_weight != total_weight_) {\n+      ARROW_LOG(ERROR) << \"tdigest total weight mismatch\";\n+      return false;\n+    }\n+    // check if buffer expanded\n+    if (tdigest_->capacity() > delta_) {\n+      ARROW_LOG(ERROR) << \"oversized tdigest buffer\";\n+      return false;\n+    }\n+    // check k-size\n+    return merger_.Verify(tdigest_, total_weight_);\n+  }\n+\n+  // dump internal data, only for debug\n+  void Dump() {\n+    MergeInput();\n+    const auto& td = *tdigest_;\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      std::cout << i << \": mean = \" << td[i].mean << \", weight = \" << td[i].weight\n+                << std::endl;\n+    }\n+    std::cout << \"min = \" << min_ << \", max = \" << max_ << std::endl;\n+    const size_t total_memory =\n+        2 * td.capacity() * sizeof(detail::Centroid) + input_.capacity() * sizeof(double);\n+    std::cout << \"total memory = \" << total_memory << \" bytes\" << std::endl;\n+  }\n+\n+  // buffer a single data point, consume internal buffer if full\n+  // this function is intensively called and performance critical\n+  void Add(double value) {\n+    DCHECK(!std::isnan(value)) << \"cannot add NAN\";\n+    if (ARROW_PREDICT_FALSE(input_.size() == input_.capacity())) {\n+      MergeInput();\n+    }\n+    input_.push_back(value);\n+  }\n+\n+  // merge with other t-digests, called infrequently\n+  void Merge(std::vector<TDigest*>& tdigests) {\n+    // current and end iterator\n+    using CentroidIter = std::vector<detail::Centroid>::const_iterator;\n+    using CentroidIterPair = std::pair<CentroidIter, CentroidIter>;\n+    // use a min-heap to find next minimal centroid from all tdigests\n+    auto centroid_gt = [](const CentroidIterPair& lhs, const CentroidIterPair& rhs) {\n+      return lhs.first->mean > rhs.first->mean;\n+    };\n+    using CentroidQueue =\n+        std::priority_queue<CentroidIterPair, std::vector<CentroidIterPair>,\n+                            decltype(centroid_gt)>;\n+\n+    // trivial dynamic memory allocated at runtime\n+    std::vector<CentroidIterPair> queue_buffer;\n+    queue_buffer.reserve(tdigests.size() + 1);\n+    CentroidQueue queue(std::move(centroid_gt), std::move(queue_buffer));\n+\n+    MergeInput();\n+    if (tdigest_->size() > 0) {\n+      queue.emplace(tdigest_->cbegin(), tdigest_->cend());\n+    }\n+    for (TDigest* td : tdigests) {\n+      td->MergeInput();\n+      if (td->tdigest_->size() > 0) {\n+        queue.emplace(td->tdigest_->cbegin(), td->tdigest_->cend());\n+        total_weight_ += td->total_weight_;\n+        min_ = std::min(min_, td->min_);\n+        max_ = std::max(max_, td->max_);\n+      }\n+    }\n+\n+    merger_.Reset(total_weight_, tdigest_next_);\n+\n+    CentroidIter current_iter, end_iter;\n+    // do k-way merge till one buffer left\n+    while (queue.size() > 1) {\n+      std::tie(current_iter, end_iter) = queue.top();\n+      merger_.Add(*current_iter);\n+      queue.pop();\n+      if (++current_iter != end_iter) {\n+        queue.emplace(current_iter, end_iter);\n+      }\n+    }\n+    // merge last buffer\n+    if (!queue.empty()) {\n+      std::tie(current_iter, end_iter) = queue.top();\n+      while (current_iter != end_iter) {\n+        merger_.Add(*current_iter++);\n+      }\n+    }\n+\n+    std::swap(tdigest_, tdigest_next_);\n+  }\n+\n+  // an ugly helper\n+  void Merge(std::vector<std::unique_ptr<TDigest>>& ptrs) {\n+    std::vector<TDigest*> tdigests;\n+    tdigests.reserve(ptrs.size());\n+    for (auto& ptr : ptrs) {\n+      tdigests.push_back(ptr.get());\n+    }\n+    Merge(tdigests);\n+  }\n+\n+  // calculate quantile\n+  double Quantile(double q) {\n+    MergeInput();\n+\n+    const auto& td = *tdigest_;\n+\n+    if (q < 0 || q > 1) {\n+      ARROW_LOG(ERROR) << \"quantile must be between 0 and 1\";\n+      return NAN;\n+    }\n+    if (td.size() == 0) {\n+      ARROW_LOG(WARNING) << \"empty tdigest\";\n+      return NAN;\n+    }\n+\n+    const double index = q * total_weight_;\n+    if (index <= 1) {\n+      return min_;\n+    } else if (index >= total_weight_ - 1) {\n+      return max_;\n+    }\n+\n+    // find centroid contains the index\n+    uint32_t ci = 0;\n+    double weight_sum = 0;\n+    for (; ci < td.size(); ++ci) {\n+      weight_sum += td[ci].weight;\n+      if (index <= weight_sum) {\n+        break;\n+      }\n+    }\n+    DCHECK_LT(ci, td.size());\n+\n+    // deviation of index from the centroid center\n+    double diff = index + td[ci].weight / 2 - weight_sum;\n+\n+    // index happen to be in a unit weight centroid\n+    if (td[ci].weight == 1 && std::abs(diff) < 0.5) {\n+      return td[ci].mean;\n+    }\n+\n+    // find adjacent centroids for interpolation\n+    uint32_t ci_left = ci, ci_right = ci;\n+    if (diff > 0) {\n+      if (ci_right == td.size() - 1) {\n+        // index larger than center of last bin\n+        DCHECK_EQ(weight_sum, total_weight_);\n+        const detail::Centroid* c = &td[ci_right];\n+        DCHECK_GE(c->weight, 2);\n+        return detail::Lerp(c->mean, max_, diff / (c->weight / 2));\n+      }\n+      ++ci_right;\n+    } else {\n+      if (ci_left == 0) {\n+        // index smaller than center of first bin\n+        const detail::Centroid* c = &td[0];\n+        DCHECK_GE(c->weight, 2);\n+        return detail::Lerp(min_, c->mean, index / (c->weight / 2));\n+      }\n+      --ci_left;\n+      diff += td[ci_left].weight / 2 + td[ci_right].weight / 2;\n\nReview comment:\n       It seems this would be the same as doing `if (diff < 0) { diff += 1; }` below?\r\n   (not sure which one is more readable or more faithful to the underlying math)\n\n##########\nFile path: cpp/src/arrow/util/tdigest_test.cc\n##########\n@@ -0,0 +1,266 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// XXX: There's no rigid error bound available. The accuracy is to some degree\n+// *random*, which depends on input data and quantiles to be calculated. I also\n+// find small gaps among linux/windows/macos.\n+// In below tests, most quantiles are within 1% deviation from exact values,\n+// while the worst test case is about 10% drift.\n+// To make test result stable, I relaxed error bound to be *good enough*.\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <vector>\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/testing/random.h\"\n+#include \"arrow/testing/util.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tdigest.h\"\n+\n+namespace arrow {\n+namespace internal {\n+\n+TEST(TDigestTest, SingleValue) {\n+  const double value = 0.12345678;\n+\n+  TDigest td;\n+  td.Add(value);\n+  EXPECT_TRUE(td.Verify());\n+  // all quantiles equal to same single vaue\n+  for (double q = 0; q <= 1; q += 0.1) {\n+    EXPECT_EQ(td.Quantile(q), value);\n+  }\n+}\n+\n+TEST(TDigestTest, FewValues) {\n+  // exact quantile at 0.1 intervanl, test sorted and unsorted input\n+  std::vector<std::vector<double>> values_vector = {\n+      {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n+      {4, 1, 9, 0, 3, 2, 5, 6, 8, 7, 10},\n+  };\n+\n+  for (auto& values : values_vector) {\n+    TDigest td;\n+    for (double v : values) {\n+      td.Add(v);\n+    }\n+    EXPECT_TRUE(td.Verify());\n+\n+    double q = 0;\n+    for (size_t i = 0; i < values.size(); ++i) {\n+      double expected = static_cast<double>(i);\n+      EXPECT_EQ(td.Quantile(q), expected);\n+      q += 0.1;\n+    }\n+  }\n+}\n+\n+// Calculate exact quantile as truth\n+std::vector<double> ExactQuantile(std::vector<double> values,\n+                                  const std::vector<double> quantiles) {\n+  std::sort(values.begin(), values.end());\n+\n+  std::vector<double> output;\n+  for (double q : quantiles) {\n+    const double index = (values.size() - 1) * q;\n+    const int64_t lower_index = static_cast<int64_t>(index);\n+    const double fraction = index - lower_index;\n+    if (fraction == 0) {\n+      output.push_back(values[lower_index]);\n+    } else {\n+      const double lerp =\n+          fraction * values[lower_index + 1] + (1 - fraction) * values[lower_index];\n+      output.push_back(lerp);\n+    }\n+  }\n+  return output;\n+}\n+\n+void TestRandom(size_t size) {\n+  const std::vector<double> fixed_quantiles = {0, 0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 0.99, 1};\n+\n+  // append random quantiles to test\n+  std::vector<double> quantiles;\n+  random_real(50, 0x11223344, 0.0, 1.0, &quantiles);\n+  quantiles.insert(quantiles.end(), fixed_quantiles.cbegin(), fixed_quantiles.cend());\n+\n+  // generate random test values\n+  const double min = 1e3, max = 1e10;\n+  std::vector<double> values;\n+  random_real(size, 0x11223344, min, max, &values);\n+\n+  TDigest td(200);\n+  for (double value : values) {\n+    td.Add(value);\n+  }\n+  EXPECT_TRUE(td.Verify());\n+\n+  std::vector<double> expected = ExactQuantile(values, quantiles);\n+  std::vector<double> approximated;\n+  for (auto q : quantiles) {\n+    approximated.push_back(td.Quantile(q));\n+  }\n+\n+  // r-square of expected and approximated quantiles should be greater than 0.999\n+  const double expected_mean =\n+      std::accumulate(expected.begin(), expected.end(), 0.0) / expected.size();\n+  double rss = 0, tss = 0;\n+  for (size_t i = 0; i < quantiles.size(); ++i) {\n+    rss += (expected[i] - approximated[i]) * (expected[i] - approximated[i]);\n+    tss += (expected[i] - expected_mean) * (expected[i] - expected_mean);\n+  }\n+  const double r2 = 1 - rss / tss;\n+  EXPECT_GT(r2, 0.999);\n+\n+  // make sure no quantile drifts more than 5% from truth\n+  for (size_t i = 0; i < quantiles.size(); ++i) {\n+    const double tolerance = std::fabs(expected[i]) * 0.05;\n+    EXPECT_NEAR(approximated[i], expected[i], tolerance) << quantiles[i];\n+  }\n+}\n+\n+TEST(TDigestTest, RandomValues) { TestRandom(100000); }\n+\n+// too heavy to run in ci\n+TEST(TDigestTest, DISABLED_HugeVolume) { TestRandom(1U << 30); }\n+\n+void TestMerge(const std::vector<std::vector<double>>& values_vector, uint32_t delta,\n+               double error_ratio) {\n+  const std::vector<double> quantiles = {0,   0.01, 0.1, 0.2, 0.3,  0.4, 0.5,\n+                                         0.6, 0.7,  0.8, 0.9, 0.99, 1};\n+\n+  std::vector<std::unique_ptr<TDigest>> tds;\n+  for (const auto& values : values_vector) {\n+    auto td = make_unique<TDigest>(delta);\n+    for (double value : values) {\n+      td->Add(value);\n+    }\n+    EXPECT_TRUE(td->Verify());\n+    tds.push_back(std::move(td));\n+  }\n+\n+  std::vector<double> values_combined;\n+  for (const auto& values : values_vector) {\n+    values_combined.insert(values_combined.end(), values.begin(), values.end());\n+  }\n+  std::vector<double> expected = ExactQuantile(values_combined, quantiles);\n+\n+  // merge into an empty tdigest\n+  {\n+    TDigest td(delta);\n+    td.Merge(tds);\n+    td.Verify();\n+    for (size_t i = 0; i < quantiles.size(); ++i) {\n+      const double tolerance = std::max(std::fabs(expected[i]) * error_ratio, 0.1);\n+      EXPECT_NEAR(td.Quantile(quantiles[i]), expected[i], tolerance) << quantiles[i];\n+    }\n+  }\n+\n+  // merge into a non empty tdigest\n+  {\n+    std::unique_ptr<TDigest> td = std::move(tds[0]);\n+    tds.erase(tds.begin(), tds.begin() + 1);\n+    td->Merge(tds);\n+    td->Verify();\n+    for (size_t i = 0; i < quantiles.size(); ++i) {\n+      const double tolerance = std::max(std::fabs(expected[i]) * error_ratio, 0.1);\n+      EXPECT_NEAR(td->Quantile(quantiles[i]), expected[i], tolerance) << quantiles[i];\n+    }\n+  }\n+}\n+\n+// merge tdigests with same distribution\n+TEST(TDigestTest, MergeUniform) {\n+  const std::vector<size_t> sizes = {20000, 3000, 1500, 18000, 9999, 6666};\n+  std::vector<std::vector<double>> values_vector;\n+  for (auto size : sizes) {\n+    std::vector<double> values;\n+    random_real(size, 0x11223344, -123456789.0, 987654321.0, &values);\n+    values_vector.push_back(std::move(values));\n+  }\n+\n+  TestMerge(values_vector, /*delta=*/200, /*error_ratio=*/0.05);\n+}\n+\n+// merge tdigests with different distributions\n+TEST(TDigestTest, MergeNonUniform) {\n+  const std::vector<std::vector<double>> configs = {\n+      // {size, min, max}\n+      {2000, 1e8, 1e9}, {0, 0, 0}, {3000, -1, 1}, {500, -1e6, -1e5}, {800, 100, 100},\n+  };\n+  std::vector<std::vector<double>> values_vector;\n+  for (const auto& cfg : configs) {\n+    std::vector<double> values;\n+    random_real(static_cast<size_t>(cfg[0]), 0x11223344, cfg[1], cfg[2], &values);\n+    values_vector.push_back(std::move(values));\n+  }\n+\n+  TestMerge(values_vector, /*delta=*/200, /*error_ratio=*/0.05);\n+}\n+\n+TEST(TDigestTest, Misc) {\n+  const size_t size = 100000;\n+  const double min = -1000, max = 1000;\n+  const std::vector<double> quantiles = {0, 0.01, 0.1, 0.4, 0.7, 0.9, 0.99, 1};\n+\n+  std::vector<double> values;\n+  random_real(size, 0x11223344, min, max, &values);\n+\n+  // test small delta and buffer\n+  {\n+    const double error_ratio = 0.15;  // low accuracy for small delta\n+\n+    TDigest td(10, 50);\n+    for (double value : values) {\n+      td.Add(value);\n+    }\n+    EXPECT_TRUE(td.Verify());\n+\n+    for (double q : quantiles) {\n+      const double truth = ExactQuantile(values, {q})[0];\n\nReview comment:\n       If you call `ExactQuantile(values, quantiles)` once, this will spare multiple sortings of the same data.\r\n   \r\n   (this test takes ~400 ms here, which is a bit long)\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-26T18:26:45.352+0000",
                    "updated": "2021-01-26T18:26:45.352+0000",
                    "started": "2021-01-26T18:26:45.352+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542296",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542299",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "pitrou commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r564736751\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest_benchmark.cc\n##########\n@@ -0,0 +1,48 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"benchmark/benchmark.h\"\n+\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/testing/random.h\"\n+#include \"arrow/util/tdigest.h\"\n+\n+namespace arrow {\n+namespace util {\n+\n+static constexpr uint32_t kDelta = 100;\n+static constexpr uint32_t kBufferSize = 500;\n+\n+static void BenchmarkTDigest(benchmark::State& state) {\n+  const size_t items = state.range(0);\n+  std::vector<double> values;\n+  random_real(items, 0x11223344, -12345678.0, 12345678.0, &values);\n+\n+  for (auto _ : state) {\n+    arrow::internal::TDigest td(kDelta, kBufferSize);\n+    for (double value : values) {\n+      td.Add(value);\n+    }\n+    benchmark::DoNotOptimize(td.Quantile(0));\n+  }\n+  state.SetItemsProcessed(state.iterations() * items);\n+}\n+\n+BENCHMARK(BenchmarkTDigest)->Arg(1 << 16)->Arg(1 << 20)->Arg(1 << 24);\n\nReview comment:\n       We should probably use smaller sizes in order to get enough iterations (the larger size here only gets 1 iteration here).\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-26T18:28:50.862+0000",
                    "updated": "2021-01-26T18:28:50.862+0000",
                    "started": "2021-01-26T18:28:50.862+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542299",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542582",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r564978390\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n\nReview comment:\n       It's the baseline to evaluate accuracy and performance of scale functions.\r\n   Have to modify source code manually to enable it.\r\n   I would like to keep it as new scale functions may be introduced in the future.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T02:21:54.612+0000",
                    "updated": "2021-01-27T02:21:54.612+0000",
                    "started": "2021-01-27T02:21:54.611+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542582",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542586",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r564981864\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n\nReview comment:\n       No. I didn't consider roundoff errors.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T02:31:55.680+0000",
                    "updated": "2021-01-27T02:31:55.680+0000",
                    "started": "2021-01-27T02:31:55.679+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542586",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542691",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565111310\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n\nReview comment:\n       Done\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:25:52.419+0000",
                    "updated": "2021-01-27T08:25:52.419+0000",
                    "started": "2021-01-27T08:25:52.419+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542691",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542692",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565111633\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n\nReview comment:\n       Done\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:26:27.336+0000",
                    "updated": "2021-01-27T08:26:27.336+0000",
                    "started": "2021-01-27T08:26:27.336+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542692",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542693",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565111868\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n\nReview comment:\n       Done\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:26:50.402+0000",
                    "updated": "2021-01-27T08:26:50.402+0000",
                    "started": "2021-01-27T08:26:50.402+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542693",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542694",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565112063\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n\nReview comment:\n       Removed\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:27:13.568+0000",
                    "updated": "2021-01-27T08:27:13.568+0000",
                    "started": "2021-01-27T08:27:13.568+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542694",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542695",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565112232\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n\nReview comment:\n       Removed\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:27:26.534+0000",
                    "updated": "2021-01-27T08:27:26.534+0000",
                    "started": "2021-01-27T08:27:26.533+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542695",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542697",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565112495\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n\nReview comment:\n       Changed to indices. Thanks.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:27:53.953+0000",
                    "updated": "2021-01-27T08:27:53.953+0000",
                    "started": "2021-01-27T08:27:53.953+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542697",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542699",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565114100\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n\nReview comment:\n       Done with returning Status.\r\n   It will merge input buffer if it's not empty before do verification. So it's not declared as `const`.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:30:17.996+0000",
                    "updated": "2021-01-27T08:30:17.996+0000",
                    "started": "2021-01-27T08:30:17.996+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542699",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542700",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565114525\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n+    MergeInput();\n+    // check weight, centroid order\n+    double total_weight = 0, prev_mean = std::numeric_limits<double>::lowest();\n+    for (const auto& centroid : *tdigest_) {\n+      if (std::isnan(centroid.mean) || std::isnan(centroid.weight)) {\n+        ARROW_LOG(ERROR) << \"NAN found in tdigest\";\n+        return false;\n+      }\n+      if (centroid.mean < prev_mean) {\n+        ARROW_LOG(ERROR) << \"centroid mean decreases\";\n+        return false;\n+      }\n+      if (centroid.weight < 1) {\n+        ARROW_LOG(ERROR) << \"invalid centroid weight\";\n+        return false;\n+      }\n+      prev_mean = centroid.mean;\n+      total_weight += centroid.weight;\n+    }\n+    if (total_weight != total_weight_) {\n\nReview comment:\n       Yes, it should always be true.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:30:52.747+0000",
                    "updated": "2021-01-27T08:30:52.747+0000",
                    "started": "2021-01-27T08:30:52.747+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542700",
                    "issueId": "13354225"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/worklog/542701",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "cyb70289 commented on a change in pull request #9310:\nURL: https://github.com/apache/arrow/pull/9310#discussion_r565114893\n\n\n\n##########\nFile path: cpp/src/arrow/util/tdigest.h\n##########\n@@ -0,0 +1,419 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+// approximate quantiles from arbitrary length dataset with O(1) space\n+// based on 'Computing Extremely Accurate Quantiles Using t-Digests' from Dunning & Ertl\n+// - https://arxiv.org/abs/1902.04023\n+// - https://github.com/tdunning/t-digest\n+\n+#pragma once\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <queue>\n+#include <vector>\n+\n+#include \"arrow/util/logging.h\"\n+\n+#ifndef M_PI\n+#define M_PI 3.14159265358979323846\n+#endif\n+\n+namespace arrow {\n+namespace internal {\n+\n+namespace detail {\n+\n+// a numerically stable lerp is unbelievably complex\n+// but we are *approximating* the quantile, so let's keep it simple\n+double Lerp(double a, double b, double t) { return a + t * (b - a); }\n+\n+// histogram bin\n+struct Centroid {\n+  double mean;\n+  double weight;  // # data points in this bin\n+\n+  // merge with another centroid\n+  void Merge(const Centroid& centroid) {\n+    weight += centroid.weight;\n+    mean += (centroid.mean - mean) * centroid.weight / weight;\n+  }\n+};\n+\n+// scale function K0: linear function, as baseline\n+struct ScalerK0 {\n+  explicit ScalerK0(uint32_t delta) : delta_norm(delta / 2.0) {}\n+\n+  double K(double q) const { return delta_norm * q; }\n+  double Q(double k) const { return k / delta_norm; }\n+\n+  const double delta_norm;\n+};\n+\n+// scale function K1\n+struct ScalerK1 {\n+  explicit ScalerK1(uint32_t delta) : delta_norm(delta / (2.0 * M_PI)) {}\n+\n+  double K(double q) const { return delta_norm * std::asin(2 * q - 1); }\n+  double Q(double k) const { return (std::sin(k / delta_norm) + 1) / 2; }\n+\n+  const double delta_norm;\n+};\n+\n+// implements t-digest merging algorithm\n+template <class T = ScalerK1>\n+class TDigestMerger : private T {\n+ public:\n+  explicit TDigestMerger(uint32_t delta) : T(delta) {}\n+\n+  void Reset(double total_weight, std::vector<Centroid>* tdigest) {\n+    total_weight_ = total_weight;\n+    tdigest_ = tdigest;\n+    tdigest_->resize(0);\n+    weight_so_far_ = 0;\n+    weight_limit_ = -1;  // trigger first centroid merge\n+  }\n+\n+  // merge one centroid from a sorted centroid stream\n+  void Add(const Centroid& centroid) {\n+    auto& td = *tdigest_;\n+    const double weight = weight_so_far_ + centroid.weight;\n+    if (weight <= weight_limit_) {\n+      td[td.size() - 1].Merge(centroid);\n+    } else {\n+      const double quantile = weight_so_far_ / total_weight_;\n+      const double next_weight_limit = total_weight_ * this->Q(this->K(quantile) + 1);\n+      // weight limit should be strictly increasing, until the last centroid\n+      if (next_weight_limit <= weight_limit_) {\n+        weight_limit_ = total_weight_;\n+      } else {\n+        weight_limit_ = next_weight_limit;\n+      }\n+      td.push_back(centroid);  // should never exceed capacity and trigger reallocation\n+    }\n+    weight_so_far_ = weight;\n+  }\n+\n+  // verify k-size of a tdigest\n+  bool Verify(const std::vector<Centroid>* tdigest, double total_weight) const {\n+    const auto& td = *tdigest;\n+    double q_prev = 0, k_prev = this->K(0);\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      const double q = q_prev + td[i].weight / total_weight;\n+      const double k = this->K(q);\n+      if (td[i].weight != 1 && (k - k_prev) > 1.001) {\n+        ARROW_LOG(ERROR) << \"oversized centroid, k2 - k1 = \" << (k - k_prev);\n+        return false;\n+      }\n+      k_prev = k;\n+      q_prev = q;\n+    }\n+    return true;\n+  }\n+\n+ private:\n+  double total_weight_;   // total weight of this tdigest\n+  double weight_so_far_;  // accumulated weight till current bin\n+  double weight_limit_;   // max accumulated weight to move to next bin\n+  std::vector<Centroid>* tdigest_;\n+};\n+\n+}  // namespace detail\n+\n+class TDigest {\n+ public:\n+  explicit TDigest(uint32_t delta = kDelta, uint32_t buffer_size = kBufferSize)\n+      : delta_(delta), merger_(delta) {\n+    if (delta < 10) {\n+      ARROW_LOG(WARNING) << \"delta is too small, increased to 10\";\n+      delta = 10;\n+    }\n+    if (buffer_size < 50 || buffer_size < delta) {\n+      ARROW_LOG(INFO) << \"increase buffer size may improve performance\";\n+    }\n+\n+    // pre-allocate input and tdigest buffers, no dynamic memory allocation at runtime\n+    input_.reserve(buffer_size);\n+    tdigest_buffers_[0].reserve(delta);\n+    tdigest_buffers_[1].reserve(delta);\n+    tdigest_ = &tdigest_buffers_[0];\n+    tdigest_next_ = &tdigest_buffers_[1];\n+\n+    Reset();\n+  }\n+\n+  // no copy/move/assign due to some dirty pointer tricks\n+  TDigest(const TDigest&) = delete;\n+  TDigest(TDigest&&) = delete;\n+  TDigest& operator=(const TDigest&) = delete;\n+  TDigest& operator=(TDigest&&) = delete;\n+\n+  // reset and re-use this tdigest\n+  void Reset() {\n+    input_.resize(0);\n+    tdigest_->resize(0);\n+    tdigest_next_->resize(0);\n+    total_weight_ = 0;\n+    min_ = std::numeric_limits<double>::max();\n+    max_ = std::numeric_limits<double>::lowest();\n+  }\n+\n+  // verify data integrity, only for test\n+  bool Verify() {\n+    MergeInput();\n+    // check weight, centroid order\n+    double total_weight = 0, prev_mean = std::numeric_limits<double>::lowest();\n+    for (const auto& centroid : *tdigest_) {\n+      if (std::isnan(centroid.mean) || std::isnan(centroid.weight)) {\n+        ARROW_LOG(ERROR) << \"NAN found in tdigest\";\n+        return false;\n+      }\n+      if (centroid.mean < prev_mean) {\n+        ARROW_LOG(ERROR) << \"centroid mean decreases\";\n+        return false;\n+      }\n+      if (centroid.weight < 1) {\n+        ARROW_LOG(ERROR) << \"invalid centroid weight\";\n+        return false;\n+      }\n+      prev_mean = centroid.mean;\n+      total_weight += centroid.weight;\n+    }\n+    if (total_weight != total_weight_) {\n+      ARROW_LOG(ERROR) << \"tdigest total weight mismatch\";\n+      return false;\n+    }\n+    // check if buffer expanded\n+    if (tdigest_->capacity() > delta_) {\n+      ARROW_LOG(ERROR) << \"oversized tdigest buffer\";\n+      return false;\n+    }\n+    // check k-size\n+    return merger_.Verify(tdigest_, total_weight_);\n+  }\n+\n+  // dump internal data, only for debug\n+  void Dump() {\n+    MergeInput();\n+    const auto& td = *tdigest_;\n+    for (size_t i = 0; i < td.size(); ++i) {\n+      std::cout << i << \": mean = \" << td[i].mean << \", weight = \" << td[i].weight\n\nReview comment:\n       Done\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-27T08:31:35.956+0000",
                    "updated": "2021-01-27T08:31:35.956+0000",
                    "started": "2021-01-27T08:31:35.955+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "542701",
                    "issueId": "13354225"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 25800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@404ac74a[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@77461b48[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@615c5bd4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@260790ec[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@f38e81e[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@2d3796d5[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2e36ea5c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@342c8f86[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7c1ce905[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@6d0c596d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@74712a9a[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@2471db2d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 25800,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Jan 28 14:40:22 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-01-28T14:40:22.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11367/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-01-25T02:32:13.000+0000",
        "updated": "2021-02-01T04:15:12.000+0000",
        "timeoriginalestimate": null,
        "description": "This utility is to approximate quantiles of arbitrary length inputs with constant space (not storing input data points) [1]. It will be used in implementing approximate quantile kernel and calculating latencies in flightrpc benchmark.\r\n\r\nt-digest [2] is state-of-the-art algorithm for quantile approximation. It performs well on both edge (0.01, 0.99, etc)  and normal cases (0.3, 0.5, etc). And t-digest can run in parallel, which means multiple chunks can be processed at same time and combined later.\r\n\r\nT-Digest author maintains an java implementation [3]. There are some open source c++ versions available, including facebook folly [4]. I planned to port one of them but finally came up with my own implementation, which (I suppose) is more efficient in space and time.\r\n\r\nReferences\r\n[1] A Survey of Approximate Quantile Computation on Large-scale Data\r\nhttps://arxiv.org/abs/2004.08255v1\r\n[2] Computing Extremely Accurate Quantiles Using t-Digests\r\nhttps://arxiv.org/abs/1902.04023\r\n[3] https://github.com/tdunning/t-digest/\r\n[4] https://github.com/facebook/folly/blob/master/folly/stats/TDigest.h\r\nhttps://github.com/derrickburns/tdigest\r\nhttps://github.com/SpirentOrion/digestible",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "7h 10m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 25800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Implement approximante quantile utility",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13354225/comment/17273770",
                    "id": "17273770",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "body": "Issue resolved by pull request 9310\n[https://github.com/apache/arrow/pull/9310]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=apitrou",
                        "name": "apitrou",
                        "key": "pitrou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=pitrou&avatarId=35049",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=pitrou&avatarId=35049",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=pitrou&avatarId=35049",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=pitrou&avatarId=35049"
                        },
                        "displayName": "Antoine Pitrou",
                        "active": true,
                        "timeZone": "Europe/Paris"
                    },
                    "created": "2021-01-28T14:40:22.719+0000",
                    "updated": "2021-01-28T14:40:22.719+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0mxao:",
        "customfield_12314139": null
    }
}