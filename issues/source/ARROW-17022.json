{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13470747",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747",
    "key": "ARROW-17022",
    "fields": {
        "fixVersions": [],
        "resolution": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": null,
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "name": "Open",
            "id": "1",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                "id": 2,
                "key": "new",
                "colorName": "blue-gray",
                "name": "To Do"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 5400,
            "total": 5400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 5400,
            "total": 5400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-17022/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 9,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796495",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#issuecomment-1199911016\n\n   https://issues.apache.org/jira/browse/ARROW-17022\n\n\n",
                    "created": "2022-07-29T20:11:45.417+0000",
                    "updated": "2022-07-29T20:11:45.417+0000",
                    "started": "2022-07-29T20:11:45.416+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796495",
                    "issueId": "13470747"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796496",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#issuecomment-1199911039\n\n   :warning: Ticket **has not been started in JIRA**, please click 'Start Progress'.\n\n\n",
                    "created": "2022-07-29T20:11:47.440+0000",
                    "updated": "2022-07-29T20:11:47.440+0000",
                    "started": "2022-07-29T20:11:47.440+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796496",
                    "issueId": "13470747"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796497",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#issuecomment-1199911235\n\n   CC @save-buffer , @michalursa do one of you mind taking a look to make sure I didn't mess up some of these comments?\n\n\n",
                    "created": "2022-07-29T20:12:07.213+0000",
                    "updated": "2022-07-29T20:12:07.213+0000",
                    "started": "2022-07-29T20:12:07.213+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796497",
                    "issueId": "13470747"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796526",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "save-buffer commented on code in PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#discussion_r933631310\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -273,15 +273,40 @@ class SwissTableMerge {\n                                     int64_t max_block_id);\n };\n \n+/// \\brief A vectorized hash-table mapping any number of key columns to a single key\n+///\n+/// Writes are assumed to be single threaded.  To build a hash table in parallel you\n+/// should one instance of this type per thread.  Once all hash tables are built you can\n+/// merge the tables together to get a single table.\n\nReview Comment:\n   Maybe mention our existing helper class for this? Maybe something like \"you can merge the tables into a single table using `SwissTableMerge`\"\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -273,15 +273,40 @@ class SwissTableMerge {\n                                     int64_t max_block_id);\n };\n \n+/// \\brief A vectorized hash-table mapping any number of key columns to a single key\n+///\n+/// Writes are assumed to be single threaded.  To build a hash table in parallel you\n+/// should one instance of this type per thread.  Once all hash tables are built you can\n\nReview Comment:\n   should make*\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -273,15 +273,40 @@ class SwissTableMerge {\n                                     int64_t max_block_id);\n };\n \n+/// \\brief A vectorized hash-table mapping any number of key columns to a single key\n+///\n+/// Writes are assumed to be single threaded.  To build a hash table in parallel you\n+/// should one instance of this type per thread.  Once all hash tables are built you can\n+/// merge the tables together to get a single table.\n+///\n+/// Reads are thread safe and can run in parallel\n+///\n+/// The table assumes the caller has already computed the hashes.  This allows the hash\n+/// calculation to be separated from the hash lookup.  This is useful, for example, in a\n+/// hash join node when you can compute hashes as soon as a probe batch arrives (and when\n+/// it is resident in cache) and then do the lookup later when the hash table has finished\n+/// building.\n struct SwissTableWithKeys {\n+  /// \\brief Input to a swiss table\n+  ///\n+  /// Every column of `in_batch` will be used to compute the key.  In other words,\n+  /// `in_batch` should only represent key columns and not payload columns.\n+  ///\n+  /// Table input can be a batch or a selection of a batch.  Table input also includes\n+  /// the scracth space that the hash table will need to work with.  This allows threads\n\nReview Comment:\n   scratch*\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -346,37 +397,69 @@ struct SwissTableWithKeys {\n   RowArray keys_;\n };\n \n-// Enhances SwissTableWithKeys with the following structures used by hash join:\n-// - storage of payloads (that unlike keys do not have to be unique)\n-// - mapping from a key to all inserted payloads corresponding to it (we can\n-// store multiple rows corresponding to a single key)\n-// - bit-vectors for keeping track of whether each payload had a match during\n-// evaluation of join.\n-//\n+/// \\brief A hash table that supports any number of key or payload columns\n+///\n+/// Enhances SwissTableWithKeys with the following structures used by hash join:\n+/// - storage of payloads (that unlike keys do not have to be unique)\n+/// - mapping from a key to all inserted payloads corresponding to it (we can\n+/// store multiple rows corresponding to a single key)\n+/// - bit-vectors for keeping track of whether each payload had a match during\n+/// evaluation of join.\n+///\n+/// Although this class is itended to be mostly used for reads (e.g. join\n\nReview Comment:\n   intended\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -403,9 +486,10 @@ class SwissTableForJoin {\n   RowArray payloads_;\n };\n \n-// Implements parallel build process for hash table for join from a sequence of\n-// exec batches with input rows.\n-//\n+/// \\brief Factory to create a SwissTableForJoin instance\n\nReview Comment:\n   Is Factory the right term here? I'd more call this a \"Helper class for building SwissTables in parallel\" or something like that. Factory to me implies the private constructor/public static Make function.\n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join.h:\n##########\n@@ -273,15 +273,40 @@ class SwissTableMerge {\n                                     int64_t max_block_id);\n };\n \n+/// \\brief A vectorized hash-table mapping any number of key columns to a single key\n+///\n+/// Writes are assumed to be single threaded.  To build a hash table in parallel you\n+/// should one instance of this type per thread.  Once all hash tables are built you can\n+/// merge the tables together to get a single table.\n+///\n+/// Reads are thread safe and can run in parallel\n+///\n+/// The table assumes the caller has already computed the hashes.  This allows the hash\n+/// calculation to be separated from the hash lookup.  This is useful, for example, in a\n+/// hash join node when you can compute hashes as soon as a probe batch arrives (and when\n\nReview Comment:\n   I think the real reason we compute hashes immediately is so that we can reuse it between Bloom filters, hash table, and spill partitioning. \n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join_test.cc:\n##########\n@@ -0,0 +1,561 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include <cstdint>\n+#include <memory>\n+#include <vector>\n+\n+#include \"arrow/array/builder_primitive.h\"\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/memory_pool.h\"\n+#include \"arrow/testing/future_util.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::Executor;\n+using internal::GetCpuThreadPool;\n+\n+namespace compute {\n+\n+void CheckRowArray(const std::vector<ExecBatch>& batches, const RowArray& row_array,\n+                   const std::vector<uint32_t>& row_ids) {\n+  ASSERT_FALSE(batches.empty());\n+  int num_rows = 0;\n+  for (const auto& batch : batches) {\n+    num_rows += static_cast<int>(batch.length);\n+  }\n+  int num_columns = batches[0].num_values();\n+  ASSERT_LE(num_rows, row_array.num_rows());\n+  for (int i = 0; i < num_columns; i++) {\n+    ResizableArrayData target;\n+    target.Init(batches[0].values[i].type(), default_memory_pool(),\n+                bit_util::Log2(num_rows));\n+    ASSERT_OK(row_array.DecodeSelected(&target, i, num_rows, row_ids.data(),\n+                                       default_memory_pool()));\n+    std::shared_ptr<ArrayData> decoded = target.array_data();\n+    std::vector<std::shared_ptr<Array>> expected_arrays;\n+    for (const auto& batch : batches) {\n+      expected_arrays.push_back(batch.values[i].make_array());\n+    }\n+    ASSERT_OK_AND_ASSIGN(std::shared_ptr<Array> expected_array,\n+                         Concatenate(expected_arrays));\n+    AssertArraysEqual(*expected_array, *MakeArray(decoded));\n+  }\n+}\n+\n+Result<int> EncodeEntireBatches(RowArray* row_array,\n+                                const std::vector<ExecBatch>& batches) {\n+  std::vector<KeyColumnArray> temp_column_arrays;\n+  int num_rows_encoded = 0;\n+  for (const auto& batch : batches) {\n+    int num_rows = static_cast<int>(batch.length);\n+    std::vector<uint16_t> in_row_ids(num_rows);\n+    std::iota(in_row_ids.begin(), in_row_ids.end(), 0);\n+    RETURN_NOT_OK(row_array->InitIfNeeded(default_memory_pool(), batch));\n+    RETURN_NOT_OK(row_array->AppendBatchSelection(default_memory_pool(), batch, 0,\n+                                                  num_rows, num_rows, in_row_ids.data(),\n+                                                  temp_column_arrays));\n+    num_rows_encoded += num_rows;\n+  }\n+  return num_rows_encoded;\n+}\n+\n+void CheckRoundTrip(const ExecBatch& batch, std::vector<uint16_t> row_ids = {}) {\n+  if (row_ids.empty()) {\n+    row_ids.resize(batch.length);\n+    std::iota(row_ids.begin(), row_ids.end(), 0);\n+  }\n+  std::vector<uint32_t> array_row_ids(row_ids.size());\n+  std::iota(array_row_ids.begin(), array_row_ids.end(), 0);\n+\n+  RowArray row_array;\n+  ASSERT_OK(EncodeEntireBatches(&row_array, {batch}));\n+  CheckRowArray({batch}, row_array, array_row_ids);\n+}\n+\n+TEST(RowArray, Basic) {\n+  // All fixed\n+  CheckRoundTrip(ExecBatchFromJSON({int32(), int32(), int32()}, R\"([\n+                   [1, 4, 7],\n+                   [2, 5, 8],\n+                   [3, 6, 9]\n+                 ])\"));\n+  // All varlen\n+  CheckRoundTrip(ExecBatchFromJSON({utf8(), utf8()}, R\"([\n+                   [\"xyz\", \"longer string\"],\n+                   [\"really even longer string\", \"\"],\n+                   [null, \"a\"],\n+                   [\"null\", null],\n+                   [\"b\", \"c\"]\n+                 ])\"));\n+  // Mixed types\n+  CheckRoundTrip(ExecBatchFromJSON({boolean(), int32(), utf8()}, R\"([\n+    [true, 3, \"test\"],\n+    [null, null, null],\n+    [false, 0, \"blah\"]\n+  ])\"));\n+  // No nulls\n+  CheckRoundTrip(ExecBatchFromJSON({int16(), utf8()}, R\"([\n+    [10, \"blahblah\"],\n+    [8, \"not-null\"]\n+  ])\"));\n+}\n+\n+TEST(RowArray, MultiBatch) {\n+  constexpr int kRowsPerBatch = std::numeric_limits<uint16_t>::max();\n+  constexpr int num_batches = 4;\n+  constexpr int num_out_rows = kRowsPerBatch * num_batches;\n+  std::vector<uint16_t> in_row_ids(kRowsPerBatch);\n+  std::vector<uint32_t> out_row_ids(num_out_rows);\n+  std::iota(in_row_ids.begin(), in_row_ids.end(), 0);\n+  std::iota(out_row_ids.begin(), out_row_ids.end(), 0);\n+  // Should be able to encode multiple batches to something\n+  // greater than 2^16 rows\n+  BatchesWithSchema test_data = MakeRandomBatches(\n+      schema({field(\"bool\", boolean()), field(\"i8\", int8()), field(\"utf8\", utf8())}), 4,\n+      kRowsPerBatch, /*add_tag=*/false);\n+\n+  RowArray row_array;\n+  ASSERT_OK_AND_ASSIGN(int num_rows_encoded,\n+                       EncodeEntireBatches(&row_array, test_data.batches));\n+  ASSERT_EQ(num_rows_encoded, row_array.num_rows());\n+\n+  CheckRowArray(test_data.batches, row_array, out_row_ids);\n+}\n+\n+Result<ExecBatch> ColumnTake(const ExecBatch& input,\n+                             const std::vector<uint16_t>& row_ids) {\n+  UInt16Builder take_indices_builder;\n+  RETURN_NOT_OK(take_indices_builder.AppendValues(row_ids));\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Array> take_indices,\n+                        take_indices_builder.Finish());\n+\n+  std::vector<Datum> taken_arrays(input.num_values());\n+  for (int i = 0; i < input.num_values(); i++) {\n+    ARROW_ASSIGN_OR_RAISE(Datum taken_array, Take(input.values[i], take_indices));\n+    taken_arrays[i] = taken_array;\n+  }\n+  return ExecBatch(taken_arrays, static_cast<int64_t>(row_ids.size()));\n+}\n+\n+TEST(RowArray, InputPartialSelection) {\n+  constexpr int num_rows = std::numeric_limits<uint16_t>::max();\n+  BatchesWithSchema test_data = MakeRandomBatches(\n+      schema({field(\"bool\", boolean()), field(\"i8\", int8()), field(\"utf8\", utf8())}), 1,\n+      num_rows, /*add_tag=*/false);\n+  ExecBatch batch = test_data.batches[0];\n+  std::vector<uint16_t> row_ids;\n+  std::default_random_engine gen(42);\n+  std::uniform_int_distribution<> dist(0, 1);\n+  for (uint16_t i = 0; i < num_rows; i++) {\n+    if (dist(gen)) {\n+      row_ids.push_back(i);\n+    }\n+  }\n+  int num_selected_rows = static_cast<int>(row_ids.size());\n+  ASSERT_GT(num_selected_rows, 0);\n+  ASSERT_LT(num_selected_rows, num_rows);\n+\n+  std::vector<uint32_t> array_row_ids(row_ids.size());\n+  std::iota(array_row_ids.begin(), array_row_ids.end(), 0);\n+\n+  RowArray row_array;\n+  std::vector<KeyColumnArray> temp_column_arrays;\n+  ASSERT_OK(row_array.InitIfNeeded(default_memory_pool(), batch));\n+  ASSERT_OK(row_array.AppendBatchSelection(default_memory_pool(), batch, 0,\n+                                           num_selected_rows, num_selected_rows,\n+                                           row_ids.data(), temp_column_arrays));\n+\n+  ASSERT_OK_AND_ASSIGN(ExecBatch expected, ColumnTake(batch, row_ids));\n+  CheckRowArray({expected}, row_array, array_row_ids);\n+}\n+\n+TEST(RowArray, ThreadedDecode) {\n+  // Create kBatchesPerThread batches per thread.  Encode all batches from the main\n+  // thread. Then decode in parallel so each thread decodes it's own batches\n+  constexpr int kRowsPerBatch = std::numeric_limits<uint16_t>::max();\n+  constexpr int kNumThreads = 16;\n+  constexpr int kBatchesPerThread = 2;\n+  constexpr int kNumBatches = kNumThreads * kBatchesPerThread;\n+  constexpr int kNumOutRows = kRowsPerBatch * kBatchesPerThread;\n+\n+  BatchesWithSchema test_data = MakeRandomBatches(\n+      schema({field(\"bool\", boolean()), field(\"i8\", int8()), field(\"utf8\", utf8())}),\n+      kNumBatches, kRowsPerBatch, /*add_tag=*/false);\n+\n+  RowArray row_array;\n+  ASSERT_OK_AND_ASSIGN(int num_rows_encoded,\n+                       EncodeEntireBatches(&row_array, test_data.batches));\n+  ASSERT_EQ(num_rows_encoded, kNumOutRows * kNumThreads);\n+\n+  std::vector<std::vector<uint32_t>> row_ids_for_threads(kNumThreads);\n+  uint32_t row_id_offset = 0;\n+  for (int i = 0; i < kNumThreads; i++) {\n+    std::vector<uint32_t>& row_ids_for_thread = row_ids_for_threads[i];\n+    row_ids_for_thread.resize(kNumOutRows);\n+    std::iota(row_ids_for_thread.begin(), row_ids_for_thread.end(), row_id_offset);\n+    row_id_offset += kNumOutRows;\n+  }\n+\n+  std::vector<std::thread> thread_tasks;\n+  for (int i = 0; i < kNumThreads; i++) {\n+    thread_tasks.emplace_back([&, i] {\n+      CheckRowArray({test_data.batches[i * 2], test_data.batches[i * 2 + 1]}, row_array,\n+                    row_ids_for_threads[i]);\n+    });\n+  }\n+  for (auto& thread_task : thread_tasks) {\n+    thread_task.join();\n+  }\n+}\n+\n+void CheckComparison(const ExecBatch& left, const ExecBatch& right,\n+                     const std::vector<bool> expected) {\n+  RowArray row_array;\n+  ASSERT_OK(EncodeEntireBatches(&row_array, {left}));\n+  std::vector<uint32_t> row_ids(right.length);\n+  std::iota(row_ids.begin(), row_ids.end(), 0);\n+  uint32_t num_not_equal = 0;\n+  std::vector<uint16_t> not_equal_selection(right.length);\n+  LightContext light_context;\n+  light_context.hardware_flags =\n+      arrow::internal::CpuInfo::GetInstance()->hardware_flags();\n+  util::TempVectorStack temp_stack;\n+  ASSERT_OK(temp_stack.Init(default_memory_pool(),\n+                            4 * util::MiniBatch::kMiniBatchLength * sizeof(uint32_t)));\n+  light_context.stack = &temp_stack;\n+  std::vector<KeyColumnArray> temp_column_arrays;\n+  std::vector<uint8_t> match_bit_vector(bit_util::Log2(right.length));\n+  row_array.Compare(\n+      right, 0, static_cast<int>(right.length), static_cast<int>(right.length), nullptr,\n+      row_ids.data(), &num_not_equal, not_equal_selection.data(),\n+      light_context.hardware_flags, light_context.stack, temp_column_arrays, nullptr);\n+\n+  // We over-allocated above\n+  not_equal_selection.resize(num_not_equal);\n+  std::vector<uint16_t> expected_not_equal_selection;\n+  for (uint16_t i = 0; i < static_cast<uint16_t>(expected.size()); i++) {\n+    if (!expected[i]) {\n+      expected_not_equal_selection.push_back(i);\n+    }\n+  }\n+  ASSERT_EQ(expected_not_equal_selection, not_equal_selection);\n+\n+  int expected_not_equal = 0;\n+  for (bool val : expected) {\n+    if (!val) {\n+      expected_not_equal++;\n+    }\n+  }\n+  ASSERT_EQ(expected_not_equal, num_not_equal);\n+}\n+\n+TEST(RowArray, Compare) {\n+  ExecBatch left = ExecBatchFromJSON({int16(), utf8()}, R\"([\n+    [1, \"blahblah\"],\n+    [2, null],\n+    [3, \"xyz\"],\n+    [4, \"sample\"],\n+    [null, \"5\"],\n+    [null, null]\n+  ])\");\n+  ExecBatch right = ExecBatchFromJSON({int16(), utf8()}, R\"([\n+    [1, \"blahblah\"],\n+    [2, \"not-null\"],\n+    [3, \"abc\"],\n+    [5, \"blah\"],\n+    [null, \"5\"],\n+    [null, null]\n+  ])\");\n+  std::vector<bool> expected_matches = {\n+      true,   // Matches exactly without nulls\n+      false,  // differs by null only\n+      false,  // differs by one value\n+      false,  // differs by all values\n+      true,   // equal but has nulls\n+      true    // equal-all-null\n+  };\n+  // Test in both directions\n+  CheckComparison(left, right, expected_matches);\n+  CheckComparison(right, left, expected_matches);\n+}\n+\n+struct SwissTableTestParams {\n+  std::shared_ptr<Schema> schema;\n+  std::string name;\n+};\n+\n+std::ostream& operator<<(std::ostream& os, const SwissTableTestParams& params) {\n+  return os << params.name;\n+}\n+\n+class SwissTableWithKeysTest : public ::testing::TestWithParam<SwissTableTestParams> {\n+ public:\n+  void SetUp() {\n+    ASSERT_OK_AND_ASSIGN(ctx_, LightContext::Make(default_memory_pool()));\n+    ASSERT_OK(table_.Init(ctx_->hardware_flags, ctx_->memory_pool));\n+  }\n+\n+  const std::shared_ptr<Schema>& test_schema() { return GetParam().schema; }\n+\n+ protected:\n+  SwissTableWithKeys table_;\n+  // Scratch space to use in tests\n+  std::unique_ptr<LightContext, LightContext::OwningLightContextDeleter> ctx_;\n\nReview Comment:\n   I don't think this needs to be `unique_ptr` - this can just be a member like `table_`.\n\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -40,9 +40,35 @@ namespace compute {\n /// allows us to take advantage of these resources without coupling the logic with\n /// the execution engine.\n struct LightContext {\n+  static constexpr int kLogMinibatchSizeMax = 10;\n+  static constexpr int kMinibatchSizeMax = 1 << kLogMinibatchSizeMax;\n+\n   bool has_avx2() const { return (hardware_flags & arrow::internal::CpuInfo::AVX2) > 0; }\n   int64_t hardware_flags;\n   util::TempVectorStack* stack;\n+  MemoryPool* memory_pool;\n+\n+  // A deleter for a light context that owns its TempVectorStack\n+  struct OwningLightContextDeleter {\n+    void operator()(LightContext* ctx) const {\n+      delete ctx->stack;\n+      delete ctx;\n+    };\n+  };\n+\n+  // Creates a new light context with an owned temp vector stack.  Only to be\n+  // used at the highest level (e.g. in ExecPlan, unit tests or benchmarks)\n+  // Nodes should get their light context from the plan\n\nReview Comment:\n   I think generally people will rely on ExecContext (or QueryContext once that merges in). LightContext is more designed to be created on the stack and passed around to lower-level code. The biggest use-case is to not have to continuously look up the thread-local TempVectorStack and instead look it up once and cache it here. \n\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -40,9 +40,35 @@ namespace compute {\n /// allows us to take advantage of these resources without coupling the logic with\n /// the execution engine.\n struct LightContext {\n+  static constexpr int kLogMinibatchSizeMax = 10;\n\nReview Comment:\n   Why not reuse this from `compute/exec/util.h`? \n\n\n\n##########\ncpp/src/arrow/compute/exec/swiss_join_test.cc:\n##########\n@@ -0,0 +1,561 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/swiss_join.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include <cstdint>\n+#include <memory>\n+#include <vector>\n+\n+#include \"arrow/array/builder_primitive.h\"\n+#include \"arrow/array/concatenate.h\"\n+#include \"arrow/compute/api_vector.h\"\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/compute/exec/util.h\"\n+#include \"arrow/memory_pool.h\"\n+#include \"arrow/testing/future_util.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/thread_pool.h\"\n+\n+namespace arrow {\n+\n+using internal::Executor;\n+using internal::GetCpuThreadPool;\n+\n+namespace compute {\n+\n+void CheckRowArray(const std::vector<ExecBatch>& batches, const RowArray& row_array,\n+                   const std::vector<uint32_t>& row_ids) {\n+  ASSERT_FALSE(batches.empty());\n+  int num_rows = 0;\n+  for (const auto& batch : batches) {\n+    num_rows += static_cast<int>(batch.length);\n+  }\n+  int num_columns = batches[0].num_values();\n+  ASSERT_LE(num_rows, row_array.num_rows());\n+  for (int i = 0; i < num_columns; i++) {\n+    ResizableArrayData target;\n+    target.Init(batches[0].values[i].type(), default_memory_pool(),\n+                bit_util::Log2(num_rows));\n+    ASSERT_OK(row_array.DecodeSelected(&target, i, num_rows, row_ids.data(),\n+                                       default_memory_pool()));\n+    std::shared_ptr<ArrayData> decoded = target.array_data();\n+    std::vector<std::shared_ptr<Array>> expected_arrays;\n+    for (const auto& batch : batches) {\n+      expected_arrays.push_back(batch.values[i].make_array());\n+    }\n+    ASSERT_OK_AND_ASSIGN(std::shared_ptr<Array> expected_array,\n+                         Concatenate(expected_arrays));\n+    AssertArraysEqual(*expected_array, *MakeArray(decoded));\n+  }\n+}\n+\n+Result<int> EncodeEntireBatches(RowArray* row_array,\n+                                const std::vector<ExecBatch>& batches) {\n+  std::vector<KeyColumnArray> temp_column_arrays;\n+  int num_rows_encoded = 0;\n+  for (const auto& batch : batches) {\n+    int num_rows = static_cast<int>(batch.length);\n+    std::vector<uint16_t> in_row_ids(num_rows);\n+    std::iota(in_row_ids.begin(), in_row_ids.end(), 0);\n+    RETURN_NOT_OK(row_array->InitIfNeeded(default_memory_pool(), batch));\n+    RETURN_NOT_OK(row_array->AppendBatchSelection(default_memory_pool(), batch, 0,\n+                                                  num_rows, num_rows, in_row_ids.data(),\n+                                                  temp_column_arrays));\n+    num_rows_encoded += num_rows;\n+  }\n+  return num_rows_encoded;\n+}\n+\n+void CheckRoundTrip(const ExecBatch& batch, std::vector<uint16_t> row_ids = {}) {\n+  if (row_ids.empty()) {\n+    row_ids.resize(batch.length);\n+    std::iota(row_ids.begin(), row_ids.end(), 0);\n+  }\n+  std::vector<uint32_t> array_row_ids(row_ids.size());\n+  std::iota(array_row_ids.begin(), array_row_ids.end(), 0);\n+\n+  RowArray row_array;\n+  ASSERT_OK(EncodeEntireBatches(&row_array, {batch}));\n+  CheckRowArray({batch}, row_array, array_row_ids);\n+}\n+\n+TEST(RowArray, Basic) {\n+  // All fixed\n+  CheckRoundTrip(ExecBatchFromJSON({int32(), int32(), int32()}, R\"([\n+                   [1, 4, 7],\n+                   [2, 5, 8],\n+                   [3, 6, 9]\n+                 ])\"));\n+  // All varlen\n+  CheckRoundTrip(ExecBatchFromJSON({utf8(), utf8()}, R\"([\n+                   [\"xyz\", \"longer string\"],\n+                   [\"really even longer string\", \"\"],\n+                   [null, \"a\"],\n+                   [\"null\", null],\n+                   [\"b\", \"c\"]\n+                 ])\"));\n+  // Mixed types\n+  CheckRoundTrip(ExecBatchFromJSON({boolean(), int32(), utf8()}, R\"([\n+    [true, 3, \"test\"],\n+    [null, null, null],\n+    [false, 0, \"blah\"]\n+  ])\"));\n+  // No nulls\n+  CheckRoundTrip(ExecBatchFromJSON({int16(), utf8()}, R\"([\n+    [10, \"blahblah\"],\n+    [8, \"not-null\"]\n+  ])\"));\n+}\n+\n+TEST(RowArray, MultiBatch) {\n+  constexpr int kRowsPerBatch = std::numeric_limits<uint16_t>::max();\n+  constexpr int num_batches = 4;\n+  constexpr int num_out_rows = kRowsPerBatch * num_batches;\n+  std::vector<uint16_t> in_row_ids(kRowsPerBatch);\n+  std::vector<uint32_t> out_row_ids(num_out_rows);\n+  std::iota(in_row_ids.begin(), in_row_ids.end(), 0);\n+  std::iota(out_row_ids.begin(), out_row_ids.end(), 0);\n+  // Should be able to encode multiple batches to something\n+  // greater than 2^16 rows\n+  BatchesWithSchema test_data = MakeRandomBatches(\n+      schema({field(\"bool\", boolean()), field(\"i8\", int8()), field(\"utf8\", utf8())}), 4,\n+      kRowsPerBatch, /*add_tag=*/false);\n+\n+  RowArray row_array;\n+  ASSERT_OK_AND_ASSIGN(int num_rows_encoded,\n+                       EncodeEntireBatches(&row_array, test_data.batches));\n+  ASSERT_EQ(num_rows_encoded, row_array.num_rows());\n+\n+  CheckRowArray(test_data.batches, row_array, out_row_ids);\n+}\n+\n+Result<ExecBatch> ColumnTake(const ExecBatch& input,\n+                             const std::vector<uint16_t>& row_ids) {\n+  UInt16Builder take_indices_builder;\n+  RETURN_NOT_OK(take_indices_builder.AppendValues(row_ids));\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Array> take_indices,\n+                        take_indices_builder.Finish());\n+\n+  std::vector<Datum> taken_arrays(input.num_values());\n+  for (int i = 0; i < input.num_values(); i++) {\n+    ARROW_ASSIGN_OR_RAISE(Datum taken_array, Take(input.values[i], take_indices));\n+    taken_arrays[i] = taken_array;\n+  }\n+  return ExecBatch(taken_arrays, static_cast<int64_t>(row_ids.size()));\n+}\n+\n+TEST(RowArray, InputPartialSelection) {\n+  constexpr int num_rows = std::numeric_limits<uint16_t>::max();\n+  BatchesWithSchema test_data = MakeRandomBatches(\n+      schema({field(\"bool\", boolean()), field(\"i8\", int8()), field(\"utf8\", utf8())}), 1,\n+      num_rows, /*add_tag=*/false);\n+  ExecBatch batch = test_data.batches[0];\n+  std::vector<uint16_t> row_ids;\n+  std::default_random_engine gen(42);\n+  std::uniform_int_distribution<> dist(0, 1);\n+  for (uint16_t i = 0; i < num_rows; i++) {\n+    if (dist(gen)) {\n+      row_ids.push_back(i);\n+    }\n+  }\n+  int num_selected_rows = static_cast<int>(row_ids.size());\n+  ASSERT_GT(num_selected_rows, 0);\n+  ASSERT_LT(num_selected_rows, num_rows);\n+\n+  std::vector<uint32_t> array_row_ids(row_ids.size());\n+  std::iota(array_row_ids.begin(), array_row_ids.end(), 0);\n+\n+  RowArray row_array;\n+  std::vector<KeyColumnArray> temp_column_arrays;\n+  ASSERT_OK(row_array.InitIfNeeded(default_memory_pool(), batch));\n+  ASSERT_OK(row_array.AppendBatchSelection(default_memory_pool(), batch, 0,\n+                                           num_selected_rows, num_selected_rows,\n+                                           row_ids.data(), temp_column_arrays));\n+\n+  ASSERT_OK_AND_ASSIGN(ExecBatch expected, ColumnTake(batch, row_ids));\n+  CheckRowArray({expected}, row_array, array_row_ids);\n+}\n+\n+TEST(RowArray, ThreadedDecode) {\n+  // Create kBatchesPerThread batches per thread.  Encode all batches from the main\n+  // thread. Then decode in parallel so each thread decodes it's own batches\n+  constexpr int kRowsPerBatch = std::numeric_limits<uint16_t>::max();\n+  constexpr int kNumThreads = 16;\n+  constexpr int kBatchesPerThread = 2;\n+  constexpr int kNumBatches = kNumThreads * kBatchesPerThread;\n+  constexpr int kNumOutRows = kRowsPerBatch * kBatchesPerThread;\n+\n+  BatchesWithSchema test_data = MakeRandomBatches(\n+      schema({field(\"bool\", boolean()), field(\"i8\", int8()), field(\"utf8\", utf8())}),\n+      kNumBatches, kRowsPerBatch, /*add_tag=*/false);\n+\n+  RowArray row_array;\n+  ASSERT_OK_AND_ASSIGN(int num_rows_encoded,\n+                       EncodeEntireBatches(&row_array, test_data.batches));\n+  ASSERT_EQ(num_rows_encoded, kNumOutRows * kNumThreads);\n+\n+  std::vector<std::vector<uint32_t>> row_ids_for_threads(kNumThreads);\n+  uint32_t row_id_offset = 0;\n+  for (int i = 0; i < kNumThreads; i++) {\n+    std::vector<uint32_t>& row_ids_for_thread = row_ids_for_threads[i];\n+    row_ids_for_thread.resize(kNumOutRows);\n+    std::iota(row_ids_for_thread.begin(), row_ids_for_thread.end(), row_id_offset);\n+    row_id_offset += kNumOutRows;\n+  }\n+\n+  std::vector<std::thread> thread_tasks;\n+  for (int i = 0; i < kNumThreads; i++) {\n+    thread_tasks.emplace_back([&, i] {\n+      CheckRowArray({test_data.batches[i * 2], test_data.batches[i * 2 + 1]}, row_array,\n+                    row_ids_for_threads[i]);\n+    });\n+  }\n+  for (auto& thread_task : thread_tasks) {\n+    thread_task.join();\n+  }\n+}\n+\n+void CheckComparison(const ExecBatch& left, const ExecBatch& right,\n+                     const std::vector<bool> expected) {\n+  RowArray row_array;\n+  ASSERT_OK(EncodeEntireBatches(&row_array, {left}));\n+  std::vector<uint32_t> row_ids(right.length);\n+  std::iota(row_ids.begin(), row_ids.end(), 0);\n+  uint32_t num_not_equal = 0;\n+  std::vector<uint16_t> not_equal_selection(right.length);\n+  LightContext light_context;\n+  light_context.hardware_flags =\n+      arrow::internal::CpuInfo::GetInstance()->hardware_flags();\n+  util::TempVectorStack temp_stack;\n+  ASSERT_OK(temp_stack.Init(default_memory_pool(),\n+                            4 * util::MiniBatch::kMiniBatchLength * sizeof(uint32_t)));\n+  light_context.stack = &temp_stack;\n+  std::vector<KeyColumnArray> temp_column_arrays;\n+  std::vector<uint8_t> match_bit_vector(bit_util::Log2(right.length));\n+  row_array.Compare(\n+      right, 0, static_cast<int>(right.length), static_cast<int>(right.length), nullptr,\n+      row_ids.data(), &num_not_equal, not_equal_selection.data(),\n+      light_context.hardware_flags, light_context.stack, temp_column_arrays, nullptr);\n+\n+  // We over-allocated above\n+  not_equal_selection.resize(num_not_equal);\n+  std::vector<uint16_t> expected_not_equal_selection;\n+  for (uint16_t i = 0; i < static_cast<uint16_t>(expected.size()); i++) {\n+    if (!expected[i]) {\n+      expected_not_equal_selection.push_back(i);\n+    }\n+  }\n+  ASSERT_EQ(expected_not_equal_selection, not_equal_selection);\n+\n+  int expected_not_equal = 0;\n+  for (bool val : expected) {\n+    if (!val) {\n+      expected_not_equal++;\n+    }\n+  }\n+  ASSERT_EQ(expected_not_equal, num_not_equal);\n+}\n+\n+TEST(RowArray, Compare) {\n+  ExecBatch left = ExecBatchFromJSON({int16(), utf8()}, R\"([\n+    [1, \"blahblah\"],\n+    [2, null],\n+    [3, \"xyz\"],\n+    [4, \"sample\"],\n+    [null, \"5\"],\n+    [null, null]\n+  ])\");\n+  ExecBatch right = ExecBatchFromJSON({int16(), utf8()}, R\"([\n+    [1, \"blahblah\"],\n+    [2, \"not-null\"],\n+    [3, \"abc\"],\n+    [5, \"blah\"],\n+    [null, \"5\"],\n+    [null, null]\n+  ])\");\n+  std::vector<bool> expected_matches = {\n+      true,   // Matches exactly without nulls\n+      false,  // differs by null only\n+      false,  // differs by one value\n+      false,  // differs by all values\n+      true,   // equal but has nulls\n+      true    // equal-all-null\n+  };\n+  // Test in both directions\n+  CheckComparison(left, right, expected_matches);\n+  CheckComparison(right, left, expected_matches);\n+}\n+\n+struct SwissTableTestParams {\n+  std::shared_ptr<Schema> schema;\n+  std::string name;\n+};\n+\n+std::ostream& operator<<(std::ostream& os, const SwissTableTestParams& params) {\n+  return os << params.name;\n+}\n+\n+class SwissTableWithKeysTest : public ::testing::TestWithParam<SwissTableTestParams> {\n+ public:\n+  void SetUp() {\n+    ASSERT_OK_AND_ASSIGN(ctx_, LightContext::Make(default_memory_pool()));\n+    ASSERT_OK(table_.Init(ctx_->hardware_flags, ctx_->memory_pool));\n+  }\n+\n+  const std::shared_ptr<Schema>& test_schema() { return GetParam().schema; }\n+\n+ protected:\n+  SwissTableWithKeys table_;\n+  // Scratch space to use in tests\n+  std::unique_ptr<LightContext, LightContext::OwningLightContextDeleter> ctx_;\n+  std::vector<KeyColumnArray> temp_column_arrays_;\n+};\n+\n+TEST_P(SwissTableWithKeysTest, Hash) {\n+  // Basic test for the hash utility function.  If I hash the same keys I should get the\n+  // same hashes.  If I hash different keys I should get different hashes.\n+  constexpr int kBatchLength = 128;\n+  BatchesWithSchema batches_with_schema =\n+      MakeRandomBatches(test_schema(),\n+                        /*num_batches=*/2, kBatchLength, /*add_tag=*/false);\n+  SwissTableWithKeys::Input input(&batches_with_schema.batches[0], ctx_->stack,\n+                                  &temp_column_arrays_);\n+\n+  std::vector<uint32_t> hashes_one(kBatchLength);\n+  SwissTableWithKeys::Hash(&input, hashes_one.data(), ctx_->hardware_flags);\n+\n+  std::vector<uint32_t> hashes_two(kBatchLength);\n+  SwissTableWithKeys::Hash(&input, hashes_two.data(), ctx_->hardware_flags);\n+\n+  ASSERT_EQ(hashes_one, hashes_two);\n+\n+  SwissTableWithKeys::Input input_two(&batches_with_schema.batches[1], ctx_->stack,\n+                                      &temp_column_arrays_);\n+  std::vector<uint32_t> hashes_three(kBatchLength);\n+  SwissTableWithKeys::Hash(&input_two, hashes_three.data(), ctx_->hardware_flags);\n+\n+  // In theory it's possible for all hashes to have collisions but it should be unlikely\n+  // enough for a unit test\n+  ASSERT_NE(hashes_one, hashes_three);\n+\n+  // Original hash values should still be in there\n+  std::vector<uint32_t> hashes_four(kBatchLength);\n+  SwissTableWithKeys::Hash(&input, hashes_four.data(), ctx_->hardware_flags);\n+\n+  ASSERT_EQ(hashes_one, hashes_four);\n+}\n+\n+std::shared_ptr<Array> ReverseIndices(int64_t len) {\n+  Int32Builder indices_builder;\n+  ARROW_EXPECT_OK(indices_builder.Reserve(len));\n+  for (int64_t i = len - 1; i >= 0; i--) {\n+    indices_builder.UnsafeAppend(static_cast<int32_t>(i));\n+  }\n+  EXPECT_OK_AND_ASSIGN(std::shared_ptr<Array> indices, indices_builder.Finish());\n+  return indices;\n+}\n+\n+ExecBatch ReverseBatch(ExecBatch batch) {\n+  ExecBatch reversed;\n+  std::shared_ptr<Array> reversed_indices = ReverseIndices(batch.length);\n+  for (const arrow::Datum& datum : batch.values) {\n+    EXPECT_OK_AND_ASSIGN(auto reversed_datum, Take(datum.make_array(), reversed_indices));\n+    reversed.values.push_back(reversed_datum);\n+  }\n+  reversed.length = batch.length;\n+  return reversed;\n+}\n+\n+bool AllUnique(const std::vector<uint32_t>& values) {\n+  std::vector<uint32_t> values_copy(values);\n+  std::sort(values_copy.begin(), values_copy.end());\n+  std::unique(values_copy.begin(), values_copy.end());\n+  return values.size() == values_copy.size();\n+}\n+\n+TEST_P(SwissTableWithKeysTest, Map) {\n+  constexpr int kBatchLength = 128;\n+  BatchesWithSchema batches_with_schema =\n+      MakeRandomBatches(test_schema(),\n+                        /*num_batches=*/2, kBatchLength, /*add_tag=*/false);\n+  SwissTableWithKeys::Input input(&batches_with_schema.batches[0], ctx_->stack,\n+                                  &temp_column_arrays_);\n+\n+  std::vector<uint32_t> hashes_one(kBatchLength);\n+  SwissTableWithKeys::Hash(&input, hashes_one.data(), ctx_->hardware_flags);\n+\n+  // Insert a bunch of keys\n+  std::vector<uint32_t> key_ids(kBatchLength);\n+  ASSERT_OK(table_.MapWithInserts(&input, hashes_one.data(), key_ids.data()));\n+\n+  // We should have all unique keys but we can't make any assertions above and beyond that\n+  ASSERT_TRUE(AllUnique(key_ids));\n+\n+  // Map the same keys, everything should match\n+  std::vector<uint8_t> match_bitvector(bit_util::CeilDiv(kBatchLength, 8));\n+  std::vector<uint32_t> new_key_ids(kBatchLength);\n+  table_.MapReadOnly(&input, hashes_one.data(), match_bitvector.data(),\n+                     new_key_ids.data());\n+\n+  for (const auto& match_byte : match_bitvector) {\n+    ASSERT_EQ(0xFF, match_byte);\n+  }\n+  ASSERT_EQ(key_ids, new_key_ids);\n+\n+  // Map the keys in reverse, should still match but key ids are reversed\n+  ExecBatch reversed_batch = ReverseBatch(batches_with_schema.batches[0]);\n+  SwissTableWithKeys::Input reversed_input(&reversed_batch, ctx_->stack,\n+                                           &temp_column_arrays_);\n+\n+  std::vector<uint32_t> hashes_one_reverse(kBatchLength);\n+  SwissTableWithKeys::Hash(&reversed_input, hashes_one_reverse.data(),\n+                           ctx_->hardware_flags);\n+\n+  new_key_ids.clear();\n+  new_key_ids.resize(kBatchLength);\n+  match_bitvector.clear();\n+  match_bitvector.resize(bit_util::CeilDiv(kBatchLength, 8));\n+  table_.MapReadOnly(&reversed_input, hashes_one_reverse.data(), match_bitvector.data(),\n+                     new_key_ids.data());\n+\n+  for (const auto& match_byte : match_bitvector) {\n+    ASSERT_EQ(0xFF, match_byte);\n+  }\n+  std::reverse(new_key_ids.begin(), new_key_ids.end());\n+  ASSERT_EQ(key_ids, new_key_ids);\n+\n+  // Map another batch of keys.  Some of them might match (since it is random data)\n+  // but it should be incredibly unlikely that any batch of 255 keys fully match\n+  SwissTableWithKeys::Input non_match_input(&batches_with_schema.batches[1], ctx_->stack,\n+                                            &temp_column_arrays_);\n+  std::vector<uint32_t> hashes_non_match(kBatchLength);\n+  SwissTableWithKeys::Hash(&non_match_input, hashes_non_match.data(),\n+                           ctx_->hardware_flags);\n+\n+  new_key_ids.clear();\n+  new_key_ids.resize(kBatchLength);\n+  match_bitvector.clear();\n+  match_bitvector.resize(bit_util::CeilDiv(kBatchLength, 8));\n+  table_.MapReadOnly(&non_match_input, hashes_non_match.data(), match_bitvector.data(),\n+                     new_key_ids.data());\n+\n+  // Can't compare directly to 0x00 since random data may allow for a few matches\n+  // A full matching block of 256 rows is extremely unlikely though.\n+  for (const auto& match_byte : match_bitvector) {\n+    ASSERT_NE(0xFF, match_byte);\n+  }\n+  uint32_t count_matching_key_ids = 0;\n+  for (const auto& key_id : new_key_ids) {\n+    if (key_id != 0) {\n+      count_matching_key_ids++;\n+    }\n+  }\n+  // Divide by 2 is a bit arbitrary but since this is random data some matches\n+  // are to be expected.\n+  ASSERT_LT(count_matching_key_ids, kBatchLength / 2);\n+}\n+\n+// The table should allow parallel read access\n+TEST_P(SwissTableWithKeysTest, ThreadedMapReadOnly) {\n+  constexpr int kBatchLength = 128;\n+  constexpr int kBatchLengthBytes = bit_util::CeilDiv(kBatchLength, 8);\n+  constexpr int kNumBatches = 8;\n+  // These tasks will be divided across all threads\n+  constexpr int kNumReadTasks = 1 << 12;\n+\n+  BatchesWithSchema batches_with_schema =\n+      MakeRandomBatches(test_schema(), kNumBatches, kBatchLength, /*add_tag=*/false);\n+\n+  // Insert all batches\n+  // Save precomputed hashes so threads can focus on the map task\n+  std::vector<std::vector<uint32_t>> hashes(kBatchLength);\n+  std::vector<std::vector<uint32_t>> expected_key_ids(kNumBatches);\n+  for (std::size_t i = 0; i < kNumBatches; i++) {\n+    hashes[i].resize(kBatchLength);\n+    expected_key_ids[i].resize(kBatchLength);\n+    SwissTableWithKeys::Input input(&batches_with_schema.batches[i], ctx_->stack,\n+                                    &temp_column_arrays_);\n+    SwissTableWithKeys::Hash(&input, hashes[i].data(), ctx_->hardware_flags);\n+    ASSERT_OK(\n+        table_.MapWithInserts(&input, hashes[i].data(), expected_key_ids[i].data()));\n+  }\n+\n+  struct ReadTaskThreadState {\n+    ReadTaskThreadState() {\n+      auto maybe_ctx = LightContext::Make(default_memory_pool());\n+      ctx = maybe_ctx.MoveValueUnsafe();\n+    }\n+    std::unique_ptr<LightContext, LightContext::OwningLightContextDeleter> ctx;\n\nReview Comment:\n   Also here, this doesn't seem like it needs to be unique_ptr\n\n\n\n",
                    "created": "2022-07-29T22:03:17.891+0000",
                    "updated": "2022-07-29T22:03:17.891+0000",
                    "started": "2022-07-29T22:03:17.891+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796526",
                    "issueId": "13470747"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796528",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#discussion_r933654764\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -40,9 +40,35 @@ namespace compute {\n /// allows us to take advantage of these resources without coupling the logic with\n /// the execution engine.\n struct LightContext {\n+  static constexpr int kLogMinibatchSizeMax = 10;\n+  static constexpr int kMinibatchSizeMax = 1 << kLogMinibatchSizeMax;\n+\n   bool has_avx2() const { return (hardware_flags & arrow::internal::CpuInfo::AVX2) > 0; }\n   int64_t hardware_flags;\n   util::TempVectorStack* stack;\n+  MemoryPool* memory_pool;\n+\n+  // A deleter for a light context that owns its TempVectorStack\n+  struct OwningLightContextDeleter {\n+    void operator()(LightContext* ctx) const {\n+      delete ctx->stack;\n+      delete ctx;\n+    };\n+  };\n+\n+  // Creates a new light context with an owned temp vector stack.  Only to be\n+  // used at the highest level (e.g. in ExecPlan, unit tests or benchmarks)\n+  // Nodes should get their light context from the plan\n\nReview Comment:\n   We have too many context objects at the moment :laughing: .  `LightContext` was my poor attempt at https://xkcd.com/927/\r\n   \r\n   However, since it holds a non-owning pointer to `TempVectorStack` it can't just be easily created for unit tests.  So this was just a quick-hack for unit tests.  The unique_ptr with a custom deleter is just a way of doing...\r\n   \r\n   ```\r\n   struct LightContextThatAlsoOwnsTempVectorStack {\r\n     LightContext ctx;\r\n     TempVectorStack stack;\r\n   }\r\n   ```\r\n   \r\n   ...without creating a new class since I was afraid of creating \"yet another context variant\".  However, I think I might as well do that explicitly and create a new class.  It will be less confusing to new readers.\n\n\n\n",
                    "created": "2022-07-29T22:19:09.710+0000",
                    "updated": "2022-07-29T22:19:09.710+0000",
                    "started": "2022-07-29T22:19:09.709+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796528",
                    "issueId": "13470747"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796529",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#discussion_r933656738\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -40,9 +40,35 @@ namespace compute {\n /// allows us to take advantage of these resources without coupling the logic with\n /// the execution engine.\n struct LightContext {\n+  static constexpr int kLogMinibatchSizeMax = 10;\n+  static constexpr int kMinibatchSizeMax = 1 << kLogMinibatchSizeMax;\n+\n   bool has_avx2() const { return (hardware_flags & arrow::internal::CpuInfo::AVX2) > 0; }\n   int64_t hardware_flags;\n   util::TempVectorStack* stack;\n+  MemoryPool* memory_pool;\n+\n+  // A deleter for a light context that owns its TempVectorStack\n+  struct OwningLightContextDeleter {\n+    void operator()(LightContext* ctx) const {\n+      delete ctx->stack;\n+      delete ctx;\n+    };\n+  };\n+\n+  // Creates a new light context with an owned temp vector stack.  Only to be\n+  // used at the highest level (e.g. in ExecPlan, unit tests or benchmarks)\n+  // Nodes should get their light context from the plan\n\nReview Comment:\n   However, I completely agree that once `QueryContext` merges in then this entire construct can go away.  I can't get away with `ExecContext` because it doesn't have `TempVectorStack`.\n\n\n\n",
                    "created": "2022-07-29T22:21:09.314+0000",
                    "updated": "2022-07-29T22:21:09.314+0000",
                    "started": "2022-07-29T22:21:09.314+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796529",
                    "issueId": "13470747"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796530",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "save-buffer commented on code in PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#discussion_r933658472\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -40,9 +40,35 @@ namespace compute {\n /// allows us to take advantage of these resources without coupling the logic with\n /// the execution engine.\n struct LightContext {\n+  static constexpr int kLogMinibatchSizeMax = 10;\n+  static constexpr int kMinibatchSizeMax = 1 << kLogMinibatchSizeMax;\n+\n   bool has_avx2() const { return (hardware_flags & arrow::internal::CpuInfo::AVX2) > 0; }\n   int64_t hardware_flags;\n   util::TempVectorStack* stack;\n+  MemoryPool* memory_pool;\n+\n+  // A deleter for a light context that owns its TempVectorStack\n+  struct OwningLightContextDeleter {\n+    void operator()(LightContext* ctx) const {\n+      delete ctx->stack;\n+      delete ctx;\n+    };\n+  };\n+\n+  // Creates a new light context with an owned temp vector stack.  Only to be\n+  // used at the highest level (e.g. in ExecPlan, unit tests or benchmarks)\n+  // Nodes should get their light context from the plan\n\nReview Comment:\n   \ud83d\ude02 Well it seems like at least in the two scenarios you used unique_ptr<LightContext> you could have just made TempVectorStack a member right? And just given LightContext a pointer to it?\n\n\n\n",
                    "created": "2022-07-29T22:26:15.624+0000",
                    "updated": "2022-07-29T22:26:15.624+0000",
                    "started": "2022-07-29T22:26:15.623+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796530",
                    "issueId": "13470747"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796542",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#discussion_r933670960\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -40,9 +40,35 @@ namespace compute {\n /// allows us to take advantage of these resources without coupling the logic with\n /// the execution engine.\n struct LightContext {\n+  static constexpr int kLogMinibatchSizeMax = 10;\n+  static constexpr int kMinibatchSizeMax = 1 << kLogMinibatchSizeMax;\n+\n   bool has_avx2() const { return (hardware_flags & arrow::internal::CpuInfo::AVX2) > 0; }\n   int64_t hardware_flags;\n   util::TempVectorStack* stack;\n+  MemoryPool* memory_pool;\n+\n+  // A deleter for a light context that owns its TempVectorStack\n+  struct OwningLightContextDeleter {\n+    void operator()(LightContext* ctx) const {\n+      delete ctx->stack;\n+      delete ctx;\n+    };\n+  };\n+\n+  // Creates a new light context with an owned temp vector stack.  Only to be\n+  // used at the highest level (e.g. in ExecPlan, unit tests or benchmarks)\n+  // Nodes should get their light context from the plan\n\nReview Comment:\n   Right but then something like this...\r\n   \r\n   ```\r\n   TEST(SomeTest, ThatNeedsLightContext) {\r\n     auto ctx = LightContext::Make();\r\n     DoThingWithCtx(*ctx);\r\n   }\r\n   ```\r\n   \r\n   ...becomes...\r\n   \r\n   ```\r\n   TEST(SomeTest, ThatNeedsLightContext) {\r\n     util::TempVectorStack stack;\r\n     stack.Init(kWhatWasThatParameterAgain);\r\n     LightContext ctx(&stack);\r\n     DoThingWithCtx(ctx);\r\n   }\r\n   ```\r\n   \r\n   Whatever our \"master context\" ends up being I would very much like to have a one-liner that creates one and all I have to do is keep track of the thing that comes back from that one-liner.  Otherwise I have to remember the recipe every time I write a unit test.\r\n   \n\n\n\n",
                    "created": "2022-07-29T23:08:03.881+0000",
                    "updated": "2022-07-29T23:08:03.881+0000",
                    "started": "2022-07-29T23:08:03.881+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796542",
                    "issueId": "13470747"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/worklog/796546",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "save-buffer commented on code in PR #13751:\nURL: https://github.com/apache/arrow/pull/13751#discussion_r933693118\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -40,9 +40,35 @@ namespace compute {\n /// allows us to take advantage of these resources without coupling the logic with\n /// the execution engine.\n struct LightContext {\n+  static constexpr int kLogMinibatchSizeMax = 10;\n+  static constexpr int kMinibatchSizeMax = 1 << kLogMinibatchSizeMax;\n+\n   bool has_avx2() const { return (hardware_flags & arrow::internal::CpuInfo::AVX2) > 0; }\n   int64_t hardware_flags;\n   util::TempVectorStack* stack;\n+  MemoryPool* memory_pool;\n+\n+  // A deleter for a light context that owns its TempVectorStack\n+  struct OwningLightContextDeleter {\n+    void operator()(LightContext* ctx) const {\n+      delete ctx->stack;\n+      delete ctx;\n+    };\n+  };\n+\n+  // Creates a new light context with an owned temp vector stack.  Only to be\n+  // used at the highest level (e.g. in ExecPlan, unit tests or benchmarks)\n+  // Nodes should get their light context from the plan\n\nReview Comment:\n   Ah makes sense, if we're getting rid of it anyway after QueryContext then it's probably fine\n\n\n\n",
                    "created": "2022-07-30T00:09:28.408+0000",
                    "updated": "2022-07-30T00:09:28.408+0000",
                    "started": "2022-07-30T00:09:28.408+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "796546",
                    "issueId": "13470747"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 5400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@175406c8[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5ce45ecf[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3148c8d2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@4f24fa12[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@26d86024[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@4f481974[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@38314617[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@50addf69[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@35114c18[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@3a9d7077[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@82e88cd[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@36465836[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 5400,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Oct 28 17:51:37 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": null,
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-17022/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2022-07-08T18:23:11.000+0000",
        "updated": "2022-10-28T17:51:37.000+0000",
        "timeoriginalestimate": null,
        "description": "The swiss join utilities being added as part of ARROW-14182 are not adequately unit tested at the moment.  They have fairly decent coverage from end-to-end random hash join testing.  However, a set of basic unit tests will help future maintenance by demonstrating basic usage and allowing for more targeted fixes when a refactor breaks something.  I'm doing some of this work as I review ARROW-14182 anyways so that I can better understand it.  Rather than complicate the review I will open this as a separate follow-up PR.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "1.5h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 5400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Add unit tests and documentation for swiss-join ",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13470747/comment/17625830",
                    "id": "17625830",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=arrowjira",
                        "name": "arrowjira",
                        "key": "arrowjira",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Apache Arrow JIRA Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "This issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned per [project policy|https://arrow.apache.org/docs/dev/developers/bug_reports.html#issue-assignment]. Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=arrowjira",
                        "name": "arrowjira",
                        "key": "arrowjira",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Apache Arrow JIRA Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2022-10-28T17:51:37.112+0000",
                    "updated": "2022-10-28T17:51:37.112+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z16py0:",
        "customfield_12314139": null
    }
}