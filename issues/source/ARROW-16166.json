{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13439066",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066",
    "key": "ARROW-16166",
    "fields": {
        "parent": {
            "id": "13404218",
            "key": "ARROW-14182",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13404218",
            "fields": {
                "summary": "[C++][Compute] Hash Join performance improvement",
                "status": {
                    "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                    "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                    "name": "Resolved",
                    "id": "5",
                    "statusCategory": {
                        "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                        "id": 3,
                        "key": "done",
                        "colorName": "green",
                        "name": "Done"
                    }
                },
                "priority": {
                    "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                    "name": "Major",
                    "id": "3"
                },
                "issuetype": {
                    "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                    "id": "4",
                    "description": "An improvement or enhancement to an existing feature or task.",
                    "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                    "name": "Improvement",
                    "subtask": false,
                    "avatarId": 21140
                }
            }
        },
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12351051",
                "id": "12351051",
                "description": "",
                "name": "8.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-05-06"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 18000,
            "total": 18000,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 18000,
            "total": 18000,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-16166/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 30,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/756185",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa opened a new pull request, #12872:\nURL: https://github.com/apache/arrow/pull/12872\n\n   Adding utilities that make it easier to filter, replicate and accumulate rows from potentially multiple sources in a single exec batch.\r\n   This is meant to be used in hash join for reducing overheads in producing its output.\r\n   \r\n   Also, moving light-weight array utilities to the same file and adding helper functions for generating them.\r\n   \r\n   Thanks to Weston Pace for updating comments and writing the unit tests.\n\n\n",
                    "created": "2022-04-13T05:50:51.143+0000",
                    "updated": "2022-04-13T05:50:51.143+0000",
                    "started": "2022-04-13T05:50:51.143+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "756185",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/756186",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#issuecomment-1097585985\n\n   https://issues.apache.org/jira/browse/ARROW-16166\n\n\n",
                    "created": "2022-04-13T05:51:08.797+0000",
                    "updated": "2022-04-13T05:51:08.797+0000",
                    "started": "2022-04-13T05:51:08.796+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "756186",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/756187",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#issuecomment-1097586004\n\n   :warning: Ticket **has not been started in JIRA**, please click 'Start Progress'.\n\n\n",
                    "created": "2022-04-13T05:51:10.648+0000",
                    "updated": "2022-04-13T05:51:10.648+0000",
                    "started": "2022-04-13T05:51:10.647+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "756187",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757173",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "save-buffer commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r850736637\n\n\n##########\ncpp/src/arrow/compute/light_array_test.cc:\n##########\n@@ -0,0 +1,504 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/testing/generator.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+const std::vector<std::shared_ptr<DataType>> kSampleFixedDataTypes = {\n+    int8(),   int16(),  int32(),  int64(),           uint8(),\n+    uint16(), uint32(), uint64(), decimal128(38, 6), decimal256(76, 6)};\n+const std::vector<std::shared_ptr<DataType>> kSampleBinaryTypes = {\n+    utf8(), binary() /*, large_utf8(), large_binary()*/};\n+\n+TEST(KeyColumnMetadata, FromDataType) {\n+  KeyColumnMetadata metadata = ColumnMetadataFromDataType(boolean());\n+  ASSERT_EQ(0, metadata.fixed_length);\n+  ASSERT_EQ(true, metadata.is_fixed_length);\n+  ASSERT_EQ(false, metadata.is_null_type);\n+\n+  metadata = ColumnMetadataFromDataType(null());\n+  ASSERT_EQ(true, metadata.is_null_type);\n+\n+  for (const auto& type : kSampleFixedDataTypes) {\n+    int byte_width =\n+        internal::checked_pointer_cast<FixedWidthType>(type)->bit_width() / 8;\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(byte_width, metadata.fixed_length);\n+    ASSERT_EQ(true, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+\n+  for (const auto& type : {binary(), utf8()}) {\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(4, metadata.fixed_length);\n+    ASSERT_EQ(false, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+\n+  for (const auto& type : {large_binary(), large_utf8()}) {\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(8, metadata.fixed_length);\n+    ASSERT_EQ(false, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayData) {\n+  for (const auto& type : kSampleFixedDataTypes) {\n+    ARROW_SCOPED_TRACE(\"Type: \", type->ToString());\n+    // `array_offset` is the offset of the source array (e.g. if we are given a sliced\n+    // source array) while `offset` is the offset we pass when constructing the\n+    // KeyColumnArray\n+    for (auto array_offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+      for (auto offset : {0, 1}) {\n+        ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+        std::shared_ptr<Array> array;\n+        int byte_width =\n+            internal::checked_pointer_cast<FixedWidthType>(type)->bit_width() / 8;\n+        if (is_decimal(type->id())) {\n+          array = ArrayFromJSON(type, R\"([\"1.123123\", \"2.123123\", null])\");\n+        } else {\n+          array = ArrayFromJSON(type, \"[1, 2, null]\");\n+        }\n+        array = array->Slice(array_offset);\n+        int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+        int buffer_offset_bytes = (offset + array_offset) * byte_width;\n+        KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+        // Maximum tested offset is < 8 so validity is just bit offset\n+        ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+        ASSERT_EQ(0, kc_array.bit_offset(1));\n+        ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+        ASSERT_EQ(array->data()->buffers[1]->data() + buffer_offset_bytes,\n+                  kc_array.data(1));\n+        ASSERT_EQ(nullptr, kc_array.data(2));\n+        ASSERT_EQ(length, kc_array.length());\n+        // When creating from ArrayData always create read-only\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(2));\n+      }\n+    }\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayDataBinary) {\n+  for (const auto& type : kSampleBinaryTypes) {\n+    ARROW_SCOPED_TRACE(\"Type: \", type->ToString());\n+    for (auto array_offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+      for (auto offset : {0, 1}) {\n+        ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+        std::shared_ptr<Array> array = ArrayFromJSON(type, R\"([\"xyz\", \"abcabc\", null])\");\n+        int offsets_width =\n+            static_cast<int>(internal::checked_pointer_cast<BaseBinaryType>(type)\n+                                 ->layout()\n+                                 .buffers[1]\n+                                 .byte_width);\n+        array = array->Slice(array_offset);\n+        int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+        int buffer_offset_bytes = (offset + array_offset) * offsets_width;\n+        KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+        ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+        ASSERT_EQ(0, kc_array.bit_offset(1));\n+        ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+        ASSERT_EQ(array->data()->buffers[1]->data() + buffer_offset_bytes,\n+                  kc_array.data(1));\n+        ASSERT_EQ(array->data()->buffers[2]->data(), kc_array.data(2));\n+        ASSERT_EQ(length, kc_array.length());\n+        // When creating from ArrayData always create read-only\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(2));\n+      }\n+    }\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayDataBool) {\n+  for (auto array_offset : {0, 1}) {\n+    ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+    for (auto offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+      std::shared_ptr<Array> array = ArrayFromJSON(boolean(), \"[true, false, null]\");\n+      array = array->Slice(array_offset);\n+      int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+      KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+      ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+      ASSERT_EQ(offset + array_offset, kc_array.bit_offset(1));\n+      ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+      ASSERT_EQ(array->data()->buffers[1]->data(), kc_array.data(1));\n+      ASSERT_EQ(length, kc_array.length());\n+      ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+      ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+    }\n+  }\n+}\n+\n+// TEST(KeyColumnArray, FromArrayDataNull) {\n\nReview Comment:\n   remove or uncomment\n\n\n\n",
                    "created": "2022-04-14T19:20:42.368+0000",
                    "updated": "2022-04-14T19:20:42.368+0000",
                    "started": "2022-04-14T19:20:42.368+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757173",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757195",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r850768949\n\n\n##########\ncpp/src/arrow/compute/light_array_test.cc:\n##########\n@@ -0,0 +1,504 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/testing/generator.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+const std::vector<std::shared_ptr<DataType>> kSampleFixedDataTypes = {\n+    int8(),   int16(),  int32(),  int64(),           uint8(),\n+    uint16(), uint32(), uint64(), decimal128(38, 6), decimal256(76, 6)};\n+const std::vector<std::shared_ptr<DataType>> kSampleBinaryTypes = {\n+    utf8(), binary() /*, large_utf8(), large_binary()*/};\n+\n+TEST(KeyColumnMetadata, FromDataType) {\n+  KeyColumnMetadata metadata = ColumnMetadataFromDataType(boolean());\n+  ASSERT_EQ(0, metadata.fixed_length);\n+  ASSERT_EQ(true, metadata.is_fixed_length);\n+  ASSERT_EQ(false, metadata.is_null_type);\n+\n+  metadata = ColumnMetadataFromDataType(null());\n+  ASSERT_EQ(true, metadata.is_null_type);\n+\n+  for (const auto& type : kSampleFixedDataTypes) {\n+    int byte_width =\n+        internal::checked_pointer_cast<FixedWidthType>(type)->bit_width() / 8;\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(byte_width, metadata.fixed_length);\n+    ASSERT_EQ(true, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+\n+  for (const auto& type : {binary(), utf8()}) {\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(4, metadata.fixed_length);\n+    ASSERT_EQ(false, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+\n+  for (const auto& type : {large_binary(), large_utf8()}) {\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(8, metadata.fixed_length);\n+    ASSERT_EQ(false, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayData) {\n+  for (const auto& type : kSampleFixedDataTypes) {\n+    ARROW_SCOPED_TRACE(\"Type: \", type->ToString());\n+    // `array_offset` is the offset of the source array (e.g. if we are given a sliced\n+    // source array) while `offset` is the offset we pass when constructing the\n+    // KeyColumnArray\n+    for (auto array_offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+      for (auto offset : {0, 1}) {\n+        ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+        std::shared_ptr<Array> array;\n+        int byte_width =\n+            internal::checked_pointer_cast<FixedWidthType>(type)->bit_width() / 8;\n+        if (is_decimal(type->id())) {\n+          array = ArrayFromJSON(type, R\"([\"1.123123\", \"2.123123\", null])\");\n+        } else {\n+          array = ArrayFromJSON(type, \"[1, 2, null]\");\n+        }\n+        array = array->Slice(array_offset);\n+        int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+        int buffer_offset_bytes = (offset + array_offset) * byte_width;\n+        KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+        // Maximum tested offset is < 8 so validity is just bit offset\n+        ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+        ASSERT_EQ(0, kc_array.bit_offset(1));\n+        ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+        ASSERT_EQ(array->data()->buffers[1]->data() + buffer_offset_bytes,\n+                  kc_array.data(1));\n+        ASSERT_EQ(nullptr, kc_array.data(2));\n+        ASSERT_EQ(length, kc_array.length());\n+        // When creating from ArrayData always create read-only\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(2));\n+      }\n+    }\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayDataBinary) {\n+  for (const auto& type : kSampleBinaryTypes) {\n+    ARROW_SCOPED_TRACE(\"Type: \", type->ToString());\n+    for (auto array_offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+      for (auto offset : {0, 1}) {\n+        ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+        std::shared_ptr<Array> array = ArrayFromJSON(type, R\"([\"xyz\", \"abcabc\", null])\");\n+        int offsets_width =\n+            static_cast<int>(internal::checked_pointer_cast<BaseBinaryType>(type)\n+                                 ->layout()\n+                                 .buffers[1]\n+                                 .byte_width);\n+        array = array->Slice(array_offset);\n+        int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+        int buffer_offset_bytes = (offset + array_offset) * offsets_width;\n+        KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+        ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+        ASSERT_EQ(0, kc_array.bit_offset(1));\n+        ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+        ASSERT_EQ(array->data()->buffers[1]->data() + buffer_offset_bytes,\n+                  kc_array.data(1));\n+        ASSERT_EQ(array->data()->buffers[2]->data(), kc_array.data(2));\n+        ASSERT_EQ(length, kc_array.length());\n+        // When creating from ArrayData always create read-only\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(2));\n+      }\n+    }\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayDataBool) {\n+  for (auto array_offset : {0, 1}) {\n+    ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+    for (auto offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+      std::shared_ptr<Array> array = ArrayFromJSON(boolean(), \"[true, false, null]\");\n+      array = array->Slice(array_offset);\n+      int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+      KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+      ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+      ASSERT_EQ(offset + array_offset, kc_array.bit_offset(1));\n+      ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+      ASSERT_EQ(array->data()->buffers[1]->data(), kc_array.data(1));\n+      ASSERT_EQ(length, kc_array.length());\n+      ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+      ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+    }\n+  }\n+}\n+\n+// TEST(KeyColumnArray, FromArrayDataNull) {\n\nReview Comment:\n   Ah, my mistake.  This test fails by the way so I wasn't sure if that was something we wanted to address in this PR or file a follow-up JIRA for.  I think the hash-join PR doesn't operate on null columns so it doesn't need it but since we specific code branches to support null in a few spots it seemed like we would want to support it at some point.\n\n\n\n",
                    "created": "2022-04-14T20:08:58.653+0000",
                    "updated": "2022-04-14T20:08:58.653+0000",
                    "started": "2022-04-14T20:08:58.652+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757195",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757217",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r850787225\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n\nReview Comment:\n   Can we add constants like:\r\n   \r\n   ```\r\n   KeyColumnArray::kValidityBuffer\r\n   KeyColumnArray::kFixedSizeBuffer\r\n   KeyColumnArray::kVariableLengthBuffer\r\n   ```\r\n   \r\n   It might help for readability.  Or we could split this into three accessors.\n\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return buffers_[i];\n+  }\n+  /// \\brief Return a mutable version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  uint32_t* mutable_offsets() {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<uint32_t*>(mutable_data(1));\n+  }\n+  /// \\brief Return a read-only version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  const uint32_t* offsets() const {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<const uint32_t*>(data(1));\n+  }\n+  /// \\brief Return the type metadata\n+  const KeyColumnMetadata& metadata() const { return metadata_; }\n+  /// \\brief Return the length (in rows) of the array\n+  int64_t length() const { return length_; }\n+  /// \\brief Return the bit offset into the corresponding vector\n+  ///\n+  /// if i == 1 then this must be a bool array\n+  int bit_offset(int i) const {\n+    ARROW_DCHECK(i >= 0 && i < max_buffers_);\n+    return bit_offset_[i];\n+  }\n+\n+ private:\n+  static constexpr int max_buffers_ = 3;\n+  const uint8_t* buffers_[max_buffers_];\n+  uint8_t* mutable_buffers_[max_buffers_];\n+  KeyColumnMetadata metadata_;\n+  int64_t length_;\n+  // Starting bit offset within the first byte (between 0 and 7)\n+  // to be used when accessing buffers that store bit vectors.\n+  int bit_offset_[max_buffers_ - 1];\n+};\n+\n+/// \\brief Create KeyColumnMetadata from a DataType\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnMetadata for\n+/// the indices type\n+///\n+/// The caller should ensure this is only called on \"key\" columns.  Calling this with\n+/// a non-key column will return a meaningless value (or abort on a debug build)\n+KeyColumnMetadata ColumnMetadataFromDataType(const std::shared_ptr<DataType>& type);\n+\n+/// \\brief Create KeyColumnArray from ArrayData\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnArray for\n+/// the indices array\n+///\n+/// The caller should ensure this is only called on \"key\" columns.\n+/// \\see ColumnMetadataFromDataType for details\n+KeyColumnArray ColumnArrayFromArrayData(const std::shared_ptr<ArrayData>& array_data,\n+                                        int start_row, int num_rows);\n+\n+/// \\brief Create KeyColumnMetadata instances from an ExecBatch\n+///\n+/// column_metadatas will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnMetadataFromDataType for more details\n+void ColumnMetadatasFromExecBatch(const ExecBatch& batch,\n+                                  std::vector<KeyColumnMetadata>* column_metadatas);\n+\n+/// \\brief Create KeyColumnArray instances from a slice of an ExecBatch\n+///\n+/// column_arrays will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnArrayFromArrayData for more details\n+void ColumnArraysFromExecBatch(const ExecBatch& batch, int start_row, int num_rows,\n+                               std::vector<KeyColumnArray>* column_arrays);\n+\n+/// \\brief Create KeyColumnArray instances from an ExecBatch\n+///\n+/// column_arrays will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnArrayFromArrayData for more details\n+void ColumnArraysFromExecBatch(const ExecBatch& batch,\n+                               std::vector<KeyColumnArray>* column_arrays);\n+\n+/// A lightweight resizable array for \"key\" columns\n+///\n+/// Unlike KeyColumnArray this instance owns its buffers\n+///\n+/// Resizing is handled by arrow::ResizableBuffer and a doubling approach is\n+/// used so that resizes will always grow up to the next power of 2\n+class ResizableArrayData {\n+ public:\n+  /// \\brief Create an uninitialized instance\n+  ///\n+  /// Init must be called before calling any other operations\n+  ResizableArrayData()\n+      : log_num_rows_min_(0),\n+        pool_(NULLPTR),\n+        num_rows_(0),\n+        num_rows_allocated_(0),\n+        var_len_buf_size_(0) {}\n+  ~ResizableArrayData() { Clear(true); }\n+  /// \\brief Initialize the array\n+  /// \\param data_type The data type this array is holding data for.\n+  /// \\param pool The pool to make allocations on\n+  /// \\param log_num_rows_min All resize operations will allocate at least enough\n+  ///                         space for (1 << log_num_rows_min) rows\n+  void Init(const std::shared_ptr<DataType>& data_type, MemoryPool* pool,\n+            int log_num_rows_min);\n+  /// \\brief Resets the array back to an empty state\n+  /// \\param release_buffers If true then allocated memory is released and the\n+  ///                        next resize operation will have to reallocate memory\n+  void Clear(bool release_buffers);\n+  /// \\brief Resize the fixed length buffers\n+  ///\n+  /// The buffers will be resized to hold at least `num_rows_new` rows of data\n+  Status ResizeFixedLengthBuffers(int num_rows_new);\n+  /// \\brief Resize the varying length buffer if this array is a variable binary type\n+  ///\n+  /// This must be called after offsets have been populated and the buffer will be\n+  /// resized to hold at least as much data as the offsets require\n+  ///\n+  /// Does nothing if the array is not a variable binary type\n+  Status ResizeVaryingLengthBuffer();\n+  /// \\brief The current length (in rows) of the array\n+  int num_rows() const { return num_rows_; }\n+  /// \\brief A non-owning view into this array\n+  KeyColumnArray column_array() const;\n+  /// \\brief A lightweight descriptor of the data held by this array\n+  KeyColumnMetadata column_metadata() const {\n+    return ColumnMetadataFromDataType(data_type_);\n+  }\n+  /// \\brief Convert the data to an arrow::ArrayData\n+  ///\n+  /// This is a zero copy operation and the created ArrayData will reference the\n+  /// buffers held by this instance.\n+  std::shared_ptr<ArrayData> array_data() const;\n+  /// \\brief A raw pointer to the requested buffer\n+  ///\n+  /// If i is 0 then this returns the validity buffer\n+  /// If i is 1 then this returns the buffer used for values (if this is a fixed\n+  ///           length data type) or offsets (if this is a variable binary type)\n+  /// If i is 2 then this returns the buffer used for variable length binary data\n+  uint8_t* mutable_data(int i) {\n+    return i == 0   ? non_null_buf_->mutable_data()\n+           : i == 1 ? fixed_len_buf_->mutable_data()\n+                    : var_len_buf_->mutable_data();\n+  }\n+\n+ private:\n+  static constexpr int64_t kNumPaddingBytes = 64;\n+  int log_num_rows_min_;\n+  std::shared_ptr<DataType> data_type_;\n+  MemoryPool* pool_;\n+  int num_rows_;\n+  int num_rows_allocated_;\n+  int var_len_buf_size_;\n+  std::shared_ptr<ResizableBuffer> non_null_buf_;\n+  std::shared_ptr<ResizableBuffer> fixed_len_buf_;\n+  std::shared_ptr<ResizableBuffer> var_len_buf_;\n+};\n+\n+/// \\brief A builder to concatenate batches of data into a larger batch\n+///\n+/// Will only store num_rows_max() rows\n+class ExecBatchBuilder {\n+ public:\n+  /// \\brief Add rows from `source` into `target` column\n+  ///\n+  /// If `target` is uninitialized or cleared it will be initialized to use\n+  /// the given pool.\n+  static Status AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                               ResizableArrayData* target, int num_rows_to_append,\n+                               const uint16_t* row_ids, MemoryPool* pool);\n+\n+  /// \\brief Add nulls into `target` column\n+  ///\n+  /// If `target` is uninitialized or cleared it will be initialized to use\n+  /// the given pool.\n+  static Status AppendNulls(const std::shared_ptr<DataType>& type,\n+                            ResizableArrayData& target, int num_rows_to_append,\n+                            MemoryPool* pool);\n+\n+  /// \\brief Add selected rows from `batch`\n+  ///\n+  /// If `col_ids` is null then `num_cols` should less than batch.num_values() and\n+  /// the first `num_cols` columns of batch will be appended.\n+  ///\n+  /// All columns in `batch` must have array shape\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int num_cols,\n+                        const int* col_ids = NULLPTR);\n\nReview Comment:\n   It's a little confusing that we have overloads that store `num_appended` and overloads that do not store `num_appended`.  The former are also able to exceed `num_rows_max()` I think.  What is the intended use case for the overloads that don't store `num_appended`?\n\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return buffers_[i];\n+  }\n+  /// \\brief Return a mutable version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  uint32_t* mutable_offsets() {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<uint32_t*>(mutable_data(1));\n+  }\n+  /// \\brief Return a read-only version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  const uint32_t* offsets() const {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<const uint32_t*>(data(1));\n+  }\n+  /// \\brief Return the type metadata\n+  const KeyColumnMetadata& metadata() const { return metadata_; }\n+  /// \\brief Return the length (in rows) of the array\n+  int64_t length() const { return length_; }\n+  /// \\brief Return the bit offset into the corresponding vector\n+  ///\n+  /// if i == 1 then this must be a bool array\n+  int bit_offset(int i) const {\n+    ARROW_DCHECK(i >= 0 && i < max_buffers_);\n+    return bit_offset_[i];\n+  }\n+\n+ private:\n+  static constexpr int max_buffers_ = 3;\n+  const uint8_t* buffers_[max_buffers_];\n+  uint8_t* mutable_buffers_[max_buffers_];\n+  KeyColumnMetadata metadata_;\n+  int64_t length_;\n+  // Starting bit offset within the first byte (between 0 and 7)\n+  // to be used when accessing buffers that store bit vectors.\n+  int bit_offset_[max_buffers_ - 1];\n+};\n+\n+/// \\brief Create KeyColumnMetadata from a DataType\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnMetadata for\n+/// the indices type\n+///\n+/// The caller should ensure this is only called on \"key\" columns.  Calling this with\n+/// a non-key column will return a meaningless value (or abort on a debug build)\n+KeyColumnMetadata ColumnMetadataFromDataType(const std::shared_ptr<DataType>& type);\n+\n+/// \\brief Create KeyColumnArray from ArrayData\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnArray for\n+/// the indices array\n+///\n+/// The caller should ensure this is only called on \"key\" columns.\n+/// \\see ColumnMetadataFromDataType for details\n+KeyColumnArray ColumnArrayFromArrayData(const std::shared_ptr<ArrayData>& array_data,\n+                                        int start_row, int num_rows);\n+\n+/// \\brief Create KeyColumnMetadata instances from an ExecBatch\n+///\n+/// column_metadatas will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnMetadataFromDataType for more details\n+void ColumnMetadatasFromExecBatch(const ExecBatch& batch,\n+                                  std::vector<KeyColumnMetadata>* column_metadatas);\n+\n+/// \\brief Create KeyColumnArray instances from a slice of an ExecBatch\n+///\n+/// column_arrays will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnArrayFromArrayData for more details\n+void ColumnArraysFromExecBatch(const ExecBatch& batch, int start_row, int num_rows,\n+                               std::vector<KeyColumnArray>* column_arrays);\n+\n+/// \\brief Create KeyColumnArray instances from an ExecBatch\n+///\n+/// column_arrays will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnArrayFromArrayData for more details\n+void ColumnArraysFromExecBatch(const ExecBatch& batch,\n+                               std::vector<KeyColumnArray>* column_arrays);\n+\n+/// A lightweight resizable array for \"key\" columns\n+///\n+/// Unlike KeyColumnArray this instance owns its buffers\n+///\n+/// Resizing is handled by arrow::ResizableBuffer and a doubling approach is\n+/// used so that resizes will always grow up to the next power of 2\n+class ResizableArrayData {\n+ public:\n+  /// \\brief Create an uninitialized instance\n+  ///\n+  /// Init must be called before calling any other operations\n+  ResizableArrayData()\n+      : log_num_rows_min_(0),\n+        pool_(NULLPTR),\n+        num_rows_(0),\n+        num_rows_allocated_(0),\n+        var_len_buf_size_(0) {}\n+  ~ResizableArrayData() { Clear(true); }\n+  /// \\brief Initialize the array\n+  /// \\param data_type The data type this array is holding data for.\n+  /// \\param pool The pool to make allocations on\n+  /// \\param log_num_rows_min All resize operations will allocate at least enough\n+  ///                         space for (1 << log_num_rows_min) rows\n+  void Init(const std::shared_ptr<DataType>& data_type, MemoryPool* pool,\n+            int log_num_rows_min);\n+  /// \\brief Resets the array back to an empty state\n+  /// \\param release_buffers If true then allocated memory is released and the\n+  ///                        next resize operation will have to reallocate memory\n+  void Clear(bool release_buffers);\n+  /// \\brief Resize the fixed length buffers\n+  ///\n+  /// The buffers will be resized to hold at least `num_rows_new` rows of data\n+  Status ResizeFixedLengthBuffers(int num_rows_new);\n+  /// \\brief Resize the varying length buffer if this array is a variable binary type\n+  ///\n+  /// This must be called after offsets have been populated and the buffer will be\n+  /// resized to hold at least as much data as the offsets require\n+  ///\n+  /// Does nothing if the array is not a variable binary type\n+  Status ResizeVaryingLengthBuffer();\n+  /// \\brief The current length (in rows) of the array\n+  int num_rows() const { return num_rows_; }\n+  /// \\brief A non-owning view into this array\n+  KeyColumnArray column_array() const;\n+  /// \\brief A lightweight descriptor of the data held by this array\n+  KeyColumnMetadata column_metadata() const {\n+    return ColumnMetadataFromDataType(data_type_);\n+  }\n+  /// \\brief Convert the data to an arrow::ArrayData\n+  ///\n+  /// This is a zero copy operation and the created ArrayData will reference the\n+  /// buffers held by this instance.\n+  std::shared_ptr<ArrayData> array_data() const;\n+  /// \\brief A raw pointer to the requested buffer\n+  ///\n+  /// If i is 0 then this returns the validity buffer\n+  /// If i is 1 then this returns the buffer used for values (if this is a fixed\n+  ///           length data type) or offsets (if this is a variable binary type)\n+  /// If i is 2 then this returns the buffer used for variable length binary data\n+  uint8_t* mutable_data(int i) {\n+    return i == 0   ? non_null_buf_->mutable_data()\n+           : i == 1 ? fixed_len_buf_->mutable_data()\n+                    : var_len_buf_->mutable_data();\n+  }\n+\n+ private:\n+  static constexpr int64_t kNumPaddingBytes = 64;\n+  int log_num_rows_min_;\n+  std::shared_ptr<DataType> data_type_;\n+  MemoryPool* pool_;\n+  int num_rows_;\n+  int num_rows_allocated_;\n+  int var_len_buf_size_;\n+  std::shared_ptr<ResizableBuffer> non_null_buf_;\n+  std::shared_ptr<ResizableBuffer> fixed_len_buf_;\n+  std::shared_ptr<ResizableBuffer> var_len_buf_;\n\nReview Comment:\n   Here we have three explicitly named buffers and for KeyColumnArray we have a fixed-size array of buffers.  We should probably align on a single approach.  Do you have a preference between the two or a reason for them to be different?\n\n\n\n##########\ncpp/src/arrow/compute/light_array.cc:\n##########\n@@ -0,0 +1,736 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <type_traits>\n+\n+#include \"arrow/util/bitmap_ops.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+KeyColumnArray::KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                               const uint8_t* buffer0, const uint8_t* buffer1,\n+                               const uint8_t* buffer2, int bit_offset0, int bit_offset1) {\n+  static_assert(std::is_pod<KeyColumnArray>::value,\n+                \"This class was intended to be a POD type\");\n+  metadata_ = metadata;\n+  length_ = length;\n+  buffers_[0] = buffer0;\n+  buffers_[1] = buffer1;\n+  buffers_[2] = buffer2;\n+  mutable_buffers_[0] = mutable_buffers_[1] = mutable_buffers_[2] = nullptr;\n+  bit_offset_[0] = bit_offset0;\n+  bit_offset_[1] = bit_offset1;\n+}\n+\n+KeyColumnArray::KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                               uint8_t* buffer0, uint8_t* buffer1, uint8_t* buffer2,\n+                               int bit_offset0, int bit_offset1) {\n+  metadata_ = metadata;\n+  length_ = length;\n+  buffers_[0] = mutable_buffers_[0] = buffer0;\n+  buffers_[1] = mutable_buffers_[1] = buffer1;\n+  buffers_[2] = mutable_buffers_[2] = buffer2;\n+  bit_offset_[0] = bit_offset0;\n+  bit_offset_[1] = bit_offset1;\n+}\n+\n+KeyColumnArray KeyColumnArray::WithBufferFrom(const KeyColumnArray& other,\n+                                              int buffer_id_to_replace) const {\n+  KeyColumnArray copy = *this;\n+  copy.mutable_buffers_[buffer_id_to_replace] =\n+      other.mutable_buffers_[buffer_id_to_replace];\n+  copy.buffers_[buffer_id_to_replace] = other.buffers_[buffer_id_to_replace];\n+  if (buffer_id_to_replace < max_buffers_ - 1) {\n+    copy.bit_offset_[buffer_id_to_replace] = other.bit_offset_[buffer_id_to_replace];\n+  }\n+  return copy;\n+}\n+\n+KeyColumnArray KeyColumnArray::WithMetadata(const KeyColumnMetadata& metadata) const {\n+  KeyColumnArray copy = *this;\n+  copy.metadata_ = metadata;\n+  return copy;\n+}\n+\n+KeyColumnArray KeyColumnArray::Slice(int64_t offset, int64_t length) const {\n+  KeyColumnArray sliced;\n+  sliced.metadata_ = metadata_;\n+  sliced.length_ = length;\n+  uint32_t fixed_size =\n+      !metadata_.is_fixed_length ? sizeof(uint32_t) : metadata_.fixed_length;\n+\n+  sliced.buffers_[0] =\n+      buffers_[0] ? buffers_[0] + (bit_offset_[0] + offset) / 8 : nullptr;\n+  sliced.mutable_buffers_[0] =\n+      mutable_buffers_[0] ? mutable_buffers_[0] + (bit_offset_[0] + offset) / 8 : nullptr;\n+  sliced.bit_offset_[0] = (bit_offset_[0] + offset) % 8;\n+\n+  if (fixed_size == 0 && !metadata_.is_null_type) {\n+    sliced.buffers_[1] =\n+        buffers_[1] ? buffers_[1] + (bit_offset_[1] + offset) / 8 : nullptr;\n+    sliced.mutable_buffers_[1] = mutable_buffers_[1]\n+                                     ? mutable_buffers_[1] + (bit_offset_[1] + offset) / 8\n+                                     : nullptr;\n+    sliced.bit_offset_[1] = (bit_offset_[1] + offset) % 8;\n+  } else {\n+    sliced.buffers_[1] = buffers_[1] ? buffers_[1] + offset * fixed_size : nullptr;\n+    sliced.mutable_buffers_[1] =\n+        mutable_buffers_[1] ? mutable_buffers_[1] + offset * fixed_size : nullptr;\n+    sliced.bit_offset_[1] = 0;\n+  }\n+\n+  sliced.buffers_[2] = buffers_[2];\n+  sliced.mutable_buffers_[2] = mutable_buffers_[2];\n+  return sliced;\n+}\n+\n+KeyColumnMetadata ColumnMetadataFromDataType(const std::shared_ptr<DataType>& type) {\n+  if (type->id() == Type::DICTIONARY) {\n+    auto bit_width =\n+        arrow::internal::checked_cast<const FixedWidthType&>(*type).bit_width();\n+    ARROW_DCHECK(bit_width % 8 == 0);\n+    return KeyColumnMetadata(true, bit_width / 8);\n+  }\n+  if (type->id() == Type::BOOL) {\n+    return KeyColumnMetadata(true, 0);\n+  }\n+  if (is_fixed_width(type->id())) {\n+    return KeyColumnMetadata(\n+        true,\n+        arrow::internal::checked_cast<const FixedWidthType&>(*type).bit_width() / 8);\n+  }\n+  if (is_binary_like(type->id())) {\n+    return KeyColumnMetadata(false, sizeof(uint32_t));\n+  }\n+  if (is_large_binary_like(type->id())) {\n+    return KeyColumnMetadata(false, sizeof(uint64_t));\n+  }\n+  if (type->id() == Type::NA) {\n+    return KeyColumnMetadata(true, 0, true);\n+  }\n+  // Should not reach this point, caller attempted to create a KeyColumnArray from an\n+  // invalid type\n+  ARROW_DCHECK(false);\n+  return KeyColumnMetadata(true, sizeof(int));\n+}\n+\n+KeyColumnArray ColumnArrayFromArrayData(const std::shared_ptr<ArrayData>& array_data,\n+                                        int start_row, int num_rows) {\n+  KeyColumnArray column_array = KeyColumnArray(\n+      ColumnMetadataFromDataType(array_data->type),\n+      array_data->offset + start_row + num_rows,\n+      array_data->buffers[0] != NULLPTR ? array_data->buffers[0]->data() : nullptr,\n+      array_data->buffers[1]->data(),\n+      (array_data->buffers.size() > 2 && array_data->buffers[2] != NULLPTR)\n+          ? array_data->buffers[2]->data()\n+          : nullptr);\n+  return column_array.Slice(array_data->offset + start_row, num_rows);\n+}\n+\n+void ColumnMetadatasFromExecBatch(const ExecBatch& batch,\n+                                  std::vector<KeyColumnMetadata>* column_metadatas) {\n+  int num_columns = static_cast<int>(batch.values.size());\n+  column_metadatas->resize(num_columns);\n+  for (int i = 0; i < num_columns; ++i) {\n+    const Datum& data = batch.values[i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    (*column_metadatas)[i] = ColumnMetadataFromDataType(array_data->type);\n+  }\n+}\n+\n+void ColumnArraysFromExecBatch(const ExecBatch& batch, int start_row, int num_rows,\n+                               std::vector<KeyColumnArray>* column_arrays) {\n+  int num_columns = static_cast<int>(batch.values.size());\n+  column_arrays->resize(num_columns);\n+  for (int i = 0; i < num_columns; ++i) {\n+    const Datum& data = batch.values[i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    (*column_arrays)[i] = ColumnArrayFromArrayData(array_data, start_row, num_rows);\n+  }\n+}\n+\n+void ColumnArraysFromExecBatch(const ExecBatch& batch,\n+                               std::vector<KeyColumnArray>* column_arrays) {\n+  ColumnArraysFromExecBatch(batch, 0, static_cast<int>(batch.length), column_arrays);\n+}\n+\n+void ResizableArrayData::Init(const std::shared_ptr<DataType>& data_type,\n+                              MemoryPool* pool, int log_num_rows_min) {\n+#ifndef NDEBUG\n+  if (num_rows_allocated_ > 0) {\n+    ARROW_DCHECK(data_type_ != NULLPTR);\n+    KeyColumnMetadata metadata_before = ColumnMetadataFromDataType(data_type_);\n+    KeyColumnMetadata metadata_after = ColumnMetadataFromDataType(data_type);\n+    ARROW_DCHECK(metadata_before.is_fixed_length == metadata_after.is_fixed_length &&\n+                 metadata_before.fixed_length == metadata_after.fixed_length);\n+  }\n+#endif\n+  Clear(/*release_buffers=*/false);\n+  log_num_rows_min_ = log_num_rows_min;\n+  data_type_ = data_type;\n+  pool_ = pool;\n+}\n+\n+void ResizableArrayData::Clear(bool release_buffers) {\n+  num_rows_ = 0;\n+  if (release_buffers) {\n+    non_null_buf_.reset();\n+    fixed_len_buf_.reset();\n+    var_len_buf_.reset();\n+    num_rows_allocated_ = 0;\n+    var_len_buf_size_ = 0;\n+  }\n+}\n+\n+Status ResizableArrayData::ResizeFixedLengthBuffers(int num_rows_new) {\n+  ARROW_DCHECK(num_rows_new >= 0);\n+  if (num_rows_new <= num_rows_allocated_) {\n+    num_rows_ = num_rows_new;\n+    return Status::OK();\n+  }\n+\n+  int num_rows_allocated_new = 1 << log_num_rows_min_;\n+  while (num_rows_allocated_new < num_rows_new) {\n+    num_rows_allocated_new *= 2;\n+  }\n+\n+  KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (fixed_len_buf_ == NULLPTR) {\n+    ARROW_DCHECK(non_null_buf_ == NULLPTR && var_len_buf_ == NULLPTR);\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        non_null_buf_,\n+        AllocateResizableBuffer(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes, pool_));\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes,\n+                pool_));\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes,\n+                pool_));\n+      }\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(\n+          fixed_len_buf_,\n+          AllocateResizableBuffer(\n+              (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes, pool_));\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(var_len_buf_, AllocateResizableBuffer(\n+                                            sizeof(uint64_t) + kNumPaddingBytes, pool_));\n+\n+    var_len_buf_size_ = sizeof(uint64_t);\n+  } else {\n+    ARROW_DCHECK(non_null_buf_ != NULLPTR && var_len_buf_ != NULLPTR);\n+\n+    RETURN_NOT_OK(non_null_buf_->Resize(bit_util::BytesForBits(num_rows_allocated_new) +\n+                                        kNumPaddingBytes));\n+\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes));\n+      } else {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes));\n+      }\n+    } else {\n+      RETURN_NOT_OK(fixed_len_buf_->Resize(\n+          (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes));\n+    }\n+  }\n+\n+  num_rows_allocated_ = num_rows_allocated_new;\n+  num_rows_ = num_rows_new;\n+\n+  return Status::OK();\n+}\n+\n+Status ResizableArrayData::ResizeVaryingLengthBuffer() {\n+  KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (!column_metadata.is_fixed_length) {\n+    int min_new_size = static_cast<int>(\n+        reinterpret_cast<const uint32_t*>(fixed_len_buf_->data())[num_rows_]);\n+    ARROW_DCHECK(var_len_buf_size_ > 0);\n+    if (var_len_buf_size_ < min_new_size) {\n+      int new_size = var_len_buf_size_;\n+      while (new_size < min_new_size) {\n+        new_size *= 2;\n+      }\n+      RETURN_NOT_OK(var_len_buf_->Resize(new_size + kNumPaddingBytes));\n+      var_len_buf_size_ = new_size;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+KeyColumnArray ResizableArrayData::column_array() const {\n+  KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+  return KeyColumnArray(column_metadata, num_rows_, non_null_buf_->mutable_data(),\n+                        fixed_len_buf_->mutable_data(), var_len_buf_->mutable_data());\n+}\n+\n+std::shared_ptr<ArrayData> ResizableArrayData::array_data() const {\n+  KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  auto valid_count = arrow::internal::CountSetBits(non_null_buf_->data(), /*offset=*/0,\n+                                                   static_cast<int64_t>(num_rows_));\n+  int null_count = static_cast<int>(num_rows_) - static_cast<int>(valid_count);\n+\n+  if (column_metadata.is_fixed_length) {\n+    return ArrayData::Make(data_type_, num_rows_, {non_null_buf_, fixed_len_buf_},\n+                           null_count);\n+  } else {\n+    return ArrayData::Make(data_type_, num_rows_,\n+                           {non_null_buf_, fixed_len_buf_, var_len_buf_}, null_count);\n+  }\n+}\n+\n+int ExecBatchBuilder::NumRowsToSkip(const std::shared_ptr<ArrayData>& column,\n+                                    int num_rows, const uint16_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+#ifndef NDEBUG\n+  // Ids must be in non-decreasing order\n+  //\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(row_ids[i] >= row_ids[i - 1]);\n+  }\n+#endif\n+\n+  KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(column->type);\n+\n+  int num_rows_left = num_rows;\n+  int num_bytes_skipped = 0;\n+  while (num_rows_left > 0 && num_bytes_skipped < num_tail_bytes_to_skip) {\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        num_rows_left = std::max(num_rows_left, 8) - 8;\n+        ++num_bytes_skipped;\n+      } else {\n+        --num_rows_left;\n+        num_bytes_skipped += column_metadata.fixed_length;\n+      }\n+    } else {\n+      --num_rows_left;\n+      int row_id_removed = row_ids[num_rows_left];\n+      const uint32_t* offsets =\n+          reinterpret_cast<const uint32_t*>(column->buffers[1]->data());\n+      num_bytes_skipped += offsets[row_id_removed + 1] - offsets[row_id_removed];\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <bool OUTPUT_BYTE_ALIGNED>\n+void ExecBatchBuilder::CollectBitsImp(const uint8_t* input_bits,\n+                                      int64_t input_bits_offset, uint8_t* output_bits,\n+                                      int64_t output_bits_offset, int num_rows,\n+                                      const uint16_t* row_ids) {\n+  if (!OUTPUT_BYTE_ALIGNED) {\n+    ARROW_DCHECK(output_bits_offset % 8 > 0);\n+    output_bits[output_bits_offset / 8] &=\n+        static_cast<uint8_t>((1 << (output_bits_offset % 8)) - 1);\n+  } else {\n+    ARROW_DCHECK(output_bits_offset % 8 == 0);\n+  }\n+  constexpr int unroll = 8;\n+  for (int i = 0; i < num_rows / unroll; ++i) {\n+    const uint16_t* row_ids_base = row_ids + unroll * i;\n+    uint8_t result;\n+    result = bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[0]) ? 1 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[1]) ? 2 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[2]) ? 4 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[3]) ? 8 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[4]) ? 16 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[5]) ? 32 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[6]) ? 64 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[7]) ? 128 : 0;\n+    if (OUTPUT_BYTE_ALIGNED) {\n+      output_bits[output_bits_offset / 8 + i] = result;\n+    } else {\n+      output_bits[output_bits_offset / 8 + i] |=\n+          static_cast<uint8_t>(result << (output_bits_offset % 8));\n+      output_bits[output_bits_offset / 8 + i + 1] =\n+          static_cast<uint8_t>(result >> (8 - (output_bits_offset % 8)));\n+    }\n+  }\n+  if (num_rows % unroll > 0) {\n+    for (int i = num_rows - (num_rows % unroll); i < num_rows; ++i) {\n+      bit_util::SetBitTo(output_bits, output_bits_offset + i,\n+                         bit_util::GetBit(input_bits, input_bits_offset + row_ids[i]));\n+    }\n+  }\n+}\n+\n+void ExecBatchBuilder::CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                                   uint8_t* output_bits, int64_t output_bits_offset,\n+                                   int num_rows, const uint16_t* row_ids) {\n+  if (output_bits_offset % 8 > 0) {\n+    CollectBitsImp<false>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                          num_rows, row_ids);\n+  } else {\n+    CollectBitsImp<true>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                         num_rows, row_ids);\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void ExecBatchBuilder::Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                             const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  KeyColumnMetadata metadata = ColumnMetadataFromDataType(column->type);\n+\n+  if (!metadata.is_fixed_length) {\n+    const uint8_t* ptr_base = column->buffers[2]->data();\n+    const uint32_t* offsets =\n+        reinterpret_cast<const uint32_t*>(column->buffers[1]->data()) + column->offset;\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr = ptr_base + offsets[row_id];\n+      uint32_t field_length = offsets[row_id + 1] - offsets[row_id];\n+      process_value_fn(i, field_ptr, field_length);\n+    }\n+  } else {\n+    ARROW_DCHECK(metadata.fixed_length > 0);\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr =\n+          column->buffers[1]->data() +\n+          (column->offset + row_id) * static_cast<int64_t>(metadata.fixed_length);\n+      process_value_fn(i, field_ptr, metadata.fixed_length);\n+    }\n+  }\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                                        ResizableArrayData* target,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        MemoryPool* pool) {\n+  int num_rows_before = target->num_rows();\n+  ARROW_DCHECK(num_rows_before >= 0);\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target->num_rows() == 0) {\n+    target->Init(source->type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target->ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(source->type);\n+\n+  if (column_metadata.is_fixed_length) {\n+    // Fixed length column\n+    //\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        CollectBits(source->buffers[1]->data(), source->offset, target->mutable_data(1),\n+                    num_rows_before, num_rows_to_append, row_ids);\n+        break;\n+      case 1:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                target->mutable_data(1)[num_rows_before + i] = *ptr;\n+              });\n+        break;\n+      case 2:\n+        Visit(\n+            source, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(target->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        Visit(\n+            source, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(target->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        Visit(\n+            source, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(target->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default: {\n+        int num_rows_to_process =\n+            num_rows_to_append -\n+            NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+        Visit(source, num_rows_to_process, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                    target->mutable_data(1) +\n+                    static_cast<int64_t>(num_bytes) * (num_rows_before + i));\n+                const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                for (uint32_t word_id = 0;\n+                     word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t));\n+                     ++word_id) {\n+                  util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+                }\n+              });\n+        if (num_rows_to_append > num_rows_to_process) {\n+          Visit(source, num_rows_to_append - num_rows_to_process,\n+                row_ids + num_rows_to_process,\n+                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                  uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                      target->mutable_data(1) +\n+                      static_cast<int64_t>(num_bytes) *\n+                          (num_rows_before + num_rows_to_process + i));\n+                  const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                  memcpy(dst, src, num_bytes);\n+                });\n+        }\n+      }\n+    }\n+  } else {\n+    // Varying length column\n+    //\n+\n+    // Step 1: calculate target offsets\n+    //\n+    uint32_t* offsets = reinterpret_cast<uint32_t*>(target->mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[num_rows_before];\n+    Visit(source, num_rows_to_append, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            offsets[num_rows_before + i] = num_bytes;\n+          });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[num_rows_before + i];\n+      offsets[num_rows_before + i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_before + num_rows_to_append] = sum;\n+\n+    // Step 2: resize output buffers\n+    //\n+    RETURN_NOT_OK(target->ResizeVaryingLengthBuffer());\n+\n+    // Step 3: copy varying-length data\n+    //\n+    int num_rows_to_process =\n+        num_rows_to_append -\n+        NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+    Visit(source, num_rows_to_process, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(target->mutable_data(2) +\n+                                                        offsets[num_rows_before + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            for (uint32_t word_id = 0;\n+                 word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+              util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+            }\n+          });\n+    Visit(source, num_rows_to_append - num_rows_to_process, row_ids + num_rows_to_process,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                target->mutable_data(2) +\n+                offsets[num_rows_before + num_rows_to_process + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            memcpy(dst, src, num_bytes);\n+          });\n+  }\n+\n+  // Process nulls\n+  //\n+  if (source->buffers[0] == NULLPTR) {\n+    uint8_t* dst = target->mutable_data(0);\n+    dst[num_rows_before / 8] |= static_cast<uint8_t>(~0ULL << (num_rows_before & 7));\n+    for (int i = num_rows_before / 8 + 1;\n+         i < bit_util::BytesForBits(num_rows_before + num_rows_to_append); ++i) {\n+      dst[i] = 0xff;\n+    }\n+  } else {\n+    CollectBits(source->buffers[0]->data(), source->offset, target->mutable_data(0),\n+                num_rows_before, num_rows_to_append, row_ids);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(const std::shared_ptr<DataType>& type,\n+                                     ResizableArrayData& target, int num_rows_to_append,\n+                                     MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(type);\n+\n+  // Process fixed length buffer\n+  //\n+  if (column_metadata.is_fixed_length) {\n+    uint8_t* dst = target.mutable_data(1);\n+    if (column_metadata.fixed_length == 0) {\n+      dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+      int64_t offset_begin = num_rows_before / 8 + 1;\n+      int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+      if (offset_end > offset_begin) {\n+        memset(dst + offset_begin, 0, offset_end - offset_begin);\n+      }\n+    } else {\n+      memset(dst + num_rows_before * static_cast<int64_t>(column_metadata.fixed_length),\n+             0, static_cast<int64_t>(column_metadata.fixed_length) * num_rows_to_append);\n+    }\n+  } else {\n+    uint32_t* dst = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : dst[num_rows_before];\n+    for (int64_t i = num_rows_before; i <= num_rows_after; ++i) {\n+      dst[i] = sum;\n+    }\n+  }\n+\n+  // Process nulls\n+  //\n+  uint8_t* dst = target.mutable_data(0);\n+  dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+  int64_t offset_begin = num_rows_before / 8 + 1;\n+  int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+  if (offset_end > offset_begin) {\n+    memset(dst + offset_begin, 0, offset_end - offset_begin);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int num_cols, const int* col_ids) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(num_cols);\n+    for (int i = 0; i < num_cols; ++i) {\n+      const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+      ARROW_DCHECK(data.is_array());\n+      const std::shared_ptr<ArrayData>& array_data = data.array();\n+      values_[i].Init(array_data->type, pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    RETURN_NOT_OK(\n+        AppendSelected(array_data, &values_[i], num_rows_to_append, row_ids, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int* num_appended, int num_cols,\n+                                        const int* col_ids) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendSelected(pool, batch, num_rows_next, row_ids, num_cols, col_ids));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(types.size());\n+    for (size_t i = 0; i < types.size(); ++i) {\n+      values_[i].Init(types[i], pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    RETURN_NOT_OK(AppendNulls(types[i], values_[i], num_rows_to_append, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append, int* num_appended) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendNulls(pool, types, num_rows_next));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+ExecBatch ExecBatchBuilder::Flush() {\n+  ARROW_DCHECK(num_rows() > 0);\n\nReview Comment:\n   This feels like it might be a difficult assertion for the caller to maintain.  What is the downside of returning an empty `ExecBatch` in this situation?\n\n\n\n",
                    "created": "2022-04-14T20:43:26.991+0000",
                    "updated": "2022-04-14T20:43:26.991+0000",
                    "started": "2022-04-14T20:43:26.991+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757217",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757218",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#issuecomment-1099604844\n\n   @pitrou @lidavidm I'd appreciate your eyes on this if you have time.  I think these utilities could be generally useful outside of hash-join and it might be nice to have some more eyes on them.\n\n\n",
                    "created": "2022-04-14T20:43:44.481+0000",
                    "updated": "2022-04-14T20:43:44.481+0000",
                    "started": "2022-04-14T20:43:44.481+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757218",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757233",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r850827924\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n\nReview Comment:\n   How do these compare to a theoretical ExecBatch refactor?\n\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n\nReview Comment:\n   Given there's already `offsets` maybe it would be better to just have 3/6 accessors and not restrict the accesors based on is_fixed_length\n\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n\nReview Comment:\n   I suppose these are far too limited in terms of data type support\n\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return buffers_[i];\n+  }\n+  /// \\brief Return a mutable version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  uint32_t* mutable_offsets() {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<uint32_t*>(mutable_data(1));\n+  }\n+  /// \\brief Return a read-only version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  const uint32_t* offsets() const {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<const uint32_t*>(data(1));\n+  }\n+  /// \\brief Return the type metadata\n+  const KeyColumnMetadata& metadata() const { return metadata_; }\n+  /// \\brief Return the length (in rows) of the array\n+  int64_t length() const { return length_; }\n+  /// \\brief Return the bit offset into the corresponding vector\n+  ///\n+  /// if i == 1 then this must be a bool array\n+  int bit_offset(int i) const {\n+    ARROW_DCHECK(i >= 0 && i < max_buffers_);\n+    return bit_offset_[i];\n+  }\n+\n+ private:\n+  static constexpr int max_buffers_ = 3;\n+  const uint8_t* buffers_[max_buffers_];\n+  uint8_t* mutable_buffers_[max_buffers_];\n+  KeyColumnMetadata metadata_;\n+  int64_t length_;\n+  // Starting bit offset within the first byte (between 0 and 7)\n+  // to be used when accessing buffers that store bit vectors.\n+  int bit_offset_[max_buffers_ - 1];\n+};\n+\n+/// \\brief Create KeyColumnMetadata from a DataType\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnMetadata for\n+/// the indices type\n+///\n+/// The caller should ensure this is only called on \"key\" columns.  Calling this with\n+/// a non-key column will return a meaningless value (or abort on a debug build)\n\nReview Comment:\n   Is this worth it compared to the usual Result<> pattern?\n\n\n\n##########\ncpp/src/arrow/compute/light_array_test.cc:\n##########\n@@ -0,0 +1,504 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/testing/generator.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+const std::vector<std::shared_ptr<DataType>> kSampleFixedDataTypes = {\n+    int8(),   int16(),  int32(),  int64(),           uint8(),\n+    uint16(), uint32(), uint64(), decimal128(38, 6), decimal256(76, 6)};\n+const std::vector<std::shared_ptr<DataType>> kSampleBinaryTypes = {\n+    utf8(), binary() /*, large_utf8(), large_binary()*/};\n\nReview Comment:\n   Why are these commented out?\n\n\n\n##########\ncpp/src/arrow/compute/light_array_test.cc:\n##########\n@@ -0,0 +1,504 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/testing/generator.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+const std::vector<std::shared_ptr<DataType>> kSampleFixedDataTypes = {\n+    int8(),   int16(),  int32(),  int64(),           uint8(),\n+    uint16(), uint32(), uint64(), decimal128(38, 6), decimal256(76, 6)};\n+const std::vector<std::shared_ptr<DataType>> kSampleBinaryTypes = {\n+    utf8(), binary() /*, large_utf8(), large_binary()*/};\n+\n+TEST(KeyColumnMetadata, FromDataType) {\n+  KeyColumnMetadata metadata = ColumnMetadataFromDataType(boolean());\n+  ASSERT_EQ(0, metadata.fixed_length);\n+  ASSERT_EQ(true, metadata.is_fixed_length);\n+  ASSERT_EQ(false, metadata.is_null_type);\n+\n+  metadata = ColumnMetadataFromDataType(null());\n+  ASSERT_EQ(true, metadata.is_null_type);\n+\n+  for (const auto& type : kSampleFixedDataTypes) {\n+    int byte_width =\n+        internal::checked_pointer_cast<FixedWidthType>(type)->bit_width() / 8;\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(byte_width, metadata.fixed_length);\n+    ASSERT_EQ(true, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+\n+  for (const auto& type : {binary(), utf8()}) {\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(4, metadata.fixed_length);\n+    ASSERT_EQ(false, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+\n+  for (const auto& type : {large_binary(), large_utf8()}) {\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(8, metadata.fixed_length);\n+    ASSERT_EQ(false, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayData) {\n+  for (const auto& type : kSampleFixedDataTypes) {\n+    ARROW_SCOPED_TRACE(\"Type: \", type->ToString());\n+    // `array_offset` is the offset of the source array (e.g. if we are given a sliced\n+    // source array) while `offset` is the offset we pass when constructing the\n+    // KeyColumnArray\n+    for (auto array_offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+      for (auto offset : {0, 1}) {\n+        ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+        std::shared_ptr<Array> array;\n+        int byte_width =\n+            internal::checked_pointer_cast<FixedWidthType>(type)->bit_width() / 8;\n+        if (is_decimal(type->id())) {\n+          array = ArrayFromJSON(type, R\"([\"1.123123\", \"2.123123\", null])\");\n+        } else {\n+          array = ArrayFromJSON(type, \"[1, 2, null]\");\n+        }\n+        array = array->Slice(array_offset);\n+        int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+        int buffer_offset_bytes = (offset + array_offset) * byte_width;\n+        KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+        // Maximum tested offset is < 8 so validity is just bit offset\n+        ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+        ASSERT_EQ(0, kc_array.bit_offset(1));\n+        ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+        ASSERT_EQ(array->data()->buffers[1]->data() + buffer_offset_bytes,\n+                  kc_array.data(1));\n+        ASSERT_EQ(nullptr, kc_array.data(2));\n+        ASSERT_EQ(length, kc_array.length());\n+        // When creating from ArrayData always create read-only\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(2));\n+      }\n+    }\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayDataBinary) {\n+  for (const auto& type : kSampleBinaryTypes) {\n+    ARROW_SCOPED_TRACE(\"Type: \", type->ToString());\n+    for (auto array_offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+      for (auto offset : {0, 1}) {\n+        ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+        std::shared_ptr<Array> array = ArrayFromJSON(type, R\"([\"xyz\", \"abcabc\", null])\");\n+        int offsets_width =\n+            static_cast<int>(internal::checked_pointer_cast<BaseBinaryType>(type)\n+                                 ->layout()\n+                                 .buffers[1]\n+                                 .byte_width);\n+        array = array->Slice(array_offset);\n+        int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+        int buffer_offset_bytes = (offset + array_offset) * offsets_width;\n+        KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+        ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+        ASSERT_EQ(0, kc_array.bit_offset(1));\n+        ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+        ASSERT_EQ(array->data()->buffers[1]->data() + buffer_offset_bytes,\n+                  kc_array.data(1));\n+        ASSERT_EQ(array->data()->buffers[2]->data(), kc_array.data(2));\n+        ASSERT_EQ(length, kc_array.length());\n+        // When creating from ArrayData always create read-only\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(2));\n+      }\n+    }\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayDataBool) {\n+  for (auto array_offset : {0, 1}) {\n+    ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+    for (auto offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+      std::shared_ptr<Array> array = ArrayFromJSON(boolean(), \"[true, false, null]\");\n+      array = array->Slice(array_offset);\n+      int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+      KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+      ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+      ASSERT_EQ(offset + array_offset, kc_array.bit_offset(1));\n+      ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+      ASSERT_EQ(array->data()->buffers[1]->data(), kc_array.data(1));\n+      ASSERT_EQ(length, kc_array.length());\n+      ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+      ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+    }\n+  }\n+}\n+\n+// TEST(KeyColumnArray, FromArrayDataNull) {\n\nReview Comment:\n   I also see above that dictionary type may be handled?\n\n\n\n",
                    "created": "2022-04-14T21:48:58.117+0000",
                    "updated": "2022-04-14T21:48:58.117+0000",
                    "started": "2022-04-14T21:48:58.117+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757233",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757299",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851066060\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n\nReview Comment:\n   Yes, this isn't intended to be a wholesale replacement of ExecBatch, rather something that is small enough to get a PR through improving the hash-join performance.  Once that is done we can either grow these tools to support more types, replace them with something more complete but more efficient, or accept multiple variations of kernels/nodes depending on benchmarks and need.\r\n   \r\n   I think, by having these tools here, it will be easier to evaluate and work on potential replacements for ExecBatch, even if someone doesn't know the ins and outs of the join algorithm.\n\n\n\n",
                    "created": "2022-04-15T05:18:25.796+0000",
                    "updated": "2022-04-15T05:18:25.796+0000",
                    "started": "2022-04-15T05:18:25.796+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757299",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757301",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851066303\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n\nReview Comment:\n   The KeyColumnMetadata and KeyColumnArray classes are used in hash group by and in new hash join for interaction with the hash table. The reason to have them was to be able to work on smaller batches than ExecBatch and therefore to keep intermediate results of multi-step computations in L1 cache. The goal was to get rid of the overheads of memory allocations and shared pointers (and atomic instructions for ref counting) when doing things like slicing.\r\n   \r\n   If we make ExecBatch more lightweight in the future, it is likely that new ExecBatch structures would be good for hash tables. But the KeyColumn... classes so far restrict the functionality to minimum needed for hash table keys in terms of data type support and abstraction of the data (e.g. hash table is interested in data movement and interpretation of the bytes in column values is left to callback functions).\n\n\n\n",
                    "created": "2022-04-15T05:19:25.839+0000",
                    "updated": "2022-04-15T05:19:25.839+0000",
                    "started": "2022-04-15T05:19:25.839+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757301",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757302",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851066765\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n\nReview Comment:\n   Understood. At the very least it helps show what a replacement might look like indeed. \n\n\n\n",
                    "created": "2022-04-15T05:21:15.984+0000",
                    "updated": "2022-04-15T05:21:15.984+0000",
                    "started": "2022-04-15T05:21:15.984+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757302",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757310",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851077345\n\n\n##########\ncpp/src/arrow/compute/light_array_test.cc:\n##########\n@@ -0,0 +1,504 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/testing/generator.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+const std::vector<std::shared_ptr<DataType>> kSampleFixedDataTypes = {\n+    int8(),   int16(),  int32(),  int64(),           uint8(),\n+    uint16(), uint32(), uint64(), decimal128(38, 6), decimal256(76, 6)};\n+const std::vector<std::shared_ptr<DataType>> kSampleBinaryTypes = {\n+    utf8(), binary() /*, large_utf8(), large_binary()*/};\n\nReview Comment:\n   There is no support for these data types yet (but there probably will be in the future). \r\n   I can remove the commented out part.\n\n\n\n",
                    "created": "2022-04-15T05:58:33.623+0000",
                    "updated": "2022-04-15T05:58:33.623+0000",
                    "started": "2022-04-15T05:58:33.623+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757310",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757311",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851077893\n\n\n##########\ncpp/src/arrow/compute/light_array_test.cc:\n##########\n@@ -0,0 +1,504 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/testing/generator.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+const std::vector<std::shared_ptr<DataType>> kSampleFixedDataTypes = {\n+    int8(),   int16(),  int32(),  int64(),           uint8(),\n+    uint16(), uint32(), uint64(), decimal128(38, 6), decimal256(76, 6)};\n+const std::vector<std::shared_ptr<DataType>> kSampleBinaryTypes = {\n+    utf8(), binary() /*, large_utf8(), large_binary()*/};\n+\n+TEST(KeyColumnMetadata, FromDataType) {\n+  KeyColumnMetadata metadata = ColumnMetadataFromDataType(boolean());\n+  ASSERT_EQ(0, metadata.fixed_length);\n+  ASSERT_EQ(true, metadata.is_fixed_length);\n+  ASSERT_EQ(false, metadata.is_null_type);\n+\n+  metadata = ColumnMetadataFromDataType(null());\n+  ASSERT_EQ(true, metadata.is_null_type);\n+\n+  for (const auto& type : kSampleFixedDataTypes) {\n+    int byte_width =\n+        internal::checked_pointer_cast<FixedWidthType>(type)->bit_width() / 8;\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(byte_width, metadata.fixed_length);\n+    ASSERT_EQ(true, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+\n+  for (const auto& type : {binary(), utf8()}) {\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(4, metadata.fixed_length);\n+    ASSERT_EQ(false, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+\n+  for (const auto& type : {large_binary(), large_utf8()}) {\n+    metadata = ColumnMetadataFromDataType(type);\n+    ASSERT_EQ(8, metadata.fixed_length);\n+    ASSERT_EQ(false, metadata.is_fixed_length);\n+    ASSERT_EQ(false, metadata.is_null_type);\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayData) {\n+  for (const auto& type : kSampleFixedDataTypes) {\n+    ARROW_SCOPED_TRACE(\"Type: \", type->ToString());\n+    // `array_offset` is the offset of the source array (e.g. if we are given a sliced\n+    // source array) while `offset` is the offset we pass when constructing the\n+    // KeyColumnArray\n+    for (auto array_offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+      for (auto offset : {0, 1}) {\n+        ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+        std::shared_ptr<Array> array;\n+        int byte_width =\n+            internal::checked_pointer_cast<FixedWidthType>(type)->bit_width() / 8;\n+        if (is_decimal(type->id())) {\n+          array = ArrayFromJSON(type, R\"([\"1.123123\", \"2.123123\", null])\");\n+        } else {\n+          array = ArrayFromJSON(type, \"[1, 2, null]\");\n+        }\n+        array = array->Slice(array_offset);\n+        int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+        int buffer_offset_bytes = (offset + array_offset) * byte_width;\n+        KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+        // Maximum tested offset is < 8 so validity is just bit offset\n+        ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+        ASSERT_EQ(0, kc_array.bit_offset(1));\n+        ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+        ASSERT_EQ(array->data()->buffers[1]->data() + buffer_offset_bytes,\n+                  kc_array.data(1));\n+        ASSERT_EQ(nullptr, kc_array.data(2));\n+        ASSERT_EQ(length, kc_array.length());\n+        // When creating from ArrayData always create read-only\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(2));\n+      }\n+    }\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayDataBinary) {\n+  for (const auto& type : kSampleBinaryTypes) {\n+    ARROW_SCOPED_TRACE(\"Type: \", type->ToString());\n+    for (auto array_offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+      for (auto offset : {0, 1}) {\n+        ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+        std::shared_ptr<Array> array = ArrayFromJSON(type, R\"([\"xyz\", \"abcabc\", null])\");\n+        int offsets_width =\n+            static_cast<int>(internal::checked_pointer_cast<BaseBinaryType>(type)\n+                                 ->layout()\n+                                 .buffers[1]\n+                                 .byte_width);\n+        array = array->Slice(array_offset);\n+        int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+        int buffer_offset_bytes = (offset + array_offset) * offsets_width;\n+        KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+        ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+        ASSERT_EQ(0, kc_array.bit_offset(1));\n+        ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+        ASSERT_EQ(array->data()->buffers[1]->data() + buffer_offset_bytes,\n+                  kc_array.data(1));\n+        ASSERT_EQ(array->data()->buffers[2]->data(), kc_array.data(2));\n+        ASSERT_EQ(length, kc_array.length());\n+        // When creating from ArrayData always create read-only\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+        ASSERT_EQ(nullptr, kc_array.mutable_data(2));\n+      }\n+    }\n+  }\n+}\n+\n+TEST(KeyColumnArray, FromArrayDataBool) {\n+  for (auto array_offset : {0, 1}) {\n+    ARROW_SCOPED_TRACE(\"Array offset: \", array_offset);\n+    for (auto offset : {0, 1}) {\n+      ARROW_SCOPED_TRACE(\"Constructor offset: \", offset);\n+      std::shared_ptr<Array> array = ArrayFromJSON(boolean(), \"[true, false, null]\");\n+      array = array->Slice(array_offset);\n+      int length = static_cast<int32_t>(array->length()) - offset - array_offset;\n+      KeyColumnArray kc_array = ColumnArrayFromArrayData(array->data(), offset, length);\n+      ASSERT_EQ(offset + array_offset, kc_array.bit_offset(0));\n+      ASSERT_EQ(offset + array_offset, kc_array.bit_offset(1));\n+      ASSERT_EQ(array->data()->buffers[0]->data(), kc_array.data(0));\n+      ASSERT_EQ(array->data()->buffers[1]->data(), kc_array.data(1));\n+      ASSERT_EQ(length, kc_array.length());\n+      ASSERT_EQ(nullptr, kc_array.mutable_data(0));\n+      ASSERT_EQ(nullptr, kc_array.mutable_data(1));\n+    }\n+  }\n+}\n+\n+// TEST(KeyColumnArray, FromArrayDataNull) {\n\nReview Comment:\n   I removed this part. \n\n\n\n",
                    "created": "2022-04-15T06:00:38.973+0000",
                    "updated": "2022-04-15T06:00:38.973+0000",
                    "started": "2022-04-15T06:00:38.972+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757311",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757312",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851078401\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return buffers_[i];\n+  }\n+  /// \\brief Return a mutable version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  uint32_t* mutable_offsets() {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<uint32_t*>(mutable_data(1));\n+  }\n+  /// \\brief Return a read-only version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  const uint32_t* offsets() const {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<const uint32_t*>(data(1));\n+  }\n+  /// \\brief Return the type metadata\n+  const KeyColumnMetadata& metadata() const { return metadata_; }\n+  /// \\brief Return the length (in rows) of the array\n+  int64_t length() const { return length_; }\n+  /// \\brief Return the bit offset into the corresponding vector\n+  ///\n+  /// if i == 1 then this must be a bool array\n+  int bit_offset(int i) const {\n+    ARROW_DCHECK(i >= 0 && i < max_buffers_);\n+    return bit_offset_[i];\n+  }\n+\n+ private:\n+  static constexpr int max_buffers_ = 3;\n+  const uint8_t* buffers_[max_buffers_];\n+  uint8_t* mutable_buffers_[max_buffers_];\n+  KeyColumnMetadata metadata_;\n+  int64_t length_;\n+  // Starting bit offset within the first byte (between 0 and 7)\n+  // to be used when accessing buffers that store bit vectors.\n+  int bit_offset_[max_buffers_ - 1];\n+};\n+\n+/// \\brief Create KeyColumnMetadata from a DataType\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnMetadata for\n+/// the indices type\n+///\n+/// The caller should ensure this is only called on \"key\" columns.  Calling this with\n+/// a non-key column will return a meaningless value (or abort on a debug build)\n\nReview Comment:\n   > Looks good to me, I guess I already reviewed a bunch of this code on the other PR. One thing: is it worth to add `Hashing32::HashBatch` and `Hashing64::HashBatch` in this PR? I need those for Bloom Filter pushdown, not sure if those fit better here or there.\r\n   \r\n   Hashing is unrelated to the topic of this PR, but we could possibly make a tiny separate PR just for HashBatch (or put it into Bloom filter pushdown).\n\n\n\n",
                    "created": "2022-04-15T06:02:29.238+0000",
                    "updated": "2022-04-15T06:02:29.238+0000",
                    "started": "2022-04-15T06:02:29.238+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757312",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757313",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851079678\n\n\n##########\ncpp/src/arrow/compute/light_array_test.cc:\n##########\n@@ -0,0 +1,504 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <gtest/gtest.h>\n+\n+#include \"arrow/compute/exec/test_util.h\"\n+#include \"arrow/testing/generator.h\"\n+#include \"arrow/testing/gtest_util.h\"\n+#include \"arrow/type.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+const std::vector<std::shared_ptr<DataType>> kSampleFixedDataTypes = {\n+    int8(),   int16(),  int32(),  int64(),           uint8(),\n+    uint16(), uint32(), uint64(), decimal128(38, 6), decimal256(76, 6)};\n+const std::vector<std::shared_ptr<DataType>> kSampleBinaryTypes = {\n+    utf8(), binary() /*, large_utf8(), large_binary()*/};\n\nReview Comment:\n   Also, dictionary type is not supported in KeyColumn... (hash table either works with indices from dictionary or decoded values). Once new hash join is ready, there will be a follow up change to port dictionary type support from old hash join to new hash join, which will not have it initially. At this point some of the code from this PR may need to be updated.\n\n\n\n",
                    "created": "2022-04-15T06:06:55.003+0000",
                    "updated": "2022-04-15T06:06:55.003+0000",
                    "started": "2022-04-15T06:06:55.003+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757313",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757314",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#issuecomment-1099882476\n\n   > Looks good to me, I guess I already reviewed a bunch of this code on the other PR. One thing: is it worth to add `Hashing32::HashBatch` and `Hashing64::HashBatch` in this PR? I need those for Bloom Filter pushdown, not sure if those fit better here or there.\r\n   \r\n   Hashing seems unrelated to the topic of this PR, but we could possibly make a tiny separate PR just for HashBatch (or put it into Bloom filter pushdown).\r\n   \n\n\n",
                    "created": "2022-04-15T06:11:12.993+0000",
                    "updated": "2022-04-15T06:11:12.993+0000",
                    "started": "2022-04-15T06:11:12.992+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757314",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757316",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851085686\n\n\n##########\ncpp/src/arrow/compute/light_array.cc:\n##########\n@@ -0,0 +1,736 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/light_array.h\"\n+\n+#include <type_traits>\n+\n+#include \"arrow/util/bitmap_ops.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+KeyColumnArray::KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                               const uint8_t* buffer0, const uint8_t* buffer1,\n+                               const uint8_t* buffer2, int bit_offset0, int bit_offset1) {\n+  static_assert(std::is_pod<KeyColumnArray>::value,\n+                \"This class was intended to be a POD type\");\n+  metadata_ = metadata;\n+  length_ = length;\n+  buffers_[0] = buffer0;\n+  buffers_[1] = buffer1;\n+  buffers_[2] = buffer2;\n+  mutable_buffers_[0] = mutable_buffers_[1] = mutable_buffers_[2] = nullptr;\n+  bit_offset_[0] = bit_offset0;\n+  bit_offset_[1] = bit_offset1;\n+}\n+\n+KeyColumnArray::KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                               uint8_t* buffer0, uint8_t* buffer1, uint8_t* buffer2,\n+                               int bit_offset0, int bit_offset1) {\n+  metadata_ = metadata;\n+  length_ = length;\n+  buffers_[0] = mutable_buffers_[0] = buffer0;\n+  buffers_[1] = mutable_buffers_[1] = buffer1;\n+  buffers_[2] = mutable_buffers_[2] = buffer2;\n+  bit_offset_[0] = bit_offset0;\n+  bit_offset_[1] = bit_offset1;\n+}\n+\n+KeyColumnArray KeyColumnArray::WithBufferFrom(const KeyColumnArray& other,\n+                                              int buffer_id_to_replace) const {\n+  KeyColumnArray copy = *this;\n+  copy.mutable_buffers_[buffer_id_to_replace] =\n+      other.mutable_buffers_[buffer_id_to_replace];\n+  copy.buffers_[buffer_id_to_replace] = other.buffers_[buffer_id_to_replace];\n+  if (buffer_id_to_replace < max_buffers_ - 1) {\n+    copy.bit_offset_[buffer_id_to_replace] = other.bit_offset_[buffer_id_to_replace];\n+  }\n+  return copy;\n+}\n+\n+KeyColumnArray KeyColumnArray::WithMetadata(const KeyColumnMetadata& metadata) const {\n+  KeyColumnArray copy = *this;\n+  copy.metadata_ = metadata;\n+  return copy;\n+}\n+\n+KeyColumnArray KeyColumnArray::Slice(int64_t offset, int64_t length) const {\n+  KeyColumnArray sliced;\n+  sliced.metadata_ = metadata_;\n+  sliced.length_ = length;\n+  uint32_t fixed_size =\n+      !metadata_.is_fixed_length ? sizeof(uint32_t) : metadata_.fixed_length;\n+\n+  sliced.buffers_[0] =\n+      buffers_[0] ? buffers_[0] + (bit_offset_[0] + offset) / 8 : nullptr;\n+  sliced.mutable_buffers_[0] =\n+      mutable_buffers_[0] ? mutable_buffers_[0] + (bit_offset_[0] + offset) / 8 : nullptr;\n+  sliced.bit_offset_[0] = (bit_offset_[0] + offset) % 8;\n+\n+  if (fixed_size == 0 && !metadata_.is_null_type) {\n+    sliced.buffers_[1] =\n+        buffers_[1] ? buffers_[1] + (bit_offset_[1] + offset) / 8 : nullptr;\n+    sliced.mutable_buffers_[1] = mutable_buffers_[1]\n+                                     ? mutable_buffers_[1] + (bit_offset_[1] + offset) / 8\n+                                     : nullptr;\n+    sliced.bit_offset_[1] = (bit_offset_[1] + offset) % 8;\n+  } else {\n+    sliced.buffers_[1] = buffers_[1] ? buffers_[1] + offset * fixed_size : nullptr;\n+    sliced.mutable_buffers_[1] =\n+        mutable_buffers_[1] ? mutable_buffers_[1] + offset * fixed_size : nullptr;\n+    sliced.bit_offset_[1] = 0;\n+  }\n+\n+  sliced.buffers_[2] = buffers_[2];\n+  sliced.mutable_buffers_[2] = mutable_buffers_[2];\n+  return sliced;\n+}\n+\n+KeyColumnMetadata ColumnMetadataFromDataType(const std::shared_ptr<DataType>& type) {\n+  if (type->id() == Type::DICTIONARY) {\n+    auto bit_width =\n+        arrow::internal::checked_cast<const FixedWidthType&>(*type).bit_width();\n+    ARROW_DCHECK(bit_width % 8 == 0);\n+    return KeyColumnMetadata(true, bit_width / 8);\n+  }\n+  if (type->id() == Type::BOOL) {\n+    return KeyColumnMetadata(true, 0);\n+  }\n+  if (is_fixed_width(type->id())) {\n+    return KeyColumnMetadata(\n+        true,\n+        arrow::internal::checked_cast<const FixedWidthType&>(*type).bit_width() / 8);\n+  }\n+  if (is_binary_like(type->id())) {\n+    return KeyColumnMetadata(false, sizeof(uint32_t));\n+  }\n+  if (is_large_binary_like(type->id())) {\n+    return KeyColumnMetadata(false, sizeof(uint64_t));\n+  }\n+  if (type->id() == Type::NA) {\n+    return KeyColumnMetadata(true, 0, true);\n+  }\n+  // Should not reach this point, caller attempted to create a KeyColumnArray from an\n+  // invalid type\n+  ARROW_DCHECK(false);\n+  return KeyColumnMetadata(true, sizeof(int));\n+}\n+\n+KeyColumnArray ColumnArrayFromArrayData(const std::shared_ptr<ArrayData>& array_data,\n+                                        int start_row, int num_rows) {\n+  KeyColumnArray column_array = KeyColumnArray(\n+      ColumnMetadataFromDataType(array_data->type),\n+      array_data->offset + start_row + num_rows,\n+      array_data->buffers[0] != NULLPTR ? array_data->buffers[0]->data() : nullptr,\n+      array_data->buffers[1]->data(),\n+      (array_data->buffers.size() > 2 && array_data->buffers[2] != NULLPTR)\n+          ? array_data->buffers[2]->data()\n+          : nullptr);\n+  return column_array.Slice(array_data->offset + start_row, num_rows);\n+}\n+\n+void ColumnMetadatasFromExecBatch(const ExecBatch& batch,\n+                                  std::vector<KeyColumnMetadata>* column_metadatas) {\n+  int num_columns = static_cast<int>(batch.values.size());\n+  column_metadatas->resize(num_columns);\n+  for (int i = 0; i < num_columns; ++i) {\n+    const Datum& data = batch.values[i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    (*column_metadatas)[i] = ColumnMetadataFromDataType(array_data->type);\n+  }\n+}\n+\n+void ColumnArraysFromExecBatch(const ExecBatch& batch, int start_row, int num_rows,\n+                               std::vector<KeyColumnArray>* column_arrays) {\n+  int num_columns = static_cast<int>(batch.values.size());\n+  column_arrays->resize(num_columns);\n+  for (int i = 0; i < num_columns; ++i) {\n+    const Datum& data = batch.values[i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    (*column_arrays)[i] = ColumnArrayFromArrayData(array_data, start_row, num_rows);\n+  }\n+}\n+\n+void ColumnArraysFromExecBatch(const ExecBatch& batch,\n+                               std::vector<KeyColumnArray>* column_arrays) {\n+  ColumnArraysFromExecBatch(batch, 0, static_cast<int>(batch.length), column_arrays);\n+}\n+\n+void ResizableArrayData::Init(const std::shared_ptr<DataType>& data_type,\n+                              MemoryPool* pool, int log_num_rows_min) {\n+#ifndef NDEBUG\n+  if (num_rows_allocated_ > 0) {\n+    ARROW_DCHECK(data_type_ != NULLPTR);\n+    KeyColumnMetadata metadata_before = ColumnMetadataFromDataType(data_type_);\n+    KeyColumnMetadata metadata_after = ColumnMetadataFromDataType(data_type);\n+    ARROW_DCHECK(metadata_before.is_fixed_length == metadata_after.is_fixed_length &&\n+                 metadata_before.fixed_length == metadata_after.fixed_length);\n+  }\n+#endif\n+  Clear(/*release_buffers=*/false);\n+  log_num_rows_min_ = log_num_rows_min;\n+  data_type_ = data_type;\n+  pool_ = pool;\n+}\n+\n+void ResizableArrayData::Clear(bool release_buffers) {\n+  num_rows_ = 0;\n+  if (release_buffers) {\n+    non_null_buf_.reset();\n+    fixed_len_buf_.reset();\n+    var_len_buf_.reset();\n+    num_rows_allocated_ = 0;\n+    var_len_buf_size_ = 0;\n+  }\n+}\n+\n+Status ResizableArrayData::ResizeFixedLengthBuffers(int num_rows_new) {\n+  ARROW_DCHECK(num_rows_new >= 0);\n+  if (num_rows_new <= num_rows_allocated_) {\n+    num_rows_ = num_rows_new;\n+    return Status::OK();\n+  }\n+\n+  int num_rows_allocated_new = 1 << log_num_rows_min_;\n+  while (num_rows_allocated_new < num_rows_new) {\n+    num_rows_allocated_new *= 2;\n+  }\n+\n+  KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (fixed_len_buf_ == NULLPTR) {\n+    ARROW_DCHECK(non_null_buf_ == NULLPTR && var_len_buf_ == NULLPTR);\n+\n+    ARROW_ASSIGN_OR_RAISE(\n+        non_null_buf_,\n+        AllocateResizableBuffer(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes, pool_));\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes,\n+                pool_));\n+      } else {\n+        ARROW_ASSIGN_OR_RAISE(\n+            fixed_len_buf_,\n+            AllocateResizableBuffer(\n+                num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes,\n+                pool_));\n+      }\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(\n+          fixed_len_buf_,\n+          AllocateResizableBuffer(\n+              (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes, pool_));\n+    }\n+\n+    ARROW_ASSIGN_OR_RAISE(var_len_buf_, AllocateResizableBuffer(\n+                                            sizeof(uint64_t) + kNumPaddingBytes, pool_));\n+\n+    var_len_buf_size_ = sizeof(uint64_t);\n+  } else {\n+    ARROW_DCHECK(non_null_buf_ != NULLPTR && var_len_buf_ != NULLPTR);\n+\n+    RETURN_NOT_OK(non_null_buf_->Resize(bit_util::BytesForBits(num_rows_allocated_new) +\n+                                        kNumPaddingBytes));\n+\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            bit_util::BytesForBits(num_rows_allocated_new) + kNumPaddingBytes));\n+      } else {\n+        RETURN_NOT_OK(fixed_len_buf_->Resize(\n+            num_rows_allocated_new * column_metadata.fixed_length + kNumPaddingBytes));\n+      }\n+    } else {\n+      RETURN_NOT_OK(fixed_len_buf_->Resize(\n+          (num_rows_allocated_new + 1) * sizeof(uint32_t) + kNumPaddingBytes));\n+    }\n+  }\n+\n+  num_rows_allocated_ = num_rows_allocated_new;\n+  num_rows_ = num_rows_new;\n+\n+  return Status::OK();\n+}\n+\n+Status ResizableArrayData::ResizeVaryingLengthBuffer() {\n+  KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  if (!column_metadata.is_fixed_length) {\n+    int min_new_size = static_cast<int>(\n+        reinterpret_cast<const uint32_t*>(fixed_len_buf_->data())[num_rows_]);\n+    ARROW_DCHECK(var_len_buf_size_ > 0);\n+    if (var_len_buf_size_ < min_new_size) {\n+      int new_size = var_len_buf_size_;\n+      while (new_size < min_new_size) {\n+        new_size *= 2;\n+      }\n+      RETURN_NOT_OK(var_len_buf_->Resize(new_size + kNumPaddingBytes));\n+      var_len_buf_size_ = new_size;\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+KeyColumnArray ResizableArrayData::column_array() const {\n+  KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+  return KeyColumnArray(column_metadata, num_rows_, non_null_buf_->mutable_data(),\n+                        fixed_len_buf_->mutable_data(), var_len_buf_->mutable_data());\n+}\n+\n+std::shared_ptr<ArrayData> ResizableArrayData::array_data() const {\n+  KeyColumnMetadata column_metadata;\n+  column_metadata = ColumnMetadataFromDataType(data_type_);\n+\n+  auto valid_count = arrow::internal::CountSetBits(non_null_buf_->data(), /*offset=*/0,\n+                                                   static_cast<int64_t>(num_rows_));\n+  int null_count = static_cast<int>(num_rows_) - static_cast<int>(valid_count);\n+\n+  if (column_metadata.is_fixed_length) {\n+    return ArrayData::Make(data_type_, num_rows_, {non_null_buf_, fixed_len_buf_},\n+                           null_count);\n+  } else {\n+    return ArrayData::Make(data_type_, num_rows_,\n+                           {non_null_buf_, fixed_len_buf_, var_len_buf_}, null_count);\n+  }\n+}\n+\n+int ExecBatchBuilder::NumRowsToSkip(const std::shared_ptr<ArrayData>& column,\n+                                    int num_rows, const uint16_t* row_ids,\n+                                    int num_tail_bytes_to_skip) {\n+#ifndef NDEBUG\n+  // Ids must be in non-decreasing order\n+  //\n+  for (int i = 1; i < num_rows; ++i) {\n+    ARROW_DCHECK(row_ids[i] >= row_ids[i - 1]);\n+  }\n+#endif\n+\n+  KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(column->type);\n+\n+  int num_rows_left = num_rows;\n+  int num_bytes_skipped = 0;\n+  while (num_rows_left > 0 && num_bytes_skipped < num_tail_bytes_to_skip) {\n+    if (column_metadata.is_fixed_length) {\n+      if (column_metadata.fixed_length == 0) {\n+        num_rows_left = std::max(num_rows_left, 8) - 8;\n+        ++num_bytes_skipped;\n+      } else {\n+        --num_rows_left;\n+        num_bytes_skipped += column_metadata.fixed_length;\n+      }\n+    } else {\n+      --num_rows_left;\n+      int row_id_removed = row_ids[num_rows_left];\n+      const uint32_t* offsets =\n+          reinterpret_cast<const uint32_t*>(column->buffers[1]->data());\n+      num_bytes_skipped += offsets[row_id_removed + 1] - offsets[row_id_removed];\n+    }\n+  }\n+\n+  return num_rows - num_rows_left;\n+}\n+\n+template <bool OUTPUT_BYTE_ALIGNED>\n+void ExecBatchBuilder::CollectBitsImp(const uint8_t* input_bits,\n+                                      int64_t input_bits_offset, uint8_t* output_bits,\n+                                      int64_t output_bits_offset, int num_rows,\n+                                      const uint16_t* row_ids) {\n+  if (!OUTPUT_BYTE_ALIGNED) {\n+    ARROW_DCHECK(output_bits_offset % 8 > 0);\n+    output_bits[output_bits_offset / 8] &=\n+        static_cast<uint8_t>((1 << (output_bits_offset % 8)) - 1);\n+  } else {\n+    ARROW_DCHECK(output_bits_offset % 8 == 0);\n+  }\n+  constexpr int unroll = 8;\n+  for (int i = 0; i < num_rows / unroll; ++i) {\n+    const uint16_t* row_ids_base = row_ids + unroll * i;\n+    uint8_t result;\n+    result = bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[0]) ? 1 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[1]) ? 2 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[2]) ? 4 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[3]) ? 8 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[4]) ? 16 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[5]) ? 32 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[6]) ? 64 : 0;\n+    result |= bit_util::GetBit(input_bits, input_bits_offset + row_ids_base[7]) ? 128 : 0;\n+    if (OUTPUT_BYTE_ALIGNED) {\n+      output_bits[output_bits_offset / 8 + i] = result;\n+    } else {\n+      output_bits[output_bits_offset / 8 + i] |=\n+          static_cast<uint8_t>(result << (output_bits_offset % 8));\n+      output_bits[output_bits_offset / 8 + i + 1] =\n+          static_cast<uint8_t>(result >> (8 - (output_bits_offset % 8)));\n+    }\n+  }\n+  if (num_rows % unroll > 0) {\n+    for (int i = num_rows - (num_rows % unroll); i < num_rows; ++i) {\n+      bit_util::SetBitTo(output_bits, output_bits_offset + i,\n+                         bit_util::GetBit(input_bits, input_bits_offset + row_ids[i]));\n+    }\n+  }\n+}\n+\n+void ExecBatchBuilder::CollectBits(const uint8_t* input_bits, int64_t input_bits_offset,\n+                                   uint8_t* output_bits, int64_t output_bits_offset,\n+                                   int num_rows, const uint16_t* row_ids) {\n+  if (output_bits_offset % 8 > 0) {\n+    CollectBitsImp<false>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                          num_rows, row_ids);\n+  } else {\n+    CollectBitsImp<true>(input_bits, input_bits_offset, output_bits, output_bits_offset,\n+                         num_rows, row_ids);\n+  }\n+}\n+\n+template <class PROCESS_VALUE_FN>\n+void ExecBatchBuilder::Visit(const std::shared_ptr<ArrayData>& column, int num_rows,\n+                             const uint16_t* row_ids, PROCESS_VALUE_FN process_value_fn) {\n+  KeyColumnMetadata metadata = ColumnMetadataFromDataType(column->type);\n+\n+  if (!metadata.is_fixed_length) {\n+    const uint8_t* ptr_base = column->buffers[2]->data();\n+    const uint32_t* offsets =\n+        reinterpret_cast<const uint32_t*>(column->buffers[1]->data()) + column->offset;\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr = ptr_base + offsets[row_id];\n+      uint32_t field_length = offsets[row_id + 1] - offsets[row_id];\n+      process_value_fn(i, field_ptr, field_length);\n+    }\n+  } else {\n+    ARROW_DCHECK(metadata.fixed_length > 0);\n+    for (int i = 0; i < num_rows; ++i) {\n+      uint16_t row_id = row_ids[i];\n+      const uint8_t* field_ptr =\n+          column->buffers[1]->data() +\n+          (column->offset + row_id) * static_cast<int64_t>(metadata.fixed_length);\n+      process_value_fn(i, field_ptr, metadata.fixed_length);\n+    }\n+  }\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                                        ResizableArrayData* target,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        MemoryPool* pool) {\n+  int num_rows_before = target->num_rows();\n+  ARROW_DCHECK(num_rows_before >= 0);\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target->num_rows() == 0) {\n+    target->Init(source->type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target->ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(source->type);\n+\n+  if (column_metadata.is_fixed_length) {\n+    // Fixed length column\n+    //\n+    uint32_t fixed_length = column_metadata.fixed_length;\n+    switch (fixed_length) {\n+      case 0:\n+        CollectBits(source->buffers[1]->data(), source->offset, target->mutable_data(1),\n+                    num_rows_before, num_rows_to_append, row_ids);\n+        break;\n+      case 1:\n+        Visit(source, num_rows_to_append, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                target->mutable_data(1)[num_rows_before + i] = *ptr;\n+              });\n+        break;\n+      case 2:\n+        Visit(\n+            source, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint16_t*>(target->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint16_t*>(ptr);\n+            });\n+        break;\n+      case 4:\n+        Visit(\n+            source, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint32_t*>(target->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint32_t*>(ptr);\n+            });\n+        break;\n+      case 8:\n+        Visit(\n+            source, num_rows_to_append, row_ids,\n+            [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+              reinterpret_cast<uint64_t*>(target->mutable_data(1))[num_rows_before + i] =\n+                  *reinterpret_cast<const uint64_t*>(ptr);\n+            });\n+        break;\n+      default: {\n+        int num_rows_to_process =\n+            num_rows_to_append -\n+            NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+        Visit(source, num_rows_to_process, row_ids,\n+              [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                    target->mutable_data(1) +\n+                    static_cast<int64_t>(num_bytes) * (num_rows_before + i));\n+                const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                for (uint32_t word_id = 0;\n+                     word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t));\n+                     ++word_id) {\n+                  util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+                }\n+              });\n+        if (num_rows_to_append > num_rows_to_process) {\n+          Visit(source, num_rows_to_append - num_rows_to_process,\n+                row_ids + num_rows_to_process,\n+                [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+                  uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                      target->mutable_data(1) +\n+                      static_cast<int64_t>(num_bytes) *\n+                          (num_rows_before + num_rows_to_process + i));\n+                  const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+                  memcpy(dst, src, num_bytes);\n+                });\n+        }\n+      }\n+    }\n+  } else {\n+    // Varying length column\n+    //\n+\n+    // Step 1: calculate target offsets\n+    //\n+    uint32_t* offsets = reinterpret_cast<uint32_t*>(target->mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : offsets[num_rows_before];\n+    Visit(source, num_rows_to_append, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            offsets[num_rows_before + i] = num_bytes;\n+          });\n+    for (int i = 0; i < num_rows_to_append; ++i) {\n+      uint32_t length = offsets[num_rows_before + i];\n+      offsets[num_rows_before + i] = sum;\n+      sum += length;\n+    }\n+    offsets[num_rows_before + num_rows_to_append] = sum;\n+\n+    // Step 2: resize output buffers\n+    //\n+    RETURN_NOT_OK(target->ResizeVaryingLengthBuffer());\n+\n+    // Step 3: copy varying-length data\n+    //\n+    int num_rows_to_process =\n+        num_rows_to_append -\n+        NumRowsToSkip(source, num_rows_to_append, row_ids, sizeof(uint64_t));\n+    Visit(source, num_rows_to_process, row_ids,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(target->mutable_data(2) +\n+                                                        offsets[num_rows_before + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            for (uint32_t word_id = 0;\n+                 word_id < bit_util::CeilDiv(num_bytes, sizeof(uint64_t)); ++word_id) {\n+              util::SafeStore<uint64_t>(dst + word_id, util::SafeLoad(src + word_id));\n+            }\n+          });\n+    Visit(source, num_rows_to_append - num_rows_to_process, row_ids + num_rows_to_process,\n+          [&](int i, const uint8_t* ptr, uint32_t num_bytes) {\n+            uint64_t* dst = reinterpret_cast<uint64_t*>(\n+                target->mutable_data(2) +\n+                offsets[num_rows_before + num_rows_to_process + i]);\n+            const uint64_t* src = reinterpret_cast<const uint64_t*>(ptr);\n+            memcpy(dst, src, num_bytes);\n+          });\n+  }\n+\n+  // Process nulls\n+  //\n+  if (source->buffers[0] == NULLPTR) {\n+    uint8_t* dst = target->mutable_data(0);\n+    dst[num_rows_before / 8] |= static_cast<uint8_t>(~0ULL << (num_rows_before & 7));\n+    for (int i = num_rows_before / 8 + 1;\n+         i < bit_util::BytesForBits(num_rows_before + num_rows_to_append); ++i) {\n+      dst[i] = 0xff;\n+    }\n+  } else {\n+    CollectBits(source->buffers[0]->data(), source->offset, target->mutable_data(0),\n+                num_rows_before, num_rows_to_append, row_ids);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(const std::shared_ptr<DataType>& type,\n+                                     ResizableArrayData& target, int num_rows_to_append,\n+                                     MemoryPool* pool) {\n+  int num_rows_before = target.num_rows();\n+  int num_rows_after = num_rows_before + num_rows_to_append;\n+  if (target.num_rows() == 0) {\n+    target.Init(type, pool, kLogNumRows);\n+  }\n+  RETURN_NOT_OK(target.ResizeFixedLengthBuffers(num_rows_after));\n+\n+  KeyColumnMetadata column_metadata = ColumnMetadataFromDataType(type);\n+\n+  // Process fixed length buffer\n+  //\n+  if (column_metadata.is_fixed_length) {\n+    uint8_t* dst = target.mutable_data(1);\n+    if (column_metadata.fixed_length == 0) {\n+      dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+      int64_t offset_begin = num_rows_before / 8 + 1;\n+      int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+      if (offset_end > offset_begin) {\n+        memset(dst + offset_begin, 0, offset_end - offset_begin);\n+      }\n+    } else {\n+      memset(dst + num_rows_before * static_cast<int64_t>(column_metadata.fixed_length),\n+             0, static_cast<int64_t>(column_metadata.fixed_length) * num_rows_to_append);\n+    }\n+  } else {\n+    uint32_t* dst = reinterpret_cast<uint32_t*>(target.mutable_data(1));\n+    uint32_t sum = num_rows_before == 0 ? 0 : dst[num_rows_before];\n+    for (int64_t i = num_rows_before; i <= num_rows_after; ++i) {\n+      dst[i] = sum;\n+    }\n+  }\n+\n+  // Process nulls\n+  //\n+  uint8_t* dst = target.mutable_data(0);\n+  dst[num_rows_before / 8] &= static_cast<uint8_t>((1 << (num_rows_before % 8)) - 1);\n+  int64_t offset_begin = num_rows_before / 8 + 1;\n+  int64_t offset_end = bit_util::BytesForBits(num_rows_after);\n+  if (offset_end > offset_begin) {\n+    memset(dst + offset_begin, 0, offset_end - offset_begin);\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int num_cols, const int* col_ids) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(num_cols);\n+    for (int i = 0; i < num_cols; ++i) {\n+      const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+      ARROW_DCHECK(data.is_array());\n+      const std::shared_ptr<ArrayData>& array_data = data.array();\n+      values_[i].Init(array_data->type, pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    const Datum& data = batch.values[col_ids ? col_ids[i] : i];\n+    ARROW_DCHECK(data.is_array());\n+    const std::shared_ptr<ArrayData>& array_data = data.array();\n+    RETURN_NOT_OK(\n+        AppendSelected(array_data, &values_[i], num_rows_to_append, row_ids, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendSelected(MemoryPool* pool, const ExecBatch& batch,\n+                                        int num_rows_to_append, const uint16_t* row_ids,\n+                                        int* num_appended, int num_cols,\n+                                        const int* col_ids) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendSelected(pool, batch, num_rows_next, row_ids, num_cols, col_ids));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append) {\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+\n+  // If this is the first time we append rows, then initialize output buffers.\n+  //\n+  if (values_.empty()) {\n+    values_.resize(types.size());\n+    for (size_t i = 0; i < types.size(); ++i) {\n+      values_[i].Init(types[i], pool, kLogNumRows);\n+    }\n+  }\n+\n+  for (size_t i = 0; i < values_.size(); ++i) {\n+    RETURN_NOT_OK(AppendNulls(types[i], values_[i], num_rows_to_append, pool));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status ExecBatchBuilder::AppendNulls(MemoryPool* pool,\n+                                     const std::vector<std::shared_ptr<DataType>>& types,\n+                                     int num_rows_to_append, int* num_appended) {\n+  *num_appended = 0;\n+  if (num_rows_to_append == 0) {\n+    return Status::OK();\n+  }\n+  int num_rows_max = 1 << kLogNumRows;\n+  int num_rows_present = num_rows();\n+  if (num_rows_present >= num_rows_max) {\n+    return Status::OK();\n+  }\n+  int num_rows_available = num_rows_max - num_rows_present;\n+  int num_rows_next = std::min(num_rows_available, num_rows_to_append);\n+  RETURN_NOT_OK(AppendNulls(pool, types, num_rows_next));\n+  *num_appended = num_rows_next;\n+  return Status::OK();\n+}\n+\n+ExecBatch ExecBatchBuilder::Flush() {\n+  ARROW_DCHECK(num_rows() > 0);\n\nReview Comment:\n   It is a very simple assertion for the caller to maintain, caller always uses the pattern: \"if (b.num_rows() > 0) {auto batch = b.Flush(); /* output batch */ }\".\r\n   \r\n   Returning empty batch is not implemented, current code will not work in that case.\r\n   \r\n   If you prefer to have Flush() return some kind of optional result, let me know what you would like function signature to look like.\n\n\n\n",
                    "created": "2022-04-15T06:25:47.274+0000",
                    "updated": "2022-04-15T06:25:47.274+0000",
                    "started": "2022-04-15T06:25:47.274+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757316",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757318",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851090256\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return buffers_[i];\n+  }\n+  /// \\brief Return a mutable version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  uint32_t* mutable_offsets() {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<uint32_t*>(mutable_data(1));\n+  }\n+  /// \\brief Return a read-only version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  const uint32_t* offsets() const {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<const uint32_t*>(data(1));\n+  }\n+  /// \\brief Return the type metadata\n+  const KeyColumnMetadata& metadata() const { return metadata_; }\n+  /// \\brief Return the length (in rows) of the array\n+  int64_t length() const { return length_; }\n+  /// \\brief Return the bit offset into the corresponding vector\n+  ///\n+  /// if i == 1 then this must be a bool array\n+  int bit_offset(int i) const {\n+    ARROW_DCHECK(i >= 0 && i < max_buffers_);\n+    return bit_offset_[i];\n+  }\n+\n+ private:\n+  static constexpr int max_buffers_ = 3;\n+  const uint8_t* buffers_[max_buffers_];\n+  uint8_t* mutable_buffers_[max_buffers_];\n+  KeyColumnMetadata metadata_;\n+  int64_t length_;\n+  // Starting bit offset within the first byte (between 0 and 7)\n+  // to be used when accessing buffers that store bit vectors.\n+  int bit_offset_[max_buffers_ - 1];\n+};\n+\n+/// \\brief Create KeyColumnMetadata from a DataType\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnMetadata for\n+/// the indices type\n+///\n+/// The caller should ensure this is only called on \"key\" columns.  Calling this with\n+/// a non-key column will return a meaningless value (or abort on a debug build)\n+KeyColumnMetadata ColumnMetadataFromDataType(const std::shared_ptr<DataType>& type);\n+\n+/// \\brief Create KeyColumnArray from ArrayData\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnArray for\n+/// the indices array\n+///\n+/// The caller should ensure this is only called on \"key\" columns.\n+/// \\see ColumnMetadataFromDataType for details\n+KeyColumnArray ColumnArrayFromArrayData(const std::shared_ptr<ArrayData>& array_data,\n+                                        int start_row, int num_rows);\n+\n+/// \\brief Create KeyColumnMetadata instances from an ExecBatch\n+///\n+/// column_metadatas will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnMetadataFromDataType for more details\n+void ColumnMetadatasFromExecBatch(const ExecBatch& batch,\n+                                  std::vector<KeyColumnMetadata>* column_metadatas);\n+\n+/// \\brief Create KeyColumnArray instances from a slice of an ExecBatch\n+///\n+/// column_arrays will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnArrayFromArrayData for more details\n+void ColumnArraysFromExecBatch(const ExecBatch& batch, int start_row, int num_rows,\n+                               std::vector<KeyColumnArray>* column_arrays);\n+\n+/// \\brief Create KeyColumnArray instances from an ExecBatch\n+///\n+/// column_arrays will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnArrayFromArrayData for more details\n+void ColumnArraysFromExecBatch(const ExecBatch& batch,\n+                               std::vector<KeyColumnArray>* column_arrays);\n+\n+/// A lightweight resizable array for \"key\" columns\n+///\n+/// Unlike KeyColumnArray this instance owns its buffers\n+///\n+/// Resizing is handled by arrow::ResizableBuffer and a doubling approach is\n+/// used so that resizes will always grow up to the next power of 2\n+class ResizableArrayData {\n+ public:\n+  /// \\brief Create an uninitialized instance\n+  ///\n+  /// Init must be called before calling any other operations\n+  ResizableArrayData()\n+      : log_num_rows_min_(0),\n+        pool_(NULLPTR),\n+        num_rows_(0),\n+        num_rows_allocated_(0),\n+        var_len_buf_size_(0) {}\n+  ~ResizableArrayData() { Clear(true); }\n+  /// \\brief Initialize the array\n+  /// \\param data_type The data type this array is holding data for.\n+  /// \\param pool The pool to make allocations on\n+  /// \\param log_num_rows_min All resize operations will allocate at least enough\n+  ///                         space for (1 << log_num_rows_min) rows\n+  void Init(const std::shared_ptr<DataType>& data_type, MemoryPool* pool,\n+            int log_num_rows_min);\n+  /// \\brief Resets the array back to an empty state\n+  /// \\param release_buffers If true then allocated memory is released and the\n+  ///                        next resize operation will have to reallocate memory\n+  void Clear(bool release_buffers);\n+  /// \\brief Resize the fixed length buffers\n+  ///\n+  /// The buffers will be resized to hold at least `num_rows_new` rows of data\n+  Status ResizeFixedLengthBuffers(int num_rows_new);\n+  /// \\brief Resize the varying length buffer if this array is a variable binary type\n+  ///\n+  /// This must be called after offsets have been populated and the buffer will be\n+  /// resized to hold at least as much data as the offsets require\n+  ///\n+  /// Does nothing if the array is not a variable binary type\n+  Status ResizeVaryingLengthBuffer();\n+  /// \\brief The current length (in rows) of the array\n+  int num_rows() const { return num_rows_; }\n+  /// \\brief A non-owning view into this array\n+  KeyColumnArray column_array() const;\n+  /// \\brief A lightweight descriptor of the data held by this array\n+  KeyColumnMetadata column_metadata() const {\n+    return ColumnMetadataFromDataType(data_type_);\n+  }\n+  /// \\brief Convert the data to an arrow::ArrayData\n+  ///\n+  /// This is a zero copy operation and the created ArrayData will reference the\n+  /// buffers held by this instance.\n+  std::shared_ptr<ArrayData> array_data() const;\n+  /// \\brief A raw pointer to the requested buffer\n+  ///\n+  /// If i is 0 then this returns the validity buffer\n+  /// If i is 1 then this returns the buffer used for values (if this is a fixed\n+  ///           length data type) or offsets (if this is a variable binary type)\n+  /// If i is 2 then this returns the buffer used for variable length binary data\n+  uint8_t* mutable_data(int i) {\n+    return i == 0   ? non_null_buf_->mutable_data()\n+           : i == 1 ? fixed_len_buf_->mutable_data()\n+                    : var_len_buf_->mutable_data();\n+  }\n+\n+ private:\n+  static constexpr int64_t kNumPaddingBytes = 64;\n+  int log_num_rows_min_;\n+  std::shared_ptr<DataType> data_type_;\n+  MemoryPool* pool_;\n+  int num_rows_;\n+  int num_rows_allocated_;\n+  int var_len_buf_size_;\n+  std::shared_ptr<ResizableBuffer> non_null_buf_;\n+  std::shared_ptr<ResizableBuffer> fixed_len_buf_;\n+  std::shared_ptr<ResizableBuffer> var_len_buf_;\n\nReview Comment:\n   Both are fine. I can change this one to an array.\n\n\n\n",
                    "created": "2022-04-15T06:38:28.143+0000",
                    "updated": "2022-04-15T06:38:28.143+0000",
                    "started": "2022-04-15T06:38:28.142+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757318",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757325",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851100413\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n\nReview Comment:\n   I added the constants.\n\n\n\n",
                    "created": "2022-04-15T07:04:47.013+0000",
                    "updated": "2022-04-15T07:04:47.013+0000",
                    "started": "2022-04-15T07:04:47.012+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757325",
                    "issueId": "13439066"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/worklog/757327",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on code in PR #12872:\nURL: https://github.com/apache/arrow/pull/12872#discussion_r851102132\n\n\n##########\ncpp/src/arrow/compute/light_array.h:\n##########\n@@ -0,0 +1,384 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <cstdint>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+/// This file contains lightweight containers for Arrow buffers.  These containers\n+/// makes compromises in terms of strong ownership and the range of data types supported\n+/// in order to gain performance and reduced overhead.\n+\n+namespace arrow {\n+namespace compute {\n+\n+/// \\brief Description of the layout of a \"key\" column\n+///\n+/// A \"key\" column is a non-nested, non-union column.\n+/// Every key column has either 0 (null), 2 (e.g. int32) or 3 (e.g. string) buffers\n+/// and no children.\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::DataType\n+struct KeyColumnMetadata {\n+  KeyColumnMetadata() = default;\n+  KeyColumnMetadata(bool is_fixed_length_in, uint32_t fixed_length_in,\n+                    bool is_null_type_in = false)\n+      : is_fixed_length(is_fixed_length_in),\n+        is_null_type(is_null_type_in),\n+        fixed_length(fixed_length_in) {}\n+  /// \\brief True if the column is not a varying-length binary type\n+  ///\n+  /// If this is true the column will have a validity buffer and\n+  /// a data buffer and the third buffer will be unused.\n+  bool is_fixed_length;\n+  /// \\brief True if this column is the null type\n+  bool is_null_type;\n+  /// \\brief The number of bytes for each item\n+  ///\n+  /// Zero has a special meaning, indicating a bit vector with one bit per value if it\n+  /// isn't a null type column.\n+  ///\n+  /// For a varying-length binary column this represents the number of bytes per offset.\n+  uint32_t fixed_length;\n+};\n+\n+/// \\brief A lightweight view into a \"key\" array\n+///\n+/// A \"key\" column is a non-nested, non-union column \\see KeyColumnMetadata\n+///\n+/// This metadata object is a zero-allocation analogue of arrow::ArrayData\n+class KeyColumnArray {\n+ public:\n+  /// \\brief Create an uninitialized KeyColumnArray\n+  KeyColumnArray() = default;\n+  /// \\brief Create a read-only view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length,\n+                 const uint8_t* buffer0, const uint8_t* buffer1, const uint8_t* buffer2,\n+                 int bit_offset0 = 0, int bit_offset1 = 0);\n+  /// \\brief Create a mutable view from buffers\n+  ///\n+  /// This is a view only and does not take ownership of the buffers.  The lifetime\n+  /// of the buffers must exceed the lifetime of this view\n+  KeyColumnArray(const KeyColumnMetadata& metadata, int64_t length, uint8_t* buffer0,\n+                 uint8_t* buffer1, uint8_t* buffer2, int bit_offset0 = 0,\n+                 int bit_offset1 = 0);\n+  /// \\brief Create a sliced view of `this`\n+  ///\n+  /// The number of rows used in offset must be divisible by 8\n+  /// in order to not split bit vectors within a single byte.\n+  KeyColumnArray Slice(int64_t offset, int64_t length) const;\n+  /// \\brief Create a copy of `this` with a buffer from `other`\n+  ///\n+  /// The copy will be identical to `this` except the buffer at buffer_id_to_replace\n+  /// will be replaced by the corresponding buffer in `other`.\n+  KeyColumnArray WithBufferFrom(const KeyColumnArray& other,\n+                                int buffer_id_to_replace) const;\n+\n+  /// \\brief Create a copy of `this` with new metadata\n+  KeyColumnArray WithMetadata(const KeyColumnMetadata& metadata) const;\n+  /// \\brief Return one of the underlying mutable buffers\n+  uint8_t* mutable_data(int i) {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return mutable_buffers_[i];\n+  }\n+  /// \\brief Return one of the underlying read-only buffers\n+  const uint8_t* data(int i) const {\n+    ARROW_DCHECK(i >= 0 && i <= max_buffers_);\n+    return buffers_[i];\n+  }\n+  /// \\brief Return a mutable version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  uint32_t* mutable_offsets() {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<uint32_t*>(mutable_data(1));\n+  }\n+  /// \\brief Return a read-only version of the offsets buffer\n+  ///\n+  /// Only valid if this is a view into a varbinary type\n+  const uint32_t* offsets() const {\n+    DCHECK(!metadata_.is_fixed_length);\n+    return reinterpret_cast<const uint32_t*>(data(1));\n+  }\n+  /// \\brief Return the type metadata\n+  const KeyColumnMetadata& metadata() const { return metadata_; }\n+  /// \\brief Return the length (in rows) of the array\n+  int64_t length() const { return length_; }\n+  /// \\brief Return the bit offset into the corresponding vector\n+  ///\n+  /// if i == 1 then this must be a bool array\n+  int bit_offset(int i) const {\n+    ARROW_DCHECK(i >= 0 && i < max_buffers_);\n+    return bit_offset_[i];\n+  }\n+\n+ private:\n+  static constexpr int max_buffers_ = 3;\n+  const uint8_t* buffers_[max_buffers_];\n+  uint8_t* mutable_buffers_[max_buffers_];\n+  KeyColumnMetadata metadata_;\n+  int64_t length_;\n+  // Starting bit offset within the first byte (between 0 and 7)\n+  // to be used when accessing buffers that store bit vectors.\n+  int bit_offset_[max_buffers_ - 1];\n+};\n+\n+/// \\brief Create KeyColumnMetadata from a DataType\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnMetadata for\n+/// the indices type\n+///\n+/// The caller should ensure this is only called on \"key\" columns.  Calling this with\n+/// a non-key column will return a meaningless value (or abort on a debug build)\n+KeyColumnMetadata ColumnMetadataFromDataType(const std::shared_ptr<DataType>& type);\n+\n+/// \\brief Create KeyColumnArray from ArrayData\n+///\n+/// If `type` is a dictionary type then this will return the KeyColumnArray for\n+/// the indices array\n+///\n+/// The caller should ensure this is only called on \"key\" columns.\n+/// \\see ColumnMetadataFromDataType for details\n+KeyColumnArray ColumnArrayFromArrayData(const std::shared_ptr<ArrayData>& array_data,\n+                                        int start_row, int num_rows);\n+\n+/// \\brief Create KeyColumnMetadata instances from an ExecBatch\n+///\n+/// column_metadatas will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnMetadataFromDataType for more details\n+void ColumnMetadatasFromExecBatch(const ExecBatch& batch,\n+                                  std::vector<KeyColumnMetadata>* column_metadatas);\n+\n+/// \\brief Create KeyColumnArray instances from a slice of an ExecBatch\n+///\n+/// column_arrays will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnArrayFromArrayData for more details\n+void ColumnArraysFromExecBatch(const ExecBatch& batch, int start_row, int num_rows,\n+                               std::vector<KeyColumnArray>* column_arrays);\n+\n+/// \\brief Create KeyColumnArray instances from an ExecBatch\n+///\n+/// column_arrays will be resized to fit\n+///\n+/// All columns in `batch` must be eligible \"key\" columns and have an array shape\n+/// \\see ColumnArrayFromArrayData for more details\n+void ColumnArraysFromExecBatch(const ExecBatch& batch,\n+                               std::vector<KeyColumnArray>* column_arrays);\n+\n+/// A lightweight resizable array for \"key\" columns\n+///\n+/// Unlike KeyColumnArray this instance owns its buffers\n+///\n+/// Resizing is handled by arrow::ResizableBuffer and a doubling approach is\n+/// used so that resizes will always grow up to the next power of 2\n+class ResizableArrayData {\n+ public:\n+  /// \\brief Create an uninitialized instance\n+  ///\n+  /// Init must be called before calling any other operations\n+  ResizableArrayData()\n+      : log_num_rows_min_(0),\n+        pool_(NULLPTR),\n+        num_rows_(0),\n+        num_rows_allocated_(0),\n+        var_len_buf_size_(0) {}\n+  ~ResizableArrayData() { Clear(true); }\n+  /// \\brief Initialize the array\n+  /// \\param data_type The data type this array is holding data for.\n+  /// \\param pool The pool to make allocations on\n+  /// \\param log_num_rows_min All resize operations will allocate at least enough\n+  ///                         space for (1 << log_num_rows_min) rows\n+  void Init(const std::shared_ptr<DataType>& data_type, MemoryPool* pool,\n+            int log_num_rows_min);\n+  /// \\brief Resets the array back to an empty state\n+  /// \\param release_buffers If true then allocated memory is released and the\n+  ///                        next resize operation will have to reallocate memory\n+  void Clear(bool release_buffers);\n+  /// \\brief Resize the fixed length buffers\n+  ///\n+  /// The buffers will be resized to hold at least `num_rows_new` rows of data\n+  Status ResizeFixedLengthBuffers(int num_rows_new);\n+  /// \\brief Resize the varying length buffer if this array is a variable binary type\n+  ///\n+  /// This must be called after offsets have been populated and the buffer will be\n+  /// resized to hold at least as much data as the offsets require\n+  ///\n+  /// Does nothing if the array is not a variable binary type\n+  Status ResizeVaryingLengthBuffer();\n+  /// \\brief The current length (in rows) of the array\n+  int num_rows() const { return num_rows_; }\n+  /// \\brief A non-owning view into this array\n+  KeyColumnArray column_array() const;\n+  /// \\brief A lightweight descriptor of the data held by this array\n+  KeyColumnMetadata column_metadata() const {\n+    return ColumnMetadataFromDataType(data_type_);\n+  }\n+  /// \\brief Convert the data to an arrow::ArrayData\n+  ///\n+  /// This is a zero copy operation and the created ArrayData will reference the\n+  /// buffers held by this instance.\n+  std::shared_ptr<ArrayData> array_data() const;\n+  /// \\brief A raw pointer to the requested buffer\n+  ///\n+  /// If i is 0 then this returns the validity buffer\n+  /// If i is 1 then this returns the buffer used for values (if this is a fixed\n+  ///           length data type) or offsets (if this is a variable binary type)\n+  /// If i is 2 then this returns the buffer used for variable length binary data\n+  uint8_t* mutable_data(int i) {\n+    return i == 0   ? non_null_buf_->mutable_data()\n+           : i == 1 ? fixed_len_buf_->mutable_data()\n+                    : var_len_buf_->mutable_data();\n+  }\n+\n+ private:\n+  static constexpr int64_t kNumPaddingBytes = 64;\n+  int log_num_rows_min_;\n+  std::shared_ptr<DataType> data_type_;\n+  MemoryPool* pool_;\n+  int num_rows_;\n+  int num_rows_allocated_;\n+  int var_len_buf_size_;\n+  std::shared_ptr<ResizableBuffer> non_null_buf_;\n+  std::shared_ptr<ResizableBuffer> fixed_len_buf_;\n+  std::shared_ptr<ResizableBuffer> var_len_buf_;\n+};\n+\n+/// \\brief A builder to concatenate batches of data into a larger batch\n+///\n+/// Will only store num_rows_max() rows\n+class ExecBatchBuilder {\n+ public:\n+  /// \\brief Add rows from `source` into `target` column\n+  ///\n+  /// If `target` is uninitialized or cleared it will be initialized to use\n+  /// the given pool.\n+  static Status AppendSelected(const std::shared_ptr<ArrayData>& source,\n+                               ResizableArrayData* target, int num_rows_to_append,\n+                               const uint16_t* row_ids, MemoryPool* pool);\n+\n+  /// \\brief Add nulls into `target` column\n+  ///\n+  /// If `target` is uninitialized or cleared it will be initialized to use\n+  /// the given pool.\n+  static Status AppendNulls(const std::shared_ptr<DataType>& type,\n+                            ResizableArrayData& target, int num_rows_to_append,\n+                            MemoryPool* pool);\n+\n+  /// \\brief Add selected rows from `batch`\n+  ///\n+  /// If `col_ids` is null then `num_cols` should less than batch.num_values() and\n+  /// the first `num_cols` columns of batch will be appended.\n+  ///\n+  /// All columns in `batch` must have array shape\n+  Status AppendSelected(MemoryPool* pool, const ExecBatch& batch, int num_rows_to_append,\n+                        const uint16_t* row_ids, int num_cols,\n+                        const int* col_ids = NULLPTR);\n\nReview Comment:\n   AppendSelected and AppendNulls variants that returned num_appended were wrappers on top of versions that did not. They can be removed as long as the caller is willing to make sure they don't exceed the batch size limit (there isn't anything incorrect about exceeding the limit at this point, but consumers of exec batches often assume that the rows can be indexed using 16-bit integers).\r\n   \r\n   I removed variants that returned num_appended (they were not used in new hash join) and added returning error status in case of exceeding limit.\n\n\n\n",
                    "created": "2022-04-15T07:08:42.376+0000",
                    "updated": "2022-04-15T07:08:42.376+0000",
                    "started": "2022-04-15T07:08:42.376+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "757327",
                    "issueId": "13439066"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
            "id": "7",
            "description": "The sub-task of the issue",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
            "name": "Sub-task",
            "subtask": true,
            "avatarId": 21146
        },
        "timespent": 18000,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@76f5d9e9[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@fd6034[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@32db3786[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@594c0d29[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@493ce5a8[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@6fae11ff[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4227c8b[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@3d3e0c63[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1301afc9[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@c3f75bf[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1b6e8697[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@567ecc8d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 18000,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Apr 21 13:13:18 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-04-21T13:13:18.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-16166/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2022-04-12T02:13:54.000+0000",
        "updated": "2022-04-24T00:40:56.000+0000",
        "timeoriginalestimate": null,
        "description": "I'm working with [~michalno] to break up ARROW-14182 into smaller PRs to ease review.  This PR adds some general purpose utilities that make it easier to work with primitive arrays, avoiding shared_ptr and allocation overhead.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "5h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 18000
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Create lightweight utilities for manipulating primitive arrays",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13439066/comment/17525703",
                    "id": "17525703",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 12872\n[https://github.com/apache/arrow/pull/12872]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2022-04-21T13:13:18.935+0000",
                    "updated": "2022-04-21T13:13:18.935+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z11dbk:",
        "customfield_12314139": null
    }
}