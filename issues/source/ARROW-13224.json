{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13386865",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865",
    "key": "ARROW-13224",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12349983",
                "id": "12349983",
                "description": "",
                "name": "5.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-07-28"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12619122",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12619122",
                "type": {
                    "id": "12310000",
                    "name": "Duplicate",
                    "inward": "is duplicated by",
                    "outward": "duplicates",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"
                },
                "inwardIssue": {
                    "id": "13386486",
                    "key": "ARROW-13207",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386486",
                    "fields": {
                        "summary": "[Python][Doc] Dataset documentation still suggests deprecated scan method as the preferred iterative approach",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                            "name": "Closed",
                            "id": "6",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                            "name": "Minor",
                            "id": "4"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "id": "1",
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "name": "Bug",
                            "subtask": false,
                            "avatarId": 21133
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332532",
                "id": "12332532",
                "name": "Documentation"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 15000,
            "total": 15000,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 15000,
            "total": 15000,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-13224/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 25,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/621221",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace opened a new pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693\n\n\n   I've added various examples of writing datasets and cleaned up a few warnings\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-10T02:02:14.333+0000",
                    "updated": "2021-07-10T02:02:14.333+0000",
                    "started": "2021-07-10T02:02:14.333+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "621221",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/621222",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#issuecomment-877537754\n\n\n   https://issues.apache.org/jira/browse/ARROW-13224\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-10T02:02:28.140+0000",
                    "updated": "2021-07-10T02:02:28.140+0000",
                    "started": "2021-07-10T02:02:28.140+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "621222",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/621223",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#issuecomment-877539137\n\n\n   Example artifacts from building locally\r\n   ![tabular_datasets](https://user-images.githubusercontent.com/1696093/125148750-285dfe00-e0d0-11eb-817d-d2b01c6a42f7.png)\r\n   ![write_dataset](https://user-images.githubusercontent.com/1696093/125148752-28f69480-e0d0-11eb-9cb4-1d203bf73de8.png)\r\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-10T02:10:45.979+0000",
                    "updated": "2021-07-10T02:10:45.979+0000",
                    "started": "2021-07-10T02:10:45.978+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "621223",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/621543",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r667897191\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    sum = 0\n\nReview comment:\n       nit: this shadows a builtin\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = sum/count\n\nReview comment:\n       Though, wouldn't the 'right' way to do it be to select `mean('col2')` as an expression? (I suppose you can't yet do that though.)\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n\nReview comment:\n       Do we actually want all these to be ipython blocks instead of code-block blocks? It does slow down building a bit.\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n\nReview comment:\n       We do sort by path: \r\n   https://github.com/apache/arrow/blob/91f261fa9a7841fd914c5ed1d8e747fb4e510a5b/cpp/src/arrow/dataset/discovery.cc#L203-L204\r\n   \r\n   Though it states that's only for unit tests. However I would personally be really surprised if data were truly returned in random order and as we've had this behavior in a few releases now I think users would also be surprised. Maybe we should instead guarantee lexicographic sort on paths?\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n+you want to partition your data or you need to write a large amount of data.  A\n+basic dataset write is similar to writing a table except that you specify a directory\n+instead of a filename.\n+\n+.. ipython:: python\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    dataset_root = base / \"sample_dataset\"\n+    dataset_root.mkdir(exist_ok=True)\n+\n+    table = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\n+    ds.write_dataset(table, dataset_root, format=\"parquet\")\n+\n+The above example will create a single file named part-0.parquet in our sample_dataset\n+diretory.\n+\n+.. warning::\n+\n+    If you run the example again it will replace the existing part-0.parquet file.\n+    Appending files to an existing dataset is not currently supported by this API and\n+    the output directory should be empty for predictable results.\n+\n+Writing partitioned data\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+A partitioning object can be used to specify how your output data should be partitioned.\n+This uses the same kind of partitioning objects we used for reading datasets.  To write\n+our above data out to a partitioned directory we only need to specify how we want the\n+dataset to be partitioned.  For example:\n+\n+.. ipython:: python\n+\n+    part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    ds.write_dataset(table, dataset_root, format=\"parquet\", partitioning=part)\n+\n+This will create two files.  Half our data will be in the dataset_root/c=1 directory and\n+the other half will be in the dataset_root/c=2 directory.\n+\n+Writing large amounts of data\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples wrote data from a table.  If you are writing a large amount of data\n+you may not be able to load everything into a single in-memory table.  Fortunately, the\n+write_dataset method also accepts an iterable of record batches.  This makes it really\n+simple, for example, to repartition a large dataset without loading the entire dataset\n+into memory:\n+\n+.. ipython:: python\n+\n+    old_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    new_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=None\n+    )\n+    input_dataset = ds.dataset(dataset_root, partitioning=old_part)\n+    new_root = base / \"repartitioned_dataset\"\n+    # A scanner can act as an iterator of record batches but you could also receive\n+    # data from the network (e.g. via flight), from your own scanning, or from any\n+    # other method that yields record batches.  In addition, you can pass a dataset\n+    # into write_dataset directly but this method is useful if you want to customize\n+    # the scanner (e.g. to filter the input dataset or set a maximum batch size)\n+    scanner = input_dataset.scanner()\n+\n+    ds.write_dataset(scanner, new_root, format=\"parquet\", partitioning=new_part)\n+\n+After the above example runs our data will be in dataset_root/1 and dataset_root/2\n+directories.  You could also use this mechnaism to change which columns you are partitioned\n+on as well.  This is useful when you expect to query your data in specific ways and\n+you can utilize partitioning to reduce the amount of data you need to read.\n+\n+.. To add when ARROW-12364 is merged\n+    Customizing & inspecting written files\n+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+    By default the dataset API will create files named \"part-i.format\" where \"i\" is a integer\n+    generated during the write and \"format\" is the file format specified in the write_dataset\n+    call.  For simple datasets it may be possible to know which files will be created but for\n+    larger or partitioned datasets it is not so easy.  The ``file_visitor`` keyword can be used \n+    to supply a visitor that will be called as each file is created:\n+\n+    .. ipython:: python\n+\n+        def file_visitor(written_file):\n+            print(f\"path={written_file.path}\")\n+            print(f\"metadata={written_file.metadata}\")\n+        ds.write_dataset(table, dataset_root, format=\"parquet\", partitioning=part,\n+                        file_visitor=file_visitor)\n+\n+    This will allow you to collect the filenames that belong to the dataset and store them elsewhere\n+    which can be useful when you want to avoid scanning directories the next time you need to read\n+    the data.  It can also be used to generate the _metadata index file used by other tools such as\n+    dask or spark to create an index of the dataset.\n+\n+Configuring format-specific parameters during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+In addition to the common options shared by all formats there are also format specific options\n+that are unique to a particular format.  For example, to allow truncated timestamps while writing\n+parquet files:\n\nReview comment:\n       Is it worth linking to the options classes for each of the formats?\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n+you want to partition your data or you need to write a large amount of data.  A\n+basic dataset write is similar to writing a table except that you specify a directory\n+instead of a filename.\n+\n+.. ipython:: python\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    dataset_root = base / \"sample_dataset\"\n+    dataset_root.mkdir(exist_ok=True)\n+\n+    table = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\n+    ds.write_dataset(table, dataset_root, format=\"parquet\")\n+\n+The above example will create a single file named part-0.parquet in our sample_dataset\n+diretory.\n+\n+.. warning::\n+\n+    If you run the example again it will replace the existing part-0.parquet file.\n+    Appending files to an existing dataset is not currently supported by this API and\n+    the output directory should be empty for predictable results.\n+\n+Writing partitioned data\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+A partitioning object can be used to specify how your output data should be partitioned.\n+This uses the same kind of partitioning objects we used for reading datasets.  To write\n+our above data out to a partitioned directory we only need to specify how we want the\n+dataset to be partitioned.  For example:\n+\n+.. ipython:: python\n+\n+    part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    ds.write_dataset(table, dataset_root, format=\"parquet\", partitioning=part)\n+\n+This will create two files.  Half our data will be in the dataset_root/c=1 directory and\n+the other half will be in the dataset_root/c=2 directory.\n+\n+Writing large amounts of data\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples wrote data from a table.  If you are writing a large amount of data\n+you may not be able to load everything into a single in-memory table.  Fortunately, the\n+write_dataset method also accepts an iterable of record batches.  This makes it really\n+simple, for example, to repartition a large dataset without loading the entire dataset\n+into memory:\n+\n+.. ipython:: python\n+\n+    old_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    new_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=None\n+    )\n+    input_dataset = ds.dataset(dataset_root, partitioning=old_part)\n+    new_root = base / \"repartitioned_dataset\"\n+    # A scanner can act as an iterator of record batches but you could also receive\n+    # data from the network (e.g. via flight), from your own scanning, or from any\n+    # other method that yields record batches.  In addition, you can pass a dataset\n+    # into write_dataset directly but this method is useful if you want to customize\n+    # the scanner (e.g. to filter the input dataset or set a maximum batch size)\n+    scanner = input_dataset.scanner()\n+\n+    ds.write_dataset(scanner, new_root, format=\"parquet\", partitioning=new_part)\n+\n+After the above example runs our data will be in dataset_root/1 and dataset_root/2\n+directories.  You could also use this mechnaism to change which columns you are partitioned\n+on as well.  This is useful when you expect to query your data in specific ways and\n+you can utilize partitioning to reduce the amount of data you need to read.\n+\n+.. To add when ARROW-12364 is merged\n+    Customizing & inspecting written files\n+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+    By default the dataset API will create files named \"part-i.format\" where \"i\" is a integer\n+    generated during the write and \"format\" is the file format specified in the write_dataset\n+    call.  For simple datasets it may be possible to know which files will be created but for\n+    larger or partitioned datasets it is not so easy.  The ``file_visitor`` keyword can be used \n+    to supply a visitor that will be called as each file is created:\n+\n+    .. ipython:: python\n+\n+        def file_visitor(written_file):\n+            print(f\"path={written_file.path}\")\n+            print(f\"metadata={written_file.metadata}\")\n+        ds.write_dataset(table, dataset_root, format=\"parquet\", partitioning=part,\n+                        file_visitor=file_visitor)\n+\n+    This will allow you to collect the filenames that belong to the dataset and store them elsewhere\n+    which can be useful when you want to avoid scanning directories the next time you need to read\n+    the data.  It can also be used to generate the _metadata index file used by other tools such as\n+    dask or spark to create an index of the dataset.\n+\n+Configuring format-specific parameters during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+In addition to the common options shared by all formats there are also format specific options\n+that are unique to a particular format.  For example, to allow truncated timestamps while writing\n+parquet files:\n\nReview comment:\n       ```suggestion\r\n   Parquet files:\r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n+you want to partition your data or you need to write a large amount of data.  A\n+basic dataset write is similar to writing a table except that you specify a directory\n+instead of a filename.\n+\n+.. ipython:: python\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    dataset_root = base / \"sample_dataset\"\n+    dataset_root.mkdir(exist_ok=True)\n+\n+    table = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\n+    ds.write_dataset(table, dataset_root, format=\"parquet\")\n+\n+The above example will create a single file named part-0.parquet in our sample_dataset\n+diretory.\n\nReview comment:\n       Is it worth noting IPC (Feather) and CSV are also supported?\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n+you want to partition your data or you need to write a large amount of data.  A\n+basic dataset write is similar to writing a table except that you specify a directory\n+instead of a filename.\n+\n+.. ipython:: python\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    dataset_root = base / \"sample_dataset\"\n+    dataset_root.mkdir(exist_ok=True)\n+\n+    table = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\n+    ds.write_dataset(table, dataset_root, format=\"parquet\")\n+\n+The above example will create a single file named part-0.parquet in our sample_dataset\n+diretory.\n\nReview comment:\n       ```suggestion\r\n   directory.\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-12T19:50:13.058+0000",
                    "updated": "2021-07-12T19:50:13.058+0000",
                    "started": "2021-07-12T19:50:13.058+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "621543",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622354",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669252727\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n\nReview comment:\n       All the scripts I added execute pretty rapidly as they are dealing with tables with less than 10 rows.  I'm not sure they add significantly to the build times.\r\n   \r\n   For a test I tried converting all ipython to code-block and saw no noticable difference in build times.  I'd prefer ipython just for the testing sake but I'm happy to go with whatever is decided in ARROW-13159.  Since it should be pretty easy to change after the fact (just a find-replace from ipython to code-block) I'd rather address it after ARROW-13159 is resolved if that is ok.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T03:27:02.297+0000",
                    "updated": "2021-07-14T03:27:02.297+0000",
                    "started": "2021-07-14T03:27:02.297+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622354",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622368",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669268967\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    sum = 0\n\nReview comment:\n       Thanks.  I renamed it.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T04:21:18.920+0000",
                    "updated": "2021-07-14T04:21:18.920+0000",
                    "started": "2021-07-14T04:21:18.920+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622368",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622369",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669269421\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n\nReview comment:\n       Hmmm, I just dropped the sentence about scanning.  If someone wants clarification we can discuss it at that time.  @bkietz any opinion?  I recall we spoke about this before.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T04:22:42.182+0000",
                    "updated": "2021-07-14T04:22:42.182+0000",
                    "started": "2021-07-14T04:22:42.182+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622369",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622370",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669270057\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = sum/count\n\nReview comment:\n       True, but then you wouldn't be using `to_batches` anyways and this whole section goes away.  I think dataset scanning is likely to be a niche API once any kind of SQL support is added.  We can address it then.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T04:24:45.748+0000",
                    "updated": "2021-07-14T04:24:45.748+0000",
                    "started": "2021-07-14T04:24:45.748+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622370",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622371",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#issuecomment-879575972\n\n\n   I believe I have addressed all comments.  I also noticed that IpcWriteOptions was undocumented so I added it in.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T04:25:39.431+0000",
                    "updated": "2021-07-14T04:25:39.431+0000",
                    "started": "2021-07-14T04:25:39.431+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622371",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622376",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669276514\n\n\n\n##########\nFile path: python/pyarrow/_hdfs.pyx\n##########\n@@ -93,9 +93,10 @@ cdef class HadoopFileSystem(FileSystem):\n         Instantiate HadoopFileSystem object from an URI string.\n \n         The following two calls are equivalent\n-        * HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test'\n-                                    '&replication=1')\n-        * HadoopFileSystem('localhost', port=8020, user='test', replication=1)\n+\n+        * ``HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test&replication=1')`` # noqa: E501\n+\n+        * ``HadoopFileSystem('localhost', port=8020, user='test', replication=1)`` # noqa: E501\n\nReview comment:\n       Anyone have any opinion on ignoring flake here?  RST didn't like the multi line strings.  I tried using literal blocks `::` but then flake complained about the empty lines required by RST.  This seems like the most readable but the lines are too long (hence the `# noqa`)\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T04:39:38.986+0000",
                    "updated": "2021-07-14T04:39:38.986+0000",
                    "started": "2021-07-14T04:39:38.986+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622376",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622476",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669546790\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n+a directory.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n\nReview comment:\n       Ah, thanks for checking.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T12:00:49.025+0000",
                    "updated": "2021-07-14T12:00:49.025+0000",
                    "started": "2021-07-14T12:00:49.025+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622476",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622477",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669547100\n\n\n\n##########\nFile path: python/pyarrow/_hdfs.pyx\n##########\n@@ -93,9 +93,10 @@ cdef class HadoopFileSystem(FileSystem):\n         Instantiate HadoopFileSystem object from an URI string.\n \n         The following two calls are equivalent\n-        * HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test'\n-                                    '&replication=1')\n-        * HadoopFileSystem('localhost', port=8020, user='test', replication=1)\n+\n+        * ``HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test&replication=1')`` # noqa: E501\n+\n+        * ``HadoopFileSystem('localhost', port=8020, user='test', replication=1)`` # noqa: E501\n\nReview comment:\n       I'm fine with this.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T12:01:22.299+0000",
                    "updated": "2021-07-14T12:01:22.299+0000",
                    "started": "2021-07-14T12:01:22.299+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622477",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622478",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669547858\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n\nReview comment:\n       I also suppose we don't have to solve this here; we can get the docs in and improve this part later.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T12:02:26.555+0000",
                    "updated": "2021-07-14T12:02:26.555+0000",
                    "started": "2021-07-14T12:02:26.555+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622478",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622624",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "bkietz commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669825752\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n\nReview comment:\n       We don't guarantee order for selectors because ARROW-8163 (asynchronous fragment discovery) might not guarantee order. Lexicographic sorting *could* be maintained for synchronous discovery from a selector, but in general we'd want to push a fragment into scan as soon as it's yielded by `FileSystem::GetFileInfoGenerator`\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T17:45:21.413+0000",
                    "updated": "2021-07-14T17:45:21.413+0000",
                    "started": "2021-07-14T17:45:21.413+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622624",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622625",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r669830370\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,163 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.  This\n+only applies when the dataset is constructed with a list of files.  There\n+are no order guarantees given when the files are instead discovered by scanning\n\nReview comment:\n       Ah thanks. Sorry for the churn here @westonpace - we should keep stating that there's no guarantee then.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-14T17:52:10.848+0000",
                    "updated": "2021-07-14T17:52:10.848+0000",
                    "started": "2021-07-14T17:52:10.848+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622625",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/622986",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorisvandenbossche commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r670370362\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n\nReview comment:\n       \"Scheduling\" is a bit strange term to use in this context? \n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n\nReview comment:\n       ```suggestion\r\n   The previous examples have demonstrated how to read the data into a table using :func:`~Dataset.to_table`.  This is\r\n   ```\r\n   \r\n   ?\n\n##########\nFile path: python/pyarrow/_hdfs.pyx\n##########\n@@ -93,9 +93,10 @@ cdef class HadoopFileSystem(FileSystem):\n         Instantiate HadoopFileSystem object from an URI string.\n \n         The following two calls are equivalent\n-        * HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test'\n-                                    '&replication=1')\n-        * HadoopFileSystem('localhost', port=8020, user='test', replication=1)\n+\n+        * ``HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test&replication=1')`` # noqa: E501\n+\n+        * ``HadoopFileSystem('localhost', port=8020, user='test', replication=1)`` # noqa: E501\n\nReview comment:\n       ```suggestion\r\n           * ``HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test\\\r\n   &replication=1')``\r\n           * ``HadoopFileSystem('localhost', port=8020, user='test', \\\r\n   replication=1)``\r\n   ```\r\n   \r\n   It's a bit ugly in the source code, but this AFAIK the best way to deal with this issue. In a console checking the docstring this will look like a single line.\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    col2_sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        col2_sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = col2_sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n+you want to partition your data or you need to write a large amount of data.  A\n+basic dataset write is similar to writing a table except that you specify a directory\n+instead of a filename.\n+\n+.. ipython:: python\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    dataset_root = base / \"sample_dataset\"\n+    dataset_root.mkdir(exist_ok=True)\n+\n+    table = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\n+    ds.write_dataset(table, dataset_root, format=\"parquet\")\n+\n+The above example will create a single file named part-0.parquet in our sample_dataset\n+directory.\n+\n+.. warning::\n+\n+    If you run the example again it will replace the existing part-0.parquet file.\n+    Appending files to an existing dataset is not currently supported by this API and\n+    the output directory should be empty for predictable results.\n+\n+Writing partitioned data\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+A partitioning object can be used to specify how your output data should be partitioned.\n+This uses the same kind of partitioning objects we used for reading datasets.  To write\n+our above data out to a partitioned directory we only need to specify how we want the\n+dataset to be partitioned.  For example:\n+\n+.. ipython:: python\n+\n+    part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    ds.write_dataset(table, dataset_root, format=\"parquet\", partitioning=part)\n+\n+This will create two files.  Half our data will be in the dataset_root/c=1 directory and\n+the other half will be in the dataset_root/c=2 directory.\n+\n+Writing large amounts of data\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples wrote data from a table.  If you are writing a large amount of data\n+you may not be able to load everything into a single in-memory table.  Fortunately, the\n+write_dataset method also accepts an iterable of record batches.  This makes it really\n+simple, for example, to repartition a large dataset without loading the entire dataset\n+into memory:\n+\n+.. ipython:: python\n+\n+    old_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    new_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=None\n+    )\n+    input_dataset = ds.dataset(dataset_root, partitioning=old_part)\n+    new_root = base / \"repartitioned_dataset\"\n+    # A scanner can act as an iterator of record batches but you could also receive\n+    # data from the network (e.g. via flight), from your own scanning, or from any\n+    # other method that yields record batches.  In addition, you can pass a dataset\n+    # into write_dataset directly but this method is useful if you want to customize\n+    # the scanner (e.g. to filter the input dataset or set a maximum batch size)\n+    scanner = input_dataset.scanner()\n+\n+    ds.write_dataset(scanner, new_root, format=\"parquet\", partitioning=new_part)\n+\n+After the above example runs our data will be in dataset_root/1 and dataset_root/2\n\nReview comment:\n       Since this example is not changing the partitioning structure (except for the flavor), it's not directly what this \"repartitioning\" will do apart from changing `/c=1/` into `/1/`. Will it also repartition individual files? (eg write more smaller files in case your input dataset has large files)\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    col2_sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        col2_sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = col2_sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n\nReview comment:\n       ```suggestion\r\n   One of those parameters is the ``batch_size``.  This controls the maximum size of the\r\n   ```\r\n   \r\n   (rst uses double backticks for code, in contrast to markdown ..)\n\n##########\nFile path: python/pyarrow/_hdfs.pyx\n##########\n@@ -93,9 +93,10 @@ cdef class HadoopFileSystem(FileSystem):\n         Instantiate HadoopFileSystem object from an URI string.\n \n         The following two calls are equivalent\n-        * HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test'\n-                                    '&replication=1')\n-        * HadoopFileSystem('localhost', port=8020, user='test', replication=1)\n+\n+        * ``HadoopFileSystem.from_uri('hdfs://localhost:8020/?user=test&replication=1')`` # noqa: E501\n+\n+        * ``HadoopFileSystem('localhost', port=8020, user='test', replication=1)`` # noqa: E501\n\nReview comment:\n       I added a suggestion below to fix this. I think that's preferrable since the ``# noqa: E501`` actually would show up as normal text which would be strange in the online rendered docstring\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    col2_sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n\nReview comment:\n       Maybe add a `columns=[\"col2\"]` as well? (it shows passing a column selection as well + it is actually what you should do here if you are only summing that column)\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    col2_sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        col2_sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = col2_sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n+you want to partition your data or you need to write a large amount of data.  A\n+basic dataset write is similar to writing a table except that you specify a directory\n+instead of a filename.\n+\n+.. ipython:: python\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    dataset_root = base / \"sample_dataset\"\n+    dataset_root.mkdir(exist_ok=True)\n+\n+    table = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\n+    ds.write_dataset(table, dataset_root, format=\"parquet\")\n+\n+The above example will create a single file named part-0.parquet in our sample_dataset\n+directory.\n+\n+.. warning::\n+\n+    If you run the example again it will replace the existing part-0.parquet file.\n+    Appending files to an existing dataset is not currently supported by this API and\n+    the output directory should be empty for predictable results.\n+\n+Writing partitioned data\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+A partitioning object can be used to specify how your output data should be partitioned.\n+This uses the same kind of partitioning objects we used for reading datasets.  To write\n+our above data out to a partitioned directory we only need to specify how we want the\n+dataset to be partitioned.  For example:\n+\n+.. ipython:: python\n+\n+    part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    ds.write_dataset(table, dataset_root, format=\"parquet\", partitioning=part)\n+\n+This will create two files.  Half our data will be in the dataset_root/c=1 directory and\n+the other half will be in the dataset_root/c=2 directory.\n+\n+Writing large amounts of data\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples wrote data from a table.  If you are writing a large amount of data\n+you may not be able to load everything into a single in-memory table.  Fortunately, the\n+write_dataset method also accepts an iterable of record batches.  This makes it really\n+simple, for example, to repartition a large dataset without loading the entire dataset\n+into memory:\n+\n+.. ipython:: python\n+\n+    old_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    new_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=None\n+    )\n+    input_dataset = ds.dataset(dataset_root, partitioning=old_part)\n+    new_root = base / \"repartitioned_dataset\"\n+    # A scanner can act as an iterator of record batches but you could also receive\n+    # data from the network (e.g. via flight), from your own scanning, or from any\n+    # other method that yields record batches.  In addition, you can pass a dataset\n+    # into write_dataset directly but this method is useful if you want to customize\n+    # the scanner (e.g. to filter the input dataset or set a maximum batch size)\n+    scanner = input_dataset.scanner()\n+\n+    ds.write_dataset(scanner, new_root, format=\"parquet\", partitioning=new_part)\n+\n+After the above example runs our data will be in dataset_root/1 and dataset_root/2\n+directories.  You could also use this mechnaism to change which columns you are partitioned\n\nReview comment:\n       ```suggestion\r\n   directories.  You could also use this mechnaism to change which columns the dataset is partitioned\r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    col2_sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        col2_sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = col2_sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n\nReview comment:\n       ```suggestion\r\n   The dataset API also simplifies writing data to a dataset using :func:`write_dataset` .  This can be useful when\r\n   ```\r\n   \r\n   (this is not necessarily the best addition, but basically I think it would be good to add this explicit reference _somewhere_ in this paragraph; it's useful to have this so that the user has an easy link to click to check the more detailed function docstring)\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-15T11:45:11.695+0000",
                    "updated": "2021-07-15T11:45:11.695+0000",
                    "started": "2021-07-15T11:45:11.694+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "622986",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/623409",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r670965306\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    col2_sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n\nReview comment:\n       Added\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-16T04:56:29.852+0000",
                    "updated": "2021-07-16T04:56:29.852+0000",
                    "started": "2021-07-16T04:56:29.852+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "623409",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/623410",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r670965398\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n\nReview comment:\n       Yes it is.  I changed it to \"listing\"\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-16T04:56:53.122+0000",
                    "updated": "2021-07-16T04:56:53.122+0000",
                    "started": "2021-07-16T04:56:53.122+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "623410",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/623411",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r670965572\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    col2_sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        col2_sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = col2_sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n+you want to partition your data or you need to write a large amount of data.  A\n+basic dataset write is similar to writing a table except that you specify a directory\n+instead of a filename.\n+\n+.. ipython:: python\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    dataset_root = base / \"sample_dataset\"\n+    dataset_root.mkdir(exist_ok=True)\n+\n+    table = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\n+    ds.write_dataset(table, dataset_root, format=\"parquet\")\n+\n+The above example will create a single file named part-0.parquet in our sample_dataset\n+directory.\n+\n+.. warning::\n+\n+    If you run the example again it will replace the existing part-0.parquet file.\n+    Appending files to an existing dataset is not currently supported by this API and\n+    the output directory should be empty for predictable results.\n+\n+Writing partitioned data\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+A partitioning object can be used to specify how your output data should be partitioned.\n+This uses the same kind of partitioning objects we used for reading datasets.  To write\n+our above data out to a partitioned directory we only need to specify how we want the\n+dataset to be partitioned.  For example:\n+\n+.. ipython:: python\n+\n+    part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    ds.write_dataset(table, dataset_root, format=\"parquet\", partitioning=part)\n+\n+This will create two files.  Half our data will be in the dataset_root/c=1 directory and\n+the other half will be in the dataset_root/c=2 directory.\n+\n+Writing large amounts of data\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples wrote data from a table.  If you are writing a large amount of data\n+you may not be able to load everything into a single in-memory table.  Fortunately, the\n+write_dataset method also accepts an iterable of record batches.  This makes it really\n+simple, for example, to repartition a large dataset without loading the entire dataset\n+into memory:\n+\n+.. ipython:: python\n+\n+    old_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    new_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=None\n+    )\n+    input_dataset = ds.dataset(dataset_root, partitioning=old_part)\n+    new_root = base / \"repartitioned_dataset\"\n+    # A scanner can act as an iterator of record batches but you could also receive\n+    # data from the network (e.g. via flight), from your own scanning, or from any\n+    # other method that yields record batches.  In addition, you can pass a dataset\n+    # into write_dataset directly but this method is useful if you want to customize\n+    # the scanner (e.g. to filter the input dataset or set a maximum batch size)\n+    scanner = input_dataset.scanner()\n+\n+    ds.write_dataset(scanner, new_root, format=\"parquet\", partitioning=new_part)\n+\n+After the above example runs our data will be in dataset_root/1 and dataset_root/2\n+directories.  You could also use this mechnaism to change which columns you are partitioned\n\nReview comment:\n       I changed to `you could also use this mechnaism to change which columns are used to partition the dataset`\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-16T04:57:26.633+0000",
                    "updated": "2021-07-16T04:57:26.633+0000",
                    "started": "2021-07-16T04:57:26.633+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "623411",
                    "issueId": "13386865"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/worklog/623413",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #10693:\nURL: https://github.com/apache/arrow/pull/10693#discussion_r670965948\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -456,20 +456,160 @@ is materialized as columns when reading the data and can be used for filtering:\n     dataset.to_table().to_pandas()\n     dataset.to_table(filter=ds.field('year') == 2019).to_pandas()\n \n+Another benefit of manually scheduling the files is that the order of the files\n+controls the order of the data.  When performing an ordered read (or a read to\n+a table) then the rows returned will match the order of the files given.\n \n-Manual scheduling\n------------------\n+Iterative (out of core or streaming) reads\n+------------------------------------------\n \n-..\n-    Possible content:\n-    - fragments (get_fragments)\n-    - scan / scan tasks / iterators of record batches\n+The previous examples have demonstrated how to read the data into a table.  This is\n+useful if the dataset is small or there is only a small amount of data that needs to\n+be read.  The dataset API contains additional methods to read and process large amounts\n+of data in a streaming fashion.\n \n-The :func:`~Dataset.to_table` method loads all selected data into memory\n-at once resulting in a pyarrow Table. Alternatively, a dataset can also be\n-scanned one RecordBatch at a time in an iterative manner using the\n-:func:`~Dataset.scan` method::\n+The easiest way to do this is to use the method :meth:`Dataset.to_batches`.  This\n+method returns an iterator of record batches.  For example, we can use this method to\n+calculate the average of a column without loading the entire column into memory:\n \n-    for scan_task in dataset.scan(columns=[...], filter=...):\n-        for record_batch in scan_task.execute():\n-            # process the record batch\n+.. ipython:: python\n+\n+    import pyarrow.compute as pc\n+\n+    col2_sum = 0\n+    count = 0\n+    for batch in dataset.to_batches(filter=~ds.field('col2').is_null()):\n+        col2_sum += pc.sum(batch.column('col2')).as_py()\n+        count += batch.num_rows\n+    mean_a = col2_sum/count\n+\n+Customizing the batch size\n+~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+An iterative read of a dataset is often called a \"scan\" of the dataset and pyarrow\n+uses an object called a :class:`Scanner` to do this.  A Scanner is created for you\n+automatically by the to_table and to_batches method of the dataset.  Any arguments\n+you pass to these methods will be passed on to the Scanner constructor.\n+\n+One of those parameters is the `batch_size`.  This controls the maximum size of the\n+batches returned by the scanner.  Batches can still be smaller than the `batch_size`\n+if the dataset consists of small files or those files themselves consist of small\n+row groups.  For example, a parquet file with 10,000 rows per row group will yield\n+batches with, at most, 10,000 rows unless the batch_size is set to a smaller value.\n+\n+The default batch size is one million rows and this is typically a good default but\n+you may want to customize it if you are reading a large number of columns.\n+\n+Writing Datasets\n+----------------\n+\n+The dataset API also simplifies writing data to a dataset.  This can be useful when\n+you want to partition your data or you need to write a large amount of data.  A\n+basic dataset write is similar to writing a table except that you specify a directory\n+instead of a filename.\n+\n+.. ipython:: python\n+\n+    base = pathlib.Path(tempfile.gettempdir())\n+    dataset_root = base / \"sample_dataset\"\n+    dataset_root.mkdir(exist_ok=True)\n+\n+    table = pa.table({\"a\": range(10), \"b\": np.random.randn(10), \"c\": [1, 2] * 5})\n+    ds.write_dataset(table, dataset_root, format=\"parquet\")\n+\n+The above example will create a single file named part-0.parquet in our sample_dataset\n+directory.\n+\n+.. warning::\n+\n+    If you run the example again it will replace the existing part-0.parquet file.\n+    Appending files to an existing dataset is not currently supported by this API and\n+    the output directory should be empty for predictable results.\n+\n+Writing partitioned data\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+A partitioning object can be used to specify how your output data should be partitioned.\n+This uses the same kind of partitioning objects we used for reading datasets.  To write\n+our above data out to a partitioned directory we only need to specify how we want the\n+dataset to be partitioned.  For example:\n+\n+.. ipython:: python\n+\n+    part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    ds.write_dataset(table, dataset_root, format=\"parquet\", partitioning=part)\n+\n+This will create two files.  Half our data will be in the dataset_root/c=1 directory and\n+the other half will be in the dataset_root/c=2 directory.\n+\n+Writing large amounts of data\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+The above examples wrote data from a table.  If you are writing a large amount of data\n+you may not be able to load everything into a single in-memory table.  Fortunately, the\n+write_dataset method also accepts an iterable of record batches.  This makes it really\n+simple, for example, to repartition a large dataset without loading the entire dataset\n+into memory:\n+\n+.. ipython:: python\n+\n+    old_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=\"hive\"\n+    )\n+    new_part = ds.partitioning(\n+        pa.schema([(\"c\", pa.int16())]), flavor=None\n+    )\n+    input_dataset = ds.dataset(dataset_root, partitioning=old_part)\n+    new_root = base / \"repartitioned_dataset\"\n+    # A scanner can act as an iterator of record batches but you could also receive\n+    # data from the network (e.g. via flight), from your own scanning, or from any\n+    # other method that yields record batches.  In addition, you can pass a dataset\n+    # into write_dataset directly but this method is useful if you want to customize\n+    # the scanner (e.g. to filter the input dataset or set a maximum batch size)\n+    scanner = input_dataset.scanner()\n+\n+    ds.write_dataset(scanner, new_root, format=\"parquet\", partitioning=new_part)\n+\n+After the above example runs our data will be in dataset_root/1 and dataset_root/2\n\nReview comment:\n       This would only change the directory structure.  I didn't want to make the example data more complicated and `a` and `b` would be bad choices for a partition column.  I modified the following paragraph to hopefully make it a bit clearer...\r\n   \r\n   > In this simple example we are not changing the structure of the data\r\n   > (only the directory naming schema) but you could also use this mechnaism to change\r\n   > which columns are used to partition the dataset.  This is useful when you expect to\r\n   > query your data in specific ways and you can utilize partitioning to reduce the\r\n   > amount of data you need to read.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-07-16T04:58:23.535+0000",
                    "updated": "2021-07-16T04:58:23.535+0000",
                    "started": "2021-07-16T04:58:23.534+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "623413",
                    "issueId": "13386865"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 15000,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@229ec32d[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@640e4b8b[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@39eb06dc[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@2f2f5079[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@71c6d24c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@5d6d8456[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3c51b4c1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@69a897aa[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2ebb6c61[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@1fc502d4[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2e035a51[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@3bb9dfc[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 15000,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Jul 20 13:38:18 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-07-20T13:38:18.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-13224/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2021-06-30T18:24:44.000+0000",
        "updated": "2021-07-20T13:38:28.000+0000",
        "timeoriginalestimate": null,
        "description": "I don't believe this is meant to be internal.\u00a0 pyarrow.parquet.write_to_dataset uses this (if use_legacy_dataset=False) but the parquet API doesn't expose the same features.\u00a0 A new example should also probably be added to the Tabular Datasets section of the docs explaining why write_dataset can take in a scanner (e.g. memory preserving, ability to write a dataset from flight or any record batch source, etc.)",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "4h 10m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 15000
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python][Doc] Documentation missing for pyarrow.dataset.write_dataset",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/comment/17372791",
                    "id": "17372791",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
                        "name": "jorisvandenbossche",
                        "key": "jorisvandenbossche",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Joris Van den Bossche",
                        "active": true,
                        "timeZone": "Europe/Brussels"
                    },
                    "body": "Indeed, we should add some documentation for writing datasets (python/dataset.rst only handles reading right now)",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorisvandenbossche",
                        "name": "jorisvandenbossche",
                        "key": "jorisvandenbossche",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Joris Van den Bossche",
                        "active": true,
                        "timeZone": "Europe/Brussels"
                    },
                    "created": "2021-07-01T13:29:51.254+0000",
                    "updated": "2021-07-01T13:29:51.254+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13386865/comment/17384261",
                    "id": "17384261",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "body": "Issue resolved by pull request 10693\n[https://github.com/apache/arrow/pull/10693]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kszucs",
                        "name": "kszucs",
                        "key": "kszucs",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "Krisztian Szucs",
                        "active": true,
                        "timeZone": "Europe/Budapest"
                    },
                    "created": "2021-07-20T13:38:18.841+0000",
                    "updated": "2021-07-20T13:38:18.841+0000"
                }
            ],
            "maxResults": 2,
            "total": 2,
            "startAt": 0
        },
        "customfield_12311820": "0|z0sh0w:",
        "customfield_12314139": null
    }
}